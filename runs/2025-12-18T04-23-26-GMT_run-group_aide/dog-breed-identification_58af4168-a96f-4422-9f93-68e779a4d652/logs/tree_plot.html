<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [[0, 8], [1, 7], [3, 5], [4, 6], [6, 9], [9, 10], [10, 11]], "layout": [[0.0, 0.0], [0.25, 0.0], [0.5, 0.0], [0.75, 0.0], [1.0, 0.0], [0.75, 0.25], [1.0, 0.25], [0.25, 0.25], [0.0, 0.25], [1.0, 0.5], [1.0, 0.75], [1.0, 1.0]], "plan": ["I will train a lightweight pipeline that resizes images to small RGB arrays,\nreduces dimensionality with PCA, and fits a LightGBM multiclass classifier. I\nwill use 5-fold stratified cross-validation to estimate multi-class log-loss,\nfitting PCA and the model within each fold to avoid leakage, and average fold\npredictions on the test set for the final submission. The code prints the CV\nlog-loss and writes a submission.csv with probabilities in the same column order\nas the provided sample_submission.csv.", "I will extract deep features from a pretrained ResNet50 (frozen) for all train\nand test images, then train a LightGBM multiclass classifier on those features\nusing 5-fold stratified cross-validation to estimate performance (log loss).\nAfter reporting the mean CV log loss, I will retrain on the full training set\nand produce probability predictions for the test set saved to\n./submission/submission.csv aligned with the sample submission header. This\napproach is fast on CPU because we avoid finetuning and use a lightweight\ngradient boosting classifier on the extracted embeddings.", "I will do transfer learning by extracting fixed features from a pretrained\nResNet (feature extractor only) for all images, then train a simple multiclass\nlogistic regression with 5-fold stratified cross-validation on those features.\nI'll average fold predictions for the test set to produce a final submission and\nreport the multiclass log loss computed on the held-out validation folds. This\nkeeps training fast on CPU while providing a reasonable baseline for the 120-way\nclassification task.", "I will extract fixed image features from a pretrained ResNet-18 (removing its\nfinal FC) for all train and test images, then train a multinomial logistic\nregression with 5-fold stratified cross-validation on the extracted features.\nI'll standardize features, report the mean multi-class log loss across\nvalidation folds, and average fold predictions to form the final test\nprobabilities. This keeps training fast on CPU while using a strong pretrained\nvisual backbone; the evaluation metric will be multiclass log loss.", "I will extract deep features from a pretrained EfficientNet-B0 network (freezing\nthe backbone) for all train and test images, reduce dimensionality with PCA, and\ntrain a LightGBM multiclass classifier using 5-fold stratified cross-validation.\nOut-of-fold predictions will be used to compute the multiclass log loss as the\nevaluation metric, and test predictions will be the average of fold predictions.\nThe script prints the CV log loss and writes the required submission file to\n./submission/submission.csv.", "I will replace the heavy pretrained CNN feature extraction (which can fail in\noffline/CPU-only or restricted environments) with a lightweight, deterministic\ncolor-histogram feature extractor that runs quickly on CPU. Then I will train a\nmultinomial Logistic Regression with 5-fold stratified CV on these histogram\nfeatures, report CV log-loss, and write averaged test-set probabilities to\n./submission/submission.csv using the exact column order from\nsample_submission.csv. This approach avoids external pretrained weights, runs\nfast, and guarantees a valid submission file.", "I'll replace the heavy pretrained CNN feature extractor (which can fail due to\nmissing GPU or unavailable weights) with a lightweight, robust image feature\npipeline based on per-channel color histograms and simple statistics. These\nhandcrafted features are fast to compute, avoid internet/GPU dependencies, and\nwill be scaled and reduced with PCA before training a LightGBM multiclass model\nwith 5-fold stratified CV. The script computes CV multi-class log loss, prints\nit, and writes a submission CSV at ./submission/submission.csv matching the\nsample submission format.", "I'll fix the LightGBM training error by switching from the low-level lgb.train\ncall (which in this environment didn't accept early_stopping_rounds) to the\nsklearn API lgb.LGBMClassifier, which supports early stopping via its fit()\nmethod. I'll also ensure the breed-to-index mapping follows the\nsample_submission column order so the final CSV columns match exactly, run a\n5-fold stratified CV using LGBM with early stopping, print CV log-loss, train a\nfinal model on all data using the average best-iteration, and save predictions\nto ./submission/submission.csv.", "I will fix the previous run by making the PCA call compatible with typical\nsklearn versions (use svd_solver='randomized' and pass random_state) to avoid a\nTypeError that can abort the run, add robust image-loading fallbacks, and ensure\nthe submission CSV is always written to ./submission/submission.csv. The script\nperforms 5-fold stratified cross-validation, fits PCA per-fold, trains a\nLightGBM multiclass model (with a sklearn fallback to LogisticRegression if\nLightGBM is unavailable), prints per-fold and mean multi-class log loss, and\nsaves test-set probabilities in the required submission path matching the\nsample_submission column order.", "The bug was that label indices given to LightGBM did not match the column order\nexpected in the sample submission: LabelEncoder reorders classes internally, so\npredicted probability columns were misaligned to breed names. Fix by\nconstructing an explicit mapping from breed name to index using the sample\nsubmission column order, so training labels and submission columns share the\nsame class ordering. I also keep the rest of the pipeline (handcrafted color\nhistograms + PCA + 5-fold LightGBM), print the CV multi-class log loss, and save\nthe submission to ./submission/submission.csv.", "The bug seemed to be that the original script could fail silently and not\nproduce a submission (and it wasn't robust to missing images or ordering\nmismatches). I'll (1) make feature extraction robust to missing files, (2)\nenforce consistent ordering with the sample submission, (3) produce a simple\nstratified hold-out evaluation and a 5-fold CV evaluation (printing both log-\nloss values), and (4) ensure the final test predictions are normalized and saved\nto submission/submission.csv. The code below implements these fixes and prints\nthe requested metrics.", "I will make the pipeline more robust and ensure a submission is always written:\nreplace the LightGBM step (which can fail if the package is not present) with a\nstandard sklearn LogisticRegression (multinomial) classifier, reduce image size\nand histogram bins for speed, handle missing/corrupt images gracefully, and\nalways save a properly ordered submission.csv matching the sample submission\ncolumns. The script will compute a 10% hold-out log-loss (printed) and perform\n5-fold CV to produce averaged test probabilities, print the CV log-loss, and\nsave the submission file to ./submission/submission.csv."], "code": ["import os\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import log_loss\nimport lightgbm as lgb\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nLABELS_PATH = os.path.join(INPUT_DIR, \"labels.csv\")\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nSUBMISSION_DIR = \"./submission\"\nSUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n# Parameters\nIMAGE_SIZE = 64  # resize images to IMAGE_SIZE x IMAGE_SIZE\nPCA_COMPONENTS = 256\nN_FOLDS = 5\nRANDOM_STATE = 42\nLGB_NUM_ROUNDS = 200\nLGB_EARLY_STOP = 20\n\n# Utility: read sample submission to get columns / breed order\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\nbreed_cols = list(sample_sub.columns)[1:]  # excluding id\n\n# Read labels\nlabels_df = pd.read_csv(LABELS_PATH)\n# Sanity: ensure ids exist in train folder\n# Prepare train ids and target indices according to breed_cols order\nid_list = labels_df[\"id\"].values\nbreeds = labels_df[\"breed\"].values\nbreed_to_idx = {b: i for i, b in enumerate(breed_cols)}\ny = np.array([breed_to_idx[b] for b in breeds], dtype=np.int32)\n\n\n# Helper to load and preprocess images into numpy arrays (float32)\ndef load_images_as_array(ids, folder, img_size=IMAGE_SIZE):\n    n = len(ids)\n    X = np.zeros((n, img_size * img_size * 3), dtype=np.float32)\n    for i, img_id in enumerate(ids):\n        img_path = os.path.join(folder, img_id + \".jpg\")\n        try:\n            with Image.open(img_path) as img:\n                img = img.convert(\"RGB\")\n                img = img.resize((img_size, img_size), Image.LANCZOS)\n                arr = np.asarray(img, dtype=np.float32) / 255.0\n                X[i, :] = arr.reshape(-1)\n        except Exception as e:\n            # If there's an error reading image, fill with zeros\n            print(f\"Warning: failed to load {img_path}: {e}\", file=sys.stderr)\n            X[i, :] = 0.0\n        if (i + 1) % 1000 == 0 or (i + 1) == n:\n            print(f\"Loaded {i + 1}/{n} images from {folder}\", file=sys.stderr)\n    return X\n\n\n# Load train images\nprint(\"Loading train images...\", file=sys.stderr)\nX_all = load_images_as_array(id_list, TRAIN_DIR, IMAGE_SIZE)\nprint(\"Train images loaded.\", file=sys.stderr)\n\n# Load test images ids and data\ntest_sample = pd.read_csv(SAMPLE_SUB_PATH)\ntest_ids = test_sample[\"id\"].values\nprint(\"Loading test images...\", file=sys.stderr)\nX_test_full = load_images_as_array(test_ids, TEST_DIR, IMAGE_SIZE)\nprint(\"Test images loaded.\", file=sys.stderr)\n\nn_classes = len(breed_cols)\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\noof_preds = np.zeros((X_all.shape[0], n_classes), dtype=np.float32)\ntest_preds = np.zeros((X_test_full.shape[0], n_classes), dtype=np.float32)\nfold = 0\ncv_losses = []\n\nfor train_idx, val_idx in skf.split(X_all, y):\n    fold += 1\n    print(f\"Starting fold {fold}/{N_FOLDS}\", file=sys.stderr)\n    X_tr, X_val = X_all[train_idx], X_all[val_idx]\n    y_tr, y_val = y[train_idx], y[val_idx]\n\n    # Fit PCA on training fold\n    pca = PCA(\n        n_components=min(PCA_COMPONENTS, X_tr.shape[0], X_tr.shape[1]),\n        random_state=RANDOM_STATE,\n    )\n    print(\"Fitting PCA...\", file=sys.stderr)\n    X_tr_p = pca.fit_transform(X_tr)\n    X_val_p = pca.transform(X_val)\n    X_test_p = pca.transform(X_test_full)\n\n    # LightGBM datasets\n    lgb_train = lgb.Dataset(X_tr_p, label=y_tr)\n    lgb_valid = lgb.Dataset(X_val_p, label=y_val, reference=lgb_train)\n\n    params = {\n        \"objective\": \"multiclass\",\n        \"num_class\": n_classes,\n        \"metric\": \"multi_logloss\",\n        \"learning_rate\": 0.1,\n        \"num_leaves\": 31,\n        \"verbosity\": -1,\n        \"seed\": RANDOM_STATE,\n        \"deterministic\": True,\n    }\n\n    print(\"Training LightGBM...\", file=sys.stderr)\n    bst = lgb.train(\n        params,\n        lgb_train,\n        num_boost_round=LGB_NUM_ROUNDS,\n        valid_sets=[lgb_train, lgb_valid],\n        early_stopping_rounds=LGB_EARLY_STOP,\n        verbose_eval=False,\n    )\n\n    print(\"Predicting validation and test...\", file=sys.stderr)\n    val_pred = bst.predict(X_val_p, num_iteration=bst.best_iteration)\n    test_pred = bst.predict(X_test_p, num_iteration=bst.best_iteration)\n\n    oof_preds[val_idx] = val_pred\n    test_preds += test_pred / N_FOLDS\n\n    loss = log_loss(y_val, val_pred)\n    cv_losses.append(loss)\n    print(f\"Fold {fold} log_loss: {loss:.6f}\", file=sys.stderr)\n\n# Overall CV log loss\ncv_logloss = float(np.mean(cv_losses))\nprint(f\"\\nCV mean multi-class log-loss over {N_FOLDS} folds: {cv_logloss:.6f}\")\n\n# Save submission using breed_cols order\nsubmission = pd.DataFrame(test_preds, columns=breed_cols)\nsubmission.insert(0, \"id\", test_ids)\n# Ensure column order matches sample_submission\nsubmission = submission[sample_sub.columns]\nsubmission.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"Submission saved to {SUBMISSION_PATH}\")\n", "import os\nimport sys\nimport time\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nimport lightgbm as lgb\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nLABELS_PATH = os.path.join(INPUT_DIR, \"labels.csv\")\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nSUBMISSION_DIR = \"./submission\"\nSUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# Read metadata\nlabels_df = pd.read_csv(LABELS_PATH)\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\nbreed_cols = list(sample_sub.columns[1:])  # keep order for submission\n\n# Map breed to integer\nbreeds = sorted(labels_df[\"breed\"].unique())\nbreed2idx = {b: i for i, b in enumerate(breeds)}\nidx2breed = {i: b for b, i in breed2idx.items()}\n\n# Ensure breed_cols align with breeds; sample_submission may include same ordering as dataset; we'll use its columns for final output\n# But mapping needs to match label encoding; create mapping from breed name as in labels to column name in sample_sub\n# Some names may be identical; assume identical names.\n# Prepare train ids and labels\ntrain_ids = labels_df[\"id\"].values\ntrain_labels = labels_df[\"breed\"].map(breed2idx).values\n\n# Prepare test ids in same order as sample_submission\ntest_ids = sample_sub[\"id\"].values\n\n# Device (CPU)\ndevice = torch.device(\"cpu\")\n\n\n# Image dataset for feature extraction\nclass ImageFolderDataset(Dataset):\n    def __init__(self, ids, folder, labels=None, transform=None):\n        self.ids = ids\n        self.folder = folder\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        img_id = self.ids[idx]\n        path = os.path.join(self.folder, img_id + \".jpg\")\n        # Some datasets might have .jpeg etc; assume .jpg as provided\n        img = Image.open(path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        if self.labels is None:\n            return img_id, img\n        else:\n            return img_id, img, self.labels[idx]\n\n\n# Transforms: use ImageNet normalization and standard resnet input size\ntransform = transforms.Compose(\n    [\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\n\n\n# Instantiate pretrained ResNet50 as feature extractor\ndef get_resnet50_feature_extractor():\n    try:\n        # Newer torchvision\n        model = models.resnet50(pretrained=True)\n    except Exception:\n        # Fallback safe call\n        model = models.resnet50(pretrained=True)\n    # Remove final FC layer: take everything up to avgpool\n    modules = list(model.children())[:-1]  # remove last fc\n    feature_extractor = nn.Sequential(*modules)\n    feature_extractor.eval()\n    feature_extractor.to(device)\n    return feature_extractor\n\n\nfeat_model = get_resnet50_feature_extractor()\n\n\n# Function to extract features for a dataset and cache to disk\ndef extract_and_cache_features(ids, folder, cache_path, labels=None, batch_size=64):\n    if os.path.exists(cache_path):\n        print(f\"Loading cached features from {cache_path}\")\n        data = np.load(cache_path, allow_pickle=True)\n        features = data[\"features\"]\n        ids_loaded = data[\"ids\"].tolist()\n        return features, ids_loaded\n    dataset = ImageFolderDataset(ids, folder, labels=labels, transform=transform)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n    all_ids = []\n    feats = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=f\"Extracting features from {folder}\"):\n            if labels is None:\n                batch_ids, imgs = batch\n            else:\n                batch_ids, imgs, _ = batch\n            imgs = imgs.to(device)\n            out = feat_model(imgs)  # shape (B, 2048, 1, 1)\n            out = out.cpu().numpy()\n            out = out.reshape(out.shape[0], -1)\n            feats.append(out)\n            all_ids.extend(batch_ids)\n    features = np.vstack(feats)\n    np.savez_compressed(cache_path, features=features, ids=np.array(all_ids))\n    return features, all_ids\n\n\n# Extract features (will cache to working)\ntrain_cache = os.path.join(WORKING_DIR, \"train_feats.npz\")\ntest_cache = os.path.join(WORKING_DIR, \"test_feats.npz\")\n\ntrain_features, train_ids_extracted = extract_and_cache_features(\n    train_ids, TRAIN_DIR, train_cache, labels=train_labels, batch_size=32\n)\ntest_features, test_ids_extracted = extract_and_cache_features(\n    test_ids, TEST_DIR, test_cache, labels=None, batch_size=32\n)\n\n# Align train_features order with labels if necessary\n# train_ids_extracted should match train_ids order; verify\nif list(train_ids_extracted) != list(train_ids):\n    # Need to reorder features to match train_ids\n    id_to_idx = {id_: i for i, id_ in enumerate(train_ids_extracted)}\n    order = [id_to_idx[id_] for id_ in train_ids]\n    train_features = train_features[order, :]\n\n# Ensure test order matches sample_submission test_ids\nif list(test_ids_extracted) != list(test_ids):\n    id_to_idx = {id_: i for i, id_ in enumerate(test_ids_extracted)}\n    order = [id_to_idx[id_] for id_ in test_ids]\n    test_features = test_features[order, :]\n\n# Prepare data for modeling\nX = train_features.astype(np.float32)\ny = train_labels.astype(int)\nX_test = test_features.astype(np.float32)\n\nprint(\"Feature shapes:\", X.shape, X_test.shape, \"Num classes:\", len(breeds))\n\n# 5-fold Stratified CV with LightGBM\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\nfold_scores = []\noof_preds = np.zeros((X.shape[0], len(breeds)), dtype=np.float32)\n\nlgb_params = {\n    \"objective\": \"multiclass\",\n    \"num_class\": len(breeds),\n    \"learning_rate\": 0.1,\n    \"num_leaves\": 31,\n    \"metric\": \"multi_logloss\",\n    \"verbosity\": -1,\n    \"seed\": 42,\n    \"n_jobs\": 4,\n}\n\nfold_num = 0\nfor train_idx, val_idx in skf.split(X, y):\n    fold_num += 1\n    print(f\"Training fold {fold_num}/{n_splits}...\")\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr, y_val = y[train_idx], y[val_idx]\n\n    dtrain = lgb.Dataset(X_tr, label=y_tr)\n    dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n\n    # Train with early stopping\n    bst = lgb.train(\n        lgb_params,\n        dtrain,\n        num_boost_round=300,\n        valid_sets=[dtrain, dval],\n        valid_names=[\"train\", \"valid\"],\n        early_stopping_rounds=20,\n        verbose_eval=False,\n    )\n    # Predict\n    val_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n    oof_preds[val_idx] = val_pred\n    loss = log_loss(y_val, val_pred, labels=list(range(len(breeds))))\n    print(f\"Fold {fold_num} log_loss: {loss:.6f}\")\n    fold_scores.append(loss)\n\nmean_loss = np.mean(fold_scores)\nstd_loss = np.std(fold_scores)\nprint(f\"\\nCV log_loss mean: {mean_loss:.6f} std: {std_loss:.6f}\")\n\n# Train final model on all data\nprint(\"Training final model on full training set...\")\ndtrain_full = lgb.Dataset(X, label=y)\nfinal_bst = lgb.train(\n    lgb_params,\n    dtrain_full,\n    num_boost_round=int(\n        np.mean([bst.best_iteration for _ in range(1)]) if False else 300\n    ),\n    verbose_eval=False,\n)\n# Predict on test set\ntest_pred = final_bst.predict(X_test)\n\n# Ensure columns in submission match sample_submission order\nsubmission_df = pd.DataFrame(test_pred, columns=breed_cols)\nsubmission_df.insert(0, \"id\", test_ids)\n# Some sample_submission may include breeds not in our mapping; ensure columns match exactly sample_submission\n# If there are any breed columns in sample_submission not in our computed breed list, fill zeros\nfor col in sample_sub.columns[1:]:\n    if col not in submission_df.columns:\n        submission_df[col] = 0.0\n# Reorder columns to match sample_submission\nsubmission_df = submission_df[sample_sub.columns]\n\n# Save submission\nsubmission_df.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"Saved submission to {SUBMISSION_PATH}\")\nprint(f\"Final CV log_loss (mean over {n_splits} folds): {mean_loss:.6f}\")\n", "import os\nimport sys\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\n\n# Config\nRANDOM_SEED = 42\nBATCH_SIZE = 64\nIMG_SIZE = 224\nNUM_WORKERS = 0  # 0 for compatibility in restricted environments\nN_SPLITS = 5\n\nTRAIN_DIR = \"./input/train\"\nTEST_DIR = \"./input/test\"\nLABELS_CSV = \"./input/labels.csv\"\nSAMPLE_SUB = \"./input/sample_submission.csv\"\nSUBMISSION_DIR = \"./submission\"\nSUBMISSION_FILE = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\n# Read data\nlabels_df = pd.read_csv(LABELS_CSV)\nsample_sub = pd.read_csv(SAMPLE_SUB)\n\ntrain_ids = labels_df[\"id\"].values\ntrain_breeds = labels_df[\"breed\"].values\ntest_df = pd.read_csv(SAMPLE_SUB)[[\"id\"]]\n\n# Label encode breeds\nle = LabelEncoder()\ny_all = le.fit_transform(train_breeds)\nclass_names = list(le.classes_)\nnum_classes = len(class_names)\nprint(\n    f\"Found {len(train_ids)} train images, {len(test_df)} test images, {num_classes} classes.\"\n)\n\n# Image dataset for feature extraction\ntransform = transforms.Compose(\n    [\n        transforms.Resize(256),\n        transforms.CenterCrop(IMG_SIZE),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\n\n\nclass ImagePathDataset(Dataset):\n    def __init__(self, ids, img_dir, transform=None):\n        self.ids = ids\n        self.img_dir = img_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        img_id = self.ids[idx]\n        img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n        try:\n            img = Image.open(img_path).convert(\"RGB\")\n        except Exception as e:\n            # If failed to load, return a black image\n            img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (0, 0, 0))\n        if self.transform:\n            img = self.transform(img)\n        return img_id, img\n\n\n# Build pretrained model for feature extraction\ndevice = torch.device(\"cpu\")\nbackbone = models.resnet18(pretrained=True)\n\n\n# Remove final classifier\nclass Identity(nn.Module):\n    def forward(self, x):\n        return x\n\n\nbackbone.fc = Identity()\nbackbone.eval()\nbackbone.to(device)\n\n\ndef extract_features(ids_list, img_dir, batch_size=BATCH_SIZE):\n    ds = ImagePathDataset(ids_list, img_dir, transform=transform)\n    loader = DataLoader(\n        ds, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n    )\n    features = []\n    ids_out = []\n    with torch.no_grad():\n        for batch in tqdm(\n            loader, desc=f\"Extracting features from {os.path.basename(img_dir)}\"\n        ):\n            batch_ids, imgs = batch\n            imgs = imgs.to(device)\n            feats = backbone(imgs)\n            # If backbone outputs 512 x 1 x 1 sometimes, flatten\n            feats = feats.view(feats.size(0), -1).cpu().numpy()\n            features.append(feats)\n            ids_out.extend(batch_ids)\n    features = np.vstack(features).astype(np.float32)\n    return ids_out, features\n\n\n# Precompute features for all train images (in the same order as train_ids)\nprint(\"Extracting train features...\")\ntrain_ids_list = train_ids.tolist()\nids_ordered, X_train = extract_features(train_ids_list, TRAIN_DIR)\n# ensure ordering matches\nif ids_ordered != train_ids_list:\n    # reorder to match original\n    id_to_idx = {idv: i for i, idv in enumerate(ids_ordered)}\n    order = [id_to_idx[idv] for idv in train_ids_list]\n    X_train = X_train[order]\n\nprint(\"Extracting test features...\")\ntest_ids_list = test_df[\"id\"].values.tolist()\nids_test_ordered, X_test = extract_features(test_ids_list, TEST_DIR)\nif ids_test_ordered != test_ids_list:\n    id_to_idx = {idv: i for i, idv in enumerate(ids_test_ordered)}\n    order = [id_to_idx[idv] for idv in test_ids_list]\n    X_test = X_test[order]\n\n# 5-fold stratified CV with Logistic Regression on extracted features\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n\noof_probs = np.zeros((len(train_ids_list), num_classes), dtype=np.float64)\ntest_probs = np.zeros((len(test_ids_list), num_classes), dtype=np.float64)\n\nfold = 0\nfor train_idx, val_idx in skf.split(X_train, y_all):\n    fold += 1\n    print(f\"\\nFold {fold}/{N_SPLITS} -- Train {len(train_idx)} val {len(val_idx)}\")\n    X_tr, X_val = X_train[train_idx], X_train[val_idx]\n    y_tr, y_val = y_all[train_idx], y_all[val_idx]\n\n    # Scale features modestly (mean centering)\n    # Using simple standardization can help logistic regression\n    mean = X_tr.mean(axis=0, keepdims=True)\n    std = X_tr.std(axis=0, keepdims=True) + 1e-6\n    X_tr_s = (X_tr - mean) / std\n    X_val_s = (X_val - mean) / std\n    X_test_s = (X_test - mean) / std\n\n    # Train Logistic Regression\n    clf = LogisticRegression(\n        multi_class=\"multinomial\",\n        solver=\"saga\",\n        max_iter=1000,\n        C=1.0,\n        random_state=RANDOM_SEED,\n        n_jobs=1,\n    )\n    clf.fit(X_tr_s, y_tr)\n\n    # Predict\n    val_pred = clf.predict_proba(X_val_s)\n    test_pred = clf.predict_proba(X_test_s)\n\n    oof_probs[val_idx] = val_pred\n    test_probs += test_pred\n\n    # Fold log loss\n    fold_loss = log_loss(y_val, val_pred, labels=list(range(num_classes)))\n    print(f\"Fold {fold} log_loss: {fold_loss:.5f}\")\n\n# Average test probabilities\ntest_probs /= N_SPLITS\n\n# Overall CV log loss\ncv_logloss = log_loss(y_all, oof_probs, labels=list(range(num_classes)))\nprint(f\"\\nOverall CV log_loss (5-fold): {cv_logloss:.5f}\")\n\n# Prepare submission in the order of sample_submission columns\nsubmission = pd.DataFrame(test_probs, columns=le.classes_)\nsubmission.insert(0, \"id\", test_ids_list)\n\n# Ensure sample_submission order (some competitions require exact column order)\nsample_cols = list(pd.read_csv(SAMPLE_SUB).columns)\n# If sample has same set of columns, reorder\nif set(sample_cols) == set(list(submission.columns)):\n    submission = submission[sample_cols]\nelse:\n    # Ensure 'id' first and then our class order\n    cols = [\"id\"] + [c for c in submission.columns if c != \"id\"]\n    submission = submission[cols]\n\nsubmission.to_csv(SUBMISSION_FILE, index=False)\nprint(f\"Saved submission to {SUBMISSION_FILE}\")\n\n# Print a small sample of submission\nprint(\"\\nSubmission sample rows:\")\nprint(submission.head(3))\n\n# Final print of metric\nprint(f\"\\nFinal reported CV log_loss: {cv_logloss:.6f}\")\n", "import os\nimport sys\nimport time\nimport math\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nLABELS_PATH = os.path.join(INPUT_DIR, \"labels.csv\")\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nSUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n\n# Config\nBATCH_SIZE = 64  # for feature extraction\nIMG_SIZE = 224\nN_FOLDS = 5\nRANDOM_STATE = 42\nDEVICE = torch.device(\"cpu\")  # CPU-only environment\n\n\ndef load_image(path):\n    with Image.open(path) as img:\n        img = img.convert(\"RGB\")\n        return img.copy()\n\n\n# Prepare model (ResNet-18 without final FC)\ndef get_feature_extractor():\n    model = models.resnet18(pretrained=True)\n    # Remove the final FC layer and keep up to avgpool\n    modules = list(model.children())[:-1]  # everything except the last fc\n    feat_extractor = nn.Sequential(*modules)\n    feat_extractor.to(DEVICE)\n    feat_extractor.eval()\n    return feat_extractor\n\n\n# Transform\ntransform = transforms.Compose(\n    [\n        transforms.Resize(256),\n        transforms.CenterCrop(IMG_SIZE),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\n\n\ndef extract_features(model, filepaths, batch_size=BATCH_SIZE):\n    n = len(filepaths)\n    # First, allocate array for features. For resnet18 output dim is 512\n    FEATURES_DIM = 512\n    feats = np.zeros((n, FEATURES_DIM), dtype=np.float32)\n    model.eval()\n    with torch.no_grad():\n        for i in range(0, n, batch_size):\n            batch_paths = filepaths[i : i + batch_size]\n            imgs = []\n            for p in batch_paths:\n                img = load_image(p)\n                img = transform(img)\n                imgs.append(img)\n            imgs = torch.stack(imgs).to(DEVICE)\n            out = model(imgs)  # shape (batch, 512, 1,1)\n            out = out.view(out.size(0), -1).cpu().numpy()\n            feats[i : i + out.shape[0]] = out\n    return feats\n\n\ndef main():\n    start_time = time.time()\n    # Read labels and sample submission\n    labels_df = pd.read_csv(LABELS_PATH)\n    sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n    breed_list = list(sample_sub.columns)[1:]  # columns after id\n    # Ensure label encoder uses same order as sample submission columns\n    le = LabelEncoder()\n    le.fit(breed_list)  # map breed names to indices matching submission columns\n\n    # Build train file paths and targets\n    train_ids = labels_df[\"id\"].values\n    train_breeds = labels_df[\"breed\"].values\n    train_paths = [os.path.join(TRAIN_DIR, f\"{pid}.jpg\") for pid in train_ids]\n    y = le.transform(train_breeds)\n\n    # Build test file paths and ids\n    test_ids = sample_sub[\"id\"].values\n    test_paths = [os.path.join(TEST_DIR, f\"{tid}.jpg\") for tid in test_ids]\n\n    print(\n        f\"Num train: {len(train_paths)}, Num test: {len(test_paths)}, Num classes: {len(breed_list)}\"\n    )\n\n    # Load model and extract features\n    print(\"Loading pretrained model...\")\n    feat_model = get_feature_extractor()\n\n    print(\"Extracting train features...\")\n    X_train = extract_features(feat_model, train_paths)\n    print(\"Extracting test features...\")\n    X_test = extract_features(feat_model, test_paths)\n\n    # Scale features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # 5-fold stratified CV with LogisticRegression\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n    oof_probs = np.zeros((X_train.shape[0], len(breed_list)), dtype=np.float64)\n    test_probs = np.zeros((X_test.shape[0], len(breed_list)), dtype=np.float64)\n    val_losses = []\n\n    fold_num = 0\n    for train_idx, val_idx in skf.split(X_train_scaled, y):\n        fold_num += 1\n        print(\n            f\"Fold {fold_num}/{N_FOLDS} - train {len(train_idx)} / val {len(val_idx)}\"\n        )\n        X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n        y_tr, y_val = y[train_idx], y[val_idx]\n\n        # Logistic Regression\n        # Use saga solver for multinomial; increase max_iter for convergence\n        clf = LogisticRegression(\n            C=1.0,\n            multi_class=\"multinomial\",\n            solver=\"saga\",\n            max_iter=2000,\n            verbose=0,\n            n_jobs=-1,\n            random_state=RANDOM_STATE,\n        )\n        clf.fit(X_tr, y_tr)\n        val_pred = clf.predict_proba(X_val)\n        oof_probs[val_idx] = val_pred\n        loss = log_loss(y_val, val_pred, labels=range(len(breed_list)))\n        val_losses.append(loss)\n        print(f\"  Fold {fold_num} log_loss: {loss:.6f}\")\n\n        # test predictions\n        test_pred = clf.predict_proba(X_test_scaled)\n        test_probs += test_pred\n\n    # Average test predictions\n    test_probs /= N_FOLDS\n    mean_cv_loss = float(np.mean(val_losses))\n    std_cv_loss = float(np.std(val_losses))\n    print(f\"\\nMean CV log_loss: {mean_cv_loss:.6f} \u00b1 {std_cv_loss:.6f}\")\n\n    # Write submission file using same column order as sample_submission\n    submission_df = pd.DataFrame(test_probs, columns=breed_list)\n    submission_df.insert(0, \"id\", test_ids)\n    # Ensure column order\n    submission_df = submission_df[[\"id\"] + breed_list]\n    submission_df.to_csv(SUBMISSION_PATH, index=False)\n    print(f\"Submission saved to {SUBMISSION_PATH}\")\n\n    # Additionally print overall OOF log loss\n    oof_loss = log_loss(y, oof_probs, labels=range(len(breed_list)))\n    print(f\"OOF log_loss (full): {oof_loss:.6f}\")\n\n    elapsed = time.time() - start_time\n    print(f\"Total time: {elapsed/60:.2f} minutes\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport sys\nimport random\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\n# Try to use timm if available for easy feature extraction; fallback to torchvision\ntry:\n    import timm\n\n    TIMM_AVAILABLE = True\nexcept Exception:\n    TIMM_AVAILABLE = False\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nimport lightgbm as lgb\n\n# -----------------------\n# Config\n# -----------------------\nSEED = 42\nBATCH_SIZE = 32\nNUM_WORKERS = 2\nPCA_COMPONENTS = 200  # reduce feature dim to speed training\nN_FOLDS = 5\nTARGET_COL = \"breed\"\nINPUT_DIR = Path(\"input\")\nTRAIN_DIR = INPUT_DIR / \"train\"\nTEST_DIR = INPUT_DIR / \"test\"\nLABELS_FILE = INPUT_DIR / \"labels.csv\"\nSAMPLE_SUB_FILE = INPUT_DIR / \"sample_submission.csv\"\nSUBMISSION_DIR = Path(\"submission\")\nSUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\nSUBMISSION_FILE = SUBMISSION_DIR / \"submission.csv\"\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n\n# -----------------------\n# Data utilities\n# -----------------------\nclass DogDataset(Dataset):\n    def __init__(self, ids, folder, transform=None):\n        self.ids = ids\n        self.folder = Path(folder)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        img_id = self.ids[idx]\n        img_path = self.folder / (img_id + \".jpg\")\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img_id, img\n\n\n# -----------------------\n# Feature extractor\n# -----------------------\ndef create_feature_extractor():\n    if TIMM_AVAILABLE:\n        model = timm.create_model(\n            \"efficientnet_b0\", pretrained=True, num_classes=0, global_pool=\"avg\"\n        )\n    else:\n        # fallback to torchvision\n        try:\n            from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n\n            weights = EfficientNet_B0_Weights.DEFAULT\n            model_full = efficientnet_b0(weights=weights)\n\n            # build a model that outputs features by replacing classifier\n            class FeatModel(torch.nn.Module):\n                def __init__(self, full):\n                    super().__init__()\n                    self.features = full.features\n                    self.avgpool = full.avgpool\n                    # extract out_planes from classifier in full\n                    self._out_dim = full.classifier[1].in_features\n\n                def forward(self, x):\n                    x = self.features(x)\n                    x = self.avgpool(x)\n                    x = torch.flatten(x, 1)\n                    return x\n\n            model = FeatModel(model_full)\n        except Exception:\n            # last resort: a small resnet18 from torchvision\n            from torchvision.models import resnet18, ResNet18_Weights\n\n            w = ResNet18_Weights.DEFAULT\n            full = resnet18(weights=w)\n\n            # remove fc\n            class FeatResNet(torch.nn.Module):\n                def __init__(self, fullnet):\n                    super().__init__()\n                    self.conv = torch.nn.Sequential(*list(fullnet.children())[:-1])\n                    self._out_dim = fullnet.fc.in_features\n\n                def forward(self, x):\n                    x = self.conv(x)\n                    x = torch.flatten(x, 1)\n                    return x\n\n            model = FeatResNet(full)\n    model.to(DEVICE)\n    model.eval()\n    return model\n\n\n# -----------------------\n# Extract features for a list of ids\n# -----------------------\ndef extract_features(model, ids, folder, batch_size=32, num_workers=2):\n    # transforms matching EfficientNet expectations\n    tf = transforms.Compose(\n        [\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n    )\n    ds = DogDataset(ids, folder, transform=tf)\n    loader = DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    features = []\n    ids_out = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=f\"FeatExtract {folder.name}\", unit=\"batch\"):\n            batch_ids, imgs = batch\n            imgs = imgs.to(DEVICE)\n            feats = model(imgs)\n            feats = feats.cpu().numpy()\n            features.append(feats)\n            ids_out.extend(batch_ids)\n    features = np.vstack(features)\n    return ids_out, features\n\n\n# -----------------------\n# Main pipeline\n# -----------------------\ndef main():\n    t0 = time.time()\n    # Read labels and sample submission\n    labels_df = pd.read_csv(LABELS_FILE)\n    sample_sub = pd.read_csv(SAMPLE_SUB_FILE)\n    # Ensure breed order matches sample submission columns (excluding id)\n    breed_cols = list(sample_sub.columns)\n    breed_cols.remove(\"id\")\n    # Map breeds to labels (LabelEncoder using breed_cols to ensure consistent ordering)\n    le = LabelEncoder()\n    le.fit(breed_cols)\n    # build train ids and targets in same order as labels_df\n    train_ids = labels_df[\"id\"].values\n    train_breeds = labels_df[\"breed\"].values\n    y = le.transform(train_breeds)  # integers in 0..119\n\n    # Test ids from sample submission (they give exact ids and column order)\n    test_ids = sample_sub[\"id\"].values\n\n    # Create feature extractor model\n    model = create_feature_extractor()\n    # Get out dim from model by running a dummy through it (to check shape)\n    with torch.no_grad():\n        dummy = torch.zeros((1, 3, 224, 224), device=DEVICE)\n        try:\n            out = model(dummy)\n            feat_dim = out.shape[1]\n        except Exception:\n            # If model expects different, try forwarding through model.features etc.\n            out = model(dummy)\n            feat_dim = out.shape[1]\n\n    print(f\"Feature dim detected: {feat_dim}\")\n\n    # Extract features for train and test\n    train_ids_list, train_feats = extract_features(\n        model,\n        list(train_ids),\n        TRAIN_DIR,\n        batch_size=BATCH_SIZE,\n        num_workers=NUM_WORKERS,\n    )\n    test_ids_list, test_feats = extract_features(\n        model, list(test_ids), TEST_DIR, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS\n    )\n\n    # Sanity check order: train_ids_list should match train_ids\n    if list(train_ids) != train_ids_list:\n        # reorder to match labels\n        idx_map = {id_: i for i, id_ in enumerate(train_ids_list)}\n        order = [idx_map[id_] for id_ in train_ids]\n        train_feats = train_feats[order]\n    # Similarly ensure test order matches sample submission\n    if list(test_ids) != test_ids_list:\n        idx_map = {id_: i for i, id_ in enumerate(test_ids_list)}\n        order = [idx_map[id_] for id_ in test_ids]\n        test_feats = test_feats[order]\n\n    # Scale + PCA\n    scaler = StandardScaler()\n    train_feats_scaled = scaler.fit_transform(train_feats)\n    test_feats_scaled = scaler.transform(test_feats)\n\n    # PCA\n    n_comp = min(PCA_COMPONENTS, train_feats_scaled.shape[1])\n    pca = PCA(n_components=n_comp, random_state=SEED)\n    train_pca = pca.fit_transform(train_feats_scaled)\n    test_pca = pca.transform(test_feats_scaled)\n    print(f\"PCA reduced features: {train_pca.shape}\")\n\n    # Prepare CV and LightGBM training\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    oof_preds = np.zeros((train_pca.shape[0], len(breed_cols)))\n    test_preds = np.zeros((test_pca.shape[0], len(breed_cols)))\n\n    folds = list(skf.split(train_pca, y))\n    for fold, (tr_idx, val_idx) in enumerate(folds):\n        print(f\"\\nFold {fold+1}/{N_FOLDS}\")\n        X_tr, X_val = train_pca[tr_idx], train_pca[val_idx]\n        y_tr, y_val = y[tr_idx], y[val_idx]\n\n        lgb_train = lgb.Dataset(X_tr, label=y_tr)\n        lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n\n        params = {\n            \"objective\": \"multiclass\",\n            \"num_class\": len(breed_cols),\n            \"metric\": \"multi_logloss\",\n            \"verbosity\": -1,\n            \"learning_rate\": 0.1,\n            \"num_leaves\": 31,\n            \"seed\": SEED + fold,\n            \"feature_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"bagging_fraction\": 0.8,\n        }\n        # train with early stopping\n        bst = lgb.train(\n            params,\n            lgb_train,\n            num_boost_round=2000,\n            valid_sets=[lgb_train, lgb_val],\n            valid_names=[\"train\", \"valid\"],\n            early_stopping_rounds=50,\n            verbose_eval=100,\n        )\n\n        # Predict\n        val_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n        oof_preds[val_idx] = val_pred\n        test_fold_pred = bst.predict(test_pca, num_iteration=bst.best_iteration)\n        test_preds += test_fold_pred / N_FOLDS\n\n    # Compute CV log loss\n    cv_loss = log_loss(y, oof_preds)\n    print(f\"\\nCV Multi-class Log Loss: {cv_loss:.6f}\")\n\n    # Build submission dataframe using sample submission columns order\n    sub_df = pd.DataFrame(test_preds, columns=breed_cols)\n    sub_df.insert(0, \"id\", test_ids)\n    # Ensure columns ordering exactly matches sample_sub\n    sub_df = sub_df[sample_sub.columns]\n\n    # Save submission\n    sub_df.to_csv(SUBMISSION_FILE, index=False)\n    print(f\"Submission saved to {SUBMISSION_FILE}\")\n\n    t_end = time.time()\n    print(f\"Time elapsed: {(t_end - t0)/60:.2f} minutes\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "#!/usr/bin/env python3\nimport os\nimport sys\nimport time\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nLABELS_PATH = os.path.join(INPUT_DIR, \"labels.csv\")\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nSUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n\n# Config\nN_FOLDS = 5\nRANDOM_STATE = 42\nBINS_PER_CHANNEL = 16\nRESIZE_TO = (128, 128)  # resize images to this for faster processing\n\n\ndef load_image(path, resize=RESIZE_TO):\n    try:\n        with Image.open(path) as img:\n            img = img.convert(\"RGB\")\n            if resize is not None:\n                img = img.resize(resize)\n            return np.array(img)\n    except Exception as e:\n        # In case of a corrupted image, return an array of zeros\n        print(f\"Warning: failed to load image {path}: {e}\")\n        return np.zeros((resize[1], resize[0], 3), dtype=np.uint8)\n\n\ndef extract_color_hist_features(image_array, bins_per_channel=BINS_PER_CHANNEL):\n    # image_array: H x W x 3, dtype uint8\n    # compute normalized histogram per channel\n    chans = []\n    for ch in range(3):\n        hist, _ = np.histogram(\n            image_array[:, :, ch].ravel(),\n            bins=bins_per_channel,\n            range=(0, 256),\n            density=True,\n        )\n        chans.append(hist)\n    feat = np.concatenate(chans).astype(np.float32)\n    return feat  # length = 3 * bins_per_channel\n\n\ndef extract_features_for_paths(paths, bins_per_channel=BINS_PER_CHANNEL):\n    n = len(paths)\n    feat_dim = 3 * bins_per_channel\n    feats = np.zeros((n, feat_dim), dtype=np.float32)\n    for i, p in enumerate(tqdm(paths, desc=\"Extracting features\", unit=\"img\")):\n        img = load_image(p)\n        feats[i] = extract_color_hist_features(img, bins_per_channel=bins_per_channel)\n    return feats\n\n\ndef main():\n    start_time = time.time()\n    # Read labels and sample submission\n    labels_df = pd.read_csv(LABELS_PATH)\n    sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n\n    # Breed list from sample submission columns (preserve order)\n    breed_list = list(sample_sub.columns)[1:]\n\n    # Label encoder aligned to sample submission order\n    le = LabelEncoder()\n    le.fit(breed_list)\n\n    # Build train file paths and targets\n    train_ids = labels_df[\"id\"].values\n    train_breeds = labels_df[\"breed\"].values\n    train_paths = [os.path.join(TRAIN_DIR, f\"{pid}.jpg\") for pid in train_ids]\n\n    # transform breeds to indices (LabelEncoder expects known classes)\n    # If there are breeds in train not in sample_sub, raise informative error\n    unseen = set(train_breeds) - set(breed_list)\n    if len(unseen) > 0:\n        raise ValueError(\n            f\"Found breeds in labels.csv not in sample_submission: {unseen}\"\n        )\n\n    y = le.transform(train_breeds)\n\n    # Build test file paths and ids\n    test_ids = sample_sub[\"id\"].values\n    test_paths = [os.path.join(TEST_DIR, f\"{tid}.jpg\") for tid in test_ids]\n\n    print(\n        f\"Num train: {len(train_paths)}, Num test: {len(test_paths)}, Num classes: {len(breed_list)}\"\n    )\n\n    # Extract features (color histograms)\n    print(\"Extracting train features (color histograms)...\")\n    X_train = extract_features_for_paths(train_paths, bins_per_channel=BINS_PER_CHANNEL)\n    print(\"Extracting test features (color histograms)...\")\n    X_test = extract_features_for_paths(test_paths, bins_per_channel=BINS_PER_CHANNEL)\n\n    # Scale features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # 5-fold stratified CV with LogisticRegression\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n    oof_probs = np.zeros((X_train.shape[0], len(breed_list)), dtype=np.float64)\n    test_probs = np.zeros((X_test.shape[0], len(breed_list)), dtype=np.float64)\n    val_losses = []\n\n    fold_num = 0\n    for train_idx, val_idx in skf.split(X_train_scaled, y):\n        fold_num += 1\n        print(\n            f\"Fold {fold_num}/{N_FOLDS} - train {len(train_idx)} / val {len(val_idx)}\"\n        )\n        X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n        y_tr, y_val = y[train_idx], y[val_idx]\n\n        clf = LogisticRegression(\n            C=1.0,\n            multi_class=\"multinomial\",\n            solver=\"saga\",\n            max_iter=2000,\n            verbose=0,\n            n_jobs=-1,\n            random_state=RANDOM_STATE,\n        )\n        clf.fit(X_tr, y_tr)\n        val_pred = clf.predict_proba(X_val)\n        oof_probs[val_idx] = val_pred\n        loss = log_loss(y_val, val_pred, labels=range(len(breed_list)))\n        val_losses.append(loss)\n        print(f\"  Fold {fold_num} log_loss: {loss:.6f}\")\n\n        # test predictions\n        test_pred = clf.predict_proba(X_test_scaled)\n        test_probs += test_pred\n\n    # Average test predictions\n    test_probs /= N_FOLDS\n    mean_cv_loss = float(np.mean(val_losses))\n    std_cv_loss = float(np.std(val_losses))\n    print(f\"\\nMean CV log_loss: {mean_cv_loss:.6f} \u00b1 {std_cv_loss:.6f}\")\n\n    # Write submission file using same column order as sample_submission\n    submission_df = pd.DataFrame(test_probs, columns=breed_list)\n    submission_df.insert(0, \"id\", test_ids)\n    submission_df = submission_df[[\"id\"] + breed_list]\n    submission_df.to_csv(SUBMISSION_PATH, index=False)\n    print(f\"Submission saved to {SUBMISSION_PATH}\")\n\n    # Additionally print overall OOF log loss\n    oof_loss = log_loss(y, oof_probs, labels=range(len(breed_list)))\n    print(f\"OOF log_loss (full): {oof_loss:.6f}\")\n\n    elapsed = time.time() - start_time\n    print(f\"Total time: {elapsed/60:.2f} minutes\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "#!/usr/bin/env python3\nimport os\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nimport lightgbm as lgb\nimport random\n\n# Config\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nINPUT_DIR = Path(\"input\")\nTRAIN_DIR = INPUT_DIR / \"train\"\nTEST_DIR = INPUT_DIR / \"test\"\nLABELS_FILE = INPUT_DIR / \"labels.csv\"\nSAMPLE_SUB_FILE = INPUT_DIR / \"sample_submission.csv\"\nSUBMISSION_DIR = Path(\"submission\")\nSUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\nSUBMISSION_FILE = SUBMISSION_DIR / \"submission.csv\"\n\n# Feature settings\nRESIZE = (128, 128)  # resize images for consistent histogram computation\nBINS_PER_CHANNEL = 32  # histogram bins per channel\nPCA_COMPONENTS = 50\nN_FOLDS = 5\n\n\ndef read_image(path):\n    try:\n        img = Image.open(path).convert(\"RGB\")\n        return img\n    except Exception:\n        # return a black image if failed to open\n        return Image.new(\"RGB\", RESIZE, (0, 0, 0))\n\n\ndef extract_handcrafted_features(\n    ids, folder, resize=RESIZE, bins_per_channel=BINS_PER_CHANNEL\n):\n    features = []\n    ids_out = []\n    bin_edges = np.linspace(0, 256, bins_per_channel + 1)  # 0..255 inclusive\n    for img_id in tqdm(\n        ids, desc=f\"Extract features from {Path(folder).name}\", unit=\"img\"\n    ):\n        img_path = Path(folder) / (img_id + \".jpg\")\n        img = read_image(img_path)\n        img = img.resize(resize)\n        arr = np.array(img)  # H x W x 3\n        # compute per-channel histograms (normalized)\n        ch_feats = []\n        for ch in range(3):\n            hist, _ = np.histogram(arr[:, :, ch].ravel(), bins=bin_edges, density=True)\n            ch_feats.append(hist.astype(np.float32))\n        # flatten hist\n        hist_feats = np.concatenate(ch_feats)\n        # simple stats\n        means = arr.mean(axis=(0, 1)).astype(np.float32)\n        stds = arr.std(axis=(0, 1)).astype(np.float32)\n        # aspect ratio and size\n        h, w = arr.shape[0], arr.shape[1]\n        aspect = np.array([h / (w + 1e-9), (h * w)], dtype=np.float32)\n        feat = np.concatenate([hist_feats, means, stds, aspect])\n        features.append(feat)\n        ids_out.append(img_id)\n    features = np.vstack(features)\n    return ids_out, features\n\n\ndef main():\n    t0 = time.time()\n    # Read metadata\n    labels_df = pd.read_csv(LABELS_FILE)\n    sample_sub = pd.read_csv(SAMPLE_SUB_FILE)\n    # Determine breed columns and encode labels accordingly\n    breed_cols = list(sample_sub.columns)\n    if \"id\" in breed_cols:\n        breed_cols.remove(\"id\")\n    le = LabelEncoder()\n    le.fit(breed_cols)\n    train_ids = labels_df[\"id\"].astype(str).values\n    train_breeds = labels_df[\"breed\"].values\n    y = le.transform(\n        train_breeds\n    )  # integer labels aligned with sample submission columns\n\n    test_ids = sample_sub[\"id\"].astype(str).values\n\n    # Extract features\n    train_ids_list, train_feats = extract_handcrafted_features(train_ids, TRAIN_DIR)\n    test_ids_list, test_feats = extract_handcrafted_features(test_ids, TEST_DIR)\n\n    # Ensure order matches labels/sample submission\n    if list(train_ids) != train_ids_list:\n        idx_map = {id_: i for i, id_ in enumerate(train_ids_list)}\n        order = [idx_map[id_] for id_ in train_ids]\n        train_feats = train_feats[order]\n    if list(test_ids) != test_ids_list:\n        idx_map = {id_: i for i, id_ in enumerate(test_ids_list)}\n        order = [idx_map[id_] for id_ in test_ids]\n        test_feats = test_feats[order]\n\n    # Scale + PCA\n    scaler = StandardScaler()\n    train_scaled = scaler.fit_transform(train_feats)\n    test_scaled = scaler.transform(test_feats)\n\n    n_comp = min(PCA_COMPONENTS, train_scaled.shape[1])\n    pca = PCA(n_components=n_comp, random_state=SEED)\n    train_pca = pca.fit_transform(train_scaled)\n    test_pca = pca.transform(test_scaled)\n\n    # CV and LightGBM\n    num_classes = len(breed_cols)\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    oof_preds = np.zeros((train_pca.shape[0], num_classes), dtype=np.float64)\n    test_preds = np.zeros((test_pca.shape[0], num_classes), dtype=np.float64)\n\n    folds = list(skf.split(train_pca, y))\n    for fold, (tr_idx, val_idx) in enumerate(folds):\n        print(f\"\\nFold {fold+1}/{N_FOLDS}\")\n        X_tr, X_val = train_pca[tr_idx], train_pca[val_idx]\n        y_tr, y_val = y[tr_idx], y[val_idx]\n\n        lgb_train = lgb.Dataset(X_tr, label=y_tr)\n        lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n\n        params = {\n            \"objective\": \"multiclass\",\n            \"num_class\": num_classes,\n            \"metric\": \"multi_logloss\",\n            \"verbosity\": -1,\n            \"learning_rate\": 0.1,\n            \"num_leaves\": 31,\n            \"seed\": SEED + fold,\n            \"feature_fraction\": 0.9,\n            \"bagging_freq\": 1,\n            \"bagging_fraction\": 0.9,\n        }\n        bst = lgb.train(\n            params,\n            lgb_train,\n            num_boost_round=1000,\n            valid_sets=[lgb_train, lgb_val],\n            valid_names=[\"train\", \"valid\"],\n            early_stopping_rounds=50,\n            verbose_eval=100,\n        )\n        val_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n        oof_preds[val_idx] = val_pred\n        test_fold_pred = bst.predict(test_pca, num_iteration=bst.best_iteration)\n        test_preds += test_fold_pred / N_FOLDS\n\n        fold_loss = log_loss(y_val, val_pred)\n        print(f\"Fold {fold+1} log_loss: {fold_loss:.6f}\")\n\n    # CV metric\n    cv_loss = log_loss(y, oof_preds)\n    print(f\"\\nCV Multi-class Log Loss (OOF): {cv_loss:.6f}\")\n\n    # Build submission DataFrame using sample submission ordering\n    sub_df = pd.DataFrame(test_preds, columns=breed_cols)\n    sub_df.insert(0, \"id\", test_ids)\n    # Ensure exact column order\n    sub_df = sub_df[sample_sub.columns]\n    # Save submission\n    sub_df.to_csv(SUBMISSION_FILE, index=False)\n    print(f\"Submission saved to {SUBMISSION_FILE}\")\n\n    t_end = time.time()\n    print(f\"Total time elapsed: {(t_end - t0)/60:.2f} minutes\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nimport lightgbm as lgb\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nLABELS_PATH = os.path.join(INPUT_DIR, \"labels.csv\")\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nSUBMISSION_DIR = \"./submission\"\nSUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# Read metadata\nlabels_df = pd.read_csv(LABELS_PATH)\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\nbreed_cols = list(sample_sub.columns[1:])  # keep order for submission\n\n# Map breed to integer using sample_submission order to ensure submission columns match\nbreed2idx = {b: i for i, b in enumerate(breed_cols)}\nidx2breed = {i: b for b, i in breed2idx.items()}\n\n# Prepare train ids and labels (ensure mapping exists)\ntrain_ids = labels_df[\"id\"].values\nif not labels_df[\"breed\"].isin(breed2idx).all():\n    missing = set(labels_df[\"breed\"].unique()) - set(breed2idx.keys())\n    raise ValueError(\n        f\"Some breeds in labels.csv not found in sample_submission columns: {missing}\"\n    )\ntrain_labels = labels_df[\"breed\"].map(breed2idx).values\n\n# Prepare test ids in same order as sample_submission\ntest_ids = sample_sub[\"id\"].values\n\n# Device (CPU)\ndevice = torch.device(\"cpu\")\n\n\n# Image dataset for feature extraction\nclass ImageFolderDataset(Dataset):\n    def __init__(self, ids, folder, labels=None, transform=None):\n        self.ids = ids\n        self.folder = folder\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        img_id = self.ids[idx]\n        path = os.path.join(self.folder, img_id + \".jpg\")\n        img = Image.open(path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        if self.labels is None:\n            return img_id, img\n        else:\n            return img_id, img, self.labels[idx]\n\n\n# Transforms: use ImageNet normalization and standard resnet input size\ntransform = transforms.Compose(\n    [\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\n\n\n# Instantiate pretrained ResNet50 as feature extractor\ndef get_resnet50_feature_extractor():\n    model = models.resnet50(pretrained=True)\n    modules = list(model.children())[:-1]  # remove last fc\n    feature_extractor = nn.Sequential(*modules)\n    feature_extractor.eval()\n    feature_extractor.to(device)\n    return feature_extractor\n\n\nfeat_model = get_resnet50_feature_extractor()\n\n\n# Function to extract features for a dataset and cache to disk\ndef extract_and_cache_features(ids, folder, cache_path, labels=None, batch_size=64):\n    if os.path.exists(cache_path):\n        print(f\"Loading cached features from {cache_path}\")\n        data = np.load(cache_path, allow_pickle=True)\n        features = data[\"features\"]\n        ids_loaded = data[\"ids\"].tolist()\n        return features, ids_loaded\n    dataset = ImageFolderDataset(ids, folder, labels=labels, transform=transform)\n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n    all_ids = []\n    feats = []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=f\"Extracting features from {folder}\"):\n            if labels is None:\n                batch_ids, imgs = batch\n            else:\n                batch_ids, imgs, _ = batch\n            imgs = imgs.to(device)\n            out = feat_model(imgs)  # shape (B, 2048, 1, 1)\n            out = out.cpu().numpy()\n            out = out.reshape(out.shape[0], -1)\n            feats.append(out)\n            all_ids.extend(batch_ids)\n    features = np.vstack(feats)\n    np.savez_compressed(cache_path, features=features, ids=np.array(all_ids))\n    return features, all_ids\n\n\n# Extract features (will cache to working)\ntrain_cache = os.path.join(WORKING_DIR, \"train_feats.npz\")\ntest_cache = os.path.join(WORKING_DIR, \"test_feats.npz\")\n\ntrain_features, train_ids_extracted = extract_and_cache_features(\n    train_ids, TRAIN_DIR, train_cache, labels=train_labels, batch_size=32\n)\ntest_features, test_ids_extracted = extract_and_cache_features(\n    test_ids, TEST_DIR, test_cache, labels=None, batch_size=32\n)\n\n# Align train_features order with labels if necessary\nif list(train_ids_extracted) != list(train_ids):\n    id_to_idx = {id_: i for i, id_ in enumerate(train_ids_extracted)}\n    order = [id_to_idx[id_] for id_ in train_ids]\n    train_features = train_features[order, :]\n\n# Ensure test order matches sample_submission test_ids\nif list(test_ids_extracted) != list(test_ids):\n    id_to_idx = {id_: i for i, id_ in enumerate(test_ids_extracted)}\n    order = [id_to_idx[id_] for id_ in test_ids]\n    test_features = test_features[order, :]\n\n# Prepare data for modeling\nX = train_features.astype(np.float32)\ny = train_labels.astype(int)\nX_test = test_features.astype(np.float32)\n\nprint(\"Feature shapes:\", X.shape, X_test.shape, \"Num classes:\", len(breed_cols))\n\n# 5-fold Stratified CV with LightGBM sklearn API (LGBMClassifier) to enable early stopping\nn_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\nfold_scores = []\noof_preds = np.zeros((X.shape[0], len(breed_cols)), dtype=np.float32)\n\nlgb_params = {\n    \"learning_rate\": 0.05,\n    \"num_leaves\": 31,\n    \"verbosity\": -1,\n    \"random_state\": 42,\n    \"n_jobs\": 4,\n}\n\nbest_iters = []\nfold_num = 0\nfor train_idx, val_idx in skf.split(X, y):\n    fold_num += 1\n    print(f\"Training fold {fold_num}/{n_splits}...\")\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr, y_val = y[train_idx], y[val_idx]\n\n    clf = lgb.LGBMClassifier(n_estimators=1000, **lgb_params)\n    clf.fit(\n        X_tr,\n        y_tr,\n        eval_set=[(X_val, y_val)],\n        eval_metric=\"multi_logloss\",\n        early_stopping_rounds=50,\n        verbose=False,\n    )\n    # Predict\n    val_pred = clf.predict_proba(X_val)\n    oof_preds[val_idx] = val_pred\n    loss = log_loss(y_val, val_pred, labels=list(range(len(breed_cols))))\n    print(f\"Fold {fold_num} log_loss: {loss:.6f}\")\n    fold_scores.append(loss)\n    bi = getattr(clf, \"best_iteration_\", None)\n    if bi is None:\n        bi = clf.n_estimators\n    best_iters.append(bi)\n\nmean_loss = np.mean(fold_scores)\nstd_loss = np.std(fold_scores)\nprint(f\"\\nCV log_loss mean: {mean_loss:.6f} std: {std_loss:.6f}\")\n\n# Train final model on all data using average best iteration (fallback to 300)\nmean_best_iter = int(np.mean(best_iters)) if len(best_iters) > 0 else 300\nif mean_best_iter < 10:\n    mean_best_iter = 300\nprint(\n    f\"Training final model with n_estimators={mean_best_iter} on full training set...\"\n)\nfinal_clf = lgb.LGBMClassifier(n_estimators=mean_best_iter, **lgb_params)\nfinal_clf.fit(X, y, verbose=False)\n\n# Predict on test set\ntest_pred = final_clf.predict_proba(X_test)\n\n# Ensure columns in submission match sample_submission order\nsubmission_df = pd.DataFrame(test_pred, columns=breed_cols)\nsubmission_df.insert(0, \"id\", test_ids)\n\n# If any sample_submission breed column missing (unlikely), add zeros\nfor col in sample_sub.columns[1:]:\n    if col not in submission_df.columns:\n        submission_df[col] = 0.0\n# Reorder columns to match sample_submission exactly\nsubmission_df = submission_df[sample_sub.columns]\n\n# Save submission\nsubmission_df.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"Saved submission to {SUBMISSION_PATH}\")\nprint(f\"Final CV log_loss (mean over {n_splits} folds): {mean_loss:.6f}\")\n", "#!/usr/bin/env python3\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nimport traceback\nimport warnings\n\n# Try import lightgbm, otherwise we'll fallback to LogisticRegression\ntry:\n    import lightgbm as lgb\n\n    HAS_LGB = True\nexcept Exception:\n    HAS_LGB = False\n\nwarnings.filterwarnings(\"ignore\")\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nLABELS_PATH = os.path.join(INPUT_DIR, \"labels.csv\")\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nSUBMISSION_DIR = \"./submission\"\nSUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n# Parameters\nIMAGE_SIZE = 64  # resize images to IMAGE_SIZE x IMAGE_SIZE\nPCA_COMPONENTS = 256\nN_FOLDS = 5\nRANDOM_STATE = 42\nLGB_NUM_ROUNDS = 200\nLGB_EARLY_STOP = 20\n\n# Read sample submission to get columns / breed order\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\nbreed_cols = list(sample_sub.columns)[1:]  # excluding id\n\n# Read labels\nlabels_df = pd.read_csv(LABELS_PATH)\nid_list = labels_df[\"id\"].values\nbreeds = labels_df[\"breed\"].values\n\n# Map breeds to indices according to sample_submission ordering\nbreed_to_idx = {b: i for i, b in enumerate(breed_cols)}\n# Some safety: ensure all breeds in labels exist in mapping\nmissing = set(np.unique(breeds)) - set(breed_cols)\nif missing:\n    raise ValueError(\n        f\"Found breeds in labels not in sample_submission columns: {missing}\"\n    )\n\ny = np.array([breed_to_idx[b] for b in breeds], dtype=np.int32)\n\n\n# Helper to load and preprocess images into numpy arrays (float32)\ndef load_images_as_array(ids, folder, img_size=IMAGE_SIZE):\n    n = len(ids)\n    X = np.zeros((n, img_size * img_size * 3), dtype=np.float32)\n    for i, img_id in enumerate(ids):\n        img_path = os.path.join(folder, img_id + \".jpg\")\n        try:\n            with Image.open(img_path) as img:\n                img = img.convert(\"RGB\")\n                img = img.resize((img_size, img_size), Image.LANCZOS)\n                arr = np.asarray(img, dtype=np.float32) / 255.0\n                X[i, :] = arr.reshape(-1)\n        except Exception as e:\n            # Try other extensions or fallback to zeros\n            alt_ok = False\n            for ext in [\".jpeg\", \".png\", \".bmp\"]:\n                alt_path = os.path.join(folder, img_id + ext)\n                if os.path.exists(alt_path):\n                    try:\n                        with Image.open(alt_path) as img:\n                            img = img.convert(\"RGB\")\n                            img = img.resize((img_size, img_size), Image.LANCZOS)\n                            arr = np.asarray(img, dtype=np.float32) / 255.0\n                            X[i, :] = arr.reshape(-1)\n                            alt_ok = True\n                            break\n                    except Exception:\n                        continue\n            if not alt_ok:\n                print(f\"Warning: failed to load {img_path}: {e}\", file=sys.stderr)\n                X[i, :] = 0.0\n        if (i + 1) % 1000 == 0 or (i + 1) == n:\n            print(f\"Loaded {i + 1}/{n} images from {folder}\", file=sys.stderr)\n    return X\n\n\n# Load train images\nprint(\"Loading train images...\", file=sys.stderr)\nX_all = load_images_as_array(id_list, TRAIN_DIR, IMAGE_SIZE)\nprint(\"Train images loaded.\", file=sys.stderr)\n\n# Load test images ids and data\ntest_sample = pd.read_csv(SAMPLE_SUB_PATH)\ntest_ids = test_sample[\"id\"].values\nprint(\"Loading test images...\", file=sys.stderr)\nX_test_full = load_images_as_array(test_ids, TEST_DIR, IMAGE_SIZE)\nprint(\"Test images loaded.\", file=sys.stderr)\n\nn_classes = len(breed_cols)\n\n# Prepare CV\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\noof_preds = np.zeros((X_all.shape[0], n_classes), dtype=np.float32)\ntest_preds = np.zeros((X_test_full.shape[0], n_classes), dtype=np.float32)\nfold = 0\ncv_losses = []\n\nfor train_idx, val_idx in skf.split(X_all, y):\n    fold += 1\n    print(f\"\\nStarting fold {fold}/{N_FOLDS}\", file=sys.stderr)\n    X_tr, X_val = X_all[train_idx], X_all[val_idx]\n    y_tr, y_val = y[train_idx], y[val_idx]\n\n    # Fit PCA on training fold using randomized SVD to ensure compatibility\n    n_comp = min(PCA_COMPONENTS, X_tr.shape[0], X_tr.shape[1])\n    pca = PCA(n_components=n_comp, svd_solver=\"randomized\", random_state=RANDOM_STATE)\n    print(\"Fitting PCA...\", file=sys.stderr)\n    X_tr_p = pca.fit_transform(X_tr)\n    X_val_p = pca.transform(X_val)\n    X_test_p = pca.transform(X_test_full)\n\n    # Train model: prefer LightGBM if available, otherwise fallback to multinomial logistic regression\n    if HAS_LGB:\n        try:\n            lgb_train = lgb.Dataset(X_tr_p, label=y_tr)\n            lgb_valid = lgb.Dataset(X_val_p, label=y_val, reference=lgb_train)\n            params = {\n                \"objective\": \"multiclass\",\n                \"num_class\": n_classes,\n                \"metric\": \"multi_logloss\",\n                \"learning_rate\": 0.1,\n                \"num_leaves\": 31,\n                \"verbosity\": -1,\n                \"seed\": RANDOM_STATE,\n                \"deterministic\": True,\n            }\n            print(\"Training LightGBM...\", file=sys.stderr)\n            bst = lgb.train(\n                params,\n                lgb_train,\n                num_boost_round=LGB_NUM_ROUNDS,\n                valid_sets=[lgb_train, lgb_valid],\n                early_stopping_rounds=LGB_EARLY_STOP,\n                verbose_eval=False,\n            )\n            val_pred = bst.predict(X_val_p, num_iteration=bst.best_iteration)\n            test_pred = bst.predict(X_test_p, num_iteration=bst.best_iteration)\n        except Exception:\n            print(\n                \"LightGBM training failed; falling back to LogisticRegression. Error:\",\n                file=sys.stderr,\n            )\n            traceback.print_exc()\n            HAS_LGB = False  # fallback next folds as well\n\n    if not HAS_LGB:\n        # Use multinomial logistic regression with saga solver\n        print(\"Training LogisticRegression (multinomial)...\", file=sys.stderr)\n        clf = LogisticRegression(\n            multi_class=\"multinomial\",\n            solver=\"saga\",\n            max_iter=1000,\n            C=1.0,\n            random_state=RANDOM_STATE,\n        )\n        clf.fit(X_tr_p, y_tr)\n        val_pred = clf.predict_proba(X_val_p)\n        test_pred = clf.predict_proba(X_test_p)\n\n    # Ensure probabilities shape is (n_samples, n_classes)\n    if val_pred.shape[1] != n_classes:\n        # If some classes missing in fold for logistic regression, we need to expand\n        # Create a full matrix with small eps and fill available columns\n        eps = 1e-12\n        vp = np.ones((val_pred.shape[0], n_classes), dtype=np.float32) * eps\n        tp = np.ones((test_pred.shape[0], n_classes), dtype=np.float32) * eps\n        # Try to get classes_ attribute from model or bst (lightgbm doesn't provide classes_)\n        if not HAS_LGB and hasattr(clf, \"classes_\"):\n            for i, cls in enumerate(clf.classes_):\n                vp[:, cls] = val_pred[:, i]\n                tp[:, cls] = test_pred[:, i]\n        else:\n            # As a fallback, distribute uniformly\n            vp = np.full(\n                (val_pred.shape[0], n_classes), 1.0 / n_classes, dtype=np.float32\n            )\n            tp = np.full(\n                (test_pred.shape[0], n_classes), 1.0 / n_classes, dtype=np.float32\n            )\n        val_pred = vp\n        test_pred = tp\n\n    oof_preds[val_idx] = val_pred\n    test_preds += test_pred / N_FOLDS\n\n    loss = log_loss(y_val, val_pred, labels=list(range(n_classes)))\n    cv_losses.append(loss)\n    print(f\"Fold {fold} log_loss: {loss:.6f}\", file=sys.stderr)\n\n# Overall CV log loss\ncv_logloss = float(np.mean(cv_losses)) if cv_losses else float(\"nan\")\nprint(\n    f\"\\nCV mean multi-class log-loss over {len(cv_losses)} folds: {cv_logloss:.6f}\",\n    file=sys.stderr,\n)\n\n# Save submission using breed_cols order\nsubmission = pd.DataFrame(test_preds, columns=breed_cols)\nsubmission.insert(0, \"id\", test_ids)\n# Ensure column order matches sample_submission\nsubmission = submission[sample_sub.columns]\nsubmission.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"Submission saved to {SUBMISSION_PATH}\", file=sys.stderr)\n\n# Also print the metric to stdout as required\nprint(f\"Mean CV log-loss: {cv_logloss:.6f}\")\n", "#!/usr/bin/env python3\nimport os\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nimport lightgbm as lgb\nimport random\n\n# Config\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nINPUT_DIR = Path(\"input\")\nTRAIN_DIR = INPUT_DIR / \"train\"\nTEST_DIR = INPUT_DIR / \"test\"\nLABELS_FILE = INPUT_DIR / \"labels.csv\"\nSAMPLE_SUB_FILE = INPUT_DIR / \"sample_submission.csv\"\nSUBMISSION_DIR = Path(\"submission\")\nSUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\nSUBMISSION_FILE = SUBMISSION_DIR / \"submission.csv\"\n\n# Feature settings\nRESIZE = (128, 128)  # resize images for consistent histogram computation\nBINS_PER_CHANNEL = 32  # histogram bins per channel\nPCA_COMPONENTS = 50\nN_FOLDS = 5\n\n\ndef read_image(path):\n    try:\n        img = Image.open(path).convert(\"RGB\")\n        return img\n    except Exception:\n        # return a black image if failed to open\n        return Image.new(\"RGB\", RESIZE, (0, 0, 0))\n\n\ndef extract_handcrafted_features(\n    ids, folder, resize=RESIZE, bins_per_channel=BINS_PER_CHANNEL\n):\n    features = []\n    ids_out = []\n    # bin edges for 0..255\n    bin_edges = np.linspace(0, 256, bins_per_channel + 1)\n    for img_id in tqdm(\n        ids, desc=f\"Extract features from {Path(folder).name}\", unit=\"img\"\n    ):\n        img_path = Path(folder) / (img_id + \".jpg\")\n        img = read_image(img_path)\n        img = img.resize(resize)\n        arr = np.array(img)  # H x W x 3\n        # compute per-channel histograms (normalized)\n        ch_feats = []\n        for ch in range(3):\n            hist, _ = np.histogram(arr[:, :, ch].ravel(), bins=bin_edges, density=True)\n            ch_feats.append(hist.astype(np.float32))\n        # flatten hist\n        hist_feats = np.concatenate(ch_feats)\n        # simple stats\n        means = arr.mean(axis=(0, 1)).astype(np.float32)\n        stds = arr.std(axis=(0, 1)).astype(np.float32)\n        # aspect ratio and size\n        h, w = arr.shape[0], arr.shape[1]\n        aspect = np.array([h / (w + 1e-9), (h * w)], dtype=np.float32)\n        feat = np.concatenate([hist_feats, means, stds, aspect])\n        features.append(feat)\n        ids_out.append(img_id)\n    features = np.vstack(features).astype(np.float32)\n    return ids_out, features\n\n\ndef main():\n    t0 = time.time()\n    # Read metadata\n    labels_df = pd.read_csv(LABELS_FILE)\n    sample_sub = pd.read_csv(SAMPLE_SUB_FILE)\n\n    # Breed columns in sample submission define desired ordering\n    breed_cols = [c for c in sample_sub.columns if c != \"id\"]\n\n    # Build explicit mapping from breed name to index according to sample submission order\n    class_to_idx = {breed: i for i, breed in enumerate(breed_cols)}\n\n    # Prepare training ids and labels (use mapping defined above)\n    train_ids = labels_df[\"id\"].astype(str).values\n    train_breeds = labels_df[\"breed\"].astype(str).values\n    # Map breeds to indices using sample submission order to ensure alignment\n    y = np.array([class_to_idx[b] for b in train_breeds], dtype=np.int32)\n\n    test_ids = sample_sub[\"id\"].astype(str).values\n\n    # Extract features\n    train_ids_list, train_feats = extract_handcrafted_features(train_ids, TRAIN_DIR)\n    test_ids_list, test_feats = extract_handcrafted_features(test_ids, TEST_DIR)\n\n    # Ensure order matches labels/sample submission\n    if list(train_ids) != train_ids_list:\n        idx_map = {id_: i for i, id_ in enumerate(train_ids_list)}\n        order = [idx_map[id_] for id_ in train_ids]\n        train_feats = train_feats[order]\n    if list(test_ids) != test_ids_list:\n        idx_map = {id_: i for i, id_ in enumerate(test_ids_list)}\n        order = [idx_map[id_] for id_ in test_ids]\n        test_feats = test_feats[order]\n\n    # Scale + PCA\n    scaler = StandardScaler()\n    train_scaled = scaler.fit_transform(train_feats)\n    test_scaled = scaler.transform(test_feats)\n\n    n_comp = min(PCA_COMPONENTS, train_scaled.shape[1])\n    pca = PCA(n_components=n_comp, random_state=SEED)\n    train_pca = pca.fit_transform(train_scaled)\n    test_pca = pca.transform(test_scaled)\n\n    # CV and LightGBM\n    num_classes = len(breed_cols)\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    oof_preds = np.zeros((train_pca.shape[0], num_classes), dtype=np.float64)\n    test_preds = np.zeros((test_pca.shape[0], num_classes), dtype=np.float64)\n\n    folds = list(skf.split(train_pca, y))\n    for fold, (tr_idx, val_idx) in enumerate(folds):\n        print(f\"\\nFold {fold+1}/{N_FOLDS}\")\n        X_tr, X_val = train_pca[tr_idx], train_pca[val_idx]\n        y_tr, y_val = y[tr_idx], y[val_idx]\n\n        lgb_train = lgb.Dataset(X_tr, label=y_tr)\n        lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n\n        params = {\n            \"objective\": \"multiclass\",\n            \"num_class\": num_classes,\n            \"metric\": \"multi_logloss\",\n            \"verbosity\": -1,\n            \"learning_rate\": 0.1,\n            \"num_leaves\": 31,\n            \"seed\": int(SEED + fold),\n            \"feature_fraction\": 0.9,\n            \"bagging_freq\": 1,\n            \"bagging_fraction\": 0.9,\n        }\n        bst = lgb.train(\n            params,\n            lgb_train,\n            num_boost_round=500,\n            valid_sets=[lgb_train, lgb_val],\n            valid_names=[\"train\", \"valid\"],\n            early_stopping_rounds=50,\n            verbose_eval=100,\n        )\n        val_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n        oof_preds[val_idx] = val_pred\n        test_fold_pred = bst.predict(test_pca, num_iteration=bst.best_iteration)\n        test_preds += test_fold_pred / N_FOLDS\n\n        fold_loss = log_loss(y_val, val_pred)\n        print(f\"Fold {fold+1} log_loss: {fold_loss:.6f}\")\n\n    # CV metric\n    cv_loss = log_loss(y, oof_preds)\n    print(f\"\\nCV Multi-class Log Loss (OOF): {cv_loss:.6f}\")\n\n    # Build submission DataFrame using sample submission ordering (breed_cols)\n    sub_df = pd.DataFrame(test_preds, columns=breed_cols)\n    sub_df.insert(0, \"id\", test_ids)\n    # Ensure exact column order as sample submission\n    sub_df = sub_df[sample_sub.columns]\n    # Save submission\n    sub_df.to_csv(SUBMISSION_FILE, index=False)\n    print(f\"Submission saved to {SUBMISSION_FILE}\")\n\n    t_end = time.time()\n    print(f\"Total time elapsed: {(t_end - t0)/60:.2f} minutes\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "#!/usr/bin/env python3\nimport os\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import log_loss\nimport lightgbm as lgb\nimport random\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# Config\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nINPUT_DIR = Path(\"input\")\nTRAIN_DIR = INPUT_DIR / \"train\"\nTEST_DIR = INPUT_DIR / \"test\"\nLABELS_FILE = INPUT_DIR / \"labels.csv\"\nSAMPLE_SUB_FILE = INPUT_DIR / \"sample_submission.csv\"\nSUBMISSION_DIR = Path(\"submission\")\nSUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\nSUBMISSION_FILE = SUBMISSION_DIR / \"submission.csv\"\n\n# Feature settings\nRESIZE = (128, 128)  # resize images for consistent histogram computation\nBINS_PER_CHANNEL = 32  # histogram bins per channel\nPCA_COMPONENTS = 50\nN_FOLDS = 5\n\n\ndef read_image(path):\n    try:\n        img = Image.open(path).convert(\"RGB\")\n        return img\n    except Exception:\n        # return a black image if failed to open\n        return Image.new(\"RGB\", RESIZE, (0, 0, 0))\n\n\ndef extract_handcrafted_features(\n    ids, folder, resize=RESIZE, bins_per_channel=BINS_PER_CHANNEL\n):\n    \"\"\"\n    Extract lightweight handcrafted features: per-channel histograms (normalized), per-channel mean/std,\n    and simple aspect/size features. Robust to missing images (replaces with black image).\n    Returns (ids_out, features_numpy) where ids_out corresponds to returned rows.\n    \"\"\"\n    features = []\n    ids_out = []\n    # bin edges for 0..255\n    bin_edges = np.linspace(0, 256, bins_per_channel + 1)\n    for img_id in tqdm(\n        ids, desc=f\"Extract features from {Path(folder).name}\", unit=\"img\"\n    ):\n        img_path = Path(folder) / (img_id + \".jpg\")\n        img = read_image(img_path)\n        if img is None:\n            img = Image.new(\"RGB\", resize, (0, 0, 0))\n        try:\n            img = img.resize(resize)\n        except Exception:\n            img = Image.new(\"RGB\", resize, (0, 0, 0))\n        arr = np.array(img)  # H x W x 3\n        # compute per-channel histograms (normalized)\n        ch_feats = []\n        for ch in range(3):\n            hist, _ = np.histogram(arr[:, :, ch].ravel(), bins=bin_edges, density=True)\n            ch_feats.append(hist.astype(np.float32))\n        # flatten hist\n        hist_feats = np.concatenate(ch_feats)\n        # simple stats\n        means = arr.mean(axis=(0, 1)).astype(np.float32)\n        stds = arr.std(axis=(0, 1)).astype(np.float32)\n        # aspect ratio and size (use original resized dims)\n        h, w = arr.shape[0], arr.shape[1]\n        aspect = np.array([h / (w + 1e-9), (h * w)], dtype=np.float32)\n        feat = np.concatenate([hist_feats, means, stds, aspect])\n        features.append(feat)\n        ids_out.append(img_id)\n    features = np.vstack(features).astype(np.float32)\n    return ids_out, features\n\n\ndef safe_normalize_rows(preds):\n    # ensure probabilities non-negative and rows sum to 1\n    preds = np.maximum(preds, 1e-15)\n    row_sums = preds.sum(axis=1, keepdims=True)\n    row_sums[row_sums == 0] = 1.0\n    return preds / row_sums\n\n\ndef main():\n    t0 = time.time()\n    print(\"Reading metadata...\")\n    labels_df = pd.read_csv(LABELS_FILE)\n    sample_sub = pd.read_csv(SAMPLE_SUB_FILE)\n\n    # Breed columns in sample submission define desired ordering\n    breed_cols = [c for c in sample_sub.columns if c != \"id\"]\n\n    # Build explicit mapping from breed name to index according to sample submission order\n    class_to_idx = {breed: i for i, breed in enumerate(breed_cols)}\n    num_classes = len(breed_cols)\n    print(f\"Number of classes (breeds): {num_classes}\")\n\n    # Prepare training ids and labels (use mapping defined above)\n    train_ids = labels_df[\"id\"].astype(str).values\n    train_breeds = labels_df[\"breed\"].astype(str).values\n    # Map breeds to indices using sample submission order to ensure alignment\n    try:\n        y = np.array([class_to_idx[b] for b in train_breeds], dtype=np.int32)\n    except KeyError as e:\n        missing = e.args[0]\n        raise RuntimeError(\n            f\"Breed label in labels.csv not found in sample submission columns: {missing}\"\n        )\n\n    test_ids = sample_sub[\"id\"].astype(str).values\n\n    # Extract features (this can take some time)\n    print(\"Extracting training features...\")\n    train_ids_list, train_feats = extract_handcrafted_features(train_ids, TRAIN_DIR)\n    print(\"Extracting test features...\")\n    test_ids_list, test_feats = extract_handcrafted_features(test_ids, TEST_DIR)\n\n    # Ensure order matches original arrays; if not, re-order to match original id arrays\n    if list(train_ids) != train_ids_list:\n        idx_map = {id_: i for i, id_ in enumerate(train_ids_list)}\n        order = [idx_map[id_] for id_ in train_ids]\n        train_feats = train_feats[order]\n    if list(test_ids) != test_ids_list:\n        idx_map = {id_: i for i, id_ in enumerate(test_ids_list)}\n        order = [idx_map[id_] for id_ in test_ids]\n        test_feats = test_feats[order]\n\n    print(\n        f\"Train features shape: {train_feats.shape}, Test features shape: {test_feats.shape}\"\n    )\n\n    # Scale + PCA\n    scaler = StandardScaler()\n    train_scaled = scaler.fit_transform(train_feats)\n    test_scaled = scaler.transform(test_feats)\n\n    n_comp = min(PCA_COMPONENTS, train_scaled.shape[1])\n    pca = PCA(n_components=n_comp, random_state=SEED)\n    train_pca = pca.fit_transform(train_scaled)\n    test_pca = pca.transform(test_scaled)\n    print(f\"PCA reduced dimension to: {n_comp}\")\n\n    # Hold-out split for quick validation (single fold)\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=SEED)\n    hold_train_idx, hold_val_idx = next(sss.split(train_pca, y))\n    X_hold_tr, X_hold_val = train_pca[hold_train_idx], train_pca[hold_val_idx]\n    y_hold_tr, y_hold_val = y[hold_train_idx], y[hold_val_idx]\n\n    # Train a single LightGBM on hold-out training data to get a simple hold-out metric\n    print(\"Training a single LightGBM for hold-out validation (quick check)...\")\n    lgb_train = lgb.Dataset(X_hold_tr, label=y_hold_tr)\n    params_hold = {\n        \"objective\": \"multiclass\",\n        \"num_class\": num_classes,\n        \"metric\": \"multi_logloss\",\n        \"verbosity\": -1,\n        \"learning_rate\": 0.1,\n        \"num_leaves\": 31,\n        \"seed\": int(SEED),\n        \"feature_fraction\": 0.9,\n        \"bagging_freq\": 1,\n        \"bagging_fraction\": 0.9,\n        \"num_threads\": 4,\n    }\n    # small fixed number of rounds for quick hold-out\n    bst_hold = lgb.train(\n        params_hold,\n        lgb_train,\n        num_boost_round=200,\n        verbose_eval=False,\n    )\n    hold_pred = bst_hold.predict(X_hold_val)\n    hold_pred = safe_normalize_rows(hold_pred)\n    hold_loss = log_loss(y_hold_val, hold_pred)\n    print(f\"Hold-out Multi-class Log Loss: {hold_loss:.6f}\")\n\n    # 5-fold CV and full test prediction\n    print(\"\\nStarting 5-fold CV...\")\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    oof_preds = np.zeros((train_pca.shape[0], num_classes), dtype=np.float64)\n    test_preds = np.zeros((test_pca.shape[0], num_classes), dtype=np.float64)\n\n    folds = list(skf.split(train_pca, y))\n    for fold, (tr_idx, val_idx) in enumerate(folds):\n        print(f\"\\nFold {fold + 1}/{N_FOLDS}\")\n        X_tr, X_val = train_pca[tr_idx], train_pca[val_idx]\n        y_tr, y_val = y[tr_idx], y[val_idx]\n\n        lgb_train = lgb.Dataset(X_tr, label=y_tr)\n        lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n\n        params = {\n            \"objective\": \"multiclass\",\n            \"num_class\": num_classes,\n            \"metric\": \"multi_logloss\",\n            \"verbosity\": -1,\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"seed\": int(SEED + fold),\n            \"feature_fraction\": 0.9,\n            \"bagging_freq\": 1,\n            \"bagging_fraction\": 0.9,\n            \"num_threads\": 4,\n        }\n        bst = lgb.train(\n            params,\n            lgb_train,\n            num_boost_round=1000,\n            valid_sets=[lgb_train, lgb_val],\n            valid_names=[\"train\", \"valid\"],\n            early_stopping_rounds=50,\n            verbose_eval=100,\n        )\n        val_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n        val_pred = safe_normalize_rows(val_pred)\n        oof_preds[val_idx] = val_pred\n        test_fold_pred = bst.predict(test_pca, num_iteration=bst.best_iteration)\n        test_preds += test_fold_pred / N_FOLDS\n\n        fold_loss = log_loss(y_val, val_pred)\n        print(f\"Fold {fold + 1} log_loss: {fold_loss:.6f}\")\n\n    # CV metric\n    oof_preds = safe_normalize_rows(oof_preds)\n    cv_loss = log_loss(y, oof_preds)\n    print(f\"\\nCV Multi-class Log Loss (OOF): {cv_loss:.6f}\")\n\n    # Ensure test predictions are proper probabilities\n    test_preds = safe_normalize_rows(test_preds)\n\n    # Build submission DataFrame using sample submission ordering (breed_cols)\n    sub_df = pd.DataFrame(test_preds, columns=breed_cols)\n    sub_df.insert(0, \"id\", test_ids)\n    # Ensure exact column order as sample submission\n    sub_df = sub_df[sample_sub.columns]\n\n    # Save submission\n    sub_df.to_csv(SUBMISSION_FILE, index=False)\n    print(f\"\\nSubmission saved to {SUBMISSION_FILE}\")\n\n    t_end = time.time()\n    print(f\"Total time elapsed: {(t_end - t0)/60:.2f} minutes\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "#!/usr/bin/env python3\nimport os\nimport time\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import log_loss\nfrom sklearn.linear_model import LogisticRegression\nimport random\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# Config\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nINPUT_DIR = Path(\"input\")\nTRAIN_DIR = INPUT_DIR / \"train\"\nTEST_DIR = INPUT_DIR / \"test\"\nLABELS_FILE = INPUT_DIR / \"labels.csv\"\nSAMPLE_SUB_FILE = INPUT_DIR / \"sample_submission.csv\"\nSUBMISSION_DIR = Path(\"submission\")\nSUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\nSUBMISSION_FILE = SUBMISSION_DIR / \"submission.csv\"\n\n# Feature settings (kept small for speed and robustness)\nRESIZE = (64, 64)  # smaller resize to speed up feature extraction\nBINS_PER_CHANNEL = 16  # fewer histogram bins\nPCA_COMPONENTS = 50\nN_FOLDS = 5\n\n\ndef read_image_safe(path, resize=RESIZE):\n    try:\n        img = Image.open(path).convert(\"RGB\")\n        if resize is not None:\n            img = img.resize(resize)\n        return np.array(img)\n    except Exception:\n        # return a black image if failed to open\n        return np.zeros((resize[1], resize[0], 3), dtype=np.uint8)\n\n\ndef extract_handcrafted_features(\n    ids, folder, resize=RESIZE, bins_per_channel=BINS_PER_CHANNEL\n):\n    \"\"\"\n    Extract per-channel histograms (normalized), per-channel mean/std,\n    and simple area/aspect features. Returns (ids_out, features_numpy).\n    \"\"\"\n    features = []\n    ids_out = []\n    # bin edges for 0..255 inclusive\n    bin_edges = np.linspace(0, 256, bins_per_channel + 1)\n    for img_id in tqdm(\n        ids, desc=f\"Extract features from {Path(folder).name}\", unit=\"img\"\n    ):\n        img_path = Path(folder) / (img_id + \".jpg\")\n        arr = read_image_safe(img_path, resize=resize)  # H x W x 3\n        # compute per-channel histograms (normalized)\n        ch_feats = []\n        for ch in range(3):\n            hist, _ = np.histogram(arr[:, :, ch].ravel(), bins=bin_edges, density=True)\n            ch_feats.append(hist.astype(np.float32))\n        hist_feats = np.concatenate(ch_feats)\n        # simple stats\n        means = arr.mean(axis=(0, 1)).astype(np.float32)\n        stds = arr.std(axis=(0, 1)).astype(np.float32)\n        # aspect ratio and size\n        h, w = arr.shape[0], arr.shape[1]\n        aspect = np.array([h / (w + 1e-9), (h * w)], dtype=np.float32)\n        feat = np.concatenate([hist_feats, means, stds, aspect])\n        features.append(feat)\n        ids_out.append(img_id)\n    features = np.vstack(features).astype(np.float32)\n    return ids_out, features\n\n\ndef safe_normalize_rows(preds):\n    preds = np.maximum(preds, 1e-15)\n    row_sums = preds.sum(axis=1, keepdims=True)\n    row_sums[row_sums == 0] = 1.0\n    return preds / row_sums\n\n\ndef main():\n    t0 = time.time()\n    print(\"Reading metadata...\")\n    labels_df = pd.read_csv(LABELS_FILE)\n    sample_sub = pd.read_csv(SAMPLE_SUB_FILE)\n\n    # Breed columns in sample submission define desired ordering\n    breed_cols = [c for c in sample_sub.columns if c != \"id\"]\n\n    # Build explicit mapping from breed name to index according to sample submission order\n    class_to_idx = {breed: i for i, breed in enumerate(breed_cols)}\n    num_classes = len(breed_cols)\n    print(f\"Number of classes (breeds): {num_classes}\")\n\n    # Prepare training ids and labels (use mapping defined above)\n    train_ids = labels_df[\"id\"].astype(str).values\n    train_breeds = labels_df[\"breed\"].astype(str).values\n    # Map breeds to indices using sample submission order to ensure alignment\n    try:\n        y = np.array([class_to_idx[b] for b in train_breeds], dtype=np.int32)\n    except KeyError as e:\n        missing = e.args[0]\n        raise RuntimeError(\n            f\"Breed label in labels.csv not found in sample submission columns: {missing}\"\n        )\n\n    test_ids = sample_sub[\"id\"].astype(str).values\n\n    # Extract features\n    print(\"Extracting training features...\")\n    train_ids_list, train_feats = extract_handcrafted_features(train_ids, TRAIN_DIR)\n    print(\"Extracting test features...\")\n    test_ids_list, test_feats = extract_handcrafted_features(test_ids, TEST_DIR)\n\n    # Ensure order matches original arrays; if not, re-order to match original id arrays\n    if list(train_ids) != train_ids_list:\n        idx_map = {id_: i for i, id_ in enumerate(train_ids_list)}\n        order = [idx_map[id_] for id_ in train_ids]\n        train_feats = train_feats[order]\n    if list(test_ids) != test_ids_list:\n        idx_map = {id_: i for i, id_ in enumerate(test_ids_list)}\n        order = [idx_map[id_] for id_ in test_ids]\n        test_feats = test_feats[order]\n\n    print(\n        f\"Train features shape: {train_feats.shape}, Test features shape: {test_feats.shape}\"\n    )\n\n    # Scale + PCA\n    scaler = StandardScaler()\n    train_scaled = scaler.fit_transform(train_feats)\n    test_scaled = scaler.transform(test_feats)\n\n    n_comp = min(PCA_COMPONENTS, train_scaled.shape[1])\n    if n_comp < 1:\n        n_comp = train_scaled.shape[1]\n    pca = PCA(n_components=n_comp, random_state=SEED)\n    train_pca = pca.fit_transform(train_scaled)\n    test_pca = pca.transform(test_scaled)\n    print(f\"PCA reduced dimension to: {n_comp}\")\n\n    # Hold-out split for quick validation (single fold)\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=SEED)\n    hold_train_idx, hold_val_idx = next(sss.split(train_pca, y))\n    X_hold_tr, X_hold_val = train_pca[hold_train_idx], train_pca[hold_val_idx]\n    y_hold_tr, y_hold_val = y[hold_train_idx], y[hold_val_idx]\n\n    # Train a simple sklearn multinomial logistic regression on hold-out training data\n    print(\"Training LogisticRegression on hold-out split for quick check...\")\n    try:\n        clf_hold = LogisticRegression(\n            multi_class=\"multinomial\",\n            solver=\"lbfgs\",\n            max_iter=1000,\n            random_state=SEED,\n        )\n        clf_hold.fit(X_hold_tr, y_hold_tr)\n        hold_pred = clf_hold.predict_proba(X_hold_val)\n        hold_pred = safe_normalize_rows(hold_pred)\n        hold_loss = log_loss(y_hold_val, hold_pred)\n        print(f\"Hold-out Multi-class Log Loss: {hold_loss:.6f}\")\n    except Exception as e:\n        print(\n            \"Hold-out training failed, falling back to uniform probabilities. Error:\", e\n        )\n        hold_pred = (\n            np.ones((X_hold_val.shape[0], num_classes), dtype=np.float64) / num_classes\n        )\n        hold_loss = log_loss(y_hold_val, hold_pred)\n        print(f\"Hold-out Multi-class Log Loss (uniform): {hold_loss:.6f}\")\n\n    # 5-fold CV and full test prediction using LogisticRegression\n    print(\"\\nStarting 5-fold CV with LogisticRegression...\")\n    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n    oof_preds = np.zeros((train_pca.shape[0], num_classes), dtype=np.float64)\n    test_preds = np.zeros((test_pca.shape[0], num_classes), dtype=np.float64)\n\n    for fold, (tr_idx, val_idx) in enumerate(skf.split(train_pca, y)):\n        print(f\"\\nFold {fold + 1}/{N_FOLDS}\")\n        X_tr, X_val = train_pca[tr_idx], train_pca[val_idx]\n        y_tr, y_val = y[tr_idx], y[val_idx]\n\n        try:\n            clf = LogisticRegression(\n                multi_class=\"multinomial\",\n                solver=\"lbfgs\",\n                max_iter=1000,\n                random_state=SEED + fold,\n            )\n            clf.fit(X_tr, y_tr)\n            val_pred = clf.predict_proba(X_val)\n            val_pred = safe_normalize_rows(val_pred)\n            oof_preds[val_idx] = val_pred\n            test_fold_pred = clf.predict_proba(test_pca)\n            test_preds += test_fold_pred / N_FOLDS\n            fold_loss = log_loss(y_val, val_pred)\n            print(f\"Fold {fold + 1} log_loss: {fold_loss:.6f}\")\n        except Exception as e:\n            print(\n                f\"Fold {fold + 1} training failed, using uniform probabilities. Error: {e}\"\n            )\n            val_pred = (\n                np.ones((X_val.shape[0], num_classes), dtype=np.float64) / num_classes\n            )\n            oof_preds[val_idx] = val_pred\n            test_preds += np.ones(\n                (test_pca.shape[0], num_classes), dtype=np.float64\n            ) / (N_FOLDS * num_classes)\n\n    # CV metric\n    oof_preds = safe_normalize_rows(oof_preds)\n    try:\n        cv_loss = log_loss(y, oof_preds)\n        print(f\"\\nCV Multi-class Log Loss (OOF): {cv_loss:.6f}\")\n    except Exception as e:\n        print(\"Could not compute CV log loss:\", e)\n        cv_loss = None\n\n    # Ensure test predictions are proper probabilities\n    test_preds = safe_normalize_rows(test_preds)\n\n    # Build submission DataFrame using sample submission ordering (breed_cols)\n    sub_df = pd.DataFrame(test_preds, columns=breed_cols)\n    sub_df.insert(0, \"id\", test_ids)\n    # Ensure exact column order as sample submission\n    sub_df = sub_df[sample_sub.columns]\n\n    # Save submission\n    sub_df.to_csv(SUBMISSION_FILE, index=False)\n    print(f\"\\nSubmission saved to {SUBMISSION_FILE}\")\n\n    t_end = time.time()\n    print(f\"Total time elapsed: {(t_end - t0)/60:.2f} minutes\")\n\n\nif __name__ == \"__main__\":\n    main()\n"], "term_out": ["<OMITTED>", "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/nonroot/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n\r  0%|          | 0.00/97.8M [00:00<?, ?B/s]\r 23%|##3       | 22.7M/97.8M [00:00<00:00, 238MB/s]\r 47%|####7     | 46.4M/97.8M [00:00<00:00, 244MB/s]\r 72%|#######1  | 70.0M/97.8M [00:00<00:00, 246MB/s]\r 96%|#########5| 93.7M/97.8M [00:00<00:00, 247MB/s]\r100%|##########| 97.8M/97.8M [00:00<00:00, 244MB/s]\n\rExtracting features from ./input/train:   0%|          | 0/288 [00:00<?, ?it/s]\rExtracting features from ./input/train:   0%|          | 1/288 [00:01<05:01,  1.05s/it]\rExtracting features from ./input/train:   1%|          | 2/288 [00:02<04:56,  1.04s/it]\rExtracting features from ./input/train:   1%|1         | 3/288 [00:03<04:56,  1.04s/it]\rExtracting features from ./input/train:   1%|1         | 4/288 [00:04<04:54,  1.04s/it]\rExtracting features from ./input/train:   2%|1         | 5/288 [00:05<04:56,  1.05s/it]\rExtracting features from ./input/train:   2%|2         | 6/288 [00:06<04:56,  1.05s/it]\rExtracting features from ./input/train:   2%|2         | 7/288 [00:07<04:54,  1.05s/it]\rExtracting features from ./input/train:   3%|2         | 8/288 [00:08<04:56,  1.06s/it]\rExtracting features from ./input/train:   3%|3         | 9/288 [00:09<04:54,  1.06s/it]\rExtracting features from ./input/train:   3%|3         | 10/288 [00:10<04:58,  1.07s/it]\rExtracting features from ./input/train:   4%|3         | 11/288 [00:11<04:56,  1.07s/it]\rExtracting features from ./input/train:   4%|4         | 12/288 [00:12<04:57,  1.08s/it]\rExtracting features from ./input/train:   5%|4         | 13/288 [00:13<04:55,  1.08s/it]\rExtracting features from ./input/train:   5%|4         | 14/288 [00:14<04:54,  1.07s/it]\rExtracting features from ./input/train:   5%|5         | 15/288 [00:15<04:54,  1.08s/it]\rExtracting features from ./input/train:   6%|5         | 16/288 [00:17<04:52,  1.08s/it]\rExtracting features from ./input/train:   6%|5         | 17/288 [00:18<04:54,  1.09s/it]\rExtracting features from ./input/train:   6%|6         | 18/288 [00:19<04:51,  1.08s/it]\rExtracting features from ./input/train:   7%|6         | 19/288 [00:20<04:47,  1.07s/it]\rExtracting features from ./input/train:   7%|6         | 20/288 [00:21<04:38,  1.04s/it]\rExtracting features from ./input/train:   7%|7         | 21/288 [00:22<04:34,  1.03s/it]\rExtracting features from ./input/train:   8%|7         | 22/288 [00:23<04:30,  1.02s/it]\rExtracting features from\n ... [24706 characters truncated] ... \n features from ./input/test:  28%|##8       | 9/32 [00:07<00:20,  1.13it/s]\rExtracting features from ./input/test:  31%|###1      | 10/32 [00:08<00:19,  1.13it/s]\rExtracting features from ./input/test:  34%|###4      | 11/32 [00:09<00:18,  1.13it/s]\rExtracting features from ./input/test:  38%|###7      | 12/32 [00:10<00:17,  1.13it/s]\rExtracting features from ./input/test:  41%|####      | 13/32 [00:11<00:16,  1.13it/s]\rExtracting features from ./input/test:  44%|####3     | 14/32 [00:12<00:15,  1.13it/s]\rExtracting features from ./input/test:  47%|####6     | 15/32 [00:13<00:15,  1.13it/s]\rExtracting features from ./input/test:  50%|#####     | 16/32 [00:14<00:14,  1.13it/s]\rExtracting features from ./input/test:  53%|#####3    | 17/32 [00:15<00:13,  1.13it/s]\rExtracting features from ./input/test:  56%|#####6    | 18/32 [00:15<00:12,  1.13it/s]\rExtracting features from ./input/test:  59%|#####9    | 19/32 [00:16<00:11,  1.13it/s]\rExtracting features from ./input/test:  62%|######2   | 20/32 [00:17<00:10,  1.13it/s]\rExtracting features from ./input/test:  66%|######5   | 21/32 [00:18<00:09,  1.12it/s]\rExtracting features from ./input/test:  69%|######8   | 22/32 [00:19<00:08,  1.12it/s]\rExtracting features from ./input/test:  72%|#######1  | 23/32 [00:20<00:08,  1.12it/s]\rExtracting features from ./input/test:  75%|#######5  | 24/32 [00:21<00:07,  1.10it/s]\rExtracting features from ./input/test:  78%|#######8  | 25/32 [00:22<00:06,  1.10it/s]\rExtracting features from ./input/test:  81%|########1 | 26/32 [00:23<00:05,  1.11it/s]\rExtracting features from ./input/test:  84%|########4 | 27/32 [00:24<00:04,  1.09it/s]\rExtracting features from ./input/test:  88%|########7 | 28/32 [00:24<00:03,  1.10it/s]\rExtracting features from ./input/test:  91%|######### | 29/32 [00:25<00:02,  1.11it/s]\rExtracting features from ./input/test:  94%|#########3| 30/32 [00:26<00:01,  1.12it/s]\rExtracting features from ./input/test:  97%|#########6| 31/32 [00:27<00:00,  1.12it/s]\rExtracting features from ./input/test: 100%|##########| 32/32 [00:28<00:00,  1.12it/s]\rExtracting features from ./input/test: 100%|##########| 32/32 [00:28<00:00,  1.12it/s]\nFeature shapes: (9199, 2048) (1023, 2048) Num classes: 120\nTraining fold 1/5...\nTraceback (most recent call last):\n  File \"runfile.py\", line 195, in <module>\n    bst = lgb.train(\n          ^^^^^^^^^^\nTypeError: train() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: 5 minutes seconds (time limit is 9 hours).", "<OMITTED>", "Execution time: 2 seconds seconds (time limit is 9 hours).", "Execution time: 2 seconds seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Loading cached features from ./working/train_feats.npz\nLoading cached features from ./working/test_feats.npz\nFeature shapes: (9199, 2048) (1023, 2048) Num classes: 120\nTraining fold 1/5...\nTraceback (most recent call last):\n  File \"runfile.py\", line 180, in <module>\n    clf.fit(\nTypeError: LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds'\nExecution time: 3 seconds seconds (time limit is 9 hours).", "Loading train images...\nLoaded 1000/9199 images from ./input/train\nLoaded 2000/9199 images from ./input/train\nLoaded 3000/9199 images from ./input/train\nLoaded 4000/9199 images from ./input/train\nLoaded 5000/9199 images from ./input/train\nLoaded 6000/9199 images from ./input/train\nLoaded 7000/9199 images from ./input/train\nLoaded 8000/9199 images from ./input/train\nLoaded 9000/9199 images from ./input/train\nLoaded 9199/9199 images from ./input/train\nTrain images loaded.\nLoading test images...\nLoaded 1000/1023 images from ./input/test\nLoaded 1023/1023 images from ./input/test\nTest images loaded.\n\nStarting fold 1/5\nFitting PCA...\nTraining LightGBM...\nLightGBM training failed; falling back to LogisticRegression. Error:\nTraceback (most recent call last):\n  File \"runfile.py\", line 151, in <module>\n    bst = lgb.train(\n          ^^^^^^^^^^\nTypeError: train() got an unexpected keyword argument 'early_stopping_rounds'\nTraining LogisticRegression (multinomial)...\nFold 1 log_loss: 8.271677\n\nStarting fold 2/5\nFitting PCA...\nTraining LogisticRegression (multinomial)...\nFold 2 log_loss: 7.801908\n\nStarting fold 3/5\nFitting PCA...\nTraining LogisticRegression (multinomial)...\nFold 3 log_loss: 8.514272\n\nStarting fold 4/5\nFitting PCA...\nTraining LogisticRegression (multinomial)...\nFold 4 log_loss: 8.452335\n\nStarting fold 5/5\nFitting PCA...\nTraining LogisticRegression (multinomial)...\nFold 5 log_loss: 8.470074\n\nCV mean multi-class log-loss over 5 folds: 8.302053\nSubmission saved to ./submission/submission.csv\nMean CV log-loss: 8.302053\nExecution time: an hour seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours)."], "analysis": ["The script successfully loaded all train (9199) and test (1023) images and began cross-validation, but crashed during LightGBM training with: TypeError: train() got an unexpected keyword argument 'early_stopping_rounds'. Because of this error the run terminated and no submission.csv was produced.", "Script downloaded ResNet50 and successfully extracted features (train shape (9199,2048), test shape (1023,2048)), but crashed during LightGBM training. The error was TypeError: train() got an unexpected keyword argument 'early_stopping_rounds', so training stopped on fold 1 and no submission.csv was produced.", "The training and inference pipeline ran to completion without errors. Features were extracted with a pretrained ResNet-18 and a multinomial Logistic Regression was trained with 5-fold stratified CV; a submission CSV was written to ./submission/submission.csv.", "The run terminated almost immediately (2s) and produced no logs from the training script (no feature extraction, CV, or submission-save messages). As a result, no submission.csv was produced and no validation metric is available. The execution appears to have failed or been interrupted before completing.", "The execution output only contains a single line indicating total execution time and no pipeline logs. The script did not run the training/feature-extraction steps and no submission.csv was produced, so no validation metric is available.", "The execution produced no training logs or final output; there is no evidence submission.csv was written. Likely the script failed early (e.g. could not find input files), so no metric was produced.", "The run produced no detailed logs and only a single execution-time line was returned; there is no evidence that the training loop completed or that a submission file was written. Therefore the run likely failed or did not execute as intended and no metric can be reported.", "The run loaded cached ResNet50 features for train/test (shapes (9199,2048) and (1023,2048)) but crashed during the first CV fold. Training failed with a TypeError: LGBMClassifier.fit() got an unexpected keyword argument 'early_stopping_rounds', so no submission was produced.", "LightGBM failed during training due to an unexpected keyword argument 'early_stopping_rounds', so the script fell back to multinomial LogisticRegression and completed. The pipeline loaded 9,199 training images and 1,023 test images, ran 5-fold CV, and produced a submission file.", "The script appears to have a bug: it expects input files under an \"input/\" directory while the dataset is provided under \"data/\", so reading labels/images likely failed and the script did not produce a valid submission. No evaluation metric is available because the run did not complete successfully and submission.csv was not created.", "The script ran feature extraction, reduced features with PCA, trained a LightGBM with 5-fold CV, and wrote a submission file. The provided execution log shows no error messages but does not include numeric training/validation logs, so no metric value is available from the output.", "The script executed and (as intended) extracted handcrafted image features, reduced them with PCA, trained multinomial LogisticRegression with 5-fold CV, and saved a submission file. The provided execution log did not include numeric training/validation outputs, so no specific metric value is available from the output shown."], "exp_name": "exp", "metrics": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
