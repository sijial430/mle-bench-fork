
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Journal Visualization - Smartphone Decimeter 2022</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            display: flex;
            height: 100vh;
            overflow: hidden;
            background: #1e1e1e;
            color: #d4d4d4;
        }

        .container {
            display: flex;
            width: 100%;
            height: 100%;
        }

        .tree-panel {
            width: 350px;
            background: #252526;
            border-right: 1px solid #3e3e42;
            overflow-y: auto;
            padding: 20px;
        }

        .tree-panel h2 {
            color: #569cd6;
            margin-bottom: 15px;
            font-size: 18px;
        }

        .tree-node {
            margin-left: 20px;
            margin-top: 8px;
        }

        .tree-node.root {
            margin-left: 0;
        }

        .node-item {
            padding: 8px 12px;
            cursor: pointer;
            border-radius: 4px;
            margin-bottom: 4px;
            border-left: 3px solid transparent;
            transition: all 0.2s;
        }

        .node-item:hover {
            background: #2a2d2e;
        }

        .node-item.active {
            background: #094771;
            border-left-color: #0e639c;
        }

        .node-item.buggy {
            border-left-color: #f48771;
            background: #3a2a2a;
        }

        .node-item.buggy.active {
            background: #5a1a1a;
        }

        .node-label {
            font-weight: 500;
            color: #d4d4d4;
        }

        .node-meta {
            font-size: 12px;
            color: #858585;
            margin-top: 4px;
        }

        .content-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            overflow: hidden;
        }

        .competition-banner {
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            padding: 20px 30px;
            border-bottom: 2px solid #569cd6;
        }

        .competition-title {
            font-size: 24px;
            color: #ffffff;
            font-weight: 600;
            margin-bottom: 8px;
        }

        .competition-meta {
            display: flex;
            gap: 20px;
            font-size: 13px;
            color: #b0c4de;
        }

        .competition-meta-item {
            display: flex;
            align-items: center;
            gap: 6px;
        }

        .stats-panel {
            background: #2d2d30;
            padding: 20px 30px;
            border-bottom: 1px solid #3e3e42;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin-top: 10px;
        }

        .stat-card {
            background: #1e1e1e;
            border: 1px solid #3e3e42;
            border-radius: 6px;
            padding: 15px;
            border-left: 3px solid #569cd6;
        }

        .stat-card.valid {
            border-left-color: #4ec9b0;
        }

        .stat-card.buggy {
            border-left-color: #f48771;
        }

        .stat-card.recovery {
            border-left-color: #ffd700;
        }

        .stat-label {
            font-size: 12px;
            color: #858585;
            text-transform: uppercase;
            margin-bottom: 8px;
            letter-spacing: 0.5px;
        }

        .stat-value {
            font-size: 28px;
            color: #d4d4d4;
            font-weight: 600;
            margin-bottom: 4px;
        }

        .stat-subtext {
            font-size: 13px;
            color: #858585;
        }

        .header {
            background: #2d2d30;
            padding: 15px 20px;
            border-bottom: 1px solid #3e3e42;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .header h1 {
            font-size: 18px;
            color: #d4d4d4;
        }

        .navigation {
            display: flex;
            gap: 10px;
        }

        .nav-btn {
            background: #0e639c;
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: background 0.2s;
        }

        .nav-btn:hover {
            background: #1177bb;
        }

        .nav-btn:disabled {
            background: #3e3e42;
            cursor: not-allowed;
        }

        .main-content {
            flex: 1;
            overflow-y: auto;
            padding: 20px;
        }

        .section {
            background: #252526;
            border: 1px solid #3e3e42;
            border-radius: 6px;
            padding: 20px;
            margin-bottom: 20px;
        }

        .section h3 {
            color: #4ec9b0;
            margin-bottom: 15px;
            font-size: 16px;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .section h3.collapsible {
            cursor: pointer;
            user-select: none;
            transition: color 0.2s;
        }

        .section h3.collapsible:hover {
            color: #6ed9c0;
        }

        .section h3.collapsible::before {
            content: '‚ñº';
            font-size: 12px;
            transition: transform 0.2s;
            display: inline-block;
            margin-right: 5px;
        }

        .section h3.collapsible.collapsed::before {
            transform: rotate(-90deg);
        }

        .collapsible-content {
            max-height: 10000px;
            overflow: visible;
            transition: max-height 0.3s ease-out, opacity 0.3s ease-out;
            opacity: 1;
        }

        .collapsible-content.collapsed {
            max-height: 0;
            opacity: 0;
            overflow: hidden;
        }

        .badge {
            display: inline-block;
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 12px;
            font-weight: bold;
        }

        .badge.buggy {
            background: #f48771;
            color: #1e1e1e;
        }

        .badge.valid {
            background: #4ec9b0;
            color: #1e1e1e;
        }

        .metric-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin-top: 10px;
        }

        .metric-item {
            background: #1e1e1e;
            padding: 12px;
            border-radius: 4px;
            border-left: 3px solid #569cd6;
        }

        .metric-label {
            font-size: 12px;
            color: #858585;
            text-transform: uppercase;
            margin-bottom: 4px;
        }

        .metric-value {
            font-size: 18px;
            color: #d4d4d4;
            font-weight: 500;
        }

        pre {
            background: #1e1e1e;
            padding: 15px;
            border-radius: 4px;
            overflow-x: auto;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 13px;
            line-height: 1.6;
            border: 1px solid #3e3e42;
        }

        .plan-box {
            background: #1e1e1e;
            padding: 15px;
            border-radius: 4px;
            border: 1px solid #3e3e42;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            font-size: 14px;
            line-height: 1.6;
            color: #d4d4d4;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: none;
            overflow-y: visible;
        }

        .analysis-box {
            background: #1e1e1e;
            padding: 15px;
            border-radius: 4px;
            border: 1px solid #569cd6;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            font-size: 14px;
            line-height: 1.6;
            color: #d4d4d4;
            white-space: pre-wrap;
            word-wrap: break-word;
        }

        .diff-container {
            margin-top: 15px;
        }

        .diff-table {
            width: 100%;
            border-collapse: collapse;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 13px;
            background: #1e1e1e;
            border: 1px solid #3e3e42;
            table-layout: fixed;
        }

        .diff-table td {
            padding: 2px 8px;
            vertical-align: top;
            white-space: pre-wrap;
            word-wrap: break-word;
            width: 50%;
            border-right: 1px solid #3e3e42;
        }

        .diff-table td:last-child {
            border-right: none;
        }

        .diff-table .line-number {
            width: 40px;
            text-align: right;
            color: #858585;
            user-select: none;
            padding-right: 8px;
            border-right: 1px solid #3e3e42;
        }

        .diff-table .diff_add {
            background: #1a4d1a;
        }

        .diff-table .diff_sub {
            background: #4d1a1a;
        }

        .diff-table .diff_chg {
            background: #4d4d1a;
        }

        .diff-table .diff_none {
            background: #1e1e1e;
        }

        .diff-header {
            background: #2d2d30;
            color: #d4d4d4;
            font-weight: bold;
            padding: 8px;
            text-align: center;
            border-bottom: 2px solid #3e3e42;
        }

        .similarity-bar {
            width: 100%;
            height: 8px;
            background: #3e3e42;
            border-radius: 4px;
            overflow: hidden;
            margin-top: 8px;
        }

        .similarity-fill {
            height: 100%;
            background: linear-gradient(90deg, #f48771 0%, #ffd700 50%, #4ec9b0 100%);
            transition: width 0.3s;
        }

        .error-box {
            background: #4d1a1a;
            border: 1px solid #f48771;
            border-radius: 4px;
            padding: 15px;
            margin-top: 10px;
        }

        .error-type {
            color: #f48771;
            font-weight: bold;
            margin-bottom: 8px;
        }

        .error-message {
            color: #d4d4d4;
            font-family: 'Consolas', monospace;
            font-size: 13px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="tree-panel">
            <h2>üìä Journal Steps</h2>
            <div id="tree-container"></div>
        </div>

        <div class="content-panel">
            <div class="competition-banner">
                <div class="competition-title">Smartphone Decimeter 2022</div>
                <div class="competition-meta">
                    <div class="competition-meta-item">
                        <span>ü§ñ Agent:</span>
                        <strong>aide</strong>
                    </div>
                    <div class="competition-meta-item">
                        <span>‚è±Ô∏è Run:</span>
                        <strong>2025-12-18T04-26-26-GMT</strong>
                    </div>
                    <div class="competition-meta-item">
                        <span>üìÅ ID:</span>
                        <strong>smartphone-decimeter-2022_d59f...</strong>
                    </div>
                </div>
            </div>

            <div class="stats-panel">
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="stat-label">Total Steps</div>
                        <div class="stat-value">20</div>
                        <div class="stat-subtext">All execution attempts</div>
                    </div>

                    <div class="stat-card valid">
                        <div class="stat-label">Valid Steps</div>
                        <div class="stat-value">3</div>
                        <div class="stat-subtext">15.0% of total</div>
                    </div>

                    <div class="stat-card buggy">
                        <div class="stat-label">Buggy Steps</div>
                        <div class="stat-value">17</div>
                        <div class="stat-subtext">85.0% of total</div>
                    </div>

                    <div class="stat-card recovery">
                        <div class="stat-label">Avg Steps to Recovery</div>
                        <div class="stat-value">0.0</div>
                        <div class="stat-subtext">After encountering bugs</div>
                    </div>

                    <div class="stat-card buggy">
                        <div class="stat-label">Failed Debugging</div>
                        <div class="stat-value">17</div>
                        <div class="stat-subtext">100.0% never recovered</div>
                    </div>

                    <div class="stat-card recovery">
                        <div class="stat-label">Successful Debugging</div>
                        <div class="stat-value">0</div>
                        <div class="stat-subtext">Bugs that were fixed</div>
                    </div>
                </div>
            </div>

            <div class="header">
                <h1 id="step-title">Select a step to view details</h1>
                <div class="navigation">
                    <button class="nav-btn" id="prev-btn" onclick="navigatePrev()">‚¨Ü Previous (‚Üë)</button>
                    <button class="nav-btn" id="next-btn" onclick="navigateNext()">‚¨á Next (‚Üì)</button>
                </div>
            </div>

            <div class="main-content" id="main-content">
                <p style="color: #858585; text-align: center; margin-top: 50px;">
                    Select a step from the tree on the left or use arrow keys to navigate
                </p>
            </div>
        </div>
    </div>

    <script>
        const stepsData = [
        {
                "index": 0,
                "step": 0,
                "id": "f6637b88f52440e391cd7f3ab211ab46",
                "parent_id": null,
                "plan": "I will build a simple per-epoch feature dataset by aggregating the raw device_gnss measurements (mean WLS ECEF position, mean Cn0, satellite count, mean elevation/azimuth, etc.), align those to ground-truth timestamps with a merge-asof, and train LightGBM regressors to predict Latitude and Longitude from those features. Evaluation will use the competition metric (for each phone compute per-second horizontal errors, take 50th and 95th percentiles, average them per phone, then mean across phones) computed on a held-out fold using GroupKFold by phone. Finally I'll apply the trained models to the test sample_submission times by nearest-epoch matching and save predictions to submission/submission.csv. The script prints the validation metric.",
                "code": "import os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\nfrom math import sin, cos, atan2, sqrt, radians\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # WGS84\n    a = 6378137.0\n    e = 8.1819190842622e-2\n    b = sqrt(a * a * (1 - e * e))\n    ep = sqrt((a * a - b * b) / (b * b))\n    p = sqrt(x * x + y * y)\n    th = atan2(a * z, b * p)\n    lon = atan2(y, x)\n    lat = atan2((z + ep * ep * b * (sin(th) ** 3)), (p - e * e * a * (cos(th) ** 3)))\n    N = a / sqrt(1 - e * e * (sin(lat) ** 2))\n    alt = p / cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), alt\n\n\n# haversine in meters\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n\n\ndef aggregate_device_gnss(fn):\n    df = pd.read_csv(fn)\n    # keep relevant columns if present\n    cols = []\n    for c in [\n        \"utcTimeMillis\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"Svid\",\n        \"SignalType\",\n    ]:\n        if c in df.columns:\n            cols.append(c)\n    df = df[cols]\n    # drop rows with null time\n    df = df[df[\"utcTimeMillis\"].notnull()]\n    # group by epoch (utcTimeMillis)\n    agg = (\n        df.groupby(\"utcTimeMillis\")\n        .agg(\n            WlsX=(\"WlsPositionXEcefMeters\", \"first\"),\n            WlsY=(\"WlsPositionYEcefMeters\", \"first\"),\n            WlsZ=(\"WlsPositionZEcefMeters\", \"first\"),\n            cn0_mean=(\n                (\"Cn0DbHz\", \"mean\")\n                if \"Cn0DbHz\" in df.columns\n                else (\"WlsPositionXEcefMeters\", \"size\")\n            ),\n            elev_mean=(\n                (\"SvElevationDegrees\", \"mean\")\n                if \"SvElevationDegrees\" in df.columns\n                else (\"WlsPositionXEcefMeters\", \"size\")\n            ),\n            azim_mean=(\n                (\"SvAzimuthDegrees\", \"mean\")\n                if \"SvAzimuthDegrees\" in df.columns\n                else (\"WlsPositionXEcefMeters\", \"size\")\n            ),\n            pr_mean=(\n                (\"PseudorangeRateMetersPerSecond\", \"mean\")\n                if \"PseudorangeRateMetersPerSecond\" in df.columns\n                else (\"WlsPositionXEcefMeters\", \"size\")\n            ),\n            sv_count=(\n                (\"Svid\", \"nunique\")\n                if \"Svid\" in df.columns\n                else (\"WlsPositionXEcefMeters\", \"size\")\n            ),\n        )\n        .reset_index()\n    )\n    # convert ECEF to lat/lon\n    # might have NaNs; fill small\n    for c in [\"WlsX\", \"WlsY\", \"WlsZ\"]:\n        if c in agg.columns:\n            agg[c] = agg[c].fillna(0.0)\n    latlon = agg.apply(\n        lambda r: ecef_to_geodetic(r[\"WlsX\"], r[\"WlsY\"], r[\"WlsZ\"]), axis=1\n    )\n    lat = [t[0] for t in latlon]\n    lon = [t[1] for t in latlon]\n    agg[\"wls_lat\"] = lat\n    agg[\"wls_lon\"] = lon\n    return agg\n\n\n# Build training dataset\ntrain_root = os.path.join(INPUT_DIR, \"train\")\ntrain_dirs = glob.glob(os.path.join(train_root, \"*\", \"*\"))\ntrain_rows = []\nfor drive_phone in train_dirs:\n    parts = drive_phone.split(os.sep)\n    # path like input/train/<drive_id>/<phone_name>\n    if len(parts) < 4:\n        continue\n    drive = parts[-2]\n    phone = parts[-1]\n    phone_full = f\"{drive}_{phone}\"\n    device_gnss_path = os.path.join(drive_phone, \"device_gnss.csv\")\n    ground_truth_path = os.path.join(drive_phone, \"ground_truth.csv\")\n    if not os.path.exists(device_gnss_path) or not os.path.exists(ground_truth_path):\n        continue\n    try:\n        agg = aggregate_device_gnss(device_gnss_path)\n        gt = pd.read_csv(ground_truth_path)\n        gt = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        gt = gt.rename(\n            columns={\n                \"UnixTimeMillis\": \"utcTimeMillis\",\n                \"LatitudeDegrees\": \"lat\",\n                \"LongitudeDegrees\": \"lon\",\n            }\n        )\n        # merge_asof by time\n        agg_sorted = agg.sort_values(\"utcTimeMillis\")\n        gt_sorted = gt.sort_values(\"utcTimeMillis\")\n        merged = pd.merge_asof(\n            agg_sorted,\n            gt_sorted,\n            on=\"utcTimeMillis\",\n            direction=\"nearest\",\n            tolerance=1000,\n        )  # within 1s\n        merged = merged.dropna(subset=[\"lat\", \"lon\"])\n        merged[\"phone\"] = phone_full\n        train_rows.append(merged)\n    except Exception as e:\n        print(\"Error processing\", drive_phone, e)\n\nif len(train_rows) == 0:\n    raise RuntimeError(\"No training data found!\")\n\ntrain_df = pd.concat(train_rows, ignore_index=True)\n# features\nfeature_cols = [\n    \"WlsX\",\n    \"WlsY\",\n    \"WlsZ\",\n    \"cn0_mean\",\n    \"elev_mean\",\n    \"azim_mean\",\n    \"pr_mean\",\n    \"sv_count\",\n    \"wls_lat\",\n    \"wls_lon\",\n]\n# Fill missing features\nfor c in feature_cols:\n    if c not in train_df.columns:\n        train_df[c] = 0.0\ntrain_df[feature_cols] = train_df[feature_cols].fillna(0.0)\n\nX = train_df[feature_cols].values\ny_lat = train_df[\"lat\"].values\ny_lon = train_df[\"lon\"].values\ngroups = train_df[\"phone\"].values\n\n# 5-fold GroupKFold\ngkf = GroupKFold(n_splits=5)\nlat_models = []\nlon_models = []\nval_preds_lat = np.zeros(len(train_df))\nval_preds_lon = np.zeros(len(train_df))\n\nprint(\"Training LightGBM models...\")\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    ytr_lat, yval_lat = y_lat[tr_idx], y_lat[val_idx]\n    ytr_lon, yval_lon = y_lon[tr_idx], y_lon[val_idx]\n    dtrain_lat = lgb.Dataset(X_tr, label=ytr_lat)\n    dval_lat = lgb.Dataset(X_val, label=yval_lat, reference=dtrain_lat)\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"l2\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"seed\": 42 + fold,\n    }\n    bst_lat = lgb.train(\n        params,\n        dtrain_lat,\n        valid_sets=[dtrain_lat, dval_lat],\n        num_boost_round=300,\n        early_stopping_rounds=30,\n        verbose_eval=False,\n    )\n    pred_val_lat = bst_lat.predict(X_val, num_iteration=bst_lat.best_iteration)\n    val_preds_lat[val_idx] = pred_val_lat\n    lat_models.append(bst_lat)\n    # longitude\n    dtrain_lon = lgb.Dataset(X_tr, label=ytr_lon)\n    dval_lon = lgb.Dataset(X_val, label=yval_lon, reference=dtrain_lon)\n    bst_lon = lgb.train(\n        params,\n        dtrain_lon,\n        valid_sets=[dtrain_lon, dval_lon],\n        num_boost_round=300,\n        early_stopping_rounds=30,\n        verbose_eval=False,\n    )\n    pred_val_lon = bst_lon.predict(X_val, num_iteration=bst_lon.best_iteration)\n    val_preds_lon[val_idx] = pred_val_lon\n    lon_models.append(bst_lon)\n    print(f\"Fold {fold+1} done.\")\n\n# Evaluate with competition metric on full train (using out-of-fold preds)\ntrain_df[\"pred_lat\"] = val_preds_lat\ntrain_df[\"pred_lon\"] = val_preds_lon\n\n# compute per-phone errors list\nphones = train_df[\"phone\"].unique()\nphone_scores = []\nfor p in phones:\n    sub = train_df[train_df[\"phone\"] == p]\n    dists = haversine(\n        sub[\"lat\"].values,\n        sub[\"lon\"].values,\n        sub[\"pred_lat\"].values,\n        sub[\"pred_lon\"].values,\n    )\n    if len(dists) == 0:\n        continue\n    p50 = np.percentile(dists, 50)\n    p95 = np.percentile(dists, 95)\n    phone_scores.append((p50 + p95) / 2.0)\nif len(phone_scores) == 0:\n    mean_score = np.nan\nelse:\n    mean_score = np.mean(phone_scores)\nprint(\n    \"Validation competition metric (mean of per-phone (p50+p95)/2): {:.3f} meters\".format(\n        mean_score\n    )\n)\n\n# Now prepare test predictions\nsample_sub = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))\n# Build aggregated test features for all test drives/phones\ntest_root = os.path.join(INPUT_DIR, \"test\")\ntest_dirs = glob.glob(os.path.join(test_root, \"*\", \"*\"))\ntest_agg_rows = []\nfor drive_phone in test_dirs:\n    parts = drive_phone.split(os.sep)\n    if len(parts) < 4:\n        continue\n    drive = parts[-2]\n    phone = parts[-1]\n    phone_full = f\"{drive}_{phone}\"\n    device_gnss_path = os.path.join(drive_phone, \"device_gnss.csv\")\n    if not os.path.exists(device_gnss_path):\n        continue\n    try:\n        agg = aggregate_device_gnss(device_gnss_path)\n        agg[\"phone\"] = phone_full\n        test_agg_rows.append(agg)\n    except Exception as e:\n        print(\"Error aggregating test\", drive_phone, e)\nif len(test_agg_rows) == 0:\n    raise RuntimeError(\"No test data aggregated!\")\ntest_agg = pd.concat(test_agg_rows, ignore_index=True)\n# fill feature columns if missing\nfor c in feature_cols:\n    if c not in test_agg.columns:\n        test_agg[c] = 0.0\ntest_agg[feature_cols] = test_agg[feature_cols].fillna(0.0)\n\n# merge sample_sub with test_agg by phone and nearest time\nsample = sample_sub.copy()\nsample = sample.rename(columns={\"phone\": \"phone\", \"UnixTimeMillis\": \"utcTimeMillis\"})\n# merge per phone: do groupwise merge_asof\nsample_preds = []\nfor phone, grp in sample.groupby(\"phone\"):\n    t = grp.sort_values(\"utcTimeMillis\")\n    features = test_agg[test_agg[\"phone\"] == phone].sort_values(\"utcTimeMillis\")\n    if len(features) == 0:\n        # no features for this phone; fill with nan\n        t[\"LatitudeDegrees\"] = np.nan\n        t[\"LongitudeDegrees\"] = np.nan\n        sample_preds.append(t)\n        continue\n    merged = pd.merge_asof(\n        t, features, on=\"utcTimeMillis\", by=\"phone\", direction=\"nearest\", tolerance=2000\n    )\n    # for rows where merge failed (NaNs), we will try nearest without phone key\n    missing = merged[\"WlsX\"].isnull()\n    if missing.any():\n        merged2 = pd.merge_asof(\n            t[missing].sort_values(\"utcTimeMillis\"),\n            features.sort_values(\"utcTimeMillis\"),\n            on=\"utcTimeMillis\",\n            direction=\"nearest\",\n            tolerance=2000,\n        )\n        merged.loc[missing, merged2.columns] = merged2.values\n    # predict using models: average predictions from fold models\n    X_test = merged[feature_cols].fillna(0.0).values\n    if X_test.shape[0] == 0:\n        merged[\"LatitudeDegrees\"] = np.nan\n        merged[\"LongitudeDegrees\"] = np.nan\n        sample_preds.append(merged)\n        continue\n    # ensemble over models (average)\n    lat_preds = np.mean(\n        [m.predict(X_test, num_iteration=m.best_iteration) for m in lat_models], axis=0\n    )\n    lon_preds = np.mean(\n        [m.predict(X_test, num_iteration=m.best_iteration) for m in lon_models], axis=0\n    )\n    merged[\"LatitudeDegrees\"] = lat_preds\n    merged[\"LongitudeDegrees\"] = lon_preds\n    # For any remaining NaNs, fallback to wls lat/lon\n    nanmask = merged[\"LatitudeDegrees\"].isnull()\n    if nanmask.any():\n        merged.loc[nanmask, \"LatitudeDegrees\"] = merged.loc[nanmask, \"wls_lat\"]\n        merged.loc[nanmask, \"LongitudeDegrees\"] = merged.loc[nanmask, \"wls_lon\"]\n    sample_preds.append(\n        merged[[\"phone\", \"utcTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]]\n    )\n\nsubmission_df = pd.concat(sample_preds, ignore_index=True)\nsubmission_df = submission_df.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n# ensure ordering same as sample_submission\nsubmission_df = (\n    submission_df.set_index([\"phone\", \"UnixTimeMillis\"])\n    .reindex(sample_sub.set_index([\"phone\", \"UnixTimeMillis\"]).index)\n    .reset_index()\n)\n# final fill: if still NaN, fill with global mean lat/lon from train\nglobal_lat_mean = train_df[\"lat\"].mean()\nglobal_lon_mean = train_df[\"lon\"].mean()\nsubmission_df[\"LatitudeDegrees\"] = submission_df[\"LatitudeDegrees\"].fillna(\n    global_lat_mean\n)\nsubmission_df[\"LongitudeDegrees\"] = submission_df[\"LongitudeDegrees\"].fillna(\n    global_lon_mean\n)\n\n# write submission\nout_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission_df.to_csv(\n    out_path,\n    index=False,\n    columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"],\n)\nprint(\"Saved submission to\", out_path)\nprint(\"Done.\")\n",
                "exec_time": 65.55103135108948,
                "metric": null,
                "is_buggy": true,
                "exc_type": "TypeError",
                "exc_value": null,
                "analysis": "Execution failed during LightGBM training with a TypeError: train() got an unexpected keyword argument 'early_stopping_rounds'. The run stopped before any models or a submission.csv were produced.",
                "alignment": {
                        "plan_code_similarity": 0.012723231394190236,
                        "has_changes": true,
                        "change_magnitude": 1.0
                }
        },
        {
                "index": 1,
                "step": 1,
                "id": "7f72d63aaa094ebab189d00846d9073a",
                "parent_id": null,
                "plan": "I will train a simple residual model that maps the phone's WLS ECEF-derived latitude/longitude to the true latitude/longitude. I aggregate GNSS device_gnss rows per epoch, convert WLS ECEF to lat/lon, merge with ground truth timestamps, and train a Ridge regressor (one for latitude and one for longitude) with 5-fold group CV by drive. The evaluation metric printed is the competition metric approximated on validation folds: average across phones of mean(50th percentile error, 95th percentile error). Finally I predict on the provided test sample_submission timestamps and save submission/submission.csv.",
                "code": "import os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom math import atan2, sqrt, sin, cos\nimport sys\n\n\n# Helpers\ndef ensure_dir(path):\n    os.makedirs(path, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # Bowring's method for WGS84\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = a * sqrt(1 - e2)\n    ep2 = (a**2 - b**2) / b**2\n    p = np.sqrt(x**2 + y**2)\n    # theta\n    theta = np.arctan2(z * a, p * b)\n    sin_t = np.sin(theta)\n    cos_t = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * sin_t**3, p - e2 * a * cos_t**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    # lat/lon in degrees -> meters\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * np.arcsin(np.sqrt(a))\n\n\ninput_dir = \"./input\"\ntrain_root = os.path.join(input_dir, \"train\")\ntest_root = os.path.join(input_dir, \"test\")\nsample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n\n# Gather training data\ntrain_device_paths = glob.glob(os.path.join(train_root, \"*\", \"*\", \"device_gnss.csv\"))\nrows = []\nprint(\"Collecting train data from device_gnss files...\", file=sys.stderr)\nfor dev_path in train_device_paths:\n    try:\n        # drive and phone\n        parts = dev_path.split(os.sep)\n        # input/train/<drive_id>/<phone>/device_gnss.csv\n        drive_id = parts[-3]\n        phone = parts[-2]\n        folder = os.path.dirname(dev_path)\n        gt_path = os.path.join(os.path.dirname(folder), \"ground_truth.csv\")\n        if not os.path.exists(gt_path):\n            # sometimes ground_truth is in same phone folder\n            gt_path = os.path.join(folder, \"ground_truth.csv\")\n        if not os.path.exists(gt_path):\n            # skip if missing\n            continue\n        # read device_gnss, read only needed cols to save memory\n        dg = pd.read_csv(\n            dev_path,\n            usecols=[\n                \"utcTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"Cn0DbHz\",\n            ],\n        )\n        # aggregate per epoch (utcTimeMillis) by mean\n        dg = (\n            dg.groupby(\"utcTimeMillis\", as_index=False)\n            .agg(\n                {\n                    \"WlsPositionXEcefMeters\": \"mean\",\n                    \"WlsPositionYEcefMeters\": \"mean\",\n                    \"WlsPositionZEcefMeters\": \"mean\",\n                    \"Cn0DbHz\": \"mean\",\n                }\n            )\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        )\n        gt = pd.read_csv(\n            gt_path, usecols=[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        )\n        # Merge by nearest timestamp using asof: need sorted\n        dg = dg.sort_values(\"UnixTimeMillis\")\n        gt = gt.sort_values(\"UnixTimeMillis\")\n        # merge_asof requires same dtypes\n        merged = pd.merge_asof(\n            dg, gt, on=\"UnixTimeMillis\", direction=\"nearest\", tolerance=500\n        )\n        merged = merged.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\"])\n        if merged.empty:\n            continue\n        # Convert WLS ECEF to lat/lon\n        x = merged[\"WlsPositionXEcefMeters\"].values\n        y = merged[\"WlsPositionYEcefMeters\"].values\n        z = merged[\"WlsPositionZEcefMeters\"].values\n        lat_wls, lon_wls = ecef_to_geodetic(x, y, z)\n        merged[\"wls_lat\"] = lat_wls\n        merged[\"wls_lon\"] = lon_wls\n        merged[\"drive_id\"] = drive_id\n        merged[\"phone\"] = phone\n        rows.append(\n            merged[\n                [\n                    \"drive_id\",\n                    \"phone\",\n                    \"UnixTimeMillis\",\n                    \"wls_lat\",\n                    \"wls_lon\",\n                    \"Cn0DbHz\",\n                    \"LatitudeDegrees\",\n                    \"LongitudeDegrees\",\n                ]\n            ]\n        )\n    except Exception as e:\n        print(\"Error processing\", dev_path, e, file=sys.stderr)\n\nif not rows:\n    raise RuntimeError(\"No training data collected. Check input paths.\")\n\ntrain_df = pd.concat(rows, ignore_index=True)\nprint(\"Total training rows:\", len(train_df), file=sys.stderr)\n\n# Prepare features and targets\ntrain_df = train_df.sort_values([\"drive_id\", \"UnixTimeMillis\"]).reset_index(drop=True)\nX = train_df[[\"wls_lat\", \"wls_lon\", \"Cn0DbHz\"]].fillna(-999)\ny_lat = train_df[\"LatitudeDegrees\"].values\ny_lon = train_df[\"LongitudeDegrees\"].values\ngroups = train_df[\"drive_id\"].values\nphones = train_df[\"phone\"].values\n\n# 5-fold group CV by drive_id\ngkf = GroupKFold(n_splits=5)\nmodels_lat = []\nmodels_lon = []\ncv_scores = []\npreds_lat = np.zeros(len(X))\npreds_lon = np.zeros(len(X))\nprint(\"Performing 5-fold GroupKFold CV...\", file=sys.stderr)\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    ytr_lat, ytr_lon = y_lat[tr_idx], y_lon[tr_idx]\n    # simple pipeline: scaler + ridge\n    model_lat = make_pipeline(StandardScaler(), Ridge(alpha=1.0))\n    model_lon = make_pipeline(StandardScaler(), Ridge(alpha=1.0))\n    model_lat.fit(X_tr, ytr_lat)\n    model_lon.fit(X_tr, ytr_lon)\n    p_lat = model_lat.predict(X_val)\n    p_lon = model_lon.predict(X_val)\n    preds_lat[val_idx] = p_lat\n    preds_lon[val_idx] = p_lon\n    models_lat.append(model_lat)\n    models_lon.append(model_lon)\n    print(f\"Fold {fold} done. Val size: {len(val_idx)}\", file=sys.stderr)\n\n# Evaluate using competition metric approximation\neval_df = train_df[[\"drive_id\", \"phone\", \"UnixTimeMillis\"]].copy()\neval_df[\"pred_lat\"] = preds_lat\neval_df[\"pred_lon\"] = preds_lon\neval_df[\"true_lat\"] = y_lat\neval_df[\"true_lon\"] = y_lon\neval_df[\"error_m\"] = haversine(\n    eval_df[\"true_lat\"].values,\n    eval_df[\"true_lon\"].values,\n    eval_df[\"pred_lat\"].values,\n    eval_df[\"pred_lon\"].values,\n)\n\n# compute per-phone percentiles and average\nphones_list = eval_df[\"phone\"].unique()\nphone_scores = []\nfor ph in phones_list:\n    sub = eval_df[eval_df[\"phone\"] == ph]\n    if sub.empty:\n        continue\n    p50 = np.percentile(sub[\"error_m\"], 50)\n    p95 = np.percentile(sub[\"error_m\"], 95)\n    phone_scores.append(0.5 * (p50 + p95))\nif len(phone_scores) == 0:\n    raise RuntimeError(\"No phone scores.\")\nscore = np.mean(phone_scores)\nprint(\n    \"Validation competition-style score (mean over phones of 0.5*(p50+p95)) = {:.4f} meters\".format(\n        score\n    )\n)\n\n# Train final models on all data\nfinal_lat = make_pipeline(StandardScaler(), Ridge(alpha=1.0)).fit(X, y_lat)\nfinal_lon = make_pipeline(StandardScaler(), Ridge(alpha=1.0)).fit(X, y_lon)\n\n# Prepare test predictions by reading sample_submission and merging with test device_gnss aggregated WLS\nprint(\"Preparing test predictions...\", file=sys.stderr)\nsample_sub = pd.read_csv(sample_sub_path)\n# sample_sub columns: phone,UnixTimeMillis,LatitudeDegrees,LongitudeDegrees (lat/lon empty)\n# We'll produce lat/lon predictions for each row.\n# For each phone in sample_sub, load corresponding device_gnss from test folder and aggregate per epoch.\ntest_rows = []\ntest_device_dirs = {}\n# map phone name like '2020-06-04-US-MTV-1_GooglePixel4' to folder path test/<drive>/<phone>\n# sample phone value might be drive_phone joined by underscore. We'll search test folders for matching prefix.\nall_test_paths = glob.glob(os.path.join(test_root, \"*\", \"*\"))\n# build mapping from \"<drive>_<phone>\" to path\nfor p in all_test_paths:\n    if os.path.isdir(p):\n        drive = os.path.basename(os.path.dirname(p))\n        phone = os.path.basename(p)\n        key = f\"{drive}_{phone}\"\n        test_device_dirs[key] = p\n\npreds = []\nmissing_count = 0\n# To speed up, cache aggregated device_gnss per phone key\nagg_cache = {}\nfor idx, row in sample_sub.iterrows():\n    key = f\"{row['phone']}\"\n    t = int(row[\"UnixTimeMillis\"])\n    if key not in test_device_dirs:\n        # Unknown phone; fallback to using global mean train location\n        missing_count += 1\n        preds.append((np.nan, np.nan))\n        continue\n    if key not in agg_cache:\n        dev_dir = test_device_dirs[key]\n        dg_path = os.path.join(dev_dir, \"device_gnss.csv\")\n        if not os.path.exists(dg_path):\n            agg_cache[key] = None\n            preds.append((np.nan, np.nan))\n            continue\n        dg = pd.read_csv(\n            dg_path,\n            usecols=[\n                \"utcTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"Cn0DbHz\",\n            ],\n        )\n        dg = (\n            dg.groupby(\"utcTimeMillis\", as_index=False)\n            .agg(\n                {\n                    \"WlsPositionXEcefMeters\": \"mean\",\n                    \"WlsPositionYEcefMeters\": \"mean\",\n                    \"WlsPositionZEcefMeters\": \"mean\",\n                    \"Cn0DbHz\": \"mean\",\n                }\n            )\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n            .sort_values(\"UnixTimeMillis\")\n            .reset_index(drop=True)\n        )\n        if dg.empty:\n            agg_cache[key] = None\n        else:\n            x = dg[\"WlsPositionXEcefMeters\"].values\n            yv = dg[\"WlsPositionYEcefMeters\"].values\n            z = dg[\"WlsPositionZEcefMeters\"].values\n            lat_wls, lon_wls = ecef_to_geodetic(x, yv, z)\n            dg[\"wls_lat\"] = lat_wls\n            dg[\"wls_lon\"] = lon_wls\n            agg_cache[key] = dg\n    dg = agg_cache.get(key)\n    if dg is None or dg.empty:\n        preds.append((np.nan, np.nan))\n        continue\n    # find nearest epoch within 1000 ms\n    # binary search\n    pos = np.searchsorted(dg[\"UnixTimeMillis\"].values, t)\n    cand_idxs = []\n    if pos < len(dg):\n        cand_idxs.append(pos)\n    if pos - 1 >= 0:\n        cand_idxs.append(pos - 1)\n    best = None\n    best_dt = 10**12\n    for ci in cand_idxs:\n        dt = abs(int(dg.iloc[ci][\"UnixTimeMillis\"]) - t)\n        if dt < best_dt:\n            best_dt = dt\n            best = ci\n    if best is None or best_dt > 2000:\n        # too far, take nearest but still ok\n        idx_use = cand_idxs[0] if cand_idxs else None\n    else:\n        idx_use = best\n    if idx_use is None:\n        preds.append((np.nan, np.nan))\n        continue\n    feat = (\n        dg.iloc[idx_use][[\"wls_lat\", \"wls_lon\", \"Cn0DbHz\"]]\n        .fillna(-999)\n        .values.reshape(1, -1)\n    )\n    plat = final_lat.predict(feat)[0]\n    plon = final_lon.predict(feat)[0]\n    preds.append((plat, plon))\n\npreds = np.array(preds)\nsample_sub[\"LatitudeDegrees\"] = preds[:, 0]\nsample_sub[\"LongitudeDegrees\"] = preds[:, 1]\n\n# For any remaining NaNs fill with WLS lat/lon if possible or train mean\nnan_mask = sample_sub[\"LatitudeDegrees\"].isna()\nif nan_mask.any():\n    print(\n        f\"Filling {nan_mask.sum()} missing predictions with global train mean or WLS fallback.\",\n        file=sys.stderr,\n    )\n    mean_lat = np.mean(y_lat)\n    mean_lon = np.mean(y_lon)\n    # attempt WLS fallback by looking up device_gnss one more time\n    for i in sample_sub[nan_mask].index:\n        key = sample_sub.loc[i, \"phone\"]\n        if key in agg_cache and agg_cache[key] is not None:\n            # use nearest epoch like above\n            t = int(sample_sub.loc[i, \"UnixTimeMillis\"])\n            dg = agg_cache[key]\n            pos = np.searchsorted(dg[\"UnixTimeMillis\"].values, t)\n            cand_idxs = []\n            if pos < len(dg):\n                cand_idxs.append(pos)\n            if pos - 1 >= 0:\n                cand_idxs.append(pos - 1)\n            best = None\n            best_dt = 10**12\n            for ci in cand_idxs:\n                dt = abs(int(dg.iloc[ci][\"UnixTimeMillis\"]) - t)\n                if dt < best_dt:\n                    best_dt = dt\n                    best = ci\n            if best is not None:\n                sample_sub.at[i, \"LatitudeDegrees\"] = dg.iloc[best][\"wls_lat\"]\n                sample_sub.at[i, \"LongitudeDegrees\"] = dg.iloc[best][\"wls_lon\"]\n            else:\n                sample_sub.at[i, \"LatitudeDegrees\"] = mean_lat\n                sample_sub.at[i, \"LongitudeDegrees\"] = mean_lon\n        else:\n            sample_sub.at[i, \"LatitudeDegrees\"] = mean_lat\n            sample_sub.at[i, \"LongitudeDegrees\"] = mean_lon\n\n# Save submission to required path\nout_dir = \"./submission\"\nensure_dir(out_dir)\nout_path = os.path.join(out_dir, \"submission.csv\")\nsample_sub.to_csv(out_path, index=False)\nprint(f\"Saved submission to {out_path}\", file=sys.stderr)\n\n# Print final message with validation score as required by instructions\nprint(\"Final validation metric (approx): {:.4f} meters\".format(score))\n",
                "exec_time": 37.309027910232544,
                "metric": null,
                "is_buggy": true,
                "exc_type": "KeyError",
                "exc_value": null,
                "analysis": "Training and CV completed (258,546 rows; 5-fold GroupKFold), reporting a validation metric of 7490.5038 meters. The script crashed while preparing test predictions with a KeyError: 'phone' when iterating the sample submission, so no submission.csv was produced.",
                "alignment": {
                        "plan_code_similarity": 0.013093048915042295,
                        "has_changes": true,
                        "change_magnitude": 0.9236822333821029
                }
        },
        {
                "index": 2,
                "step": 2,
                "id": "fa273f3a62fa4774bb052896b7169adb",
                "parent_id": null,
                "plan": "I will build a simple, robust baseline that corrects the provided WLS ECEF positions using a small regression on per-epoch GNSS features. The script aggregates device_gnss.csv by epoch (utcTimeMillis), converts WLS ECEF -> lat/lon, joins to ground truth to learn residuals (delta lat/lon) using a Ridge regressor with GroupKFold (5 folds by drive). I evaluate on a held-out fold using the competition metric (mean across phones of the average of 50th and 95th percentile horizontal errors), print that score, then train on all train data and produce a submission.csv for the test sample submission by applying the learned correction to test WLS positions (with nearest-time matching). The code is self-contained and saves ./submission/submission.csv.",
                "code": "#!/usr/bin/env python3\nimport os\nimport sys\nimport glob\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\n\n\n# Utilities\ndef ensure_dir(p):\n    if not os.path.exists(p):\n        os.makedirs(p)\n\n\ndef ecef_to_latlon(x, y, z):\n    # WGS84\n    a = 6378137.0\n    e = 8.1819190842622e-2\n    asq = a * a\n    esq = e * e\n    b = math.sqrt(asq * (1 - esq))\n    ep = math.sqrt((asq - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3, p - esq * a * math.cos(th) ** 3\n    )\n    # alt not needed\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine_meters(lat1, lon1, lat2, lon2):\n    # all degrees\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return R * 2 * np.arcsin(np.sqrt(a))\n\n\n# Paths\nINPUT = \"./input\"\nTRAIN_DIR = os.path.join(INPUT, \"train\")\nTEST_DIR = os.path.join(INPUT, \"test\")\nSAMPLE_SUB = os.path.join(INPUT, \"sample_submission.csv\")\nSUBMISSION_OUT = os.path.join(\"submission\")\nensure_dir(SUBMISSION_OUT)\nSUBMISSION_PATH = os.path.join(SUBMISSION_OUT, \"submission.csv\")\n\n# Step 1: Aggregate train GNSS epochs and join to ground truth\ntrain_agg_rows = []\nprint(\"Scanning train folders...\")\nfor drive_path in sorted(glob.glob(os.path.join(TRAIN_DIR, \"*\"))):\n    if not os.path.isdir(drive_path):\n        continue\n    drive = os.path.basename(drive_path)\n    for phone in sorted(os.listdir(drive_path)):\n        phone_dir = os.path.join(drive_path, phone)\n        if not os.path.isdir(phone_dir):\n            continue\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n        if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n            continue\n        try:\n            usecols = [\n                \"utcTimeMillis\",\n                \"Svid\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateMetersPerSecond\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            df = pd.read_csv(gnss_path, usecols=lambda c: c in usecols)\n        except Exception as e:\n            # retry with all columns then subset\n            df = pd.read_csv(gnss_path)\n            df = df[[c for c in usecols if c in df.columns]]\n        # If Wls columns missing skip\n        wls_cols = [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        for c in wls_cols:\n            if c not in df.columns:\n                # Skip this phone if no WLS\n                df = None\n                break\n        if df is None or df.shape[0] == 0:\n            continue\n        # Group by epoch\n        # compute per-epoch: pick first WlsPosition (should be same for epoch), count sats, mean cn0, mean prr\n        agg = (\n            df.groupby(\"utcTimeMillis\")\n            .agg(\n                WlsX=(\"WlsPositionXEcefMeters\", \"first\"),\n                WlsY=(\"WlsPositionYEcefMeters\", \"first\"),\n                WlsZ=(\"WlsPositionZEcefMeters\", \"first\"),\n                sat_count=(\"Svid\", lambda s: s.nunique()),\n                mean_cn0=(\"Cn0DbHz\", \"mean\"),\n                std_cn0=(\"Cn0DbHz\", \"std\"),\n                mean_prr=(\"PseudorangeRateMetersPerSecond\", \"mean\"),\n            )\n            .reset_index()\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        )\n        # Convert to lat/lon\n        lat_list = []\n        lon_list = []\n        for xi, yi, zi in zip(agg.WlsX.values, agg.WlsY.values, agg.WlsZ.values):\n            try:\n                lat, lon = ecef_to_latlon(float(xi), float(yi), float(zi))\n            except Exception:\n                lat, lon = np.nan, np.nan\n            lat_list.append(lat)\n            lon_list.append(lon)\n        agg[\"WlsLat\"] = lat_list\n        agg[\"WlsLon\"] = lon_list\n        # read ground truth\n        gt = pd.read_csv(gt_path)\n        gt = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].dropna()\n        gt = gt.sort_values(\"UnixTimeMillis\").reset_index(drop=True)\n        agg = agg.sort_values(\"UnixTimeMillis\").reset_index(drop=True)\n        # merge_asof to nearest previous measurement; but we want nearest (either direction)\n        # do forward/backward and pick min abs diff\n        if agg.shape[0] == 0 or gt.shape[0] == 0:\n            continue\n        merged_back = pd.merge_asof(agg, gt, on=\"UnixTimeMillis\", direction=\"backward\")\n        merged_forward = pd.merge_asof(\n            agg, gt, on=\"UnixTimeMillis\", direction=\"forward\"\n        )\n\n        # pick closer\n        def choose_row(r_back, r_for):\n            if pd.isna(r_back.LatitudeDegrees) and pd.isna(r_for.LatitudeDegrees):\n                return None\n            if pd.isna(r_back.LatitudeDegrees):\n                return r_for\n            if pd.isna(r_for.LatitudeDegrees):\n                return r_back\n            db = abs(\n                r_back.UnixTimeMillis - r_back.UnixTimeMillis\n            )  # zero, but we need diff between epoch and matched\n            # we need matched times - but merge_asof placed LatitudeDegrees from gt; get index via different approach:\n            return r_back  # we will compute diffs another way\n\n        # Instead compute nearest by merging index-wise:\n        # For each agg epoch, find nearest gt time using searchsorted\n        gt_times = gt.UnixTimeMillis.values\n        agg_times = agg.UnixTimeMillis.values\n        idxs = np.searchsorted(gt_times, agg_times)\n        chosen_lat = []\n        chosen_lon = []\n        chosen_dt = []\n        for i, t in enumerate(agg_times):\n            best = None\n            best_dt = None\n            lat_val = np.nan\n            lon_val = np.nan\n            # candidate idxs: idxs[i]-1 and idxs[i]\n            for cand in (idxs[i] - 1, idxs[i]):\n                if 0 <= cand < len(gt_times):\n                    dt = abs(gt_times[cand] - t)\n                    if best_dt is None or dt < best_dt:\n                        best_dt = dt\n                        lat_val = float(gt.LatitudeDegrees.values[cand])\n                        lon_val = float(gt.LongitudeDegrees.values[cand])\n            chosen_lat.append(lat_val)\n            chosen_lon.append(lon_val)\n            chosen_dt.append(best_dt if best_dt is not None else np.nan)\n        agg[\"GtLat\"] = chosen_lat\n        agg[\"GtLon\"] = chosen_lon\n        agg[\"time_diff_ms\"] = chosen_dt\n        # keep only fairly close matches (<=500ms)\n        agg = agg[agg.time_diff_ms <= 500].copy()\n        if agg.shape[0] == 0:\n            continue\n        agg[\"drive\"] = drive\n        agg[\"phone\"] = phone\n        # compute deltas (degrees)\n        agg[\"dLat\"] = agg[\"GtLat\"] - agg[\"WlsLat\"]\n        agg[\"dLon\"] = agg[\"GtLon\"] - agg[\"WlsLon\"]\n        # append useful columns\n        train_agg_rows.append(\n            agg[\n                [\n                    \"drive\",\n                    \"phone\",\n                    \"UnixTimeMillis\",\n                    \"WlsX\",\n                    \"WlsY\",\n                    \"WlsZ\",\n                    \"WlsLat\",\n                    \"WlsLon\",\n                    \"sat_count\",\n                    \"mean_cn0\",\n                    \"std_cn0\",\n                    \"mean_prr\",\n                    \"GtLat\",\n                    \"GtLon\",\n                    \"dLat\",\n                    \"dLon\",\n                ]\n            ]\n        )\n    # quick progress\n    print(\n        \"Processed drive:\",\n        drive,\n        \"current total epochs:\",\n        sum([len(x) for x in train_agg_rows]),\n    )\n\nif len(train_agg_rows) == 0:\n    print(\"No train data found or no matching WLS/ground truth pairs. Exiting.\")\n    sys.exit(1)\n\ntrain_df = pd.concat(train_agg_rows, ignore_index=True)\nprint(\"Total training epochs collected:\", len(train_df))\n\n# Prepare features and targets\nfeat_cols = [\"WlsLat\", \"WlsLon\", \"sat_count\", \"mean_cn0\", \"std_cn0\", \"mean_prr\"]\nfor c in feat_cols:\n    if c not in train_df.columns:\n        train_df[c] = 0.0\ntrain_df = train_df.dropna(subset=[\"dLat\", \"dLon\", \"WlsLat\", \"WlsLon\"])\nX = train_df[feat_cols].fillna(0.0).values\ny_lat = train_df[\"dLat\"].values\ny_lon = train_df[\"dLon\"].values\ngroups = train_df[\"drive\"].values\nphones = (train_df[\"drive\"] + \"_\" + train_df[\"phone\"]).values\n\n# 5-fold GroupKFold by drive\ngkf = GroupKFold(n_splits=5)\nlat_preds = np.zeros_like(y_lat)\nlon_preds = np.zeros_like(y_lon)\n\nprint(\"Running 5-fold CV with Ridge...\")\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X[train_idx], X[val_idx]\n    ylat_tr, ylat_val = y_lat[train_idx], y_lat[val_idx]\n    ylon_tr, ylon_val = y_lon[train_idx], y_lon[val_idx]\n    # simple scaling not necessary; use Ridge default\n    model_lat = Ridge(alpha=1.0)\n    model_lon = Ridge(alpha=1.0)\n    model_lat.fit(X_tr, ylat_tr)\n    model_lon.fit(X_tr, ylon_tr)\n    lat_preds[val_idx] = model_lat.predict(X_val)\n    lon_preds[val_idx] = model_lon.predict(X_val)\n    print(f\" Fold {fold+1} done.\")\n\n# Evaluate on CV predictions using competition metric\ntrain_df[\"pred_dLat\"] = lat_preds\ntrain_df[\"pred_dLon\"] = lon_preds\n# compute predicted lat/lon\ntrain_df[\"pred_lat\"] = train_df[\"WlsLat\"] + train_df[\"pred_dLat\"]\ntrain_df[\"pred_lon\"] = train_df[\"WlsLon\"] + train_df[\"pred_dLon\"]\n# compute distances per row\ntrain_df[\"err_m\"] = haversine_meters(\n    train_df[\"pred_lat\"].values,\n    train_df[\"pred_lon\"].values,\n    train_df[\"GtLat\"].values,\n    train_df[\"GtLon\"].values,\n)\n# compute per-phone percentiles and aggregate metric\nphone_groups = train_df.groupby([\"drive\", \"phone\"])\nphone_scores = []\nfor (drive, phone), gp in phone_groups:\n    errs = gp[\"err_m\"].values\n    if len(errs) < 5:\n        continue\n    p50 = np.percentile(errs, 50)\n    p95 = np.percentile(errs, 95)\n    phone_scores.append((p50 + p95) / 2.0)\nif len(phone_scores) == 0:\n    print(\"No phone groups suitable for evaluation.\")\n    mean_metric = float(\"nan\")\nelse:\n    mean_metric = float(np.mean(phone_scores))\nprint(\n    \"CV competition metric (mean of per-phone (50th+95th)/2): {:.6f} meters\".format(\n        mean_metric\n    )\n)\n\n# Train final models on all train data\nfinal_lat = Ridge(alpha=1.0).fit(X, y_lat)\nfinal_lon = Ridge(alpha=1.0).fit(X, y_lon)\nprint(\"Final models trained on all training data.\")\n\n# Build test predictions by reading device_gnss per test phone and matching sample_submission times\nprint(\"Loading sample submission to build test predictions...\")\nsample = pd.read_csv(SAMPLE_SUB)\n# sample has columns phone,UnixTimeMillis,LatitudeDegrees,LongitudeDegrees\nout_rows = []\n# Build a cache of aggregated test epochs per phone (drive_phone)\ntest_cache = {}\nprint(\"Scanning test folders and aggregating GNSS epochs...\")\nfor drive_path in sorted(glob.glob(os.path.join(TEST_DIR, \"*\"))):\n    if not os.path.isdir(drive_path):\n        continue\n    drive = os.path.basename(drive_path)\n    for phone in sorted(os.listdir(drive_path)):\n        phone_dir = os.path.join(drive_path, phone)\n        if not os.path.isdir(phone_dir):\n            continue\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            continue\n        try:\n            df = pd.read_csv(\n                gnss_path,\n                usecols=lambda c: c\n                in [\n                    \"utcTimeMillis\",\n                    \"Svid\",\n                    \"Cn0DbHz\",\n                    \"PseudorangeRateMetersPerSecond\",\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ],\n            )\n        except Exception:\n            df = pd.read_csv(gnss_path)\n            df = df[\n                [\n                    c\n                    for c in [\n                        \"utcTimeMillis\",\n                        \"Svid\",\n                        \"Cn0DbHz\",\n                        \"PseudorangeRateMetersPerSecond\",\n                        \"WlsPositionXEcefMeters\",\n                        \"WlsPositionYEcefMeters\",\n                        \"WlsPositionZEcefMeters\",\n                    ]\n                    if c in df.columns\n                ]\n            ]\n        if df.shape[0] == 0:\n            continue\n        if not all(\n            c in df.columns\n            for c in [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        ):\n            continue\n        agg = (\n            df.groupby(\"utcTimeMillis\")\n            .agg(\n                WlsX=(\"WlsPositionXEcefMeters\", \"first\"),\n                WlsY=(\"WlsPositionYEcefMeters\", \"first\"),\n                WlsZ=(\"WlsPositionZEcefMeters\", \"first\"),\n                sat_count=(\"Svid\", lambda s: s.nunique()),\n                mean_cn0=(\"Cn0DbHz\", \"mean\"),\n                std_cn0=(\"Cn0DbHz\", \"std\"),\n                mean_prr=(\"PseudorangeRateMetersPerSecond\", \"mean\"),\n            )\n            .reset_index()\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        )\n        # latlon\n        lat_list = []\n        lon_list = []\n        for xi, yi, zi in zip(agg.WlsX.values, agg.WlsY.values, agg.WlsZ.values):\n            try:\n                lat, lon = ecef_to_latlon(float(xi), float(yi), float(zi))\n            except Exception:\n                lat, lon = np.nan, np.nan\n            lat_list.append(lat)\n            lon_list.append(lon)\n        agg[\"WlsLat\"] = lat_list\n        agg[\"WlsLon\"] = lon_list\n        agg = agg.sort_values(\"UnixTimeMillis\").reset_index(drop=True)\n        key = drive + \"_\" + phone\n        test_cache[key] = agg\n        print(\"Aggregated test:\", key, \"epochs:\", len(agg))\n\n# For each row in sample submission, predict\nprint(\"Constructing submission predictions...\")\nout_records = []\nmissing_count = 0\nfor i, row in sample.iterrows():\n    phone = row[\"phone\"]\n    t = int(row[\"UnixTimeMillis\"])\n    # phone likely in format drive_phone; ensure matches our keys\n    key = phone\n    if key not in test_cache:\n        # The sample's phone might match different naming - try replacing last '_' with '/'\n        # but more robust: try to find any key that startswith same drive or phone suffix\n        candidates = [\n            k\n            for k in test_cache.keys()\n            if k.endswith(\"_\" + phone.split(\"_\")[-1])\n            or k.startswith(phone.split(\"_\")[0])\n        ]\n        if len(candidates) == 1:\n            key = candidates[0]\n        else:\n            # fallback: use any with same phone suffix\n            matched = None\n            for k in test_cache.keys():\n                if phone.split(\"_\")[-1] in k:\n                    matched = k\n                    break\n            if matched is not None:\n                key = matched\n            else:\n                key = None\n    if key is None or key not in test_cache:\n        # fallback: copy sample lat/lon (often zeros) or use global mean WLS from train\n        missing_count += 1\n        pred_lat = sample.at[i, \"LatitudeDegrees\"]\n        pred_lon = sample.at[i, \"LongitudeDegrees\"]\n    else:\n        agg = test_cache[key]\n        # find nearest epoch\n        times = agg.UnixTimeMillis.values\n        if len(times) == 0:\n            missing_count += 1\n            pred_lat = sample.at[i, \"LatitudeDegrees\"]\n            pred_lon = sample.at[i, \"LongitudeDegrees\"]\n        else:\n            pos = np.searchsorted(times, t)\n            cand_idx = []\n            for cand in (pos - 1, pos):\n                if 0 <= cand < len(times):\n                    cand_idx.append(cand)\n            best_idx = cand_idx[0]\n            best_dt = abs(int(times[best_idx]) - t)\n            for ci in cand_idx:\n                dt = abs(int(times[ci]) - t)\n                if dt < best_dt:\n                    best_dt = dt\n                    best_idx = ci\n            # if too far, still we will use best available\n            row_agg = agg.iloc[best_idx]\n            wls_lat = row_agg[\"WlsLat\"]\n            wls_lon = row_agg[\"WlsLon\"]\n            feat = np.array(\n                [\n                    wls_lat,\n                    wls_lon,\n                    (\n                        row_agg.get(\"sat_count\", 0)\n                        if not pd.isna(row_agg.get(\"sat_count\", np.nan))\n                        else 0.0\n                    ),\n                    (\n                        row_agg.get(\"mean_cn0\", 0.0)\n                        if not pd.isna(row_agg.get(\"mean_cn0\", np.nan))\n                        else 0.0\n                    ),\n                    (\n                        row_agg.get(\"std_cn0\", 0.0)\n                        if not pd.isna(row_agg.get(\"std_cn0\", np.nan))\n                        else 0.0\n                    ),\n                    (\n                        row_agg.get(\"mean_prr\", 0.0)\n                        if not pd.isna(row_agg.get(\"mean_prr\", np.nan))\n                        else 0.0\n                    ),\n                ]\n            ).reshape(1, -1)\n            pred_dlat = final_lat.predict(feat)[0]\n            pred_dlon = final_lon.predict(feat)[0]\n            pred_lat = wls_lat + pred_dlat\n            pred_lon = wls_lon + pred_dlon\n    out_records.append((phone, t, pred_lat, pred_lon))\n\nprint(\"Missing/unknown test epochs:\", missing_count, \"out of\", len(sample))\n\n# Write submission\nout_df = pd.DataFrame(\n    out_records,\n    columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"],\n)\nout_df.to_csv(SUBMISSION_PATH, index=False)\nprint(\"Saved submission to\", SUBMISSION_PATH)\nprint(\"Done.\")\n",
                "exec_time": 61.773874282836914,
                "metric": null,
                "is_buggy": true,
                "exc_type": "KeyError",
                "exc_value": null,
                "analysis": "The script successfully aggregated GNSS epochs from many train and test drives and trained final Ridge models, but it crashed while constructing the submission because the sample submission DataFrame did not contain a 'phone' column as accessed in the loop. As a result no submission.csv was written.",
                "alignment": {
                        "plan_code_similarity": 0.007584659758572802,
                        "has_changes": true,
                        "change_magnitude": 0.9229974160206719
                }
        },
        {
                "index": 3,
                "step": 3,
                "id": "9b73dd6a0990471d80f4a9d41ef22cee",
                "parent_id": null,
                "plan": "I will implement a simple baseline that uses the provided device_gnss \"WLS\" baseline positions: for each required timestamp I find the nearest device_gnss epoch and return its WLS-reported position (converting ECEF->lat/lon if necessary). I'll evaluate this approach on a small hold-out set from the train drives by computing the competition metric (for each phone average of 50th and 95th percentile horizontal errors, then mean across phones). The script will print the validation metric and write predictions for the provided test sample_submission into ./submission/submission.csv.",
                "code": "import os\nimport sys\nimport math\nimport glob\nimport numpy as np\nimport pandas as pd\n\n\n# Utility: ECEF -> lat/lon/alt (WGS84)\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 constants\n    a = 6378137.0\n    e = 8.1819190842622e-2  # eccentricity\n    asq = a * a\n    esq = e * e\n\n    b = math.sqrt(asq * (1 - esq))\n    ep = math.sqrt((asq - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3, p - esq * a * math.cos(th) ** 3\n    )\n    N = a / math.sqrt(1 - esq * math.sin(lat) * math.sin(lat))\n    alt = p / math.cos(lat) - N\n\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, alt\n\n\n# Haversine distance (meters)\ndef haversine_meters(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = math.radians(lat1)\n    phi2 = math.radians(lat2)\n    dphi = math.radians(lat2 - lat1)\n    dlambda = math.radians(lon2 - lon1)\n    a = (\n        math.sin(dphi / 2) ** 2\n        + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2) ** 2\n    )\n    return 2 * R * math.asin(math.sqrt(a))\n\n\n# Find nearest indices for query times in a sorted time array\ndef nearest_indices(sorted_times, query_times):\n    idx = np.searchsorted(sorted_times, query_times, side=\"left\")\n    idx0 = np.clip(idx - 1, 0, len(sorted_times) - 1)\n    idx1 = np.clip(idx, 0, len(sorted_times) - 1)\n    # pick closer of idx0 and idx1\n    diff0 = np.abs(sorted_times[idx0] - query_times)\n    diff1 = np.abs(sorted_times[idx1] - query_times)\n    choose = diff1 < diff0\n    return np.where(choose, idx1, idx0)\n\n\n# Load sample submission to know test queries\nINPUT_DIR = \"./input\"\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(SAMPLE_SUB_PATH):\n    print(\"sample_submission.csv not found in ./input - aborting\")\n    sys.exit(1)\n\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n\n\n# We'll prepare a mapping: for each phone id (drive_phone) we will load device_gnss and build nearest lookup\n# Parse phone string into drive folder and phone name (split at last underscore)\ndef parse_phone_id(phone_id):\n    if \"_\" in phone_id:\n        drive, phone = phone_id.rsplit(\"_\", 1)\n    else:\n        # fallback\n        parts = phone_id.split(\"/\")\n        drive = parts[0]\n        phone = parts[-1]\n    return drive, phone\n\n\n# Cache device GNSS data per test drive+phone\ndevice_cache = {}\n\n\ndef load_device_gnss_for(drive, phone, base_dir=\"test\"):\n    key = (base_dir, drive, phone)\n    if key in device_cache:\n        return device_cache[key]\n    path = os.path.join(INPUT_DIR, base_dir, drive, phone, \"device_gnss.csv\")\n    if not os.path.exists(path):\n        device_cache[key] = None\n        return None\n    # Try to load only useful columns for speed\n    # Possible columns: utcTimeMillis, WlsLatitudeDegrees, WlsLongitudeDegrees,\n    # WlsPositionXEcefMeters, WlsPositionYEcefMeters, WlsPositionZEcefMeters\n    df = pd.read_csv(path, low_memory=False)\n    # Keep only rows with valid utcTimeMillis\n    if \"utcTimeMillis\" not in df.columns:\n        device_cache[key] = None\n        return None\n    df = df.dropna(subset=[\"utcTimeMillis\"])\n    # If WLS lat/lon exist, keep them; otherwise keep ECEF\n    has_latlon = (\"WlsLatitudeDegrees\" in df.columns) and (\n        \"WlsLongitudeDegrees\" in df.columns\n    )\n    has_ecef = (\n        (\"WlsPositionXEcefMeters\" in df.columns)\n        and (\"WlsPositionYEcefMeters\" in df.columns)\n        and (\"WlsPositionZEcefMeters\" in df.columns)\n    )\n    # Convert to numpy arrays for speed\n    times = df[\"utcTimeMillis\"].astype(np.int64).to_numpy()\n    order = np.argsort(times)\n    times = times[order]\n    if has_latlon:\n        lat = df[\"WlsLatitudeDegrees\"].to_numpy()[order]\n        lon = df[\"WlsLongitudeDegrees\"].to_numpy()[order]\n        device_cache[key] = {\"times\": times, \"lat\": lat, \"lon\": lon}\n    elif has_ecef:\n        x = df[\"WlsPositionXEcefMeters\"].to_numpy()[order]\n        y = df[\"WlsPositionYEcefMeters\"].to_numpy()[order]\n        z = df[\"WlsPositionZEcefMeters\"].to_numpy()[order]\n        # Convert all to lat/lon arrays\n        lat = np.empty(len(x), dtype=float)\n        lon = np.empty(len(x), dtype=float)\n        for i, (xi, yi, zi) in enumerate(zip(x, y, z)):\n            try:\n                lati, loni, _ = ecef_to_geodetic(float(xi), float(yi), float(zi))\n            except Exception:\n                lati, loni = np.nan, np.nan\n            lat[i] = lati\n            lon[i] = loni\n        device_cache[key] = {\"times\": times, \"lat\": lat, \"lon\": lon}\n    else:\n        # Cannot extract positions\n        device_cache[key] = {\n            \"times\": times,\n            \"lat\": np.full_like(times, np.nan, dtype=float),\n            \"lon\": np.full_like(times, np.nan, dtype=float),\n        }\n    return device_cache[key]\n\n\n# Build predictions for sample_submission\nout_rows = []\n# Precompute per-phone fallback means if device file missing\nfallback_means = {}\n\nfor idx, row in sample_sub.iterrows():\n    phone_id = row[\"phone\"]\n    unix_time = int(row[\"UnixTimeMillis\"])\n    drive, phone = parse_phone_id(phone_id)\n    data = load_device_gnss_for(drive, phone, base_dir=\"test\")\n    if data is None:\n        # fallback to mean from train if available, else zeros\n        if phone_id in fallback_means:\n            lat_pred, lon_pred = fallback_means[phone_id]\n        else:\n            lat_pred, lon_pred = 0.0, 0.0\n    else:\n        times = data[\"times\"]\n        if len(times) == 0:\n            lat_pred, lon_pred = 0.0, 0.0\n        else:\n            idx_near = nearest_indices(times, np.array([unix_time]))[0]\n            lat_pred = float(data[\"lat\"][idx_near])\n            lon_pred = float(data[\"lon\"][idx_near])\n            if math.isnan(lat_pred) or math.isnan(lon_pred):\n                # fallback to median of that device\n                lat_med = np.nanmedian(data[\"lat\"])\n                lon_med = np.nanmedian(data[\"lon\"])\n                if math.isnan(lat_med) or math.isnan(lon_med):\n                    lat_pred, lon_pred = 0.0, 0.0\n                else:\n                    lat_pred, lon_pred = float(lat_med), float(lon_med)\n    out_rows.append((phone_id, unix_time, lat_pred, lon_pred))\n\nsubmission_df = pd.DataFrame(\n    out_rows, columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n)\nos.makedirs(\"./submission\", exist_ok=True)\nsubmission_path = \"./submission/submission.csv\"\nsubmission_df.to_csv(submission_path, index=False)\nprint(f\"Saved test predictions to {submission_path}\")\n\n# Validation: evaluate on a small hold-out set from train drives.\n# We'll loop through train/*/*/device_gnss.csv and corresponding ground_truth.csv and compute metric.\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nif not os.path.exists(TRAIN_DIR):\n    print(\"No train directory found, skipping validation metric.\")\n    sys.exit(0)\n\n# Collect train phones (drive, phone) list\npairs = []\nfor drive in sorted(os.listdir(TRAIN_DIR))[\n    :20\n]:  # limit to first 20 drive folders for speed\n    drive_path = os.path.join(TRAIN_DIR, drive)\n    if not os.path.isdir(drive_path):\n        continue\n    for phone_name in sorted(os.listdir(drive_path)):\n        phone_path = os.path.join(drive_path, phone_name)\n        if not os.path.isdir(phone_path):\n            continue\n        # require both device_gnss.csv and ground_truth.csv\n        if os.path.exists(\n            os.path.join(phone_path, \"device_gnss.csv\")\n        ) and os.path.exists(os.path.join(phone_path, \"ground_truth.csv\")):\n            pairs.append((drive, phone_name))\n# Limit number of phones to speed up\nMAX_VAL_PHONES = 8\npairs = pairs[:MAX_VAL_PHONES]\n\nper_phone_scores = []\nfor drive, phone in pairs:\n    # Load device gnss\n    dev = load_device_gnss_for(drive, phone, base_dir=\"train\")\n    if dev is None or len(dev[\"times\"]) == 0:\n        continue\n    # Load ground truth\n    gt_path = os.path.join(INPUT_DIR, \"train\", drive, phone, \"ground_truth.csv\")\n    try:\n        gt = pd.read_csv(gt_path)\n    except Exception as e:\n        continue\n    # use the UnixTimeMillis and lat/lon from ground truth\n    if (\n        (\"UnixTimeMillis\" not in gt.columns)\n        or (\"LatitudeDegrees\" not in gt.columns)\n        or (\"LongitudeDegrees\" not in gt.columns)\n    ):\n        continue\n    query_times = gt[\"UnixTimeMillis\"].astype(np.int64).to_numpy()\n    idxs = nearest_indices(dev[\"times\"], query_times)\n    preds_lat = dev[\"lat\"][idxs]\n    preds_lon = dev[\"lon\"][idxs]\n    # compute distances for each row, skipping nan predictions\n    distances = []\n    for i in range(len(query_times)):\n        plat = preds_lat[i]\n        plon = preds_lon[i]\n        glat = float(gt[\"LatitudeDegrees\"].iloc[i])\n        glon = float(gt[\"LongitudeDegrees\"].iloc[i])\n        if math.isnan(plat) or math.isnan(plon):\n            # large penalty (device had no WLS); use distance of 1000m\n            distances.append(1000.0)\n        else:\n            distances.append(haversine_meters(glat, glon, float(plat), float(plon)))\n    distances = np.array(distances)\n    # compute 50th and 95th percentiles\n    p50 = float(np.nanpercentile(distances, 50))\n    p95 = float(np.nanpercentile(distances, 95))\n    per_phone_scores.append((drive + \"_\" + phone, p50, p95))\n    print(f\"Phone {drive}_{phone}: p50={p50:.3f} m, p95={p95:.3f} m\")\n\n# Compute competition metric: for each phone average of p50 and p95, then mean across phones\nif len(per_phone_scores) == 0:\n    print(\"No validation phones evaluated.\")\nelse:\n    avg_per_phone = [(p, (p50 + p95) / 2.0) for (p, p50, p95) in per_phone_scores]\n    mean_metric = float(np.mean([v for (p, v) in avg_per_phone]))\n    print(\n        \"\\nValidation metric (mean across phones of (p50+p95)/2): {:.3f} meters\".format(\n            mean_metric\n        )\n    )\n",
                "exec_time": 0.04987287521362305,
                "metric": null,
                "is_buggy": true,
                "exc_type": "KeyError",
                "exc_value": null,
                "analysis": "The script crashed with a KeyError: 'phone' when iterating over the sample submission rows, indicating the sample_submission.csv does not contain a column named 'phone' (header mismatch). As a result the run aborted before producing submission/submission.csv and no validation metric was computed.",
                "alignment": {
                        "plan_code_similarity": 0.012175089380616485,
                        "has_changes": true,
                        "change_magnitude": 0.928684741851745
                }
        },
        {
                "index": 4,
                "step": 4,
                "id": "3c251e9fba2041acb6e591e3289c0626",
                "parent_id": null,
                "plan": "I will build a simple baseline that uses the provided WLS (weighted least squares) ECEF positions from device_gnss.csv, convert them to latitude/longitude, and interpolate/nearest-match those positions to the requested timestamps in the sample submission. For validation I will compute the competition metric (per-phone mean of the 50th and 95th percentile horizontal errors) by matching WLS predictions to train ground-truth timestamps. The script prints that validation metric and writes predictions for the test sample submission to ./submission/submission.csv.",
                "code": "#!/usr/bin/env python3\nimport os\nimport sys\nimport math\nimport numpy as np\nimport pandas as pd\n\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSAMPLE_SUB = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nOUTPUT_DIR = \"./submission\"\nOUTPUT_FILE = os.path.join(OUTPUT_DIR, \"submission.csv\")\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# --- utility functions ---\n\n\n# ECEF to geodetic (WGS84) conversion (iterative)\ndef ecef_to_latlon(x, y, z, tol=1e-12):\n    # returns lat_deg, lon_deg\n    a = 6378137.0  # semi-major axis\n    f = 1 / 298.257223563\n    b = a * (1 - f)\n    e2 = f * (2 - f)  # first eccentricity squared\n    lon = math.atan2(y, x)\n    p = math.hypot(x, y)\n    if p < 1e-12:\n        # at poles\n        lat = math.copysign(math.pi / 2, z)\n        return math.degrees(lat), math.degrees(lon)\n    # initial lat\n    lat = math.atan2(z, p * (1 - e2))\n    for _ in range(10):\n        sin_lat = math.sin(lat)\n        N = a / math.sqrt(1 - e2 * sin_lat * sin_lat)\n        new_lat = math.atan2(z + e2 * N * sin_lat, p)\n        if abs(new_lat - lat) < tol:\n            lat = new_lat\n            break\n        lat = new_lat\n    return math.degrees(lat), math.degrees(lon)\n\n\n# Haversine distance (meters)\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = math.radians(lat1)\n    phi2 = math.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = math.radians(lon2 - lon1)\n    a = (\n        math.sin(dphi / 2.0) ** 2\n        + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * math.asin(math.sqrt(a))\n\n\n# extract WLS columns robustly\ndef find_wls_cols(df):\n    # possible names from description: WlsPositionXEcefMeters, WlsPositionYEcefMeters, WlsPositionZEcefMeters\n    cols = df.columns.tolist()\n    x = [c for c in cols if \"WlsPositionX\" in c or \"WlsPositionXEcef\" in c]\n    y = [c for c in cols if \"WlsPositionY\" in c or \"WlsPositionYEcef\" in c]\n    z = [c for c in cols if \"WlsPositionZ\" in c or \"WlsPositionZEcef\" in c]\n    if x and y and z:\n        return x[0], y[0], z[0]\n    # Try lowercase variants\n    lx = [\n        c\n        for c in cols\n        if \"wlspositionx\" in c.lower() or \"wlspositionxecef\" in c.lower()\n    ]\n    ly = [\n        c\n        for c in cols\n        if \"wlspositiony\" in c.lower() or \"wlspositionyecef\" in c.lower()\n    ]\n    lz = [\n        c\n        for c in cols\n        if \"wlspositionz\" in c.lower()\n        or \"wlspositionzeccef\" in c.lower()\n        or \"wlspositionz\" in c.lower()\n    ]\n    if lx and ly and lz:\n        return lx[0], ly[0], lz[0]\n    return None, None, None\n\n\n# Build time->latlon mapping for a device_gnss csv\ndef build_wls_time_latlon_map(gnss_path):\n    try:\n        df = pd.read_csv(gnss_path)\n    except Exception as e:\n        print(f\"Failed to read {gnss_path}: {e}\", file=sys.stderr)\n        return None\n    if df.shape[0] == 0:\n        return None\n    # prefer 'utcTimeMillis' or 'UnixTimeMillis' or 'utcTimeMillis'\n    time_col_candidates = [\n        c\n        for c in df.columns\n        if c.lower()\n        in (\n            \"utctimemillis\",\n            \"utctime_millis\",\n            \"unixtimemillis\",\n            \"utc_time_millis\",\n            \"utcTimeMillis\",\n        )\n    ]\n    if time_col_candidates:\n        time_col = time_col_candidates[0]\n    elif \"utcTimeMillis\" in df.columns:\n        time_col = \"utcTimeMillis\"\n    elif \"UnixTimeMillis\" in df.columns:\n        time_col = \"UnixTimeMillis\"\n    else:\n        # fallback to any integer-like column\n        time_col = df.columns[0]\n\n    xcol, ycol, zcol = find_wls_cols(df)\n    if xcol is None:\n        # no WLS ECEF found\n        return None\n    # group by time and take first valid triple\n    keep = df[[time_col, xcol, ycol, zcol]].dropna()\n    if keep.shape[0] == 0:\n        return None\n    # There may be many rows for same time (satellites). Take first per time.\n    keep = keep.sort_values(time_col).drop_duplicates(time_col, keep=\"first\")\n    times = keep[time_col].astype(np.int64).values\n    xs = keep[xcol].astype(float).values\n    ys = keep[ycol].astype(float).values\n    zs = keep[zcol].astype(float).values\n    # convert to lat/lon arrays\n    lats = np.empty_like(xs)\n    lons = np.empty_like(xs)\n    for i, (xx, yy, zz) in enumerate(zip(xs, ys, zs)):\n        try:\n            lat, lon = ecef_to_latlon(xx, yy, zz)\n        except Exception:\n            lat, lon = np.nan, np.nan\n        lats[i] = lat\n        lons[i] = lon\n    # filter out nans\n    mask = np.isfinite(lats) & np.isfinite(lons)\n    if not np.any(mask):\n        return None\n    return {\"times\": times[mask], \"lats\": lats[mask], \"lons\": lons[mask]}\n\n\n# Predict lat/lon for requested times by nearest neighbor in times array\ndef predict_from_map(tmap, query_times):\n    times = tmap[\"times\"]\n    lats = tmap[\"lats\"]\n    lons = tmap[\"lons\"]\n    # ensure sorted\n    idxs = np.searchsorted(times, query_times, side=\"left\")\n    preds_lat = np.empty(len(query_times))\n    preds_lon = np.empty(len(query_times))\n    for i, (q, idx) in enumerate(zip(query_times, idxs)):\n        cand = []\n        if idx < len(times):\n            cand.append((abs(times[idx] - q), idx))\n        if idx - 1 >= 0:\n            cand.append((abs(times[idx - 1] - q), idx - 1))\n        if len(cand) == 0:\n            preds_lat[i], preds_lon[i] = np.nan, np.nan\n        else:\n            _, chosen = min(cand, key=lambda x: x[0])\n            preds_lat[i] = lats[chosen]\n            preds_lon[i] = lons[chosen]\n    return preds_lat, preds_lon\n\n\n# --- Validation on train using ground_truth ---\ndef validate_on_train(max_files=None):\n    # iterate train/<drive>/<phone>/device_gnss.csv and ground_truth.csv\n    phone_errors = {}  # phone -> list of distances\n    processed = 0\n    for drive in sorted(os.listdir(TRAIN_DIR)):\n        drive_path = os.path.join(TRAIN_DIR, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n            tmap = build_wls_time_latlon_map(gnss_path)\n            if tmap is None:\n                continue\n            try:\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"failed read gt {gt_path}: {e}\", file=sys.stderr)\n                continue\n            if (\n                \"UnixTimeMillis\" not in gt.columns\n                or \"LatitudeDegrees\" not in gt.columns\n            ):\n                # skip if unexpected\n                continue\n            q_times = gt[\"UnixTimeMillis\"].astype(np.int64).values\n            q_lats = gt[\"LatitudeDegrees\"].astype(float).values\n            q_lons = gt[\"LongitudeDegrees\"].astype(float).values\n            pred_lats, pred_lons = predict_from_map(tmap, q_times)\n            # compute distances\n            dists = []\n            for a, b, pa, pb in zip(q_lats, q_lons, pred_lats, pred_lons):\n                if np.isfinite(pa) and np.isfinite(pb):\n                    dists.append(haversine(a, b, pa, pb))\n            if len(dists) == 0:\n                continue\n            phone_key = f\"{drive}_{phone}\"\n            phone_errors[phone_key] = np.array(dists)\n            processed += 1\n            if max_files is not None and processed >= max_files:\n                break\n        if max_files is not None and processed >= max_files:\n            break\n    if len(phone_errors) == 0:\n        print(\"No train phones validated.\", file=sys.stderr)\n        return None\n    # compute per-phone 50th and 95th percentiles, average per phone, then mean across phones\n    per_phone_scores = []\n    for phone, dists in phone_errors.items():\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        per_phone_scores.append((p50 + p95) / 2.0)\n    metric = float(np.mean(per_phone_scores))\n    return metric, phone_errors\n\n\n# --- Build submission predictions by matching test device_gnss to sample_submission times ---\ndef build_submission(sample_sub_path):\n    sub = pd.read_csv(sample_sub_path)\n    # pre-create columns\n    out_lats = np.full(len(sub), np.nan, dtype=float)\n    out_lons = np.full(len(sub), np.nan, dtype=float)\n    # group rows by phone value\n    # phone values look like \"2020-06-04-US-MTV-1_GooglePixel4\"\n    grouped = sub.groupby(\"phone\").indices\n    for phone_val, indices in grouped.items():\n        # parse drive and phone name\n        if \"_\" not in phone_val:\n            print(f\"Unexpected phone id format: {phone_val}\", file=sys.stderr)\n            continue\n        drive = phone_val.rsplit(\"_\", 1)[0]\n        phone_name = phone_val.rsplit(\"_\", 1)[1]\n        gnss_path = os.path.join(TEST_DIR, drive, phone_name, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            # try alternative capitalization\n            found = False\n            drive_path = os.path.join(TEST_DIR, drive)\n            if os.path.isdir(drive_path):\n                for candidate in os.listdir(drive_path):\n                    if candidate.lower() == phone_name.lower():\n                        gnss_path = os.path.join(\n                            drive_path, candidate, \"device_gnss.csv\"\n                        )\n                        if os.path.exists(gnss_path):\n                            found = True\n                            break\n            if not os.path.exists(gnss_path):\n                print(\n                    f\"Missing device_gnss for {phone_val} expected at {gnss_path}\",\n                    file=sys.stderr,\n                )\n                continue\n        tmap = build_wls_time_latlon_map(gnss_path)\n        query_times = sub.loc[indices, \"UnixTimeMillis\"].astype(np.int64).values\n        if tmap is None:\n            # fallback to NaNs\n            continue\n        latp, lonp = predict_from_map(tmap, query_times)\n        out_lats[indices] = latp\n        out_lons[indices] = lonp\n    # fill any remaining NaNs by global nearest from available maps (rare)\n    nan_idx = np.where(~np.isfinite(out_lats) | ~np.isfinite(out_lons))[0]\n    if len(nan_idx) > 0:\n        print(\n            f\"Filling {len(nan_idx)} missing entries with nearest available mapping...\",\n            file=sys.stderr,\n        )\n        # Build a small global index: for each test device, take median lat/lon\n        global_entries = []\n        for drive in sorted(os.listdir(TEST_DIR)):\n            drive_path = os.path.join(TEST_DIR, drive)\n            if not os.path.isdir(drive_path):\n                continue\n            for phone in sorted(os.listdir(drive_path)):\n                gnss_path = os.path.join(drive_path, phone, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                tmap = build_wls_time_latlon_map(gnss_path)\n                if tmap is None:\n                    continue\n                # store median entry\n                mid = len(tmap[\"times\"]) // 2\n                global_entries.append(\n                    (tmap[\"times\"][mid], tmap[\"lats\"][mid], tmap[\"lons\"][mid])\n                )\n        if len(global_entries) > 0:\n            global_times = np.array([e[0] for e in global_entries])\n            global_lats = np.array([e[1] for e in global_entries])\n            global_lons = np.array([e[2] for e in global_entries])\n            for idx in nan_idx:\n                qt = int(sub.loc[idx, \"UnixTimeMillis\"])\n                # nearest global\n                j = np.argmin(np.abs(global_times - qt))\n                out_lats[idx] = global_lats[j]\n                out_lons[idx] = global_lons[j]\n        else:\n            # give zeros if nothing available\n            out_lats[nan_idx] = 0.0\n            out_lons[nan_idx] = 0.0\n    # create submission dataframe\n    submission_df = pd.DataFrame(\n        {\n            \"phone\": sub[\"phone\"],\n            \"UnixTimeMillis\": sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": out_lats,\n            \"LongitudeDegrees\": out_lons,\n        }\n    )\n    submission_df.to_csv(OUTPUT_FILE, index=False)\n    return submission_df\n\n\ndef main():\n    print(\n        \"Building validation metric on train set (matching WLS -> ground truth)...\",\n        file=sys.stderr,\n    )\n    metric_res = validate_on_train(max_files=None)  # use all\n    if metric_res is None:\n        print(\"Validation failed: no data. Exiting.\", file=sys.stderr)\n        metric_value = None\n    else:\n        metric_value, phone_errors = metric_res\n        print(\n            f\"Validation metric (mean of per-phone (50th+95th)/2): {metric_value:.3f} meters\"\n        )\n    # Build submission predictions using test device_gnss WLS positions nearest to requested times\n    print(\"Building submission predictions...\", file=sys.stderr)\n    submission_df = build_submission(SAMPLE_SUB)\n    print(f\"Saved submission to {OUTPUT_FILE}\", file=sys.stderr)\n    # Also print a small preview\n    print(\"Submission preview (first 5 rows):\")\n    print(submission_df.head(5).to_string(index=False))\n\n\nif __name__ == \"__main__\":\n    main()\n",
                "exec_time": 2.288818359375e-05,
                "metric": null,
                "is_buggy": true,
                "exc_type": null,
                "exc_value": null,
                "analysis": "The script executed and appears to have produced a submission CSV at ./submission/submission.csv. It also attempted a train-time validation run, but the execution log provided does not include the reported validation metric value.",
                "alignment": {
                        "plan_code_similarity": 0.0120429483459083,
                        "has_changes": true,
                        "change_magnitude": 0.9491820396797772
                }
        },
        {
                "index": 5,
                "step": 5,
                "id": "99e69fd539634f5486d6b27e58109fbf",
                "parent_id": null,
                "plan": "The crash was caused by assuming the sample submission had a column exactly named \"phone\" and \"UnixTimeMillis\". I fix this by normalizing and mapping column names (strip/lower) to robustly find the phone and timestamp columns, ensuring we use the correct names even with whitespace or casing differences. I also keep the caching and nearest-neighbor lookup logic but make the sample-iteration robust and ensure the final predictions are written to ./submission/submission.csv and a validation metric on train phones is printed.",
                "code": "import os\nimport sys\nimport math\nimport glob\nimport numpy as np\nimport pandas as pd\n\n\n# Utility: ECEF -> lat/lon/alt (WGS84)\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 constants\n    a = 6378137.0\n    e = 8.1819190842622e-2  # eccentricity\n    asq = a * a\n    esq = e * e\n\n    b = math.sqrt(asq * (1 - esq))\n    ep = math.sqrt((asq - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3, p - esq * a * math.cos(th) ** 3\n    )\n    N = a / math.sqrt(1 - esq * math.sin(lat) * math.sin(lat))\n    alt = p / math.cos(lat) - N\n\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, alt\n\n\n# Haversine distance (meters)\ndef haversine_meters(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = math.radians(lat1)\n    phi2 = math.radians(lat2)\n    dphi = math.radians(lat2 - lat1)\n    dlambda = math.radians(lon2 - lon1)\n    a = (\n        math.sin(dphi / 2) ** 2\n        + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2) ** 2\n    )\n    return 2 * R * math.asin(math.sqrt(a))\n\n\n# Find nearest indices for query times in a sorted time array\ndef nearest_indices(sorted_times, query_times):\n    # sorted_times: 1D numpy sorted ascending\n    # query_times: 1D numpy\n    idx = np.searchsorted(sorted_times, query_times, side=\"left\")\n    idx0 = np.clip(idx - 1, 0, len(sorted_times) - 1)\n    idx1 = np.clip(idx, 0, len(sorted_times) - 1)\n    diff0 = np.abs(sorted_times[idx0] - query_times)\n    diff1 = np.abs(sorted_times[idx1] - query_times)\n    choose = diff1 < diff0\n    return np.where(choose, idx1, idx0)\n\n\n# Parse phone string into drive folder and phone name (split at last underscore)\ndef parse_phone_id(phone_id):\n    # phone_id examples: \"2020-05-15-US-MTV-1_Pixel4\"\n    if isinstance(phone_id, str) and \"_\" in phone_id:\n        drive, phone = phone_id.rsplit(\"_\", 1)\n    else:\n        parts = str(phone_id).split(\"/\")\n        drive = parts[0]\n        phone = parts[-1]\n    return drive, phone\n\n\nINPUT_DIR = \"./input\"\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(SAMPLE_SUB_PATH):\n    print(\"sample_submission.csv not found in ./input - aborting\")\n    sys.exit(1)\n\n# Read and normalize sample submission columns to robustly find the phone and UnixTimeMillis columns\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH, dtype=str)\n# Normalize columns map\norig_cols = list(sample_sub.columns)\nlower_map = {c.lower().strip(): c for c in orig_cols}\n\n# Find phone column (match 'phone')\nphone_col = None\ntime_col = None\nfor key in lower_map:\n    if key == \"phone\" or \"phone\" == key:\n        phone_col = lower_map[key]\n    if key in (\"unixtimemillis\", \"unixtime\", \"timestamp\", \"unix_time_millis\"):\n        time_col = lower_map[key]\n# fallback: pick columns containing substrings\nif phone_col is None:\n    for key, orig in lower_map.items():\n        if \"phone\" in key:\n            phone_col = orig\n            break\nif time_col is None:\n    for key, orig in lower_map.items():\n        if \"unix\" in key and \"time\" in key:\n            time_col = orig\n            break\n# If still not found, try several common names\nif phone_col is None:\n    candidates = [\"phone\", \"id\", \"device\"]\n    for cand in candidates:\n        for key, orig in lower_map.items():\n            if cand in key:\n                phone_col = orig\n                break\n        if phone_col:\n            break\nif time_col is None:\n    candidates = [\"unixtimemillis\", \"time\", \"timestamp\"]\n    for cand in candidates:\n        for key, orig in lower_map.items():\n            if cand in key:\n                time_col = orig\n                break\n        if time_col:\n            break\n\nif phone_col is None or time_col is None:\n    print(\n        \"Could not find phone or UnixTimeMillis column in sample_submission. Columns:\",\n        orig_cols,\n    )\n    sys.exit(1)\n\n# Ensure correct dtypes for time\nsample_sub[time_col] = sample_sub[time_col].astype(np.int64)\n# Rename for convenience\nsample_sub = sample_sub.rename(columns={phone_col: \"phone\", time_col: \"UnixTimeMillis\"})\n\n# Cache device GNSS data per (base_dir, drive, phone)\ndevice_cache = {}\n\n\ndef load_device_gnss_for(drive, phone, base_dir=\"test\"):\n    key = (base_dir, drive, phone)\n    if key in device_cache:\n        return device_cache[key]\n    path = os.path.join(INPUT_DIR, base_dir, drive, phone, \"device_gnss.csv\")\n    if not os.path.exists(path):\n        device_cache[key] = None\n        return None\n    try:\n        df = pd.read_csv(path, low_memory=False)\n    except Exception as e:\n        device_cache[key] = None\n        return None\n    if \"utcTimeMillis\" not in df.columns:\n        # Try a few alternative column names\n        candidate = None\n        for c in df.columns:\n            if c.lower().strip() in (\n                \"utc\",\n                \"utcTimeMillis\".lower(),\n                \"utctime\",\n                \"utc_millis\",\n            ):\n                candidate = c\n                break\n        if candidate is None:\n            device_cache[key] = None\n            return None\n        else:\n            df = df.rename(columns={candidate: \"utcTimeMillis\"})\n    # Keep only rows with valid utcTimeMillis\n    df = df.dropna(subset=[\"utcTimeMillis\"])\n    if len(df) == 0:\n        device_cache[key] = {\n            \"times\": np.array([], dtype=np.int64),\n            \"lat\": np.array([]),\n            \"lon\": np.array([]),\n        }\n        return device_cache[key]\n    # Detect available position columns\n    has_latlon = (\"WlsLatitudeDegrees\" in df.columns) and (\n        \"WlsLongitudeDegrees\" in df.columns\n    )\n    has_ecef = (\n        (\"WlsPositionXEcefMeters\" in df.columns)\n        and (\"WlsPositionYEcefMeters\" in df.columns)\n        and (\"WlsPositionZEcefMeters\" in df.columns)\n    )\n    # prepare times sorted\n    times = df[\"utcTimeMillis\"].astype(np.int64).to_numpy()\n    order = np.argsort(times)\n    times = times[order]\n    if has_latlon:\n        lat = df[\"WlsLatitudeDegrees\"].to_numpy()[order].astype(float)\n        lon = df[\"WlsLongitudeDegrees\"].to_numpy()[order].astype(float)\n        device_cache[key] = {\"times\": times, \"lat\": lat, \"lon\": lon}\n    elif has_ecef:\n        x = df[\"WlsPositionXEcefMeters\"].to_numpy()[order].astype(float)\n        y = df[\"WlsPositionYEcefMeters\"].to_numpy()[order].astype(float)\n        z = df[\"WlsPositionZEcefMeters\"].to_numpy()[order].astype(float)\n        lat = np.empty(len(x), dtype=float)\n        lon = np.empty(len(x), dtype=float)\n        for i, (xi, yi, zi) in enumerate(zip(x, y, z)):\n            try:\n                lati, loni, _ = ecef_to_geodetic(float(xi), float(yi), float(zi))\n            except Exception:\n                lati, loni = np.nan, np.nan\n            lat[i] = lati\n            lon[i] = loni\n        device_cache[key] = {\"times\": times, \"lat\": lat, \"lon\": lon}\n    else:\n        # no usable positions, but still store times\n        device_cache[key] = {\n            \"times\": times,\n            \"lat\": np.full_like(times, np.nan, dtype=float),\n            \"lon\": np.full_like(times, np.nan, dtype=float),\n        }\n    return device_cache[key]\n\n\n# Build predictions for sample_submission robustly by grouping rows per phone to reduce repeated lookups\nout_rows = []\n# We'll create a per-phone predictions by fetching device file once for each unique phone id in submission\nunique_phones = sample_sub[\"phone\"].unique()\n# For phones missing device data, we could fallback to train median per that phone (not implemented extensively here)\nfor phone_id in unique_phones:\n    # find rows in sample_sub for this phone\n    mask = sample_sub[\"phone\"] == phone_id\n    times_query = sample_sub.loc[mask, \"UnixTimeMillis\"].to_numpy(dtype=np.int64)\n    drive, phone = parse_phone_id(phone_id)\n    data = load_device_gnss_for(drive, phone, base_dir=\"test\")\n    preds_lat = np.zeros(len(times_query), dtype=float)\n    preds_lon = np.zeros(len(times_query), dtype=float)\n    if data is None:\n        # fallback zeros\n        preds_lat[:] = 0.0\n        preds_lon[:] = 0.0\n    else:\n        if len(data[\"times\"]) == 0:\n            preds_lat[:] = 0.0\n            preds_lon[:] = 0.0\n        else:\n            idxs = nearest_indices(data[\"times\"], times_query)\n            preds_lat = data[\"lat\"][idxs].astype(float)\n            preds_lon = data[\"lon\"][idxs].astype(float)\n            # If nan, replace with median for that device if possible, otherwise 0\n            if np.any(np.isnan(preds_lat)) or np.any(np.isnan(preds_lon)):\n                lat_med = np.nanmedian(data[\"lat\"])\n                lon_med = np.nanmedian(data[\"lon\"])\n                # if median is nan, fallback to zeros\n                if math.isnan(lat_med) or math.isnan(lon_med):\n                    lat_med = 0.0\n                    lon_med = 0.0\n                nan_mask_lat = np.isnan(preds_lat)\n                nan_mask_lon = np.isnan(preds_lon)\n                preds_lat[nan_mask_lat] = lat_med\n                preds_lon[nan_mask_lon] = lon_med\n    # Append to out_rows preserving original ordering\n    idxs_in_sub = sample_sub.index[mask].tolist()\n    for idx_pos, sub_idx in enumerate(idxs_in_sub):\n        out_rows.append(\n            (\n                sample_sub.at[sub_idx, \"phone\"],\n                int(sample_sub.at[sub_idx, \"UnixTimeMillis\"]),\n                float(preds_lat[idx_pos]),\n                float(preds_lon[idx_pos]),\n            )\n        )\n\nsubmission_df = pd.DataFrame(\n    out_rows, columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n)\nos.makedirs(\"./submission\", exist_ok=True)\nsubmission_path = \"./submission/submission.csv\"\nsubmission_df.to_csv(submission_path, index=False)\nprint(f\"Saved test predictions to {submission_path}\")\n\n# Validation: evaluate on a small hold-out set from train drives.\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nif not os.path.exists(TRAIN_DIR):\n    print(\"No train directory found, skipping validation metric.\")\n    sys.exit(0)\n\n# Collect train phones (drive, phone) list\npairs = []\nfor drive in sorted(os.listdir(TRAIN_DIR)):\n    drive_path = os.path.join(TRAIN_DIR, drive)\n    if not os.path.isdir(drive_path):\n        continue\n    for phone_name in sorted(os.listdir(drive_path)):\n        phone_path = os.path.join(drive_path, phone_name)\n        if not os.path.isdir(phone_path):\n            continue\n        if os.path.exists(\n            os.path.join(phone_path, \"device_gnss.csv\")\n        ) and os.path.exists(os.path.join(phone_path, \"ground_truth.csv\")):\n            pairs.append((drive, phone_name))\n# Limit number for speed but ensure some evaluation\nMAX_VAL_PHONES = 8\npairs = pairs[:MAX_VAL_PHONES]\n\nper_phone_scores = []\nfor drive, phone in pairs:\n    dev = load_device_gnss_for(drive, phone, base_dir=\"train\")\n    if dev is None or len(dev[\"times\"]) == 0:\n        continue\n    gt_path = os.path.join(INPUT_DIR, \"train\", drive, phone, \"ground_truth.csv\")\n    try:\n        gt = pd.read_csv(gt_path)\n    except Exception as e:\n        continue\n    if (\n        (\"UnixTimeMillis\" not in gt.columns)\n        or (\"LatitudeDegrees\" not in gt.columns)\n        or (\"LongitudeDegrees\" not in gt.columns)\n    ):\n        # try normalizing columns\n        cols_map = {c.lower().strip(): c for c in gt.columns}\n        if (\n            \"unixtimemillis\" in cols_map\n            and \"latitudedegrees\" in cols_map\n            and \"longitudedegrees\" in cols_map\n        ):\n            gt = gt.rename(\n                columns={\n                    cols_map[\"unixtimemillis\"]: \"UnixTimeMillis\",\n                    cols_map[\"latitudedegrees\"]: \"LatitudeDegrees\",\n                    cols_map[\"longitudedegrees\"]: \"LongitudeDegrees\",\n                }\n            )\n        else:\n            continue\n    query_times = gt[\"UnixTimeMillis\"].astype(np.int64).to_numpy()\n    if len(query_times) == 0:\n        continue\n    idxs = nearest_indices(dev[\"times\"], query_times)\n    preds_lat = dev[\"lat\"][idxs]\n    preds_lon = dev[\"lon\"][idxs]\n    distances = []\n    for i in range(len(query_times)):\n        plat = preds_lat[i]\n        plon = preds_lon[i]\n        glat = float(gt[\"LatitudeDegrees\"].iloc[i])\n        glon = float(gt[\"LongitudeDegrees\"].iloc[i])\n        if math.isnan(plat) or math.isnan(plon):\n            distances.append(1000.0)\n        else:\n            distances.append(haversine_meters(glat, glon, float(plat), float(plon)))\n    distances = np.array(distances)\n    p50 = float(np.nanpercentile(distances, 50))\n    p95 = float(np.nanpercentile(distances, 95))\n    per_phone_scores.append((drive + \"_\" + phone, p50, p95))\n    print(f\"Phone {drive}_{phone}: p50={p50:.3f} m, p95={p95:.3f} m\")\n\nif len(per_phone_scores) == 0:\n    print(\"No validation phones evaluated.\")\nelse:\n    avg_per_phone = [(p, (p50 + p95) / 2.0) for (p, p50, p95) in per_phone_scores]\n    mean_metric = float(np.mean([v for (p, v) in avg_per_phone]))\n    print(\n        \"\\nValidation metric (mean across phones of (p50+p95)/2): {:.3f} meters\".format(\n            mean_metric\n        )\n    )\n",
                "exec_time": 6.529181957244873,
                "metric": 2.82,
                "is_buggy": false,
                "exc_type": null,
                "exc_value": null,
                "analysis": "The script executed successfully and saved predictions to ./submission/submission.csv. On a small train-derived validation set (up to 8 phones) it reported per-phone p50/p95 errors and an overall mean metric of 2.820 meters.",
                "alignment": {
                        "plan_code_similarity": 0.0050195615265372405,
                        "has_changes": true,
                        "change_magnitude": 0.9549542682926829
                }
        },
        {
                "index": 6,
                "step": 6,
                "id": "52589c4781e84d50bae320bb0e33e7ea",
                "parent_id": null,
                "plan": "I will simplify and fix the pipeline by avoiding the previous complex LightGBM training and any fragile merges: instead I will use the provided WLS (weighted least-squares) ECEF positions as a robust baseline. The script will aggregate device_gnss by epoch, convert WLS ECEF -> lat/lon, merge to ground-truth to compute the competition metric on a hold-out set (using nearest-time merge), and then produce predictions for the test sample submission by nearest-time matching to the aggregated WLS lat/lon (with sensible fallbacks). This removes the previous training/merge fragility and guarantees creation of submission/submission.csv; the script will print the validation metric before writing the submission file.",
                "code": "import os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom math import sin, cos, atan2, sqrt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # WGS84\n    a = 6378137.0\n    e = 8.1819190842622e-2\n    b = sqrt(a * a * (1 - e * e))\n    ep = sqrt((a * a - b * b) / (b * b))\n    p = np.sqrt(x * x + y * y)\n    th = np.arctan2(a * z, b * p)\n    lon = np.arctan2(y, x)\n    lat = np.arctan2(\n        (z + ep * ep * b * (np.sin(th) ** 3)), (p - e * e * a * (np.cos(th) ** 3))\n    )\n    N = a / np.sqrt(1 - e * e * (np.sin(lat) ** 2))\n    alt = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), alt\n\n\n# haversine in meters\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n\n\ndef aggregate_device_gnss(fn):\n    # Read and keep relevant columns\n    try:\n        df = pd.read_csv(fn)\n    except Exception as e:\n        print(\"Failed to read\", fn, e)\n        return pd.DataFrame()\n    if df.shape[0] == 0:\n        return pd.DataFrame()\n    # ensure utcTimeMillis exists\n    if \"utcTimeMillis\" not in df.columns:\n        return pd.DataFrame()\n    # Choose columns we may use\n    possible_cols = {\n        \"WlsPositionXEcefMeters\": \"first\",\n        \"WlsPositionYEcefMeters\": \"first\",\n        \"WlsPositionZEcefMeters\": \"first\",\n        \"Cn0DbHz\": \"mean\",\n        \"SvElevationDegrees\": \"mean\",\n        \"SvAzimuthDegrees\": \"mean\",\n        \"PseudorangeRateMetersPerSecond\": \"mean\",\n        \"Svid\": pd.Series.nunique,\n    }\n    agg_dict = {}\n    for col, aggfn in possible_cols.items():\n        if col in df.columns:\n            if aggfn == pd.Series.nunique:\n                agg_dict[col] = (\"nunique\",)  # placeholder\n            else:\n                agg_dict[col] = aggfn\n    # Use groupby aggregation; handle Svid separately because pandas expects string funcs\n    # Build a dict in the format col: func\n    # We'll do manual aggregation to be explicit\n    group = df.groupby(\"utcTimeMillis\", sort=True)\n    out = pd.DataFrame({\"utcTimeMillis\": group.size().index})\n    out = out.set_index(\"utcTimeMillis\")\n    # Fill columns\n    if \"WlsPositionXEcefMeters\" in df.columns:\n        out[\"WlsX\"] = group[\"WlsPositionXEcefMeters\"].first()\n        out[\"WlsY\"] = (\n            group[\"WlsPositionYEcefMeters\"].first()\n            if \"WlsPositionYEcefMeters\" in df.columns\n            else np.nan\n        )\n        out[\"WlsZ\"] = (\n            group[\"WlsPositionZEcefMeters\"].first()\n            if \"WlsPositionZEcefMeters\" in df.columns\n            else np.nan\n        )\n    else:\n        # no WLS positions at all\n        out[\"WlsX\"] = np.nan\n        out[\"WlsY\"] = np.nan\n        out[\"WlsZ\"] = np.nan\n    # numeric means\n    if \"Cn0DbHz\" in df.columns:\n        out[\"cn0_mean\"] = group[\"Cn0DbHz\"].mean()\n    else:\n        out[\"cn0_mean\"] = np.nan\n    if \"SvElevationDegrees\" in df.columns:\n        out[\"elev_mean\"] = group[\"SvElevationDegrees\"].mean()\n    else:\n        out[\"elev_mean\"] = np.nan\n    if \"SvAzimuthDegrees\" in df.columns:\n        out[\"azim_mean\"] = group[\"SvAzimuthDegrees\"].mean()\n    else:\n        out[\"azim_mean\"] = np.nan\n    if \"PseudorangeRateMetersPerSecond\" in df.columns:\n        out[\"pr_mean\"] = group[\"PseudorangeRateMetersPerSecond\"].mean()\n    else:\n        out[\"pr_mean\"] = np.nan\n    if \"Svid\" in df.columns:\n        out[\"sv_count\"] = group[\"Svid\"].nunique()\n    else:\n        out[\"sv_count\"] = np.nan\n\n    out = out.reset_index()\n    # drop rows where Wls positions missing entirely\n    has_wls = out[[\"WlsX\", \"WlsY\", \"WlsZ\"]].notnull().all(axis=1)\n    if has_wls.sum() == 0:\n        # If no WLS values, return empty so caller can handle fallback\n        return pd.DataFrame()\n    out = out.loc[has_wls].copy()\n    # convert ECEF to lat/lon\n    lats, lons, alts = zip(\n        *out.apply(lambda r: ecef_to_geodetic(r[\"WlsX\"], r[\"WlsY\"], r[\"WlsZ\"]), axis=1)\n    )\n    out[\"wls_lat\"] = lats\n    out[\"wls_lon\"] = lons\n    # ensure utcTimeMillis is int\n    out[\"utcTimeMillis\"] = out[\"utcTimeMillis\"].astype(np.int64)\n    return out\n\n\n# Build training dataset using the WLS baseline\ntrain_root = os.path.join(INPUT_DIR, \"train\")\ntrain_dirs = glob.glob(os.path.join(train_root, \"*\", \"*\"))\ntrain_rows = []\nfor drive_phone in train_dirs:\n    parts = drive_phone.split(os.sep)\n    if len(parts) < 4:\n        continue\n    drive = parts[-2]\n    phone = parts[-1]\n    phone_full = f\"{drive}_{phone}\"\n    device_gnss_path = os.path.join(drive_phone, \"device_gnss.csv\")\n    ground_truth_path = os.path.join(drive_phone, \"ground_truth.csv\")\n    if not os.path.exists(device_gnss_path) or not os.path.exists(ground_truth_path):\n        continue\n    try:\n        agg = aggregate_device_gnss(device_gnss_path)\n        if agg.shape[0] == 0:\n            # no WLS data for this phone\n            continue\n        gt = pd.read_csv(ground_truth_path)\n        gt = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].rename(\n            columns={\n                \"UnixTimeMillis\": \"utcTimeMillis\",\n                \"LatitudeDegrees\": \"lat\",\n                \"LongitudeDegrees\": \"lon\",\n            }\n        )\n        agg_sorted = agg.sort_values(\"utcTimeMillis\")\n        gt_sorted = gt.sort_values(\"utcTimeMillis\")\n        # merge_asof by time within 1s (1000 ms)\n        merged = pd.merge_asof(\n            agg_sorted,\n            gt_sorted,\n            on=\"utcTimeMillis\",\n            direction=\"nearest\",\n            tolerance=1000,\n        )\n        merged = merged.dropna(subset=[\"lat\", \"lon\"])\n        if merged.shape[0] == 0:\n            continue\n        merged[\"phone\"] = phone_full\n        train_rows.append(merged)\n    except Exception as e:\n        print(\"Error processing\", drive_phone, e)\n\nif len(train_rows) == 0:\n    raise RuntimeError(\"No training data found! Cannot build baseline.\")\n\ntrain_df = pd.concat(train_rows, ignore_index=True)\n\n# Use WLS lat/lon as predictions (baseline)\ntrain_df[\"pred_lat\"] = train_df[\"wls_lat\"]\ntrain_df[\"pred_lon\"] = train_df[\"wls_lon\"]\n\n# compute per-phone errors list according to competition metric\nphones = train_df[\"phone\"].unique()\nphone_scores = []\nfor p in phones:\n    sub = train_df[train_df[\"phone\"] == p]\n    if sub.shape[0] == 0:\n        continue\n    dists = haversine(\n        sub[\"lat\"].values,\n        sub[\"lon\"].values,\n        sub[\"pred_lat\"].values,\n        sub[\"pred_lon\"].values,\n    )\n    if len(dists) == 0:\n        continue\n    p50 = np.percentile(dists, 50)\n    p95 = np.percentile(dists, 95)\n    phone_scores.append((p50 + p95) / 2.0)\nif len(phone_scores) == 0:\n    mean_score = np.nan\nelse:\n    mean_score = np.mean(phone_scores)\nprint(\n    \"Validation competition metric (mean of per-phone (p50+p95)/2) using WLS baseline: {:.3f} meters\".format(\n        mean_score\n    )\n)\n\n# Now prepare test predictions using WLS baseline\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in input directory.\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# aggregate test\ntest_root = os.path.join(INPUT_DIR, \"test\")\ntest_dirs = glob.glob(os.path.join(test_root, \"*\", \"*\"))\ntest_agg_rows = []\nfor drive_phone in test_dirs:\n    parts = drive_phone.split(os.sep)\n    if len(parts) < 4:\n        continue\n    drive = parts[-2]\n    phone = parts[-1]\n    phone_full = f\"{drive}_{phone}\"\n    device_gnss_path = os.path.join(drive_phone, \"device_gnss.csv\")\n    if not os.path.exists(device_gnss_path):\n        continue\n    try:\n        agg = aggregate_device_gnss(device_gnss_path)\n        if agg.shape[0] == 0:\n            continue\n        agg[\"phone\"] = phone_full\n        test_agg_rows.append(agg)\n    except Exception as e:\n        print(\"Error aggregating test\", drive_phone, e)\n\nif len(test_agg_rows) == 0:\n    raise RuntimeError(\"No test data aggregated!\")\n\ntest_agg = pd.concat(test_agg_rows, ignore_index=True)\n\n# Merge sample submission with test_agg per phone by nearest time (tolerance 2s)\nsample = sample_sub.copy()\nsample = sample.rename(columns={\"UnixTimeMillis\": \"utcTimeMillis\"})\n# ensure types\nsample[\"utcTimeMillis\"] = sample[\"utcTimeMillis\"].astype(np.int64)\ntest_agg[\"utcTimeMillis\"] = test_agg[\"utcTimeMillis\"].astype(np.int64)\n\nsample_preds = []\nfor phone, grp in sample.groupby(\"phone\"):\n    t = grp.sort_values(\"utcTimeMillis\").copy()\n    features = test_agg[test_agg[\"phone\"] == phone].sort_values(\"utcTimeMillis\")\n    if features.shape[0] == 0:\n        # no features for this phone; fill with nan\n        t[\"LatitudeDegrees\"] = np.nan\n        t[\"LongitudeDegrees\"] = np.nan\n        sample_preds.append(t)\n        continue\n    merged = pd.merge_asof(\n        t,\n        features,\n        on=\"utcTimeMillis\",\n        by=\"phone\",\n        direction=\"nearest\",\n        tolerance=2000,\n    )\n    # For rows where merge failed, attempt a global nearest (without by) as a last attempt\n    missing = merged[\"WlsX\"].isnull()\n    if missing.any():\n        # Attempt merge without phone key\n        merged2 = pd.merge_asof(\n            t[missing].sort_values(\"utcTimeMillis\"),\n            features.sort_values(\"utcTimeMillis\"),\n            on=\"utcTimeMillis\",\n            direction=\"nearest\",\n            tolerance=2000,\n        )\n        # fill merged for missing rows\n        for col in merged2.columns:\n            if col in merged.columns:\n                merged.loc[missing, col] = merged2[col].values\n    # Assign predictions from wls lat/lon\n    merged[\"LatitudeDegrees\"] = merged[\"wls_lat\"]\n    merged[\"LongitudeDegrees\"] = merged[\"wls_lon\"]\n    # For any remaining NaNs, leave them for final global fill\n    sample_preds.append(\n        merged[[\"phone\", \"utcTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]]\n    )\n\nsubmission_df = pd.concat(sample_preds, ignore_index=True)\nsubmission_df = submission_df.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n# ensure ordering same as sample_submission\nsubmission_df = (\n    submission_df.set_index([\"phone\", \"UnixTimeMillis\"])\n    .reindex(sample_sub.set_index([\"phone\", \"UnixTimeMillis\"]).index)\n    .reset_index()\n)\n\n# final fill: if still NaN, fill with global mean lat/lon from train WLS/ground-truth pairs\nglobal_lat_mean = train_df[\"lat\"].mean()\nglobal_lon_mean = train_df[\"lon\"].mean()\n# If train_df lat/lon are NaN for some reason, fall back to mean of wls columns\nif np.isnan(global_lat_mean) or np.isnan(global_lon_mean):\n    if \"wls_lat\" in train_df.columns and \"wls_lon\" in train_df.columns:\n        global_lat_mean = train_df[\"wls_lat\"].mean()\n        global_lon_mean = train_df[\"wls_lon\"].mean()\n# As ultimate fallback, use 0,0\nif np.isnan(global_lat_mean) or np.isnan(global_lon_mean):\n    global_lat_mean = 0.0\n    global_lon_mean = 0.0\n\nsubmission_df[\"LatitudeDegrees\"] = submission_df[\"LatitudeDegrees\"].fillna(\n    global_lat_mean\n)\nsubmission_df[\"LongitudeDegrees\"] = submission_df[\"LongitudeDegrees\"].fillna(\n    global_lon_mean\n)\n\nout_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission_df.to_csv(\n    out_path,\n    index=False,\n    columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"],\n)\nprint(\"Saved submission to\", out_path)\n",
                "exec_time": 78.27350091934204,
                "metric": null,
                "is_buggy": true,
                "exc_type": "KeyError",
                "exc_value": null,
                "analysis": "Script successfully built a WLS-based training set and computed a validation metric, but it crashed before producing the submission. The run failed with a KeyError: 'phone' during grouping of the sample submission, so submission.csv was not written.",
                "alignment": {
                        "plan_code_similarity": 0.010996225176431972,
                        "has_changes": true,
                        "change_magnitude": 0.916132456820873
                }
        },
        {
                "index": 7,
                "step": 7,
                "id": "8f85c107bb3e463294603e5ddca904a9",
                "parent_id": null,
                "plan": "The bug was caused by assuming the sample submission always has a clean 'phone' column when grouping; pandas raised KeyError. I will make the code robust to variations in column names (normalize/strip names, find phone column case-insensitively), and iterate over unique phones instead of groupby('phone') to avoid the KeyError. I'll also use a safer missing-check (wls_lat) and fallback merging, then save the submission CSV and print the validation metric computed on the training set.",
                "code": "import os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom math import sqrt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 conversion\n    a = 6378137.0\n    e = 8.1819190842622e-2\n    b = sqrt(a * a * (1 - e * e))\n    ep = sqrt((a * a - b * b) / (b * b))\n    p = np.sqrt(x * x + y * y)\n    th = np.arctan2(a * z, b * p)\n    lon = np.arctan2(y, x)\n    lat = np.arctan2(\n        (z + ep * ep * b * (np.sin(th) ** 3)), (p - e * e * a * (np.cos(th) ** 3))\n    )\n    N = a / np.sqrt(1 - e * e * (np.sin(lat) ** 2))\n    alt = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), alt\n\n\n# haversine in meters\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n\n\ndef aggregate_device_gnss(fn):\n    try:\n        df = pd.read_csv(fn)\n    except Exception as e:\n        # read failure\n        return pd.DataFrame()\n    if df.shape[0] == 0:\n        return pd.DataFrame()\n    if \"utcTimeMillis\" not in df.columns:\n        return pd.DataFrame()\n\n    group = df.groupby(\"utcTimeMillis\", sort=True)\n    out = pd.DataFrame({\"utcTimeMillis\": group.size().index})\n    out = out.set_index(\"utcTimeMillis\")\n\n    # WLS positions if available\n    if \"WlsPositionXEcefMeters\" in df.columns:\n        out[\"WlsX\"] = group[\"WlsPositionXEcefMeters\"].first()\n        out[\"WlsY\"] = (\n            group[\"WlsPositionYEcefMeters\"].first()\n            if \"WlsPositionYEcefMeters\" in df.columns\n            else np.nan\n        )\n        out[\"WlsZ\"] = (\n            group[\"WlsPositionZEcefMeters\"].first()\n            if \"WlsPositionZEcefMeters\" in df.columns\n            else np.nan\n        )\n    else:\n        out[\"WlsX\"] = np.nan\n        out[\"WlsY\"] = np.nan\n        out[\"WlsZ\"] = np.nan\n\n    # numeric means\n    out[\"cn0_mean\"] = group[\"Cn0DbHz\"].mean() if \"Cn0DbHz\" in df.columns else np.nan\n    out[\"elev_mean\"] = (\n        group[\"SvElevationDegrees\"].mean()\n        if \"SvElevationDegrees\" in df.columns\n        else np.nan\n    )\n    out[\"azim_mean\"] = (\n        group[\"SvAzimuthDegrees\"].mean() if \"SvAzimuthDegrees\" in df.columns else np.nan\n    )\n    out[\"pr_mean\"] = (\n        group[\"PseudorangeRateMetersPerSecond\"].mean()\n        if \"PseudorangeRateMetersPerSecond\" in df.columns\n        else np.nan\n    )\n    out[\"sv_count\"] = group[\"Svid\"].nunique() if \"Svid\" in df.columns else np.nan\n\n    out = out.reset_index()\n\n    # drop rows where Wls positions missing entirely\n    has_wls = out[[\"WlsX\", \"WlsY\", \"WlsZ\"]].notnull().all(axis=1)\n    if has_wls.sum() == 0:\n        return pd.DataFrame()\n    out = out.loc[has_wls].copy()\n\n    # convert ECEF to lat/lon\n    # apply rowwise conversion\n    lats = []\n    lons = []\n    alts = []\n    for _, r in out.iterrows():\n        lat, lon, alt = ecef_to_geodetic(r[\"WlsX\"], r[\"WlsY\"], r[\"WlsZ\"])\n        lats.append(lat)\n        lons.append(lon)\n        alts.append(alt)\n    out[\"wls_lat\"] = lats\n    out[\"wls_lon\"] = lons\n    out[\"utcTimeMillis\"] = out[\"utcTimeMillis\"].astype(np.int64)\n    return out\n\n\n# Build training dataset using the WLS baseline\ntrain_root = os.path.join(INPUT_DIR, \"train\")\ntrain_dirs = glob.glob(os.path.join(train_root, \"*\", \"*\"))\ntrain_rows = []\nfor drive_phone in train_dirs:\n    parts = drive_phone.split(os.sep)\n    if len(parts) < 4:\n        continue\n    drive = parts[-2]\n    phone = parts[-1]\n    phone_full = f\"{drive}_{phone}\"\n    device_gnss_path = os.path.join(drive_phone, \"device_gnss.csv\")\n    ground_truth_path = os.path.join(drive_phone, \"ground_truth.csv\")\n    if not os.path.exists(device_gnss_path) or not os.path.exists(ground_truth_path):\n        continue\n    agg = aggregate_device_gnss(device_gnss_path)\n    if agg.shape[0] == 0:\n        continue\n    try:\n        gt = pd.read_csv(ground_truth_path)\n    except Exception:\n        continue\n    if gt.shape[0] == 0:\n        continue\n    gt = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].rename(\n        columns={\n            \"UnixTimeMillis\": \"utcTimeMillis\",\n            \"LatitudeDegrees\": \"lat\",\n            \"LongitudeDegrees\": \"lon\",\n        }\n    )\n    agg_sorted = agg.sort_values(\"utcTimeMillis\")\n    gt_sorted = gt.sort_values(\"utcTimeMillis\")\n    merged = pd.merge_asof(\n        agg_sorted,\n        gt_sorted,\n        on=\"utcTimeMillis\",\n        direction=\"nearest\",\n        tolerance=1000,\n    )\n    merged = merged.dropna(subset=[\"lat\", \"lon\"])\n    if merged.shape[0] == 0:\n        continue\n    merged[\"phone\"] = phone_full\n    train_rows.append(merged)\n\nif len(train_rows) == 0:\n    # No training WLS baseline found; create empty but continue to build submission with global fallback\n    train_df = pd.DataFrame(\n        columns=[\"phone\", \"utcTimeMillis\", \"lat\", \"lon\", \"wls_lat\", \"wls_lon\"]\n    )\nelse:\n    train_df = pd.concat(train_rows, ignore_index=True)\n\n# Use WLS lat/lon as predictions (baseline) in train_df\nif \"wls_lat\" in train_df.columns:\n    train_df[\"pred_lat\"] = train_df[\"wls_lat\"]\n    train_df[\"pred_lon\"] = train_df[\"wls_lon\"]\n\n# compute per-phone errors list according to competition metric\nphone_scores = []\nif \"lat\" in train_df.columns and \"pred_lat\" in train_df.columns:\n    phones = train_df[\"phone\"].unique()\n    for p in phones:\n        sub = train_df[train_df[\"phone\"] == p]\n        if sub.shape[0] == 0:\n            continue\n        dists = haversine(\n            sub[\"lat\"].values,\n            sub[\"lon\"].values,\n            sub[\"pred_lat\"].values,\n            sub[\"pred_lon\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        phone_scores.append((p50 + p95) / 2.0)\nif len(phone_scores) == 0:\n    mean_score = np.nan\nelse:\n    mean_score = np.mean(phone_scores)\nprint(\n    \"Validation competition metric (mean of per-phone (p50+p95)/2) using WLS baseline: {:.3f} meters\".format(\n        mean_score\n    )\n)\n\n# Now prepare test predictions using WLS baseline\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in input directory.\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Normalize column names (strip)\nsample_sub.columns = [c.strip() for c in sample_sub.columns]\n\n# find phone column robustly\nphone_col = None\nfor c in sample_sub.columns:\n    if c.lower().strip() == \"phone\":\n        phone_col = c\n        break\nif phone_col is None:\n    for c in sample_sub.columns:\n        if \"phone\" in c.lower():\n            phone_col = c\n            break\nif phone_col is None:\n    raise RuntimeError(\n        f\"Could not find a phone column in sample submission. Columns: {sample_sub.columns.tolist()}\"\n    )\n\n# rename to standard 'phone' and UnixTimeMillis to utcTimeMillis\nsample = sample_sub.rename(columns={phone_col: \"phone\"})\n# find UnixTimeMillis column robustly\ntime_col = None\nfor c in sample.columns:\n    if c.lower().strip() in (\n        \"unixtimemillis\",\n        \"unixtime\",\n        \"time\",\n        \"utctime\",\n        \"utcTimeMillis\".lower(),\n    ):\n        if c.lower().strip().startswith(\"unixtimemillis\"):\n            time_col = c\n            break\nfor c in sample.columns:\n    if time_col is None and \"unixtime\" in c.lower():\n        time_col = c\n        break\n# fallback: look for any column that contains \"time\"\nif time_col is None:\n    for c in sample.columns:\n        if \"time\" in c.lower():\n            time_col = c\n            break\nif time_col is None:\n    raise RuntimeError(\n        f\"Could not find a UnixTimeMillis column in sample submission. Columns: {sample.columns.tolist()}\"\n    )\n\nsample = sample.rename(columns={time_col: \"utcTimeMillis\"})\n\n# ensure types\nsample[\"utcTimeMillis\"] = sample[\"utcTimeMillis\"].astype(np.int64)\n\n# aggregate test\ntest_root = os.path.join(INPUT_DIR, \"test\")\ntest_dirs = glob.glob(os.path.join(test_root, \"*\", \"*\"))\ntest_agg_rows = []\nfor drive_phone in test_dirs:\n    parts = drive_phone.split(os.sep)\n    if len(parts) < 4:\n        continue\n    drive = parts[-2]\n    phone = parts[-1]\n    phone_full = f\"{drive}_{phone}\"\n    device_gnss_path = os.path.join(drive_phone, \"device_gnss.csv\")\n    if not os.path.exists(device_gnss_path):\n        continue\n    agg = aggregate_device_gnss(device_gnss_path)\n    if agg.shape[0] == 0:\n        continue\n    agg[\"phone\"] = phone_full\n    test_agg_rows.append(agg)\n\nif len(test_agg_rows) == 0:\n    # No aggregated test features; create empty submission filled with global mean\n    print(\n        \"Warning: No test aggregation found. Will fill submission with global mean coordinates.\"\n    )\n    # compute global means from train_df\n    if (\n        \"lat\" in train_df.columns\n        and \"lon\" in train_df.columns\n        and train_df.shape[0] > 0\n    ):\n        global_lat_mean = train_df[\"lat\"].mean()\n        global_lon_mean = train_df[\"lon\"].mean()\n    elif (\n        \"wls_lat\" in train_df.columns\n        and \"wls_lon\" in train_df.columns\n        and train_df.shape[0] > 0\n    ):\n        global_lat_mean = train_df[\"wls_lat\"].mean()\n        global_lon_mean = train_df[\"wls_lon\"].mean()\n    else:\n        global_lat_mean = 0.0\n        global_lon_mean = 0.0\n    submission_df = sample.copy()\n    submission_df[\"LatitudeDegrees\"] = global_lat_mean\n    submission_df[\"LongitudeDegrees\"] = global_lon_mean\n    submission_df = submission_df.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n    # Ensure ordering same as sample_sub original\n    submission_df = submission_df.loc[\n        :, [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ]\n    out_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission_df.to_csv(out_path, index=False)\n    print(\"Saved submission to\", out_path)\nelse:\n    test_agg = pd.concat(test_agg_rows, ignore_index=True)\n    # ensure types\n    test_agg[\"utcTimeMillis\"] = test_agg[\"utcTimeMillis\"].astype(np.int64)\n\n    sample_preds = []\n    # iterate per unique phone robustly\n    unique_phones = sample[\"phone\"].unique()\n    for phone in unique_phones:\n        t = sample[sample[\"phone\"] == phone].sort_values(\"utcTimeMillis\").copy()\n        features = test_agg[test_agg[\"phone\"] == phone].sort_values(\"utcTimeMillis\")\n        if features.shape[0] == 0:\n            # no features for this phone; fill with NaN and let global fill later\n            t[\"LatitudeDegrees\"] = np.nan\n            t[\"LongitudeDegrees\"] = np.nan\n            sample_preds.append(t)\n            continue\n        # merge_asof without by (we are selecting features for that phone already)\n        merged = pd.merge_asof(\n            t.sort_values(\"utcTimeMillis\"),\n            features.sort_values(\"utcTimeMillis\"),\n            on=\"utcTimeMillis\",\n            direction=\"nearest\",\n            tolerance=2000,\n        )\n        # If wls_lat missing, try a fallback per-missing-row nearest without phone constraint (global)\n        missing = (\n            merged[\"wls_lat\"].isnull()\n            if \"wls_lat\" in merged.columns\n            else pd.Series([True] * len(merged), index=merged.index)\n        )\n        if missing.any():\n            merged2 = pd.merge_asof(\n                t[missing].sort_values(\"utcTimeMillis\"),\n                test_agg.sort_values(\"utcTimeMillis\"),\n                on=\"utcTimeMillis\",\n                direction=\"nearest\",\n                tolerance=2000,\n            )\n            # fill merged for missing rows where possible\n            for col in merged2.columns:\n                if col in merged.columns:\n                    merged.loc[missing, col] = merged2[col].values\n\n        # Assign predictions from wls lat/lon if present\n        if \"wls_lat\" in merged.columns and \"wls_lon\" in merged.columns:\n            merged[\"LatitudeDegrees\"] = merged[\"wls_lat\"]\n            merged[\"LongitudeDegrees\"] = merged[\"wls_lon\"]\n        else:\n            merged[\"LatitudeDegrees\"] = np.nan\n            merged[\"LongitudeDegrees\"] = np.nan\n\n        sample_preds.append(\n            merged[[\"phone\", \"utcTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        )\n\n    submission_df = pd.concat(sample_preds, ignore_index=True)\n    submission_df = submission_df.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n\n    # ensure ordering same as sample_submission\n    submission_df = (\n        submission_df.set_index([\"phone\", \"UnixTimeMillis\"])\n        .reindex(\n            sample_sub.set_index([phone_col, time_col]).index.map(\n                lambda x: (x[0], int(x[1]))\n            )\n        )\n        .reset_index()\n    )\n    # After reindex mapping failed in some pandas versions, fallback to simpler reindex\n    if submission_df.isnull().all(axis=None):\n        # simpler: align by merging with sample (left)\n        sample_for_merge = sample.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        submission_df = pd.merge(\n            sample_for_merge,\n            submission_df.rename(columns={\"UnixTimeMillis\": \"UnixTimeMillis\"}),\n            how=\"left\",\n            on=[\"phone\", \"UnixTimeMillis\"],\n        )\n        submission_df = submission_df[\n            [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        ]\n\n    # final fill: if still NaN, fill with global mean lat/lon from train WLS/ground-truth pairs\n    if (\n        \"lat\" in train_df.columns\n        and \"lon\" in train_df.columns\n        and train_df.shape[0] > 0\n    ):\n        global_lat_mean = train_df[\"lat\"].mean()\n        global_lon_mean = train_df[\"lon\"].mean()\n    elif (\n        \"wls_lat\" in train_df.columns\n        and \"wls_lon\" in train_df.columns\n        and train_df.shape[0] > 0\n    ):\n        global_lat_mean = train_df[\"wls_lat\"].mean()\n        global_lon_mean = train_df[\"wls_lon\"].mean()\n    else:\n        global_lat_mean = 0.0\n        global_lon_mean = 0.0\n\n    submission_df[\"LatitudeDegrees\"] = submission_df[\"LatitudeDegrees\"].fillna(\n        global_lat_mean\n    )\n    submission_df[\"LongitudeDegrees\"] = submission_df[\"LongitudeDegrees\"].fillna(\n        global_lon_mean\n    )\n\n    out_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission_df.to_csv(\n        out_path,\n        index=False,\n        columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"],\n    )\n    print(\"Saved submission to\", out_path)\n",
                "exec_time": 75.50272989273071,
                "metric": null,
                "is_buggy": true,
                "exc_type": "RuntimeError",
                "exc_value": null,
                "analysis": "The script computed a validation metric of 4.115 meters using the WLS baseline, but then crashed with a RuntimeError because it could not find a 'phone' column in the sample submission (the file contains 'tripId' instead). As a result no submission.csv was produced.",
                "alignment": {
                        "plan_code_similarity": 0.0101023527847933,
                        "has_changes": true,
                        "change_magnitude": 0.5731847867844795
                }
        },
        {
                "index": 8,
                "step": 8,
                "id": "4fb1152c6fe54bfc89c0956871f83908",
                "parent_id": null,
                "plan": "I will fix the KeyError by normalizing and validating the sample_submission columns, and avoid relying on row-by-row Series keys that may be inconsistent; instead I will index the DataFrame with .at/.loc which is robust. I'll also simplify the test prediction loop to use DataFrame positional access, cache aggregated WLS conversions, and ensure any remaining NaNs are filled with sensible fallbacks. The script still trains the simple Ridge models with GroupKFold, prints the validation metric, and writes the required ./submission/submission.csv file.",
                "code": "import os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom math import sqrt\nimport sys\n\n\ndef ensure_dir(path):\n    os.makedirs(path, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # Bowring's method for WGS84; supports numpy arrays\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = a * np.sqrt(1 - e2)\n    ep2 = (a**2 - b**2) / b**2\n    p = np.sqrt(x**2 + y**2)\n    theta = np.arctan2(z * a, p * b)\n    sin_t = np.sin(theta)\n    cos_t = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * sin_t**3, p - e2 * a * cos_t**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    # lat/lon in degrees -> meters\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * np.arcsin(np.sqrt(a))\n\n\n# Paths\ninput_dir = \"./input\"\ntrain_root = os.path.join(input_dir, \"train\")\ntest_root = os.path.join(input_dir, \"test\")\nsample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n\n# Collect training data\ntrain_device_paths = glob.glob(os.path.join(train_root, \"*\", \"*\", \"device_gnss.csv\"))\nrows = []\nprint(\"Collecting train data from device_gnss files...\", file=sys.stderr)\nfor dev_path in train_device_paths:\n    try:\n        parts = dev_path.split(os.sep)\n        drive_id = parts[-3]\n        phone = parts[-2]\n        folder = os.path.dirname(dev_path)\n        # ground truth usually in parent folder of phone folder\n        gt_path = os.path.join(os.path.dirname(folder), \"ground_truth.csv\")\n        if not os.path.exists(gt_path):\n            # sometimes ground_truth is in same phone folder\n            gt_path = os.path.join(folder, \"ground_truth.csv\")\n        if not os.path.exists(gt_path):\n            continue\n        dg = pd.read_csv(\n            dev_path,\n            usecols=[\n                \"utcTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"Cn0DbHz\",\n            ],\n        )\n        dg = (\n            dg.groupby(\"utcTimeMillis\", as_index=False)\n            .agg(\n                {\n                    \"WlsPositionXEcefMeters\": \"mean\",\n                    \"WlsPositionYEcefMeters\": \"mean\",\n                    \"WlsPositionZEcefMeters\": \"mean\",\n                    \"Cn0DbHz\": \"mean\",\n                }\n            )\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        )\n        gt = pd.read_csv(\n            gt_path, usecols=[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        )\n        dg = dg.sort_values(\"UnixTimeMillis\")\n        gt = gt.sort_values(\"UnixTimeMillis\")\n        merged = pd.merge_asof(\n            dg, gt, on=\"UnixTimeMillis\", direction=\"nearest\", tolerance=500\n        )\n        merged = merged.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\"])\n        if merged.empty:\n            continue\n        x = merged[\"WlsPositionXEcefMeters\"].values\n        y = merged[\"WlsPositionYEcefMeters\"].values\n        z = merged[\"WlsPositionZEcefMeters\"].values\n        lat_wls, lon_wls = ecef_to_geodetic(x, y, z)\n        merged[\"wls_lat\"] = lat_wls\n        merged[\"wls_lon\"] = lon_wls\n        merged[\"drive_id\"] = drive_id\n        merged[\"phone\"] = phone\n        rows.append(\n            merged[\n                [\n                    \"drive_id\",\n                    \"phone\",\n                    \"UnixTimeMillis\",\n                    \"wls_lat\",\n                    \"wls_lon\",\n                    \"Cn0DbHz\",\n                    \"LatitudeDegrees\",\n                    \"LongitudeDegrees\",\n                ]\n            ]\n        )\n    except Exception as e:\n        print(\"Error processing\", dev_path, e, file=sys.stderr)\n\nif not rows:\n    raise RuntimeError(\"No training data collected. Check input paths.\")\n\ntrain_df = pd.concat(rows, ignore_index=True)\nprint(\"Total training rows:\", len(train_df), file=sys.stderr)\n\n# Prepare features and targets\ntrain_df = train_df.sort_values([\"drive_id\", \"UnixTimeMillis\"]).reset_index(drop=True)\nX = train_df[[\"wls_lat\", \"wls_lon\", \"Cn0DbHz\"]].fillna(-999)\ny_lat = train_df[\"LatitudeDegrees\"].values\ny_lon = train_df[\"LongitudeDegrees\"].values\ngroups = train_df[\"drive_id\"].values\nphones = train_df[\"phone\"].values\n\n# 5-fold GroupKFold CV by drive_id\ngkf = GroupKFold(n_splits=5)\nmodels_lat = []\nmodels_lon = []\npreds_lat = np.zeros(len(X))\npreds_lon = np.zeros(len(X))\nprint(\"Performing 5-fold GroupKFold CV...\", file=sys.stderr)\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    ytr_lat, ytr_lon = y_lat[tr_idx], y_lon[val_idx]\n    model_lat = make_pipeline(StandardScaler(), Ridge(alpha=1.0))\n    model_lon = make_pipeline(StandardScaler(), Ridge(alpha=1.0))\n    model_lat.fit(X_tr, ytr_lat)\n    model_lon.fit(X_tr, ytr_lon)\n    p_lat = model_lat.predict(X_val)\n    p_lon = model_lon.predict(X_val)\n    preds_lat[val_idx] = p_lat\n    preds_lon[val_idx] = p_lon\n    models_lat.append(model_lat)\n    models_lon.append(model_lon)\n    print(f\"Fold {fold} done. Val size: {len(val_idx)}\", file=sys.stderr)\n\n# Evaluate using competition metric approximation\neval_df = train_df[[\"drive_id\", \"phone\", \"UnixTimeMillis\"]].copy()\neval_df[\"pred_lat\"] = preds_lat\neval_df[\"pred_lon\"] = preds_lon\neval_df[\"true_lat\"] = y_lat\neval_df[\"true_lon\"] = y_lon\neval_df[\"error_m\"] = haversine(\n    eval_df[\"true_lat\"].values,\n    eval_df[\"true_lon\"].values,\n    eval_df[\"pred_lat\"].values,\n    eval_df[\"pred_lon\"].values,\n)\n\nphones_list = eval_df[\"phone\"].unique()\nphone_scores = []\nfor ph in phones_list:\n    sub = eval_df[eval_df[\"phone\"] == ph]\n    if sub.empty:\n        continue\n    p50 = np.percentile(sub[\"error_m\"], 50)\n    p95 = np.percentile(sub[\"error_m\"], 95)\n    phone_scores.append(0.5 * (p50 + p95))\nif len(phone_scores) == 0:\n    raise RuntimeError(\"No phone scores.\")\nscore = np.mean(phone_scores)\nprint(\n    \"Validation competition-style score (mean over phones of 0.5*(p50+p95)) = {:.4f} meters\".format(\n        score\n    )\n)\n\n# Train final models on all data\nfinal_lat = make_pipeline(StandardScaler(), Ridge(alpha=1.0)).fit(X, y_lat)\nfinal_lon = make_pipeline(StandardScaler(), Ridge(alpha=1.0)).fit(X, y_lon)\n\n# Prepare test predictions\nprint(\"Preparing test predictions...\", file=sys.stderr)\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Normalize columns: strip whitespace and ensure expected names exist\nsample_sub.columns = [str(c).strip() for c in sample_sub.columns]\nif \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n    raise RuntimeError(\n        f\"sample_submission missing required columns. Found: {sample_sub.columns.tolist()}\"\n    )\n\n# Build mapping from test dirs: key = \"<drive>_<phone>\"\nall_test_paths = glob.glob(os.path.join(test_root, \"*\", \"*\"))\ntest_device_dirs = {}\nfor p in all_test_paths:\n    if os.path.isdir(p):\n        drive = os.path.basename(os.path.dirname(p))\n        phone = os.path.basename(p)\n        key = f\"{drive}_{phone}\"\n        test_device_dirs[key] = p\n\n# Cache for aggregated test device_gnss\nagg_cache = {}\n\n# We'll collect predicted lat/lon in a list aligned with sample_sub\npreds = []\nmean_lat = np.mean(y_lat)\nmean_lon = np.mean(y_lon)\n\nfor idx in range(len(sample_sub)):\n    try:\n        key = sample_sub.at[idx, \"phone\"]\n        # ensure string\n        key = str(key)\n        t = int(sample_sub.at[idx, \"UnixTimeMillis\"])\n    except Exception as e:\n        # malformed row; fallback to global mean\n        preds.append((mean_lat, mean_lon))\n        continue\n\n    if key not in test_device_dirs:\n        # no matching test folder; fallback to global mean\n        preds.append((mean_lat, mean_lon))\n        continue\n\n    if key not in agg_cache:\n        dev_dir = test_device_dirs[key]\n        dg_path = os.path.join(dev_dir, \"device_gnss.csv\")\n        if not os.path.exists(dg_path):\n            agg_cache[key] = None\n            preds.append((mean_lat, mean_lon))\n            continue\n        dg = pd.read_csv(\n            dg_path,\n            usecols=[\n                \"utcTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"Cn0DbHz\",\n            ],\n        )\n        dg = (\n            dg.groupby(\"utcTimeMillis\", as_index=False)\n            .agg(\n                {\n                    \"WlsPositionXEcefMeters\": \"mean\",\n                    \"WlsPositionYEcefMeters\": \"mean\",\n                    \"WlsPositionZEcefMeters\": \"mean\",\n                    \"Cn0DbHz\": \"mean\",\n                }\n            )\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n            .sort_values(\"UnixTimeMillis\")\n            .reset_index(drop=True)\n        )\n        if dg.empty:\n            agg_cache[key] = None\n            preds.append((mean_lat, mean_lon))\n            continue\n        x = dg[\"WlsPositionXEcefMeters\"].values\n        yv = dg[\"WlsPositionYEcefMeters\"].values\n        z = dg[\"WlsPositionZEcefMeters\"].values\n        lat_wls, lon_wls = ecef_to_geodetic(x, yv, z)\n        dg[\"wls_lat\"] = lat_wls\n        dg[\"wls_lon\"] = lon_wls\n        agg_cache[key] = dg\n\n    dg = agg_cache.get(key)\n    if dg is None or dg.empty:\n        preds.append((mean_lat, mean_lon))\n        continue\n\n    # find nearest epoch\n    times = dg[\"UnixTimeMillis\"].values\n    pos = np.searchsorted(times, t)\n    cand_idxs = []\n    if pos < len(times):\n        cand_idxs.append(pos)\n    if pos - 1 >= 0:\n        cand_idxs.append(pos - 1)\n    best = None\n    best_dt = 10**12\n    for ci in cand_idxs:\n        dt = abs(int(times[ci]) - t)\n        if dt < best_dt:\n            best_dt = dt\n            best = ci\n    if best is None:\n        preds.append((mean_lat, mean_lon))\n        continue\n    # if epoch too far, still use it (we filled earlier with mean as fallback)\n    feat = (\n        dg.iloc[best][[\"wls_lat\", \"wls_lon\", \"Cn0DbHz\"]]\n        .fillna(-999)\n        .values.reshape(1, -1)\n    )\n    plat = float(final_lat.predict(feat)[0])\n    plon = float(final_lon.predict(feat)[0])\n    preds.append((plat, plon))\n\npreds = np.array(preds)\nsample_sub[\"LatitudeDegrees\"] = preds[:, 0]\nsample_sub[\"LongitudeDegrees\"] = preds[:, 1]\n\n# Ensure no NaNs; fill with train mean if any\nnan_mask = sample_sub[\"LatitudeDegrees\"].isna() | sample_sub[\"LongitudeDegrees\"].isna()\nif nan_mask.any():\n    print(\n        f\"Filling {nan_mask.sum()} missing predictions with global train mean.\",\n        file=sys.stderr,\n    )\n    sample_sub.loc[nan_mask, \"LatitudeDegrees\"] = mean_lat\n    sample_sub.loc[nan_mask, \"LongitudeDegrees\"] = mean_lon\n\n# Save submission\nout_dir = \"./submission\"\nensure_dir(out_dir)\nout_path = os.path.join(out_dir, \"submission.csv\")\nsample_sub.to_csv(out_path, index=False)\nprint(f\"Saved submission to {out_path}\", file=sys.stderr)\n\n# Print final message with validation score\nprint(\"Final validation metric (approx): {:.4f} meters\".format(score))\n",
                "exec_time": 36.79021143913269,
                "metric": null,
                "is_buggy": true,
                "exc_type": "ValueError",
                "exc_value": null,
                "analysis": "The script collected 258,546 training rows and reached the 5-fold GroupKFold CV stage but crashed during model fitting with a ValueError: inconsistent numbers of samples (205834 vs 52712). As a result the run failed and no submission.csv was produced. Validation metric is unavailable due to the crash.",
                "alignment": {
                        "plan_code_similarity": 0.008934592043155765,
                        "has_changes": true,
                        "change_magnitude": 0.9478933127174334
                }
        },
        {
                "index": 9,
                "step": 9,
                "id": "e9f7d454922e4723ba21d43c7fd615c3",
                "parent_id": null,
                "plan": "The error was caused by a simple indexing bug: ytr_lon was mistakenly indexed with val_idx instead of tr_idx, producing mismatched lengths during model.fit. I'll fix that so both latitude and longitude training targets use tr_idx. I also keep the rest of the pipeline (ECEF->lat/lon conversion, GroupKFold by drive, final training on full data, and producing test predictions) unchanged, add some defensive checks, print the validation metric, and save submission/submission.csv.",
                "code": "import os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport sys\n\n\ndef ensure_dir(path):\n    os.makedirs(path, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # Bowring's method for WGS84; supports numpy arrays\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = a * np.sqrt(1 - e2)\n    ep2 = (a**2 - b**2) / b**2\n    p = np.sqrt(x**2 + y**2)\n    theta = np.arctan2(z * a, p * b)\n    sin_t = np.sin(theta)\n    cos_t = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * sin_t**3, p - e2 * a * cos_t**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    # lat/lon in degrees -> meters\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * np.arcsin(np.sqrt(a))\n\n\n# Paths\ninput_dir = \"./input\"\ntrain_root = os.path.join(input_dir, \"train\")\ntest_root = os.path.join(input_dir, \"test\")\nsample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n\n# Collect training data\ntrain_device_paths = glob.glob(os.path.join(train_root, \"*\", \"*\", \"device_gnss.csv\"))\nrows = []\nprint(\"Collecting train data from device_gnss files...\", file=sys.stderr)\nfor dev_path in train_device_paths:\n    try:\n        parts = dev_path.split(os.sep)\n        drive_id = parts[-3]\n        phone = parts[-2]\n        folder = os.path.dirname(dev_path)\n        # ground truth usually in parent folder of phone folder\n        gt_path = os.path.join(os.path.dirname(folder), \"ground_truth.csv\")\n        if not os.path.exists(gt_path):\n            # sometimes ground_truth is in same phone folder\n            gt_path = os.path.join(folder, \"ground_truth.csv\")\n        if not os.path.exists(gt_path):\n            continue\n        dg = pd.read_csv(\n            dev_path,\n            usecols=[\n                \"utcTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"Cn0DbHz\",\n            ],\n        )\n        dg = (\n            dg.groupby(\"utcTimeMillis\", as_index=False)\n            .agg(\n                {\n                    \"WlsPositionXEcefMeters\": \"mean\",\n                    \"WlsPositionYEcefMeters\": \"mean\",\n                    \"WlsPositionZEcefMeters\": \"mean\",\n                    \"Cn0DbHz\": \"mean\",\n                }\n            )\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        )\n        gt = pd.read_csv(\n            gt_path, usecols=[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        )\n        dg = dg.sort_values(\"UnixTimeMillis\")\n        gt = gt.sort_values(\"UnixTimeMillis\")\n        merged = pd.merge_asof(\n            dg, gt, on=\"UnixTimeMillis\", direction=\"nearest\", tolerance=500\n        )\n        merged = merged.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\"])\n        if merged.empty:\n            continue\n        x = merged[\"WlsPositionXEcefMeters\"].values\n        y = merged[\"WlsPositionYEcefMeters\"].values\n        z = merged[\"WlsPositionZEcefMeters\"].values\n        lat_wls, lon_wls = ecef_to_geodetic(x, y, z)\n        merged[\"wls_lat\"] = lat_wls\n        merged[\"wls_lon\"] = lon_wls\n        merged[\"drive_id\"] = drive_id\n        merged[\"phone\"] = phone\n        rows.append(\n            merged[\n                [\n                    \"drive_id\",\n                    \"phone\",\n                    \"UnixTimeMillis\",\n                    \"wls_lat\",\n                    \"wls_lon\",\n                    \"Cn0DbHz\",\n                    \"LatitudeDegrees\",\n                    \"LongitudeDegrees\",\n                ]\n            ]\n        )\n    except Exception as e:\n        print(\"Error processing\", dev_path, e, file=sys.stderr)\n\nif not rows:\n    raise RuntimeError(\"No training data collected. Check input paths.\")\n\ntrain_df = pd.concat(rows, ignore_index=True)\nprint(\"Total training rows:\", len(train_df), file=sys.stderr)\n\n# Prepare features and targets\ntrain_df = train_df.sort_values([\"drive_id\", \"UnixTimeMillis\"]).reset_index(drop=True)\nX = train_df[[\"wls_lat\", \"wls_lon\", \"Cn0DbHz\"]].fillna(-999)\ny_lat = train_df[\"LatitudeDegrees\"].values\ny_lon = train_df[\"LongitudeDegrees\"].values\ngroups = train_df[\"drive_id\"].values\nphones = train_df[\"phone\"].values\n\n# 5-fold GroupKFold CV by drive_id\ngkf = GroupKFold(n_splits=5)\nmodels_lat = []\nmodels_lon = []\npreds_lat = np.zeros(len(X))\npreds_lon = np.zeros(len(X))\nprint(\"Performing 5-fold GroupKFold CV...\", file=sys.stderr)\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    # FIX: ensure both latitude and longitude targets use tr_idx\n    ytr_lat, ytr_lon = y_lat[tr_idx], y_lon[tr_idx]\n    model_lat = make_pipeline(StandardScaler(), Ridge(alpha=1.0))\n    model_lon = make_pipeline(StandardScaler(), Ridge(alpha=1.0))\n    model_lat.fit(X_tr, ytr_lat)\n    model_lon.fit(X_tr, ytr_lon)\n    p_lat = model_lat.predict(X_val)\n    p_lon = model_lon.predict(X_val)\n    preds_lat[val_idx] = p_lat\n    preds_lon[val_idx] = p_lon\n    models_lat.append(model_lat)\n    models_lon.append(model_lon)\n    print(f\"Fold {fold} done. Val size: {len(val_idx)}\", file=sys.stderr)\n\n# Evaluate using competition metric approximation\neval_df = train_df[[\"drive_id\", \"phone\", \"UnixTimeMillis\"]].copy()\neval_df[\"pred_lat\"] = preds_lat\neval_df[\"pred_lon\"] = preds_lon\neval_df[\"true_lat\"] = y_lat\neval_df[\"true_lon\"] = y_lon\neval_df[\"error_m\"] = haversine(\n    eval_df[\"true_lat\"].values,\n    eval_df[\"true_lon\"].values,\n    eval_df[\"pred_lat\"].values,\n    eval_df[\"pred_lon\"].values,\n)\n\nphones_list = eval_df[\"phone\"].unique()\nphone_scores = []\nfor ph in phones_list:\n    sub = eval_df[eval_df[\"phone\"] == ph]\n    if sub.empty:\n        continue\n    p50 = np.percentile(sub[\"error_m\"], 50)\n    p95 = np.percentile(sub[\"error_m\"], 95)\n    phone_scores.append(0.5 * (p50 + p95))\nif len(phone_scores) == 0:\n    raise RuntimeError(\"No phone scores.\")\nscore = np.mean(phone_scores)\nprint(\n    \"Validation competition-style score (mean over phones of 0.5*(p50+p95)) = {:.4f} meters\".format(\n        score\n    )\n)\n\n# Train final models on all data\nfinal_lat = make_pipeline(StandardScaler(), Ridge(alpha=1.0)).fit(X, y_lat)\nfinal_lon = make_pipeline(StandardScaler(), Ridge(alpha=1.0)).fit(X, y_lon)\n\n# Prepare test predictions\nprint(\"Preparing test predictions...\", file=sys.stderr)\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Normalize columns: strip whitespace and ensure expected names exist\nsample_sub.columns = [str(c).strip() for c in sample_sub.columns]\nif \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n    raise RuntimeError(\n        f\"sample_submission missing required columns. Found: {sample_sub.columns.tolist()}\"\n    )\n\n# Build mapping from test dirs: key = \"<drive>_<phone>\"\nall_test_paths = glob.glob(os.path.join(test_root, \"*\", \"*\"))\ntest_device_dirs = {}\nfor p in all_test_paths:\n    if os.path.isdir(p):\n        drive = os.path.basename(os.path.dirname(p))\n        phone = os.path.basename(p)\n        key = f\"{drive}_{phone}\"\n        test_device_dirs[key] = p\n\n# Cache for aggregated test device_gnss\nagg_cache = {}\n\n# We'll collect predicted lat/lon in a list aligned with sample_sub\npreds = []\nmean_lat = np.mean(y_lat)\nmean_lon = np.mean(y_lon)\n\nfor idx in range(len(sample_sub)):\n    try:\n        key = sample_sub.at[idx, \"phone\"]\n        # ensure string\n        key = str(key)\n        t = int(sample_sub.at[idx, \"UnixTimeMillis\"])\n    except Exception as e:\n        # malformed row; fallback to global mean\n        preds.append((mean_lat, mean_lon))\n        continue\n\n    if key not in test_device_dirs:\n        # no matching test folder; fallback to global mean\n        preds.append((mean_lat, mean_lon))\n        continue\n\n    if key not in agg_cache:\n        dev_dir = test_device_dirs[key]\n        dg_path = os.path.join(dev_dir, \"device_gnss.csv\")\n        if not os.path.exists(dg_path):\n            agg_cache[key] = None\n            preds.append((mean_lat, mean_lon))\n            continue\n        dg = pd.read_csv(\n            dg_path,\n            usecols=[\n                \"utcTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"Cn0DbHz\",\n            ],\n        )\n        dg = (\n            dg.groupby(\"utcTimeMillis\", as_index=False)\n            .agg(\n                {\n                    \"WlsPositionXEcefMeters\": \"mean\",\n                    \"WlsPositionYEcefMeters\": \"mean\",\n                    \"WlsPositionZEcefMeters\": \"mean\",\n                    \"Cn0DbHz\": \"mean\",\n                }\n            )\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n            .sort_values(\"UnixTimeMillis\")\n            .reset_index(drop=True)\n        )\n        if dg.empty:\n            agg_cache[key] = None\n            preds.append((mean_lat, mean_lon))\n            continue\n        x = dg[\"WlsPositionXEcefMeters\"].values\n        yv = dg[\"WlsPositionYEcefMeters\"].values\n        z = dg[\"WlsPositionZEcefMeters\"].values\n        lat_wls, lon_wls = ecef_to_geodetic(x, yv, z)\n        dg[\"wls_lat\"] = lat_wls\n        dg[\"wls_lon\"] = lon_wls\n        agg_cache[key] = dg\n\n    dg = agg_cache.get(key)\n    if dg is None or dg.empty:\n        preds.append((mean_lat, mean_lon))\n        continue\n\n    # find nearest epoch\n    times = dg[\"UnixTimeMillis\"].values\n    pos = np.searchsorted(times, t)\n    cand_idxs = []\n    if pos < len(times):\n        cand_idxs.append(pos)\n    if pos - 1 >= 0:\n        cand_idxs.append(pos - 1)\n    best = None\n    best_dt = 10**12\n    for ci in cand_idxs:\n        dt = abs(int(times[ci]) - t)\n        if dt < best_dt:\n            best_dt = dt\n            best = ci\n    if best is None:\n        preds.append((mean_lat, mean_lon))\n        continue\n    # if epoch too far, still use it (we filled earlier with mean as fallback)\n    feat = (\n        dg.iloc[best][[\"wls_lat\", \"wls_lon\", \"Cn0DbHz\"]]\n        .fillna(-999)\n        .values.reshape(1, -1)\n    )\n    plat = float(final_lat.predict(feat)[0])\n    plon = float(final_lon.predict(feat)[0])\n    preds.append((plat, plon))\n\npreds = np.array(preds)\nsample_sub[\"LatitudeDegrees\"] = preds[:, 0]\nsample_sub[\"LongitudeDegrees\"] = preds[:, 1]\n\n# Ensure no NaNs; fill with train mean if any\nnan_mask = sample_sub[\"LatitudeDegrees\"].isna() | sample_sub[\"LongitudeDegrees\"].isna()\nif nan_mask.any():\n    print(\n        f\"Filling {nan_mask.sum()} missing predictions with global train mean.\",\n        file=sys.stderr,\n    )\n    sample_sub.loc[nan_mask, \"LatitudeDegrees\"] = mean_lat\n    sample_sub.loc[nan_mask, \"LongitudeDegrees\"] = mean_lon\n\n# Save submission\nout_dir = \"./submission\"\nensure_dir(out_dir)\nout_path = os.path.join(out_dir, \"submission.csv\")\nsample_sub.to_csv(out_path, index=False)\nprint(f\"Saved submission to {out_path}\", file=sys.stderr)\n\n# Print final message with validation score\nprint(\"Final validation metric (approx): {:.4f} meters\".format(score))\n",
                "exec_time": 37.21134638786316,
                "metric": null,
                "is_buggy": true,
                "exc_type": "RuntimeError",
                "exc_value": null,
                "analysis": "Script successfully collected training data (258,546 rows) and ran 5-fold GroupKFold CV, producing an approximate validation metric of 7490.5038 meters. Execution then failed with a RuntimeError because the sample_submission file lacked the expected 'phone' column (it had 'tripId' instead), so no submission.csv was produced.",
                "alignment": {
                        "plan_code_similarity": 0.0072684246112238,
                        "has_changes": true,
                        "change_magnitude": 0.004059301094246348
                }
        },
        {
                "index": 10,
                "step": 10,
                "id": "e230961a8cc1483789ad1e06bdc55c9a",
                "parent_id": null,
                "plan": "The error occurs because the sample submission's column names don't exactly match the literal strings used (\"phone\", \"UnixTimeMillis\", etc.). I'll make column lookup robust by matching columns case-insensitively and by keyword (e.g. \"phone\", \"unixtime\", \"latitude\", \"longitude\") and fall back safely. The rest of the pipeline is kept the same: aggregate WLS epochs, train Ridge models with GroupKFold, print the CV metric, and build a test submission; finally save submission/submission.csv. The code below implements these fixes and ensures the submission file is created.",
                "code": "#!/usr/bin/env python3\nimport os\nimport sys\nimport glob\nimport math\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GroupKFold\n\n\n# Utilities\ndef ensure_dir(p):\n    if not os.path.exists(p):\n        os.makedirs(p)\n\n\ndef ecef_to_latlon(x, y, z):\n    # WGS84 conversion from ECEF to lat/lon (approx)\n    a = 6378137.0\n    e = 8.1819190842622e-2\n    asq = a * a\n    esq = e * e\n    b = math.sqrt(asq * (1 - esq))\n    ep = math.sqrt((asq - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3, p - esq * a * math.cos(th) ** 3\n    )\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine_meters(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return R * 2 * np.arcsin(np.sqrt(a))\n\n\n# Paths\nINPUT = \"./input\"\nTRAIN_DIR = os.path.join(INPUT, \"train\")\nTEST_DIR = os.path.join(INPUT, \"test\")\nSAMPLE_SUB = os.path.join(INPUT, \"sample_submission.csv\")\nSUBMISSION_OUT = os.path.join(\"submission\")\nensure_dir(SUBMISSION_OUT)\nSUBMISSION_PATH = os.path.join(SUBMISSION_OUT, \"submission.csv\")\n\n\n# Helper to find column by keywords (case-insensitive)\ndef find_col(cols, keywords):\n    cols_l = [c.lower() for c in cols]\n    for kw in keywords:\n        for i, c in enumerate(cols_l):\n            if kw in c:\n                return cols[i]\n    return None\n\n\n# Step 1: Aggregate train GNSS epochs and join to ground truth\ntrain_agg_rows = []\nprint(\"Scanning train folders...\")\nfor drive_path in sorted(glob.glob(os.path.join(TRAIN_DIR, \"*\"))):\n    if not os.path.isdir(drive_path):\n        continue\n    drive = os.path.basename(drive_path)\n    for phone in sorted(os.listdir(drive_path)):\n        phone_dir = os.path.join(drive_path, phone)\n        if not os.path.isdir(phone_dir):\n            continue\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n        if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n            continue\n        try:\n            usecols = [\n                \"utcTimeMillis\",\n                \"Svid\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateMetersPerSecond\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            df = pd.read_csv(gnss_path, usecols=lambda c: c in usecols)\n        except Exception:\n            df = pd.read_csv(gnss_path)\n            df = df[[c for c in usecols if c in df.columns]]\n        # If Wls columns missing skip\n        wls_cols = [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        missing_wls = any(c not in df.columns for c in wls_cols)\n        if missing_wls or df.shape[0] == 0:\n            continue\n        # Group by epoch\n        agg = (\n            df.groupby(\"utcTimeMillis\")\n            .agg(\n                WlsX=(\"WlsPositionXEcefMeters\", \"first\"),\n                WlsY=(\"WlsPositionYEcefMeters\", \"first\"),\n                WlsZ=(\"WlsPositionZEcefMeters\", \"first\"),\n                sat_count=(\"Svid\", lambda s: s.nunique()),\n                mean_cn0=(\"Cn0DbHz\", \"mean\"),\n                std_cn0=(\"Cn0DbHz\", \"std\"),\n                mean_prr=(\"PseudorangeRateMetersPerSecond\", \"mean\"),\n            )\n            .reset_index()\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        )\n        # Convert to lat/lon\n        lat_list = []\n        lon_list = []\n        for xi, yi, zi in zip(agg.WlsX.values, agg.WlsY.values, agg.WlsZ.values):\n            try:\n                lat, lon = ecef_to_latlon(float(xi), float(yi), float(zi))\n            except Exception:\n                lat, lon = np.nan, np.nan\n            lat_list.append(lat)\n            lon_list.append(lon)\n        agg[\"WlsLat\"] = lat_list\n        agg[\"WlsLon\"] = lon_list\n        # read ground truth\n        gt = pd.read_csv(gt_path)\n        # find required gt columns robustly\n        gt_cols = gt.columns.tolist()\n        time_col = find_col(gt_cols, [\"unixtimemillis\", \"unixtime\", \"time\"])\n        lat_col = find_col(gt_cols, [\"latitude\", \"lat\"])\n        lon_col = find_col(gt_cols, [\"longitude\", \"lon\", \"long\"])\n        if time_col is None or lat_col is None or lon_col is None:\n            continue\n        gt = gt[[time_col, lat_col, lon_col]].dropna()\n        gt = gt.rename(\n            columns={\n                time_col: \"UnixTimeMillis\",\n                lat_col: \"LatitudeDegrees\",\n                lon_col: \"LongitudeDegrees\",\n            }\n        )\n        gt = gt.sort_values(\"UnixTimeMillis\").reset_index(drop=True)\n        agg = agg.sort_values(\"UnixTimeMillis\").reset_index(drop=True)\n        if agg.shape[0] == 0 or gt.shape[0] == 0:\n            continue\n        # For each agg epoch, find nearest gt time using searchsorted\n        gt_times = gt.UnixTimeMillis.values\n        agg_times = agg.UnixTimeMillis.values\n        idxs = np.searchsorted(gt_times, agg_times)\n        chosen_lat = []\n        chosen_lon = []\n        chosen_dt = []\n        for i, t in enumerate(agg_times):\n            best_dt = None\n            lat_val = np.nan\n            lon_val = np.nan\n            for cand in (idxs[i] - 1, idxs[i]):\n                if 0 <= cand < len(gt_times):\n                    dt = abs(gt_times[cand] - t)\n                    if best_dt is None or dt < best_dt:\n                        best_dt = dt\n                        lat_val = float(gt.LatitudeDegrees.values[cand])\n                        lon_val = float(gt.LongitudeDegrees.values[cand])\n            chosen_lat.append(lat_val)\n            chosen_lon.append(lon_val)\n            chosen_dt.append(best_dt if best_dt is not None else np.nan)\n        agg[\"GtLat\"] = chosen_lat\n        agg[\"GtLon\"] = chosen_lon\n        agg[\"time_diff_ms\"] = chosen_dt\n        # keep only fairly close matches (<=500ms)\n        agg = agg[agg.time_diff_ms <= 500].copy()\n        if agg.shape[0] == 0:\n            continue\n        agg[\"drive\"] = drive\n        agg[\"phone\"] = phone\n        # compute deltas (degrees)\n        agg[\"dLat\"] = agg[\"GtLat\"] - agg[\"WlsLat\"]\n        agg[\"dLon\"] = agg[\"GtLon\"] - agg[\"WlsLon\"]\n        train_agg_rows.append(\n            agg[\n                [\n                    \"drive\",\n                    \"phone\",\n                    \"UnixTimeMillis\",\n                    \"WlsX\",\n                    \"WlsY\",\n                    \"WlsZ\",\n                    \"WlsLat\",\n                    \"WlsLon\",\n                    \"sat_count\",\n                    \"mean_cn0\",\n                    \"std_cn0\",\n                    \"mean_prr\",\n                    \"GtLat\",\n                    \"GtLon\",\n                    \"dLat\",\n                    \"dLon\",\n                ]\n            ]\n        )\n    print(\n        \"Processed drive:\",\n        drive,\n        \"current total epochs:\",\n        sum([len(x) for x in train_agg_rows]) if len(train_agg_rows) > 0 else 0,\n    )\n\nif len(train_agg_rows) == 0:\n    print(\"No train data found or no matching WLS/ground truth pairs. Exiting.\")\n    sys.exit(1)\n\ntrain_df = pd.concat(train_agg_rows, ignore_index=True)\nprint(\"Total training epochs collected:\", len(train_df))\n\n# Prepare features and targets\nfeat_cols = [\"WlsLat\", \"WlsLon\", \"sat_count\", \"mean_cn0\", \"std_cn0\", \"mean_prr\"]\nfor c in feat_cols:\n    if c not in train_df.columns:\n        train_df[c] = 0.0\ntrain_df = train_df.dropna(subset=[\"dLat\", \"dLon\", \"WlsLat\", \"WlsLon\"])\nX = train_df[feat_cols].fillna(0.0).values\ny_lat = train_df[\"dLat\"].values\ny_lon = train_df[\"dLon\"].values\ngroups = train_df[\"drive\"].values\n\n# 5-fold GroupKFold by drive\ngkf = GroupKFold(n_splits=5)\nlat_preds = np.zeros_like(y_lat)\nlon_preds = np.zeros_like(y_lon)\n\nprint(\"Running 5-fold CV with Ridge...\")\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X[train_idx], X[val_idx]\n    ylat_tr, ylat_val = y_lat[train_idx], y_lat[val_idx]\n    ylon_tr, ylon_val = y_lon[train_idx], y_lon[val_idx]\n    model_lat = Ridge(alpha=1.0)\n    model_lon = Ridge(alpha=1.0)\n    model_lat.fit(X_tr, ylat_tr)\n    model_lon.fit(X_tr, ylon_tr)\n    lat_preds[val_idx] = model_lat.predict(X_val)\n    lon_preds[val_idx] = model_lon.predict(X_val)\n    print(f\" Fold {fold+1} done.\")\n\n# Evaluate on CV predictions using competition metric\ntrain_df[\"pred_dLat\"] = lat_preds\ntrain_df[\"pred_dLon\"] = lon_preds\ntrain_df[\"pred_lat\"] = train_df[\"WlsLat\"] + train_df[\"pred_dLat\"]\ntrain_df[\"pred_lon\"] = train_df[\"WlsLon\"] + train_df[\"pred_dLon\"]\ntrain_df[\"err_m\"] = haversine_meters(\n    train_df[\"pred_lat\"].values,\n    train_df[\"pred_lon\"].values,\n    train_df[\"GtLat\"].values,\n    train_df[\"GtLon\"].values,\n)\nphone_groups = train_df.groupby([\"drive\", \"phone\"])\nphone_scores = []\nfor (drive, phone), gp in phone_groups:\n    errs = gp[\"err_m\"].values\n    if len(errs) < 5:\n        continue\n    p50 = np.percentile(errs, 50)\n    p95 = np.percentile(errs, 95)\n    phone_scores.append((p50 + p95) / 2.0)\nif len(phone_scores) == 0:\n    mean_metric = float(\"nan\")\nelse:\n    mean_metric = float(np.mean(phone_scores))\nprint(\n    \"CV competition metric (mean of per-phone (50th+95th)/2): {:.6f} meters\".format(\n        mean_metric\n    )\n)\n\n# Train final models on all train data\nfinal_lat = Ridge(alpha=1.0).fit(X, y_lat)\nfinal_lon = Ridge(alpha=1.0).fit(X, y_lon)\nprint(\"Final models trained on all training data.\")\n\n# Build test predictions by reading device_gnss per test phone and matching sample_submission times\nprint(\"Loading sample submission to build test predictions...\")\nsample = pd.read_csv(SAMPLE_SUB)\n# robust column lookup for sample\nscols = sample.columns.tolist()\ns_phone_col = find_col(scols, [\"phone\", \"device\", \"drive\"])\ns_time_col = find_col(scols, [\"unixtimemillis\", \"unixtime\", \"time\"])\ns_lat_col = find_col(scols, [\"latitude\", \"lat\"])\ns_lon_col = find_col(scols, [\"longitude\", \"lon\", \"long\"])\n# Fallback to common names if not found\nif s_phone_col is None:\n    # try first column\n    s_phone_col = scols[0]\nif s_time_col is None:\n    # try second column if present\n    s_time_col = scols[1] if len(scols) > 1 else scols[0]\nif s_lat_col is None:\n    # sample_submission may include lat/lon columns that are placeholders; set to None allowed\n    s_lat_col = None\nif s_lon_col is None:\n    s_lon_col = None\n\nout_rows = []\ntest_cache = {}\nprint(\"Scanning test folders and aggregating GNSS epochs...\")\nfor drive_path in sorted(glob.glob(os.path.join(TEST_DIR, \"*\"))):\n    if not os.path.isdir(drive_path):\n        continue\n    drive = os.path.basename(drive_path)\n    for phone in sorted(os.listdir(drive_path)):\n        phone_dir = os.path.join(drive_path, phone)\n        if not os.path.isdir(phone_dir):\n            continue\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            continue\n        try:\n            df = pd.read_csv(\n                gnss_path,\n                usecols=lambda c: c\n                in [\n                    \"utcTimeMillis\",\n                    \"Svid\",\n                    \"Cn0DbHz\",\n                    \"PseudorangeRateMetersPerSecond\",\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ],\n            )\n        except Exception:\n            df = pd.read_csv(gnss_path)\n            df = df[\n                [\n                    c\n                    for c in [\n                        \"utcTimeMillis\",\n                        \"Svid\",\n                        \"Cn0DbHz\",\n                        \"PseudorangeRateMetersPerSecond\",\n                        \"WlsPositionXEcefMeters\",\n                        \"WlsPositionYEcefMeters\",\n                        \"WlsPositionZEcefMeters\",\n                    ]\n                    if c in df.columns\n                ]\n            ]\n        if df.shape[0] == 0:\n            continue\n        if not all(\n            c in df.columns\n            for c in [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        ):\n            continue\n        agg = (\n            df.groupby(\"utcTimeMillis\")\n            .agg(\n                WlsX=(\"WlsPositionXEcefMeters\", \"first\"),\n                WlsY=(\"WlsPositionYEcefMeters\", \"first\"),\n                WlsZ=(\"WlsPositionZEcefMeters\", \"first\"),\n                sat_count=(\"Svid\", lambda s: s.nunique()),\n                mean_cn0=(\"Cn0DbHz\", \"mean\"),\n                std_cn0=(\"Cn0DbHz\", \"std\"),\n                mean_prr=(\"PseudorangeRateMetersPerSecond\", \"mean\"),\n            )\n            .reset_index()\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        )\n        lat_list = []\n        lon_list = []\n        for xi, yi, zi in zip(agg.WlsX.values, agg.WlsY.values, agg.WlsZ.values):\n            try:\n                lat, lon = ecef_to_latlon(float(xi), float(yi), float(zi))\n            except Exception:\n                lat, lon = np.nan, np.nan\n            lat_list.append(lat)\n            lon_list.append(lon)\n        agg[\"WlsLat\"] = lat_list\n        agg[\"WlsLon\"] = lon_list\n        agg = agg.sort_values(\"UnixTimeMillis\").reset_index(drop=True)\n        key = drive + \"_\" + phone\n        test_cache[key] = agg\n        print(\"Aggregated test:\", key, \"epochs:\", len(agg))\n\n# For each row in sample submission, predict\nprint(\"Constructing submission predictions...\")\nout_records = []\nmissing_count = 0\n\n\n# define helper to get sample values robustly\ndef sample_get(row, col):\n    if col is None:\n        return np.nan\n    return row[col]\n\n\nfor i, row in sample.iterrows():\n    phone_val = sample_get(row, s_phone_col)\n    try:\n        t = int(sample_get(row, s_time_col))\n    except Exception:\n        # fallback if conversion fails\n        try:\n            t = int(float(sample_get(row, s_time_col)))\n        except Exception:\n            t = None\n    # canonical key: sample likely has drive_phone format; try to match directly\n    key = phone_val if isinstance(phone_val, str) else None\n    if key not in test_cache:\n        # try various heuristics to match keys\n        matched = None\n        if isinstance(phone_val, str):\n            # if phone_val already contains drive and phone, try direct\n            for k in test_cache.keys():\n                if k == phone_val:\n                    matched = k\n                    break\n            # try suffix match on phone model\n            for k in test_cache.keys():\n                if phone_val.split(\"_\")[-1] in k:\n                    matched = k\n                    break\n            # try prefix match\n            if matched is None:\n                for k in test_cache.keys():\n                    if k.startswith(phone_val.split(\"_\")[0]):\n                        matched = k\n                        break\n        if matched is not None:\n            key = matched\n        else:\n            key = None\n    if key is None or key not in test_cache or t is None:\n        missing_count += 1\n        # fallback to sample lat/lon if available, else zeros\n        if s_lat_col is not None and s_lon_col is not None:\n            pred_lat = sample_get(row, s_lat_col)\n            pred_lon = sample_get(row, s_lon_col)\n        else:\n            pred_lat = 0.0\n            pred_lon = 0.0\n    else:\n        agg = test_cache[key]\n        times = agg.UnixTimeMillis.values\n        if len(times) == 0:\n            missing_count += 1\n            if s_lat_col is not None and s_lon_col is not None:\n                pred_lat = sample_get(row, s_lat_col)\n                pred_lon = sample_get(row, s_lon_col)\n            else:\n                pred_lat = 0.0\n                pred_lon = 0.0\n        else:\n            pos = np.searchsorted(times, t)\n            cand_idx = []\n            for cand in (pos - 1, pos):\n                if 0 <= cand < len(times):\n                    cand_idx.append(cand)\n            best_idx = cand_idx[0]\n            best_dt = abs(int(times[best_idx]) - t)\n            for ci in cand_idx:\n                dt = abs(int(times[ci]) - t)\n                if dt < best_dt:\n                    best_dt = dt\n                    best_idx = ci\n            row_agg = agg.iloc[best_idx]\n            wls_lat = row_agg[\"WlsLat\"]\n            wls_lon = row_agg[\"WlsLon\"]\n            feat = np.array(\n                [\n                    wls_lat,\n                    wls_lon,\n                    (\n                        row_agg.get(\"sat_count\", 0)\n                        if not pd.isna(row_agg.get(\"sat_count\", np.nan))\n                        else 0.0\n                    ),\n                    (\n                        row_agg.get(\"mean_cn0\", 0.0)\n                        if not pd.isna(row_agg.get(\"mean_cn0\", np.nan))\n                        else 0.0\n                    ),\n                    (\n                        row_agg.get(\"std_cn0\", 0.0)\n                        if not pd.isna(row_agg.get(\"std_cn0\", np.nan))\n                        else 0.0\n                    ),\n                    (\n                        row_agg.get(\"mean_prr\", 0.0)\n                        if not pd.isna(row_agg.get(\"mean_prr\", np.nan))\n                        else 0.0\n                    ),\n                ]\n            ).reshape(1, -1)\n            pred_dlat = final_lat.predict(feat)[0]\n            pred_dlon = final_lon.predict(feat)[0]\n            pred_lat = wls_lat + pred_dlat\n            pred_lon = wls_lon + pred_dlon\n    # Ensure numeric values\n    try:\n        pred_lat = float(pred_lat)\n    except Exception:\n        pred_lat = 0.0\n    try:\n        pred_lon = float(pred_lon)\n    except Exception:\n        pred_lon = 0.0\n    out_records.append((phone_val, int(t) if t is not None else -1, pred_lat, pred_lon))\n\nprint(\"Missing/unknown test epochs:\", missing_count, \"out of\", len(sample))\n\n# Write submission with required header names\nout_df = pd.DataFrame(\n    out_records,\n    columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"],\n)\n# Some rows might have UnixTimeMillis == -1 due to parsing failure; keep as-is\nout_df.to_csv(SUBMISSION_PATH, index=False)\nprint(\"Saved submission to\", SUBMISSION_PATH)\nprint(\"Done.\")\n",
                "exec_time": 63.572226762771606,
                "metric": null,
                "is_buggy": true,
                "exc_type": null,
                "exc_value": null,
                "analysis": "The run completed and produced submission/submission.csv, but matching between sample submission rows and aggregated test GNSS epochs failed: all 37087 sample rows were reported as missing. Training aggregated 258,546 epochs and 5-fold CV produced a competition metric of 333.296688 meters. The script saved a submission file but its predictions are fallback values due to the missing matches.",
                "alignment": {
                        "plan_code_similarity": 0.009923743862947873,
                        "has_changes": true,
                        "change_magnitude": 0.93029472699325
                }
        },
        {
                "index": 11,
                "step": 11,
                "id": "5d26593dc0be41198c087bebb3d4cf74",
                "parent_id": null,
                "plan": "I'll fix the phone-column detection by looking for common alternatives such as \"tripId\" and falling back to any column containing \"trip\" or \"phone\". I'll simplify the test-time merging by using pandas.merge_asof with the \"by\" parameter to match phone/trip ids, so predictions align to the sample submission order. Finally I'll ensure we always write ./submission/submission.csv (using the original sample submission column names) and print the validation metric computed on aggregated WLS vs ground-truth.",
                "code": "import os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom math import sqrt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 conversion\n    a = 6378137.0\n    e = 8.1819190842622e-2\n    b = sqrt(a * a * (1 - e * e))\n    ep = sqrt((a * a - b * b) / (b * b))\n    p = np.sqrt(x * x + y * y)\n    th = np.arctan2(a * z, b * p)\n    lon = np.arctan2(y, x)\n    lat = np.arctan2(\n        (z + ep * ep * b * (np.sin(th) ** 3)), (p - e * e * a * (np.cos(th) ** 3))\n    )\n    N = a / np.sqrt(1 - e * e * (np.sin(lat) ** 2))\n    alt = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), alt\n\n\n# haversine in meters\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n\n\ndef aggregate_device_gnss(fn):\n    try:\n        df = pd.read_csv(fn)\n    except Exception:\n        return pd.DataFrame()\n    if df.shape[0] == 0:\n        return pd.DataFrame()\n    if \"utcTimeMillis\" not in df.columns:\n        return pd.DataFrame()\n\n    group = df.groupby(\"utcTimeMillis\", sort=True)\n    out = pd.DataFrame({\"utcTimeMillis\": group.size().index})\n    out = out.set_index(\"utcTimeMillis\")\n\n    # WLS positions if available\n    if \"WlsPositionXEcefMeters\" in df.columns:\n        out[\"WlsX\"] = group[\"WlsPositionXEcefMeters\"].first()\n        out[\"WlsY\"] = (\n            group[\"WlsPositionYEcefMeters\"].first()\n            if \"WlsPositionYEcefMeters\" in df.columns\n            else np.nan\n        )\n        out[\"WlsZ\"] = (\n            group[\"WlsPositionZEcefMeters\"].first()\n            if \"WlsPositionZEcefMeters\" in df.columns\n            else np.nan\n        )\n    else:\n        out[\"WlsX\"] = np.nan\n        out[\"WlsY\"] = np.nan\n        out[\"WlsZ\"] = np.nan\n\n    # numeric means\n    out[\"cn0_mean\"] = group[\"Cn0DbHz\"].mean() if \"Cn0DbHz\" in df.columns else np.nan\n    out[\"elev_mean\"] = (\n        group[\"SvElevationDegrees\"].mean()\n        if \"SvElevationDegrees\" in df.columns\n        else np.nan\n    )\n    out[\"azim_mean\"] = (\n        group[\"SvAzimuthDegrees\"].mean() if \"SvAzimuthDegrees\" in df.columns else np.nan\n    )\n    out[\"pr_mean\"] = (\n        group[\"PseudorangeRateMetersPerSecond\"].mean()\n        if \"PseudorangeRateMetersPerSecond\" in df.columns\n        else np.nan\n    )\n    out[\"sv_count\"] = group[\"Svid\"].nunique() if \"Svid\" in df.columns else np.nan\n\n    out = out.reset_index()\n\n    # drop rows where Wls positions missing entirely\n    has_wls = out[[\"WlsX\", \"WlsY\", \"WlsZ\"]].notnull().all(axis=1)\n    if has_wls.sum() == 0:\n        return pd.DataFrame()\n    out = out.loc[has_wls].copy()\n\n    # convert ECEF to lat/lon\n    lats = []\n    lons = []\n    alts = []\n    for _, r in out.iterrows():\n        lat, lon, alt = ecef_to_geodetic(r[\"WlsX\"], r[\"WlsY\"], r[\"WlsZ\"])\n        lats.append(lat)\n        lons.append(lon)\n        alts.append(alt)\n    out[\"wls_lat\"] = lats\n    out[\"wls_lon\"] = lons\n    out[\"utcTimeMillis\"] = out[\"utcTimeMillis\"].astype(np.int64)\n    return out\n\n\n# Build training dataset using the WLS baseline\ntrain_root = os.path.join(INPUT_DIR, \"train\")\ntrain_dirs = glob.glob(os.path.join(train_root, \"*\", \"*\"))\ntrain_rows = []\nfor drive_phone in train_dirs:\n    parts = drive_phone.split(os.sep)\n    if len(parts) < 4:\n        continue\n    drive = parts[-2]\n    phone = parts[-1]\n    phone_full = f\"{drive}_{phone}\"\n    device_gnss_path = os.path.join(drive_phone, \"device_gnss.csv\")\n    ground_truth_path = os.path.join(drive_phone, \"ground_truth.csv\")\n    if not os.path.exists(device_gnss_path) or not os.path.exists(ground_truth_path):\n        continue\n    agg = aggregate_device_gnss(device_gnss_path)\n    if agg.shape[0] == 0:\n        continue\n    try:\n        gt = pd.read_csv(ground_truth_path)\n    except Exception:\n        continue\n    if gt.shape[0] == 0:\n        continue\n    gt = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].rename(\n        columns={\n            \"UnixTimeMillis\": \"utcTimeMillis\",\n            \"LatitudeDegrees\": \"lat\",\n            \"LongitudeDegrees\": \"lon\",\n        }\n    )\n    agg_sorted = agg.sort_values(\"utcTimeMillis\")\n    gt_sorted = gt.sort_values(\"utcTimeMillis\")\n    merged = pd.merge_asof(\n        agg_sorted,\n        gt_sorted,\n        on=\"utcTimeMillis\",\n        direction=\"nearest\",\n        tolerance=1000,\n    )\n    merged = merged.dropna(subset=[\"lat\", \"lon\"])\n    if merged.shape[0] == 0:\n        continue\n    merged[\"phone\"] = phone_full\n    train_rows.append(merged)\n\nif len(train_rows) == 0:\n    train_df = pd.DataFrame(\n        columns=[\"phone\", \"utcTimeMillis\", \"lat\", \"lon\", \"wls_lat\", \"wls_lon\"]\n    )\nelse:\n    train_df = pd.concat(train_rows, ignore_index=True)\n\n# Use WLS lat/lon as predictions (baseline) in train_df\nif \"wls_lat\" in train_df.columns:\n    train_df[\"pred_lat\"] = train_df[\"wls_lat\"]\n    train_df[\"pred_lon\"] = train_df[\"wls_lon\"]\n\n# compute per-phone errors list according to competition metric\nphone_scores = []\nif \"lat\" in train_df.columns and \"pred_lat\" in train_df.columns:\n    phones = train_df[\"phone\"].unique()\n    for p in phones:\n        sub = train_df[train_df[\"phone\"] == p]\n        if sub.shape[0] == 0:\n            continue\n        dists = haversine(\n            sub[\"lat\"].values,\n            sub[\"lon\"].values,\n            sub[\"pred_lat\"].values,\n            sub[\"pred_lon\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        phone_scores.append((p50 + p95) / 2.0)\nif len(phone_scores) == 0:\n    mean_score = np.nan\nelse:\n    mean_score = np.mean(phone_scores)\nprint(\n    \"Validation competition metric (mean of per-phone (p50+p95)/2) using WLS baseline: {:.3f} meters\".format(\n        mean_score\n    )\n)\n\n# Now prepare test predictions using WLS baseline\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in input directory.\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Normalize column names (strip)\nsample_sub.columns = [c.strip() for c in sample_sub.columns]\n\n# Keep original column names to preserve output header style\norig_cols = list(sample_sub.columns)\n\n# find phone column robustly (accept tripId etc.)\nphone_col = None\nfor c in sample_sub.columns:\n    low = c.lower()\n    if low == \"phone\" or \"phone\" == low:\n        phone_col = c\n        break\n    if low == \"tripid\" or low == \"trip_id\" or low == \"tripid\":\n        phone_col = c\n        break\n# broader search\nif phone_col is None:\n    for c in sample_sub.columns:\n        low = c.lower()\n        if (\n            \"trip\" in low\n            or \"phone\" in low\n            or \"id\" in low\n            and len(sample_sub[c].astype(str).iloc[0]) > 5\n        ):\n            phone_col = c\n            break\nif phone_col is None:\n    # fallback: first column\n    phone_col = sample_sub.columns[0]\n\n# find time column robustly\ntime_col = None\nfor c in sample_sub.columns:\n    if c.lower().strip() in (\n        \"unixtimemillis\",\n        \"unixtime\",\n        \"time\",\n        \"utctime\",\n        \"utctimemillis\",\n        \"unixtimeMillis\".lower(),\n    ):\n        time_col = c\n        break\nif time_col is None:\n    for c in sample_sub.columns:\n        if \"time\" in c.lower() or \"unix\" in c.lower():\n            time_col = c\n            break\nif time_col is None:\n    # fallback: second column if there are at least 2\n    if len(sample_sub.columns) >= 2:\n        time_col = sample_sub.columns[1]\n    else:\n        raise RuntimeError(\n            f\"Could not find a UnixTimeMillis column in sample submission. Columns: {sample_sub.columns.tolist()}\"\n        )\n\n# For processing, rename to standardized names\nsample = sample_sub.rename(columns={phone_col: \"phone\", time_col: \"utcTimeMillis\"})\n# ensure types\nsample[\"utcTimeMillis\"] = sample[\"utcTimeMillis\"].astype(np.int64)\n\n# aggregate test\ntest_root = os.path.join(INPUT_DIR, \"test\")\ntest_dirs = glob.glob(os.path.join(test_root, \"*\", \"*\"))\ntest_agg_rows = []\nfor drive_phone in test_dirs:\n    parts = drive_phone.split(os.sep)\n    if len(parts) < 4:\n        continue\n    drive = parts[-2]\n    phone = parts[-1]\n    phone_full = f\"{drive}_{phone}\"\n    device_gnss_path = os.path.join(drive_phone, \"device_gnss.csv\")\n    if not os.path.exists(device_gnss_path):\n        continue\n    agg = aggregate_device_gnss(device_gnss_path)\n    if agg.shape[0] == 0:\n        continue\n    agg[\"phone\"] = phone_full\n    test_agg_rows.append(agg)\n\n# compute global means if needed from train\nif \"lat\" in train_df.columns and \"lon\" in train_df.columns and train_df.shape[0] > 0:\n    global_lat_mean = train_df[\"lat\"].mean()\n    global_lon_mean = train_df[\"lon\"].mean()\nelif (\n    \"wls_lat\" in train_df.columns\n    and \"wls_lon\" in train_df.columns\n    and train_df.shape[0] > 0\n):\n    global_lat_mean = train_df[\"wls_lat\"].mean()\n    global_lon_mean = train_df[\"wls_lon\"].mean()\nelse:\n    # fallback coordinates (someplace plausible)\n    global_lat_mean = 37.3875\n    global_lon_mean = -122.0575\n\nif len(test_agg_rows) == 0:\n    print(\n        \"Warning: No test aggregation found. Will fill submission with global mean coordinates.\"\n    )\n    submission_df = sample.copy()\n    submission_df[\"LatitudeDegrees\"] = global_lat_mean\n    submission_df[\"LongitudeDegrees\"] = global_lon_mean\n    # rename back to original column names for output\n    submission_df = submission_df.rename(\n        columns={\"phone\": phone_col, \"utcTimeMillis\": time_col}\n    )\n    out_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission_df.to_csv(\n        out_path,\n        index=False,\n        columns=[phone_col, time_col, \"LatitudeDegrees\", \"LongitudeDegrees\"],\n    )\n    print(\"Saved submission to\", out_path)\nelse:\n    test_agg = pd.concat(test_agg_rows, ignore_index=True)\n    # ensure types\n    test_agg[\"utcTimeMillis\"] = test_agg[\"utcTimeMillis\"].astype(np.int64)\n\n    # We'll use merge_asof with 'by' = phone to align nearest timestamps for each phone\n    # Prepare dataframes sorted\n    sample_sorted = sample.sort_values([\"phone\", \"utcTimeMillis\"]).reset_index(\n        drop=True\n    )\n    test_sorted = test_agg.sort_values([\"phone\", \"utcTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    merged = pd.merge_asof(\n        sample_sorted,\n        test_sorted,\n        on=\"utcTimeMillis\",\n        by=\"phone\",\n        direction=\"nearest\",\n        tolerance=2000,\n    )\n\n    # Fill predictions from wls columns if present\n    if \"wls_lat\" in merged.columns and \"wls_lon\" in merged.columns:\n        merged[\"LatitudeDegrees\"] = merged[\"wls_lat\"]\n        merged[\"LongitudeDegrees\"] = merged[\"wls_lon\"]\n    else:\n        merged[\"LatitudeDegrees\"] = np.nan\n        merged[\"LongitudeDegrees\"] = np.nan\n\n    # For any missing rows try a relaxed merge ignoring phone (find global nearest)\n    missing_mask = (\n        merged[\"LatitudeDegrees\"].isnull() | merged[\"LongitudeDegrees\"].isnull()\n    )\n    if missing_mask.any():\n        # do global nearest merge for missing times\n        sample_missing = sample_sorted.loc[missing_mask].sort_values(\"utcTimeMillis\")\n        global_merge = pd.merge_asof(\n            sample_missing,\n            test_sorted.sort_values(\"utcTimeMillis\"),\n            on=\"utcTimeMillis\",\n            direction=\"nearest\",\n            tolerance=2000,\n        )\n        # fill where available\n        for idx, row in global_merge.iterrows():\n            # find index in merged corresponding to this sample row position (match phone & time)\n            mask = (merged[\"phone\"] == row[\"phone\"]) & (\n                merged[\"utcTimeMillis\"] == row[\"utcTimeMillis\"]\n            )\n            if mask.any():\n                mi = merged.index[mask][0]\n                if \"wls_lat\" in global_merge.columns and not pd.isna(\n                    row.get(\"wls_lat\", np.nan)\n                ):\n                    merged.at[mi, \"LatitudeDegrees\"] = row[\"wls_lat\"]\n                    merged.at[mi, \"LongitudeDegrees\"] = row[\"wls_lon\"]\n\n    # final fill: fill any still-NaN with global means\n    merged[\"LatitudeDegrees\"] = merged[\"LatitudeDegrees\"].fillna(global_lat_mean)\n    merged[\"LongitudeDegrees\"] = merged[\"LongitudeDegrees\"].fillna(global_lon_mean)\n\n    # restore original sample order and original column names\n    # merged currently follows sample_sorted order; we want original sample_sub order\n    # We'll merge back to original sample_sub to preserve any extra columns and order\n    merged_out = sample.copy().reset_index(drop=True)\n    merged_out = merged_out.merge(\n        merged[[\"phone\", \"utcTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]],\n        on=[\"phone\", \"utcTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n\n    # Any remaining NaNs (shouldn't be) fill with global\n    merged_out[\"LatitudeDegrees\"] = merged_out[\"LatitudeDegrees\"].fillna(\n        global_lat_mean\n    )\n    merged_out[\"LongitudeDegrees\"] = merged_out[\"LongitudeDegrees\"].fillna(\n        global_lon_mean\n    )\n\n    # rename columns back to original sample names\n    merged_out = merged_out.rename(\n        columns={\"phone\": phone_col, \"utcTimeMillis\": time_col}\n    )\n\n    out_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    # Ensure output columns include original phone and time column names (as in sample) followed by lat/lon\n    merged_out.to_csv(\n        out_path,\n        index=False,\n        columns=[phone_col, time_col, \"LatitudeDegrees\", \"LongitudeDegrees\"],\n    )\n    print(\"Saved submission to\", out_path)\n",
                "exec_time": 86.18689274787903,
                "metric": null,
                "is_buggy": true,
                "exc_type": "ValueError",
                "exc_value": null,
                "analysis": "The script computed a validation metric using the WLS baseline (mean per-phone (p50+p95)/2) of 4.115 meters, but then crashed with a pandas ValueError ('left keys must be sorted') during a merge_asof step while preparing test predictions. Because of the crash, no submission.csv was produced in ./submission/.",
                "alignment": {
                        "plan_code_similarity": 0.004751921733053809,
                        "has_changes": true,
                        "change_magnitude": 0.9576255482117487
                }
        },
        {
                "index": 12,
                "step": 12,
                "id": "5ca505b866a947b28e766706d25ba327",
                "parent_id": null,
                "plan": "I'll fix the mapping and submission pipeline by making the WLS column detection robust (use regex case-insensitive matches for common variants like WlsPositionXEcefMeters) and improving time-column detection. I'll also ensure we always sort times, handle capitalization mismatches when locating test device files, and fill any remaining missing predictions with a sensible fallback so that a complete submission.csv is written. The script will validate on train (when ground truth exists), print the computed metric, and always save ./submission/submission.csv.",
                "code": "#!/usr/bin/env python3\nimport os\nimport sys\nimport math\nimport re\nimport numpy as np\nimport pandas as pd\n\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSAMPLE_SUB = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nOUTPUT_DIR = \"./submission\"\nOUTPUT_FILE = os.path.join(OUTPUT_DIR, \"submission.csv\")\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# --- utility functions ---\n\n\n# ECEF to geodetic (WGS84) conversion (iterative)\ndef ecef_to_latlon(x, y, z, tol=1e-12):\n    # returns lat_deg, lon_deg\n    a = 6378137.0  # semi-major axis\n    f = 1 / 298.257223563\n    e2 = f * (2 - f)  # first eccentricity squared\n    lon = math.atan2(y, x)\n    p = math.hypot(x, y)\n    if p < 1e-12:\n        # at poles\n        lat = math.copysign(math.pi / 2, z)\n        return math.degrees(lat), math.degrees(lon)\n    # initial lat\n    lat = math.atan2(z, p * (1 - e2))\n    for _ in range(50):\n        sin_lat = math.sin(lat)\n        N = a / math.sqrt(1 - e2 * sin_lat * sin_lat)\n        new_lat = math.atan2(z + e2 * N * sin_lat, p)\n        if abs(new_lat - lat) < tol:\n            lat = new_lat\n            break\n        lat = new_lat\n    return math.degrees(lat), math.degrees(lon)\n\n\n# Haversine distance (meters)\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = math.radians(lat1)\n    phi2 = math.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = math.radians(lon2 - lon1)\n    a = (\n        math.sin(dphi / 2.0) ** 2\n        + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * math.asin(math.sqrt(max(0.0, a)))\n\n\n# More robust WLS column finder using regex (case-insensitive)\ndef find_wls_cols(df):\n    cols = df.columns.tolist()\n    # First try the typical precise pattern: WlsPositionXEcefMeters etc.\n    xcol = None\n    ycol = None\n    zcol = None\n    for c in cols:\n        if re.search(r\"wls.*x.*ecef.*meter\", c, re.I):\n            xcol = c\n        if re.search(r\"wls.*y.*ecef.*meter\", c, re.I):\n            ycol = c\n        if re.search(r\"wls.*z.*ecef.*meter\", c, re.I):\n            zcol = c\n    if xcol and ycol and zcol:\n        return xcol, ycol, zcol\n    # If that fails, try more relaxed patterns: look for wls + X/Y/Z\n    for c in cols:\n        if re.search(r\"wls.*\\bpos.*x\\b\", c, re.I) or re.search(r\"wls.*x\\b\", c, re.I):\n            if xcol is None:\n                xcol = c\n        if re.search(r\"wls.*\\bpos.*y\\b\", c, re.I) or re.search(r\"wls.*y\\b\", c, re.I):\n            if ycol is None:\n                ycol = c\n        if re.search(r\"wls.*\\bpos.*z\\b\", c, re.I) or re.search(r\"wls.*z\\b\", c, re.I):\n            if zcol is None:\n                zcol = c\n    if xcol and ycol and zcol:\n        return xcol, ycol, zcol\n    # Final fallback: look for any columns that include 'wlsposition' and an axis letter\n    for c in cols:\n        lc = c.lower()\n        if \"wlsposition\" in lc or \"wls\" in lc:\n            if \"x\" in lc and xcol is None and \"y\" not in lc and \"z\" not in lc:\n                xcol = c\n            if \"y\" in lc and ycol is None and \"x\" not in lc and \"z\" not in lc:\n                ycol = c\n            if \"z\" in lc and zcol is None and \"x\" not in lc and \"y\" not in lc:\n                zcol = c\n    if xcol and ycol and zcol:\n        return xcol, ycol, zcol\n    return None, None, None\n\n\n# Build time->latlon mapping for a device_gnss csv\ndef build_wls_time_latlon_map(gnss_path):\n    try:\n        df = pd.read_csv(gnss_path)\n    except Exception as e:\n        print(f\"Failed to read {gnss_path}: {e}\", file=sys.stderr)\n        return None\n    if df.shape[0] == 0:\n        return None\n    # robust time column detection (case-insensitive)\n    time_col = None\n    candidates = [\n        \"utctimemillis\",\n        \"unixtimemillis\",\n        \"unixTimeMillis\".lower(),\n        \"unixtime_ms\",\n        \"utc_time_millis\",\n        \"utcTimeMillis\",\n        \"UnixTimeMillis\",\n    ]\n    for c in df.columns:\n        if c.lower() in [t.lower() for t in candidates]:\n            time_col = c\n            break\n    if time_col is None:\n        # fallback: pick numeric integer-like column that looks like time (large values)\n        for c in df.columns:\n            if np.issubdtype(df[c].dtype, np.integer) or np.issubdtype(\n                df[c].dtype, np.floating\n            ):\n                s = df[c].dropna()\n                if s.size > 0:\n                    v = float(s.iloc[0])\n                    # heuristics: milliseconds since epoch > 1e9\n                    if abs(v) > 1e9:\n                        time_col = c\n                        break\n        if time_col is None:\n            # final fallback to first column\n            time_col = df.columns[0]\n\n    xcol, ycol, zcol = find_wls_cols(df)\n    if xcol is None or ycol is None or zcol is None:\n        return None\n    # group by time and take first valid triple\n    keep = df[[time_col, xcol, ycol, zcol]].dropna()\n    if keep.shape[0] == 0:\n        return None\n    # Ensure time is integer-like\n    try:\n        keep[time_col] = keep[time_col].astype(np.int64)\n    except Exception:\n        # coerce\n        keep[time_col] = pd.to_numeric(keep[time_col], errors=\"coerce\").astype(\n            np.float64\n        )\n        keep = keep.dropna(subset=[time_col])\n        keep[time_col] = keep[time_col].astype(np.int64)\n    # There may be many rows for same time (satellites). Take first per time.\n    keep = keep.sort_values(time_col).drop_duplicates(time_col, keep=\"first\")\n    times = keep[time_col].values.astype(np.int64)\n    xs = keep[xcol].astype(float).values\n    ys = keep[ycol].astype(float).values\n    zs = keep[zcol].astype(float).values\n    # convert to lat/lon arrays\n    lats = np.empty_like(xs)\n    lons = np.empty_like(xs)\n    for i, (xx, yy, zz) in enumerate(zip(xs, ys, zs)):\n        try:\n            lat, lon = ecef_to_latlon(xx, yy, zz)\n        except Exception:\n            lat, lon = np.nan, np.nan\n        lats[i] = lat\n        lons[i] = lon\n    # filter out nans\n    mask = np.isfinite(lats) & np.isfinite(lons)\n    if not np.any(mask):\n        return None\n    # ensure sorting by times\n    order = np.argsort(times[mask])\n    return {\n        \"times\": times[mask][order],\n        \"lats\": lats[mask][order],\n        \"lons\": lons[mask][order],\n    }\n\n\n# Predict lat/lon for requested times by nearest neighbor in times array\ndef predict_from_map(tmap, query_times):\n    times = tmap[\"times\"]\n    lats = tmap[\"lats\"]\n    lons = tmap[\"lons\"]\n    if len(times) == 0:\n        return np.full(len(query_times), np.nan), np.full(len(query_times), np.nan)\n    # ensure sorted\n    idxs = np.searchsorted(times, query_times, side=\"left\")\n    preds_lat = np.empty(len(query_times))\n    preds_lon = np.empty(len(query_times))\n    for i, (q, idx) in enumerate(zip(query_times, idxs)):\n        cand = []\n        if idx < len(times):\n            cand.append((abs(int(times[idx]) - int(q)), idx))\n        if idx - 1 >= 0:\n            cand.append((abs(int(times[idx - 1]) - int(q)), idx - 1))\n        if len(cand) == 0:\n            preds_lat[i], preds_lon[i] = np.nan, np.nan\n        else:\n            _, chosen = min(cand, key=lambda x: x[0])\n            preds_lat[i] = lats[chosen]\n            preds_lon[i] = lons[chosen]\n    return preds_lat, preds_lon\n\n\n# --- Validation on train using ground_truth ---\ndef validate_on_train(max_files=None):\n    phone_errors = {}  # phone -> list of distances\n    processed = 0\n    if not os.path.isdir(TRAIN_DIR):\n        print(\"No train directory found, skipping validation.\", file=sys.stderr)\n        return None\n    for drive in sorted(os.listdir(TRAIN_DIR)):\n        drive_path = os.path.join(TRAIN_DIR, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n            tmap = build_wls_time_latlon_map(gnss_path)\n            if tmap is None:\n                continue\n            try:\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"failed read gt {gt_path}: {e}\", file=sys.stderr)\n                continue\n            if (\"UnixTimeMillis\" not in gt.columns) or (\n                \"LatitudeDegrees\" not in gt.columns\n            ):\n                # skip if unexpected\n                continue\n            q_times = gt[\"UnixTimeMillis\"].astype(np.int64).values\n            q_lats = gt[\"LatitudeDegrees\"].astype(float).values\n            q_lons = gt[\"LongitudeDegrees\"].astype(float).values\n            pred_lats, pred_lons = predict_from_map(tmap, q_times)\n            # compute distances\n            dists = []\n            for a, b, pa, pb in zip(q_lats, q_lons, pred_lats, pred_lons):\n                if np.isfinite(pa) and np.isfinite(pb):\n                    dists.append(haversine(a, b, pa, pb))\n            if len(dists) == 0:\n                continue\n            phone_key = f\"{drive}_{phone}\"\n            phone_errors[phone_key] = np.array(dists)\n            processed += 1\n            if max_files is not None and processed >= max_files:\n                break\n        if max_files is not None and processed >= max_files:\n            break\n    if len(phone_errors) == 0:\n        print(\"No train phones validated.\", file=sys.stderr)\n        return None\n    # compute per-phone 50th and 95th percentiles, average per phone, then mean across phones\n    per_phone_scores = []\n    for phone, dists in phone_errors.items():\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        per_phone_scores.append((p50 + p95) / 2.0)\n    metric = float(np.mean(per_phone_scores))\n    return metric, phone_errors\n\n\n# --- Build submission predictions by matching test device_gnss to sample_submission times ---\ndef build_submission(sample_sub_path):\n    sub = pd.read_csv(sample_sub_path)\n    # pre-create columns\n    out_lats = np.full(len(sub), np.nan, dtype=float)\n    out_lons = np.full(len(sub), np.nan, dtype=float)\n    # group rows by phone value\n    grouped = sub.groupby(\"phone\").indices\n    # Cache tmaps for test devices to avoid reading files multiple times\n    tmap_cache = {}\n    for phone_val, indices in grouped.items():\n        # parse drive and phone name\n        if \"_\" not in phone_val:\n            print(f\"Unexpected phone id format: {phone_val}\", file=sys.stderr)\n            continue\n        drive = phone_val.rsplit(\"_\", 1)[0]\n        phone_name = phone_val.rsplit(\"_\", 1)[1]\n        gnss_path = os.path.join(TEST_DIR, drive, phone_name, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            # try alternative capitalization or name matching\n            found = False\n            drive_path = os.path.join(TEST_DIR, drive)\n            if os.path.isdir(drive_path):\n                for candidate in os.listdir(drive_path):\n                    if candidate.lower() == phone_name.lower():\n                        gnss_path = os.path.join(\n                            drive_path, candidate, \"device_gnss.csv\"\n                        )\n                        if os.path.exists(gnss_path):\n                            found = True\n                            break\n            if not os.path.exists(gnss_path):\n                print(\n                    f\"Missing device_gnss for {phone_val} expected at {gnss_path}\",\n                    file=sys.stderr,\n                )\n                continue\n        # load or get from cache\n        cache_key = gnss_path\n        if cache_key in tmap_cache:\n            tmap = tmap_cache[cache_key]\n        else:\n            tmap = build_wls_time_latlon_map(gnss_path)\n            tmap_cache[cache_key] = tmap\n        query_times = sub.loc[indices, \"UnixTimeMillis\"].astype(np.int64).values\n        if tmap is None:\n            # fallback to NaNs for now; will fill later\n            continue\n        latp, lonp = predict_from_map(tmap, query_times)\n        out_lats[indices] = latp\n        out_lons[indices] = lonp\n    # fill any remaining NaNs by nearest available mapping (global)\n    nan_idx = np.where((~np.isfinite(out_lats)) | (~np.isfinite(out_lons)))[0]\n    if len(nan_idx) > 0:\n        print(\n            f\"Filling {len(nan_idx)} missing entries with nearest available mapping...\",\n            file=sys.stderr,\n        )\n        global_entries = []\n        # try to use cached tmaps first\n        for cache_key, tmap in tmap_cache.items():\n            if tmap is None:\n                continue\n            mid = len(tmap[\"times\"]) // 2\n            global_entries.append(\n                (tmap[\"times\"][mid], tmap[\"lats\"][mid], tmap[\"lons\"][mid])\n            )\n        # if still empty, scan test dir for any maps\n        if len(global_entries) == 0:\n            for drive in sorted(os.listdir(TEST_DIR)):\n                drive_path = os.path.join(TEST_DIR, drive)\n                if not os.path.isdir(drive_path):\n                    continue\n                for phone in sorted(os.listdir(drive_path)):\n                    gnss_path = os.path.join(drive_path, phone, \"device_gnss.csv\")\n                    if not os.path.exists(gnss_path):\n                        continue\n                    tmap = build_wls_time_latlon_map(gnss_path)\n                    if tmap is None:\n                        continue\n                    mid = len(tmap[\"times\"]) // 2\n                    global_entries.append(\n                        (tmap[\"times\"][mid], tmap[\"lats\"][mid], tmap[\"lons\"][mid])\n                    )\n        if len(global_entries) > 0:\n            global_times = np.array([e[0] for e in global_entries]).astype(np.int64)\n            global_lats = np.array([e[1] for e in global_entries])\n            global_lons = np.array([e[2] for e in global_entries])\n            for idx in nan_idx:\n                qt = int(sub.loc[idx, \"UnixTimeMillis\"])\n                j = np.argmin(np.abs(global_times - qt))\n                out_lats[idx] = global_lats[j]\n                out_lons[idx] = global_lons[j]\n        else:\n            # as last resort, fill with zeros (shouldn't happen with provided data)\n            out_lats[nan_idx] = 0.0\n            out_lons[nan_idx] = 0.0\n    # create submission dataframe\n    submission_df = pd.DataFrame(\n        {\n            \"phone\": sub[\"phone\"],\n            \"UnixTimeMillis\": sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": out_lats,\n            \"LongitudeDegrees\": out_lons,\n        }\n    )\n    submission_df.to_csv(OUTPUT_FILE, index=False)\n    return submission_df\n\n\ndef main():\n    print(\"Starting run...\", file=sys.stderr)\n    print(\n        \"Building validation metric on train set (matching WLS -> ground truth)...\",\n        file=sys.stderr,\n    )\n    metric_res = validate_on_train(max_files=None)  # use all available training drives\n    if metric_res is None:\n        print(\n            \"Validation failed or no data available. Proceeding to build submission.\",\n            file=sys.stderr,\n        )\n    else:\n        metric_value, phone_errors = metric_res\n        print(\n            f\"Validation metric (mean of per-phone (50th+95th)/2): {metric_value:.3f} meters\"\n        )\n    # Build submission predictions using test device_gnss WLS positions nearest to requested times\n    print(\"Building submission predictions...\", file=sys.stderr)\n    if not os.path.exists(SAMPLE_SUB):\n        print(f\"Sample submission not found at {SAMPLE_SUB}\", file=sys.stderr)\n        return\n    submission_df = build_submission(SAMPLE_SUB)\n    print(f\"Saved submission to {OUTPUT_FILE}\", file=sys.stderr)\n    # preview\n    print(\"Submission preview (first 5 rows):\")\n    if submission_df is not None:\n        print(submission_df.head(5).to_string(index=False))\n    else:\n        print(\"No submission dataframe produced.\", file=sys.stderr)\n\n\nif __name__ == \"__main__\":\n    main()\n",
                "exec_time": 2.3365020751953125e-05,
                "metric": null,
                "is_buggy": true,
                "exc_type": null,
                "exc_value": null,
                "analysis": "The run produced no detailed logs in the execution output, so I cannot confirm that a submission.csv was written. The script appears to have executed (no error messages shown), but the validation metric and submission file creation are not visible in the provided output. Metric is therefore unavailable.",
                "alignment": {
                        "plan_code_similarity": 0.006897803594118715,
                        "has_changes": true,
                        "change_magnitude": 0.9527728326223506
                }
        },
        {
                "index": 13,
                "step": 13,
                "id": "38c4fc173a36470e9ac3be46c7c25263",
                "parent_id": null,
                "plan": "I will fix the submission bug by making the test-sample -> aggregated test key matching much more robust: build multiple lookup maps (full key, drive-only, phone-only suffix) and use a cascading set of matching heuristics (exact, case-insensitive, suffix, substring, startswith) to find the best test aggregation for each sample row. I keep the original pipeline to aggregate WLS epochs, train Ridge models with GroupKFold CV (printing the CV metric), then predict test rows using the improved matching and save submission/submission.csv. The script also falls back to sample lat/lon or zeros if no reasonable match is found.",
                "code": "#!/usr/bin/env python3\nimport os\nimport sys\nimport glob\nimport math\nfrom collections import defaultdict, Counter\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GroupKFold\n\n\n# Utilities\ndef ensure_dir(p):\n    if not os.path.exists(p):\n        os.makedirs(p)\n\n\ndef ecef_to_latlon(x, y, z):\n    # WGS84 conversion from ECEF to lat/lon (approx)\n    a = 6378137.0\n    e = 8.1819190842622e-2\n    asq = a * a\n    esq = e * e\n    b = math.sqrt(asq * (1 - esq))\n    ep = math.sqrt((asq - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3, p - esq * a * math.cos(th) ** 3\n    )\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine_meters(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return R * 2 * np.arcsin(np.sqrt(a))\n\n\n# Paths\nINPUT = \"./input\"\nTRAIN_DIR = os.path.join(INPUT, \"train\")\nTEST_DIR = os.path.join(INPUT, \"test\")\nSAMPLE_SUB = os.path.join(INPUT, \"sample_submission.csv\")\nSUBMISSION_OUT = os.path.join(\"submission\")\nensure_dir(SUBMISSION_OUT)\nSUBMISSION_PATH = os.path.join(SUBMISSION_OUT, \"submission.csv\")\n\n\n# Helper to find column by keywords (case-insensitive)\ndef find_col(cols, keywords):\n    cols_l = [c.lower() for c in cols]\n    for kw in keywords:\n        for i, c in enumerate(cols_l):\n            if kw in c:\n                return cols[i]\n    return None\n\n\nprint(\"Starting aggregation and training...\")\n\n# Step 1: Aggregate train GNSS epochs and join to ground truth\ntrain_agg_rows = []\nprint(\"Scanning train folders...\")\nfor drive_path in sorted(glob.glob(os.path.join(TRAIN_DIR, \"*\"))):\n    if not os.path.isdir(drive_path):\n        continue\n    drive = os.path.basename(drive_path)\n    for phone in sorted(os.listdir(drive_path)):\n        phone_dir = os.path.join(drive_path, phone)\n        if not os.path.isdir(phone_dir):\n            continue\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n        if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n            continue\n        try:\n            usecols = [\n                \"utcTimeMillis\",\n                \"Svid\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateMetersPerSecond\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            df = pd.read_csv(gnss_path, usecols=lambda c: c in usecols)\n        except Exception:\n            df = pd.read_csv(gnss_path)\n            df = df[[c for c in usecols if c in df.columns]]\n        # If Wls columns missing skip\n        wls_cols = [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        missing_wls = any(c not in df.columns for c in wls_cols)\n        if missing_wls or df.shape[0] == 0:\n            continue\n        # Group by epoch\n        agg = (\n            df.groupby(\"utcTimeMillis\")\n            .agg(\n                WlsX=(\"WlsPositionXEcefMeters\", \"first\"),\n                WlsY=(\"WlsPositionYEcefMeters\", \"first\"),\n                WlsZ=(\"WlsPositionZEcefMeters\", \"first\"),\n                sat_count=(\"Svid\", lambda s: s.nunique()),\n                mean_cn0=(\"Cn0DbHz\", \"mean\"),\n                std_cn0=(\"Cn0DbHz\", \"std\"),\n                mean_prr=(\"PseudorangeRateMetersPerSecond\", \"mean\"),\n            )\n            .reset_index()\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        )\n        # Convert to lat/lon\n        lat_list = []\n        lon_list = []\n        for xi, yi, zi in zip(agg.WlsX.values, agg.WlsY.values, agg.WlsZ.values):\n            try:\n                lat, lon = ecef_to_latlon(float(xi), float(yi), float(zi))\n            except Exception:\n                lat, lon = np.nan, np.nan\n            lat_list.append(lat)\n            lon_list.append(lon)\n        agg[\"WlsLat\"] = lat_list\n        agg[\"WlsLon\"] = lon_list\n        # read ground truth\n        gt = pd.read_csv(gt_path)\n        # find required gt columns robustly\n        gt_cols = gt.columns.tolist()\n        time_col = find_col(gt_cols, [\"unixtimemillis\", \"unixtime\", \"time\"])\n        lat_col = find_col(gt_cols, [\"latitude\", \"lat\"])\n        lon_col = find_col(gt_cols, [\"longitude\", \"lon\", \"long\"])\n        if time_col is None or lat_col is None or lon_col is None:\n            continue\n        gt = gt[[time_col, lat_col, lon_col]].dropna()\n        gt = gt.rename(\n            columns={\n                time_col: \"UnixTimeMillis\",\n                lat_col: \"LatitudeDegrees\",\n                lon_col: \"LongitudeDegrees\",\n            }\n        )\n        gt = gt.sort_values(\"UnixTimeMillis\").reset_index(drop=True)\n        agg = agg.sort_values(\"UnixTimeMillis\").reset_index(drop=True)\n        if agg.shape[0] == 0 or gt.shape[0] == 0:\n            continue\n        # For each agg epoch, find nearest gt time using searchsorted\n        gt_times = gt.UnixTimeMillis.values\n        agg_times = agg.UnixTimeMillis.values\n        idxs = np.searchsorted(gt_times, agg_times)\n        chosen_lat = []\n        chosen_lon = []\n        chosen_dt = []\n        for i, t in enumerate(agg_times):\n            best_dt = None\n            lat_val = np.nan\n            lon_val = np.nan\n            for cand in (idxs[i] - 1, idxs[i]):\n                if 0 <= cand < len(gt_times):\n                    dt = abs(gt_times[cand] - t)\n                    if best_dt is None or dt < best_dt:\n                        best_dt = dt\n                        lat_val = float(gt.LatitudeDegrees.values[cand])\n                        lon_val = float(gt.LongitudeDegrees.values[cand])\n            chosen_lat.append(lat_val)\n            chosen_lon.append(lon_val)\n            chosen_dt.append(best_dt if best_dt is not None else np.nan)\n        agg[\"GtLat\"] = chosen_lat\n        agg[\"GtLon\"] = chosen_lon\n        agg[\"time_diff_ms\"] = chosen_dt\n        # keep only fairly close matches (<=500ms)\n        agg = agg[agg.time_diff_ms <= 500].copy()\n        if agg.shape[0] == 0:\n            continue\n        agg[\"drive\"] = drive\n        agg[\"phone\"] = phone\n        # compute deltas (degrees)\n        agg[\"dLat\"] = agg[\"GtLat\"] - agg[\"WlsLat\"]\n        agg[\"dLon\"] = agg[\"GtLon\"] - agg[\"WlsLon\"]\n        train_agg_rows.append(\n            agg[\n                [\n                    \"drive\",\n                    \"phone\",\n                    \"UnixTimeMillis\",\n                    \"WlsX\",\n                    \"WlsY\",\n                    \"WlsZ\",\n                    \"WlsLat\",\n                    \"WlsLon\",\n                    \"sat_count\",\n                    \"mean_cn0\",\n                    \"std_cn0\",\n                    \"mean_prr\",\n                    \"GtLat\",\n                    \"GtLon\",\n                    \"dLat\",\n                    \"dLon\",\n                ]\n            ]\n        )\n    print(\n        \"Processed drive:\",\n        drive,\n        \"current total epochs:\",\n        sum([len(x) for x in train_agg_rows]) if len(train_agg_rows) > 0 else 0,\n    )\n\nif len(train_agg_rows) == 0:\n    print(\"No train data found or no matching WLS/ground truth pairs. Exiting.\")\n    sys.exit(1)\n\ntrain_df = pd.concat(train_agg_rows, ignore_index=True)\nprint(\"Total training epochs collected:\", len(train_df))\n\n# Prepare features and targets\nfeat_cols = [\"WlsLat\", \"WlsLon\", \"sat_count\", \"mean_cn0\", \"std_cn0\", \"mean_prr\"]\nfor c in feat_cols:\n    if c not in train_df.columns:\n        train_df[c] = 0.0\ntrain_df = train_df.dropna(subset=[\"dLat\", \"dLon\", \"WlsLat\", \"WlsLon\"])\nX = train_df[feat_cols].fillna(0.0).values\ny_lat = train_df[\"dLat\"].values\ny_lon = train_df[\"dLon\"].values\ngroups = train_df[\"drive\"].values\n\n# 5-fold GroupKFold by drive\ngkf = GroupKFold(n_splits=5)\nlat_preds = np.zeros_like(y_lat)\nlon_preds = np.zeros_like(y_lon)\n\nprint(\"Running 5-fold CV with Ridge...\")\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X[train_idx], X[val_idx]\n    ylat_tr, ylat_val = y_lat[train_idx], y_lat[val_idx]\n    ylon_tr, ylon_val = y_lon[train_idx], y_lon[val_idx]\n    model_lat = Ridge(alpha=1.0)\n    model_lon = Ridge(alpha=1.0)\n    model_lat.fit(X_tr, ylat_tr)\n    model_lon.fit(X_tr, ylon_tr)\n    lat_preds[val_idx] = model_lat.predict(X_val)\n    lon_preds[val_idx] = model_lon.predict(X_val)\n    print(f\" Fold {fold+1} done.\")\n\n# Evaluate on CV predictions using competition metric\ntrain_df[\"pred_dLat\"] = lat_preds\ntrain_df[\"pred_dLon\"] = lon_preds\ntrain_df[\"pred_lat\"] = train_df[\"WlsLat\"] + train_df[\"pred_dLat\"]\ntrain_df[\"pred_lon\"] = train_df[\"WlsLon\"] + train_df[\"pred_dLon\"]\ntrain_df[\"err_m\"] = haversine_meters(\n    train_df[\"pred_lat\"].values,\n    train_df[\"pred_lon\"].values,\n    train_df[\"GtLat\"].values,\n    train_df[\"GtLon\"].values,\n)\nphone_groups = train_df.groupby([\"drive\", \"phone\"])\nphone_scores = []\nfor (drive, phone), gp in phone_groups:\n    errs = gp[\"err_m\"].values\n    if len(errs) < 5:\n        continue\n    p50 = np.percentile(errs, 50)\n    p95 = np.percentile(errs, 95)\n    phone_scores.append((p50 + p95) / 2.0)\nif len(phone_scores) == 0:\n    mean_metric = float(\"nan\")\nelse:\n    mean_metric = float(np.mean(phone_scores))\nprint(\n    \"CV competition metric (mean of per-phone (50th+95th)/2): {:.6f} meters\".format(\n        mean_metric\n    )\n)\n\n# Train final models on all train data\nfinal_lat = Ridge(alpha=1.0).fit(X, y_lat)\nfinal_lon = Ridge(alpha=1.0).fit(X, y_lon)\nprint(\"Final models trained on all training data.\")\n\n# Build test predictions by reading device_gnss per test phone and matching sample_submission times\nprint(\"Loading sample submission to build test predictions...\")\nsample = pd.read_csv(SAMPLE_SUB)\nscols = sample.columns.tolist()\ns_phone_col = find_col(scols, [\"phone\", \"device\", \"drive\"])\ns_time_col = find_col(scols, [\"unixtimemillis\", \"unixtime\", \"time\"])\ns_lat_col = find_col(scols, [\"latitude\", \"lat\"])\ns_lon_col = find_col(scols, [\"longitude\", \"lon\", \"long\"])\nif s_phone_col is None:\n    s_phone_col = scols[0]\nif s_time_col is None:\n    s_time_col = scols[1] if len(scols) > 1 else scols[0]\n\nout_rows = []\ntest_cache = {}\nprint(\"Scanning test folders and aggregating GNSS epochs...\")\nfor drive_path in sorted(glob.glob(os.path.join(TEST_DIR, \"*\"))):\n    if not os.path.isdir(drive_path):\n        continue\n    drive = os.path.basename(drive_path)\n    for phone in sorted(os.listdir(drive_path)):\n        phone_dir = os.path.join(drive_path, phone)\n        if not os.path.isdir(phone_dir):\n            continue\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            continue\n        try:\n            df = pd.read_csv(\n                gnss_path,\n                usecols=lambda c: c\n                in [\n                    \"utcTimeMillis\",\n                    \"Svid\",\n                    \"Cn0DbHz\",\n                    \"PseudorangeRateMetersPerSecond\",\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ],\n            )\n        except Exception:\n            df = pd.read_csv(gnss_path)\n            df = df[\n                [\n                    c\n                    for c in [\n                        \"utcTimeMillis\",\n                        \"Svid\",\n                        \"Cn0DbHz\",\n                        \"PseudorangeRateMetersPerSecond\",\n                        \"WlsPositionXEcefMeters\",\n                        \"WlsPositionYEcefMeters\",\n                        \"WlsPositionZEcefMeters\",\n                    ]\n                    if c in df.columns\n                ]\n            ]\n        if df.shape[0] == 0:\n            continue\n        if not all(\n            c in df.columns\n            for c in [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        ):\n            continue\n        agg = (\n            df.groupby(\"utcTimeMillis\")\n            .agg(\n                WlsX=(\"WlsPositionXEcefMeters\", \"first\"),\n                WlsY=(\"WlsPositionYEcefMeters\", \"first\"),\n                WlsZ=(\"WlsPositionZEcefMeters\", \"first\"),\n                sat_count=(\"Svid\", lambda s: s.nunique()),\n                mean_cn0=(\"Cn0DbHz\", \"mean\"),\n                std_cn0=(\"Cn0DbHz\", \"std\"),\n                mean_prr=(\"PseudorangeRateMetersPerSecond\", \"mean\"),\n            )\n            .reset_index()\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        )\n        lat_list = []\n        lon_list = []\n        for xi, yi, zi in zip(agg.WlsX.values, agg.WlsY.values, agg.WlsZ.values):\n            try:\n                lat, lon = ecef_to_latlon(float(xi), float(yi), float(zi))\n            except Exception:\n                lat, lon = np.nan, np.nan\n            lat_list.append(lat)\n            lon_list.append(lon)\n        agg[\"WlsLat\"] = lat_list\n        agg[\"WlsLon\"] = lon_list\n        agg = agg.sort_values(\"UnixTimeMillis\").reset_index(drop=True)\n        key = drive + \"_\" + phone\n        test_cache[key] = agg\n        print(\"Aggregated test:\", key, \"epochs:\", len(agg))\n\n# Build auxiliary indexes for robust matching\nkeys = list(test_cache.keys())\nkeys_lower = {k.lower(): k for k in keys}\nkeys_by_suffix = defaultdict(list)\nkeys_by_drive = defaultdict(list)\nkeys_by_phone = defaultdict(list)\nfor k in keys:\n    parts = k.split(\"_\")\n    if len(parts) >= 2:\n        drive_part = \"_\".join(parts[:-1])\n        phone_part = parts[-1]\n    else:\n        drive_part = parts[0]\n        phone_part = parts[-1]\n    keys_by_drive[drive_part].append(k)\n    keys_by_phone[phone_part].append(k)\n    keys_by_suffix[phone_part.lower()].append(k)\n\n\ndef match_test_key(phone_val):\n    if not isinstance(phone_val, str):\n        return None\n    pv = phone_val.strip()\n    if pv in test_cache:\n        return pv\n    pv_low = pv.lower()\n    # direct case-insensitive match\n    if pv_low in keys_lower:\n        return keys_lower[pv_low]\n    # if sample already just phone suffix like \"GooglePixel4\"\n    suffix = pv.split(\"_\")[-1]\n    s_low = suffix.lower()\n    if s_low in keys_by_suffix and len(keys_by_suffix[s_low]) == 1:\n        return keys_by_suffix[s_low][0]\n    # if sample contains drive + phone but with slightly different separators, try find key that contains both parts\n    parts = pv.split(\"_\")\n    for k in keys:\n        kl = k.lower()\n        if all(\n            any(part.lower() in kl for part in parts) for part in [parts[0], parts[-1]]\n        ):\n            return k\n    # if suffix match but multiple candidates, try to narrow by drive prefix presence\n    if len(parts) >= 2:\n        drive_part = parts[0]\n        candidates = [\n            k for k in keys_by_suffix.get(s_low, []) if k.startswith(drive_part)\n        ]\n        if len(candidates) == 1:\n            return candidates[0]\n    # try any key that contains the suffix as substring\n    for k in keys:\n        if s_low in k.lower():\n            return k\n    # final fallback: if pv contains only drive part, return first key for that drive\n    drive_guess = pv.split(\"_\")[0]\n    if drive_guess in keys_by_drive and len(keys_by_drive[drive_guess]) >= 1:\n        return keys_by_drive[drive_guess][0]\n    return None\n\n\nprint(\"Constructing submission predictions with robust matching...\")\nout_records = []\nmissing_count = 0\ntotal = len(sample)\nfor i, row in sample.iterrows():\n    phone_val = row[s_phone_col]\n    # parse time\n    tval = row[s_time_col]\n    try:\n        t = int(tval)\n    except Exception:\n        try:\n            t = int(float(tval))\n        except Exception:\n            t = None\n    key = None\n    if isinstance(phone_val, str):\n        key = match_test_key(phone_val)\n    if key is None or key not in test_cache or t is None:\n        missing_count += 1\n        # fallback to sample lat/lon if available, else zeros\n        if (\n            s_lat_col is not None\n            and s_lon_col is not None\n            and (not pd.isna(row[s_lat_col]))\n            and (not pd.isna(row[s_lon_col]))\n        ):\n            pred_lat = row[s_lat_col]\n            pred_lon = row[s_lon_col]\n        else:\n            pred_lat = 0.0\n            pred_lon = 0.0\n    else:\n        agg = test_cache[key]\n        times = agg.UnixTimeMillis.values\n        if len(times) == 0:\n            missing_count += 1\n            if (\n                s_lat_col is not None\n                and s_lon_col is not None\n                and (not pd.isna(row[s_lat_col]))\n                and (not pd.isna(row[s_lon_col]))\n            ):\n                pred_lat = row[s_lat_col]\n                pred_lon = row[s_lon_col]\n            else:\n                pred_lat = 0.0\n                pred_lon = 0.0\n        else:\n            pos = np.searchsorted(times, t)\n            cand_idx = []\n            for cand in (pos - 1, pos):\n                if 0 <= cand < len(times):\n                    cand_idx.append(cand)\n            # choose nearest\n            best_idx = None\n            best_dt = None\n            for ci in cand_idx:\n                dt = abs(int(times[ci]) - t)\n                if best_dt is None or dt < best_dt:\n                    best_dt = dt\n                    best_idx = ci\n            row_agg = agg.iloc[best_idx]\n            wls_lat = row_agg[\"WlsLat\"]\n            wls_lon = row_agg[\"WlsLon\"]\n            feat = np.array(\n                [\n                    wls_lat,\n                    wls_lon,\n                    (\n                        row_agg.get(\"sat_count\", 0)\n                        if not pd.isna(row_agg.get(\"sat_count\", np.nan))\n                        else 0.0\n                    ),\n                    (\n                        row_agg.get(\"mean_cn0\", 0.0)\n                        if not pd.isna(row_agg.get(\"mean_cn0\", np.nan))\n                        else 0.0\n                    ),\n                    (\n                        row_agg.get(\"std_cn0\", 0.0)\n                        if not pd.isna(row_agg.get(\"std_cn0\", np.nan))\n                        else 0.0\n                    ),\n                    (\n                        row_agg.get(\"mean_prr\", 0.0)\n                        if not pd.isna(row_agg.get(\"mean_prr\", np.nan))\n                        else 0.0\n                    ),\n                ]\n            ).reshape(1, -1)\n            pred_dlat = final_lat.predict(feat)[0]\n            pred_dlon = final_lon.predict(feat)[0]\n            pred_lat = wls_lat + pred_dlat\n            pred_lon = wls_lon + pred_dlon\n    # Ensure numeric values\n    try:\n        pred_lat = float(pred_lat)\n    except Exception:\n        pred_lat = 0.0\n    try:\n        pred_lon = float(pred_lon)\n    except Exception:\n        pred_lon = 0.0\n    out_records.append((phone_val, int(t) if t is not None else -1, pred_lat, pred_lon))\n\nprint(\n    \"Missing/unknown test epochs (after robust matching):\",\n    missing_count,\n    \"out of\",\n    total,\n)\n\n# Write submission with required header names\nout_df = pd.DataFrame(\n    out_records,\n    columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"],\n)\nout_df.to_csv(SUBMISSION_PATH, index=False)\nprint(\"Saved submission to\", SUBMISSION_PATH)\nprint(\"Done.\")\n",
                "exec_time": 63.985435009002686,
                "metric": null,
                "is_buggy": true,
                "exc_type": null,
                "exc_value": null,
                "analysis": "The script completed end-to-end and saved submission/submission.csv, but the robust matching failed to match any sample submission rows to the aggregated test data, so all 37087 test epochs were treated as missing and fallback values were used. The CV evaluation on the training data reported a mean per-phone metric of 333.296688 meters.",
                "alignment": {
                        "plan_code_similarity": 0.007451194674879539,
                        "has_changes": true,
                        "change_magnitude": 0.9689896256202075
                }
        },
        {
                "index": 14,
                "step": 14,
                "id": "7efe0cb1fe524617a6096e9e6e408069",
                "parent_id": null,
                "plan": "I will fix the merge_asof error by avoiding a single global merge_asof that requires the entire left keys to be strictly sorted; instead I will perform the nearest-time alignment per-phone (merge_asof within each phone group after sorting by utcTimeMillis). For rows where a phone has no test WLS data, I'll fall back to a global nearest-time merge and finally fill any remaining missing predictions with a global mean. The script preserves the original sample submission format, prints the validation metric computed on the training WLS vs ground-truth, and writes the required submission CSV to ./submission/submission.csv.",
                "code": "import os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom math import sqrt\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 conversion\n    a = 6378137.0\n    e = 8.1819190842622e-2\n    b = sqrt(a * a * (1 - e * e))\n    ep = sqrt((a * a - b * b) / (b * b))\n    p = np.sqrt(x * x + y * y)\n    th = np.arctan2(a * z, b * p)\n    lon = np.arctan2(y, x)\n    lat = np.arctan2(\n        (z + ep * ep * b * (np.sin(th) ** 3)), (p - e * e * a * (np.cos(th) ** 3))\n    )\n    N = a / np.sqrt(1 - e * e * (np.sin(lat) ** 2))\n    alt = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), alt\n\n\n# haversine in meters\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n\n\ndef aggregate_device_gnss(fn):\n    try:\n        df = pd.read_csv(fn)\n    except Exception:\n        return pd.DataFrame()\n    if df.shape[0] == 0:\n        return pd.DataFrame()\n    if \"utcTimeMillis\" not in df.columns:\n        return pd.DataFrame()\n\n    group = df.groupby(\"utcTimeMillis\", sort=True)\n    out = pd.DataFrame({\"utcTimeMillis\": group.size().index})\n    out = out.set_index(\"utcTimeMillis\")\n\n    # WLS positions if available\n    if \"WlsPositionXEcefMeters\" in df.columns:\n        out[\"WlsX\"] = group[\"WlsPositionXEcefMeters\"].first()\n        out[\"WlsY\"] = (\n            group[\"WlsPositionYEcefMeters\"].first()\n            if \"WlsPositionYEcefMeters\" in df.columns\n            else np.nan\n        )\n        out[\"WlsZ\"] = (\n            group[\"WlsPositionZEcefMeters\"].first()\n            if \"WlsPositionZEcefMeters\" in df.columns\n            else np.nan\n        )\n    else:\n        out[\"WlsX\"] = np.nan\n        out[\"WlsY\"] = np.nan\n        out[\"WlsZ\"] = np.nan\n\n    # numeric means\n    out[\"cn0_mean\"] = group[\"Cn0DbHz\"].mean() if \"Cn0DbHz\" in df.columns else np.nan\n    out[\"elev_mean\"] = (\n        group[\"SvElevationDegrees\"].mean()\n        if \"SvElevationDegrees\" in df.columns\n        else np.nan\n    )\n    out[\"azim_mean\"] = (\n        group[\"SvAzimuthDegrees\"].mean() if \"SvAzimuthDegrees\" in df.columns else np.nan\n    )\n    out[\"pr_mean\"] = (\n        group[\"PseudorangeRateMetersPerSecond\"].mean()\n        if \"PseudorangeRateMetersPerSecond\" in df.columns\n        else np.nan\n    )\n    out[\"sv_count\"] = group[\"Svid\"].nunique() if \"Svid\" in df.columns else np.nan\n\n    out = out.reset_index()\n\n    # drop rows where Wls positions missing entirely\n    has_wls = out[[\"WlsX\", \"WlsY\", \"WlsZ\"]].notnull().all(axis=1)\n    if has_wls.sum() == 0:\n        return pd.DataFrame()\n    out = out.loc[has_wls].copy()\n\n    # convert ECEF to lat/lon\n    lats = []\n    lons = []\n    alts = []\n    for _, r in out.iterrows():\n        lat, lon, alt = ecef_to_geodetic(r[\"WlsX\"], r[\"WlsY\"], r[\"WlsZ\"])\n        lats.append(lat)\n        lons.append(lon)\n        alts.append(alt)\n    out[\"wls_lat\"] = lats\n    out[\"wls_lon\"] = lons\n    out[\"utcTimeMillis\"] = out[\"utcTimeMillis\"].astype(np.int64)\n    return out\n\n\n# Build training dataset using the WLS baseline\ntrain_root = os.path.join(INPUT_DIR, \"train\")\ntrain_dirs = glob.glob(os.path.join(train_root, \"*\", \"*\"))\ntrain_rows = []\nfor drive_phone in train_dirs:\n    parts = drive_phone.split(os.sep)\n    if len(parts) < 4:\n        continue\n    drive = parts[-2]\n    phone = parts[-1]\n    phone_full = f\"{drive}_{phone}\"\n    device_gnss_path = os.path.join(drive_phone, \"device_gnss.csv\")\n    ground_truth_path = os.path.join(drive_phone, \"ground_truth.csv\")\n    if not os.path.exists(device_gnss_path) or not os.path.exists(ground_truth_path):\n        continue\n    agg = aggregate_device_gnss(device_gnss_path)\n    if agg.shape[0] == 0:\n        continue\n    try:\n        gt = pd.read_csv(ground_truth_path)\n    except Exception:\n        continue\n    if gt.shape[0] == 0:\n        continue\n    gt = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].rename(\n        columns={\n            \"UnixTimeMillis\": \"utcTimeMillis\",\n            \"LatitudeDegrees\": \"lat\",\n            \"LongitudeDegrees\": \"lon\",\n        }\n    )\n    agg_sorted = agg.sort_values(\"utcTimeMillis\")\n    gt_sorted = gt.sort_values(\"utcTimeMillis\")\n    merged = pd.merge_asof(\n        agg_sorted,\n        gt_sorted,\n        on=\"utcTimeMillis\",\n        direction=\"nearest\",\n        tolerance=1000,\n    )\n    merged = merged.dropna(subset=[\"lat\", \"lon\"])\n    if merged.shape[0] == 0:\n        continue\n    merged[\"phone\"] = phone_full\n    train_rows.append(merged)\n\nif len(train_rows) == 0:\n    train_df = pd.DataFrame(\n        columns=[\"phone\", \"utcTimeMillis\", \"lat\", \"lon\", \"wls_lat\", \"wls_lon\"]\n    )\nelse:\n    train_df = pd.concat(train_rows, ignore_index=True)\n\n# Use WLS lat/lon as predictions (baseline) in train_df\nif \"wls_lat\" in train_df.columns:\n    train_df[\"pred_lat\"] = train_df[\"wls_lat\"]\n    train_df[\"pred_lon\"] = train_df[\"wls_lon\"]\n\n# compute per-phone errors list according to competition metric\nphone_scores = []\nif \"lat\" in train_df.columns and \"pred_lat\" in train_df.columns:\n    phones = train_df[\"phone\"].unique()\n    for p in phones:\n        sub = train_df[train_df[\"phone\"] == p]\n        if sub.shape[0] == 0:\n            continue\n        dists = haversine(\n            sub[\"lat\"].values,\n            sub[\"lon\"].values,\n            sub[\"pred_lat\"].values,\n            sub[\"pred_lon\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        phone_scores.append((p50 + p95) / 2.0)\nif len(phone_scores) == 0:\n    mean_score = np.nan\nelse:\n    mean_score = np.mean(phone_scores)\nprint(\n    \"Validation competition metric (mean of per-phone (p50+p95)/2) using WLS baseline: {:.3f} meters\".format(\n        mean_score\n    )\n)\n\n# Now prepare test predictions using WLS baseline\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in input directory.\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Normalize column names (strip)\nsample_sub.columns = [c.strip() for c in sample_sub.columns]\n\n# Keep original column names to preserve output header style\norig_cols = list(sample_sub.columns)\n\n# find phone column robustly\nphone_col = None\nfor c in sample_sub.columns:\n    low = c.lower()\n    if low == \"phone\" or low == \"tripid\" or low == \"trip_id\":\n        phone_col = c\n        break\nif phone_col is None:\n    # broader heuristics\n    for c in sample_sub.columns:\n        low = c.lower()\n        if (\n            \"phone\" in low\n            or \"trip\" in low\n            or (\"id\" in low and len(sample_sub[c].astype(str).iloc[0]) > 5)\n        ):\n            phone_col = c\n            break\nif phone_col is None:\n    phone_col = sample_sub.columns[0]\n\n# find time column robustly\ntime_col = None\nfor c in sample_sub.columns:\n    if c.lower().strip() in (\n        \"unixtimemillis\",\n        \"unixtime\",\n        \"time\",\n        \"utctime\",\n        \"utctimemillis\",\n    ):\n        time_col = c\n        break\nif time_col is None:\n    for c in sample_sub.columns:\n        if \"time\" in c.lower() or \"unix\" in c.lower():\n            time_col = c\n            break\nif time_col is None:\n    if len(sample_sub.columns) >= 2:\n        time_col = sample_sub.columns[1]\n    else:\n        raise RuntimeError(\n            f\"Could not find a UnixTimeMillis column in sample submission. Columns: {sample_sub.columns.tolist()}\"\n        )\n\n# For processing, rename to standardized names\nsample = sample_sub.rename(columns={phone_col: \"phone\", time_col: \"utcTimeMillis\"})\n# ensure types\nsample[\"utcTimeMillis\"] = sample[\"utcTimeMillis\"].astype(np.int64)\n\n# aggregate test\ntest_root = os.path.join(INPUT_DIR, \"test\")\ntest_dirs = glob.glob(os.path.join(test_root, \"*\", \"*\"))\ntest_agg_rows = []\nfor drive_phone in test_dirs:\n    parts = drive_phone.split(os.sep)\n    if len(parts) < 4:\n        continue\n    drive = parts[-2]\n    phone = parts[-1]\n    phone_full = f\"{drive}_{phone}\"\n    device_gnss_path = os.path.join(drive_phone, \"device_gnss.csv\")\n    if not os.path.exists(device_gnss_path):\n        continue\n    agg = aggregate_device_gnss(device_gnss_path)\n    if agg.shape[0] == 0:\n        continue\n    agg[\"phone\"] = phone_full\n    test_agg_rows.append(agg)\n\n# compute global means if needed from train\nif \"lat\" in train_df.columns and \"lon\" in train_df.columns and train_df.shape[0] > 0:\n    global_lat_mean = train_df[\"lat\"].mean()\n    global_lon_mean = train_df[\"lon\"].mean()\nelif (\n    \"wls_lat\" in train_df.columns\n    and \"wls_lon\" in train_df.columns\n    and train_df.shape[0] > 0\n):\n    global_lat_mean = train_df[\"wls_lat\"].mean()\n    global_lon_mean = train_df[\"wls_lon\"].mean()\nelse:\n    global_lat_mean = 37.3875\n    global_lon_mean = -122.0575\n\nif len(test_agg_rows) == 0:\n    print(\n        \"Warning: No test aggregation found. Will fill submission with global mean coordinates.\"\n    )\n    submission_df = sample.copy()\n    submission_df[\"LatitudeDegrees\"] = global_lat_mean\n    submission_df[\"LongitudeDegrees\"] = global_lon_mean\n    # rename back to original column names for output\n    submission_df = submission_df.rename(\n        columns={\"phone\": phone_col, \"utcTimeMillis\": time_col}\n    )\n    out_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission_df.to_csv(\n        out_path,\n        index=False,\n        columns=[phone_col, time_col, \"LatitudeDegrees\", \"LongitudeDegrees\"],\n    )\n    print(\"Saved submission to\", out_path)\nelse:\n    test_agg = pd.concat(test_agg_rows, ignore_index=True)\n    test_agg[\"utcTimeMillis\"] = test_agg[\"utcTimeMillis\"].astype(np.int64)\n\n    # Build a mapping phone -> test subset sorted by time for per-phone merge_asof\n    test_by_phone = {}\n    for phone_val, g in test_agg.groupby(\"phone\"):\n        test_by_phone[phone_val] = g.sort_values(\"utcTimeMillis\").reset_index(drop=True)\n\n    # Prepare output frame as copy of sample\n    out_df = sample.copy().reset_index(drop=True)\n    out_df[\"LatitudeDegrees\"] = np.nan\n    out_df[\"LongitudeDegrees\"] = np.nan\n\n    # For each phone in the sample, try to align with test data for the same phone\n    for phone_val, group_idx in out_df.groupby(\"phone\").groups.items():\n        sub_sample = out_df.loc[group_idx].copy()\n        if phone_val in test_by_phone:\n            test_sub = test_by_phone[phone_val]\n            # both sorted by utcTimeMillis\n            sub_sorted = sub_sample.sort_values(\"utcTimeMillis\")\n            merged = pd.merge_asof(\n                sub_sorted,\n                test_sub,\n                on=\"utcTimeMillis\",\n                direction=\"nearest\",\n                tolerance=2000,\n            )\n            # Fill lat/lon where available\n            fill_mask = merged[\"wls_lat\"].notnull() & merged[\"wls_lon\"].notnull()\n            for idx, row in merged[fill_mask].iterrows():\n                out_df_idx = merged.loc[idx, \"utcTimeMillis\"]\n            # assign back to out_df\n            out_df.loc[merged.index, \"LatitudeDegrees\"] = merged[\"wls_lat\"].values\n            out_df.loc[merged.index, \"LongitudeDegrees\"] = merged[\"wls_lon\"].values\n        else:\n            # No per-phone test data, leave NaNs for now\n            continue\n\n    # For any rows still missing, attempt a global nearest-time match ignoring phone\n    missing_mask = (\n        out_df[\"LatitudeDegrees\"].isnull() | out_df[\"LongitudeDegrees\"].isnull()\n    )\n    if missing_mask.any():\n        sample_missing = (\n            out_df.loc[missing_mask].sort_values(\"utcTimeMillis\").reset_index()\n        )\n        # merge with entire test_agg sorted by utcTimeMillis\n        test_sorted_global = test_agg.sort_values(\"utcTimeMillis\").reset_index(\n            drop=True\n        )\n        global_merge = pd.merge_asof(\n            sample_missing.sort_values(\"utcTimeMillis\"),\n            test_sorted_global,\n            on=\"utcTimeMillis\",\n            direction=\"nearest\",\n            tolerance=2000,\n        )\n        # Fill where available\n        for i, gm_row in global_merge.iterrows():\n            orig_index = sample_missing.loc[gm_row[\"index\"], \"index\"]\n            # gm_row may have NaN wls_lat\n            if not pd.isna(gm_row.get(\"wls_lat\", np.nan)) and not pd.isna(\n                gm_row.get(\"wls_lon\", np.nan)\n            ):\n                out_df.at[orig_index, \"LatitudeDegrees\"] = gm_row[\"wls_lat\"]\n                out_df.at[orig_index, \"LongitudeDegrees\"] = gm_row[\"wls_lon\"]\n\n    # final fill: fill any still-NaN with global means\n    out_df[\"LatitudeDegrees\"] = out_df[\"LatitudeDegrees\"].fillna(global_lat_mean)\n    out_df[\"LongitudeDegrees\"] = out_df[\"LongitudeDegrees\"].fillna(global_lon_mean)\n\n    # restore original sample order and original column names\n    merged_out = out_df.copy()\n    merged_out = merged_out.rename(\n        columns={\"phone\": phone_col, \"utcTimeMillis\": time_col}\n    )\n\n    # Ensure the output includes original sample columns plus predictions; but competition expects phone,time,lat,lon\n    final_out = merged_out[[phone_col, time_col, \"LatitudeDegrees\", \"LongitudeDegrees\"]]\n\n    out_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    final_out.to_csv(out_path, index=False)\n    print(\"Saved submission to\", out_path)\n",
                "exec_time": 89.48636436462402,
                "metric": 4.115,
                "is_buggy": false,
                "exc_type": null,
                "exc_value": null,
                "analysis": "The script ran successfully, computed a validation metric using the WLS baseline and saved a submission file. The mean per-phone (p50+p95)/2 validation metric reported was 4.115 meters and submission.csv was written to ./submission/submission.csv.",
                "alignment": {
                        "plan_code_similarity": 0.014846235418875928,
                        "has_changes": true,
                        "change_magnitude": 0.958335856597832
                }
        },
        {
                "index": 15,
                "step": 15,
                "id": "7554f3836d904e258afc1971d5b5eb9d",
                "parent_id": null,
                "plan": "I will fix robustness and assignment bugs that likely prevented a submission: (1) make column detection for time and WLS ECEF coordinates more flexible and case-insensitive, with several fallbacks; (2) use boolean masks rather than group indices to safely assign predicted rows; (3) ensure all times are coerced to int64 and handle missing maps by filling from global medians; (4) run validation on training drives (limited for speed), print the evaluation metric, and always write ./submission/submission.csv. The code below implements these fixes and prints the validation metric and a preview of the saved submission.",
                "code": "#!/usr/bin/env python3\nimport os\nimport sys\nimport math\nimport re\nimport numpy as np\nimport pandas as pd\n\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSAMPLE_SUB = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nOUTPUT_DIR = \"./submission\"\nOUTPUT_FILE = os.path.join(OUTPUT_DIR, \"submission.csv\")\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# ECEF to geodetic (WGS84) conversion (iterative)\ndef ecef_to_latlon(x, y, z, tol=1e-12):\n    # supports scalar or numpy arrays\n    scalar_input = False\n    if np.isscalar(x):\n        scalar_input = True\n        x = np.array([x], dtype=float)\n        y = np.array([y], dtype=float)\n        z = np.array([z], dtype=float)\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    a = 6378137.0  # semi-major axis\n    f = 1 / 298.257223563\n    e2 = f * (2 - f)  # first eccentricity squared\n    lon = np.arctan2(y, x)\n    p = np.hypot(x, y)\n    lat = np.arctan2(z, p * (1 - e2))  # initial\n    # handle poles where p ~ 0\n    pole_mask = p < 1e-12\n    lat[pole_mask] = np.sign(z[pole_mask]) * (math.pi / 2.0)\n    # iterative refinement for non-pole\n    nonpole = ~pole_mask\n    for _ in range(50):\n        sin_lat = np.sin(lat[nonpole])\n        N = a / np.sqrt(1 - e2 * sin_lat * sin_lat)\n        new_lat = np.arctan2(z[nonpole] + e2 * N * sin_lat, p[nonpole])\n        if np.all(np.abs(new_lat - lat[nonpole]) < tol):\n            lat[nonpole] = new_lat\n            break\n        lat[nonpole] = new_lat\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    if scalar_input:\n        return float(lat_deg[0]), float(lon_deg[0])\n    return lat_deg, lon_deg\n\n\n# Haversine distance (meters)\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = math.radians(lat1)\n    phi2 = math.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = math.radians(lon2 - lon1)\n    a = (\n        math.sin(dphi / 2.0) ** 2\n        + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * math.asin(math.sqrt(max(0.0, a)))\n\n\n# Robust WLS column finder using multiple patterns\ndef find_wls_cols(df):\n    cols = df.columns.tolist()\n    lc = [c.lower() for c in cols]\n    # Common exact names\n    patterns = [\n        (\"wlspositionxecefmeters\", \"wlspositionyecefmeters\", \"wlspositionzecefmeters\"),\n        (\n            \"wlsposition_x_ecef_meters\",\n            \"wlsposition_y_ecef_meters\",\n            \"wlsposition_z_ecef_meters\",\n        ),\n        (\"wlsposition_x_ecef\", \"wlsposition_y_ecef\", \"wlsposition_z_ecef\"),\n        (\"wls_x\", \"wls_y\", \"wls_z\"),\n    ]\n    for px, py, pz in patterns:\n        try:\n            xi = lc.index(px)\n            yi = lc.index(py)\n            zi = lc.index(pz)\n            return cols[xi], cols[yi], cols[zi]\n        except Exception:\n            continue\n    # regex search\n    xcol = ycol = zcol = None\n    for c, c_l in zip(cols, lc):\n        if re.search(r\"wls.*x.*ecef.*meter\", c_l):\n            xcol = c\n        if re.search(r\"wls.*y.*ecef.*meter\", c_l):\n            ycol = c\n        if re.search(r\"wls.*z.*ecef.*meter\", c_l):\n            zcol = c\n    if xcol and ycol and zcol:\n        return xcol, ycol, zcol\n    # relaxed: look for 'wls' and axis letters\n    for c, c_l in zip(cols, lc):\n        if \"wls\" in c_l:\n            if (\"x\" in c_l and xcol is None) or re.search(r\"\\bx\\b\", c_l):\n                xcol = xcol or c\n            if (\"y\" in c_l and ycol is None) or re.search(r\"\\by\\b\", c_l):\n                ycol = ycol or c\n            if (\"z\" in c_l and zcol is None) or re.search(r\"\\bz\\b\", c_l):\n                zcol = zcol or c\n    if xcol and ycol and zcol:\n        return xcol, ycol, zcol\n    # final fallback: look for 'ecef' and axis letters\n    for c, c_l in zip(cols, lc):\n        if \"ecef\" in c_l:\n            if \"x\" in c_l and xcol is None:\n                xcol = c\n            if \"y\" in c_l and ycol is None:\n                ycol = c\n            if \"z\" in c_l and zcol is None:\n                zcol = c\n    if xcol and ycol and zcol:\n        return xcol, ycol, zcol\n    return None, None, None\n\n\n# Build time->latlon mapping for a device_gnss csv\ndef build_wls_time_latlon_map(gnss_path):\n    try:\n        df = pd.read_csv(gnss_path)\n    except Exception as e:\n        print(f\"Failed to read {gnss_path}: {e}\", file=sys.stderr)\n        return None\n    if df.shape[0] == 0:\n        return None\n    # detect time column robustly\n    time_col = None\n    for c in df.columns:\n        if c.lower() in (\n            \"utctimemillis\",\n            \"utc_time_millis\",\n            \"utcTimeMillis\".lower(),\n            \"unixtimemillis\",\n            \"unixtime_millis\",\n            \"unixtime\",\n            \"utc\",\n            \"unixtimemillis\".lower(),\n        ):\n            time_col = c\n            break\n    if time_col is None:\n        # pick column with large integer-like values\n        for c in df.columns:\n            if np.issubdtype(df[c].dtype, np.integer) or np.issubdtype(\n                df[c].dtype, np.floating\n            ):\n                s = df[c].dropna()\n                if s.size > 0:\n                    try:\n                        v = float(s.iloc[0])\n                    except Exception:\n                        continue\n                    if abs(v) > 1e9:\n                        time_col = c\n                        break\n    if time_col is None:\n        # give up\n        time_col = df.columns[0]\n    xcol, ycol, zcol = find_wls_cols(df)\n    if xcol is None or ycol is None or zcol is None:\n        # no WLS position columns found\n        return None\n    # select and drop rows without positional data\n    keep = df[[time_col, xcol, ycol, zcol]].copy()\n    keep = keep.dropna(subset=[xcol, ycol, zcol])\n    if keep.shape[0] == 0:\n        return None\n    # coerce time to int64 (milliseconds)\n    try:\n        keep[time_col] = pd.to_numeric(keep[time_col], errors=\"coerce\")\n    except Exception:\n        keep[time_col] = keep[time_col]\n    keep = keep.dropna(subset=[time_col])\n    if keep.shape[0] == 0:\n        return None\n    # times may be floats; round/coerce to int64\n    try:\n        keep[time_col] = keep[time_col].astype(np.int64)\n    except Exception:\n        keep[time_col] = (keep[time_col].astype(np.float64)).astype(np.int64)\n    # drop duplicates per time, keep first\n    keep = keep.sort_values(by=time_col).drop_duplicates(\n        subset=[time_col], keep=\"first\"\n    )\n    times = keep[time_col].values.astype(np.int64)\n    xs = keep[xcol].astype(float).values\n    ys = keep[ycol].astype(float).values\n    zs = keep[zcol].astype(float).values\n    # convert to lat/lon arrays (vectorized)\n    try:\n        lats, lons = ecef_to_latlon(xs, ys, zs)\n    except Exception:\n        # fallback iterative\n        lats = np.empty_like(xs)\n        lons = np.empty_like(xs)\n        for i, (xx, yy, zz) in enumerate(zip(xs, ys, zs)):\n            try:\n                lat, lon = ecef_to_latlon(xx, yy, zz)\n            except Exception:\n                lat, lon = np.nan, np.nan\n            lats[i] = lat\n            lons[i] = lon\n    mask = np.isfinite(lats) & np.isfinite(lons)\n    if not np.any(mask):\n        return None\n    order = np.argsort(times[mask])\n    return {\n        \"times\": times[mask][order],\n        \"lats\": lats[mask][order],\n        \"lons\": lons[mask][order],\n    }\n\n\n# Predict lat/lon for requested times by nearest neighbor in times array\ndef predict_from_map(tmap, query_times):\n    times = tmap[\"times\"]\n    lats = tmap[\"lats\"]\n    lons = tmap[\"lons\"]\n    if len(times) == 0:\n        return np.full(len(query_times), np.nan), np.full(len(query_times), np.nan)\n    # ensure arrays\n    qt = np.asarray(query_times, dtype=np.int64)\n    idxs = np.searchsorted(times, qt, side=\"left\")\n    preds_lat = np.empty(len(qt))\n    preds_lon = np.empty(len(qt))\n    for i, (q, idx) in enumerate(zip(qt, idxs)):\n        cand = []\n        if idx < len(times):\n            cand.append((abs(int(times[idx]) - int(q)), idx))\n        if idx - 1 >= 0:\n            cand.append((abs(int(times[idx - 1]) - int(q)), idx - 1))\n        if len(cand) == 0:\n            preds_lat[i], preds_lon[i] = np.nan, np.nan\n        else:\n            _, chosen = min(cand, key=lambda x: x[0])\n            preds_lat[i] = lats[chosen]\n            preds_lon[i] = lons[chosen]\n    return preds_lat, preds_lon\n\n\n# --- Validation on train using ground_truth ---\ndef validate_on_train(max_files=None):\n    phone_errors = {}  # phone -> list of distances\n    processed = 0\n    if not os.path.isdir(TRAIN_DIR):\n        print(\"No train directory found, skipping validation.\", file=sys.stderr)\n        return None\n    # iterate drives\n    for drive in sorted(os.listdir(TRAIN_DIR)):\n        drive_path = os.path.join(TRAIN_DIR, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n            tmap = build_wls_time_latlon_map(gnss_path)\n            if tmap is None:\n                continue\n            try:\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"failed read gt {gt_path}: {e}\", file=sys.stderr)\n                continue\n            # robust column names\n            if not any(c.lower() == \"unixtimemillis\" for c in gt.columns) or not any(\n                c.lower() == \"latitudedegrees\" for c in gt.columns\n            ):\n                continue\n            # standardize column names\n            time_col = [c for c in gt.columns if c.lower() == \"unixtimemillis\"][0]\n            lat_col = [c for c in gt.columns if c.lower() == \"latitudedegrees\"][0]\n            lon_col = [c for c in gt.columns if c.lower() == \"longitudedegrees\"][0]\n            q_times = gt[time_col].astype(np.int64).values\n            q_lats = gt[lat_col].astype(float).values\n            q_lons = gt[lon_col].astype(float).values\n            pred_lats, pred_lons = predict_from_map(tmap, q_times)\n            dists = []\n            for a, b, pa, pb in zip(q_lats, q_lons, pred_lats, pred_lons):\n                if np.isfinite(pa) and np.isfinite(pb):\n                    dists.append(haversine(a, b, pa, pb))\n            if len(dists) == 0:\n                continue\n            phone_key = f\"{drive}_{phone}\"\n            phone_errors[phone_key] = np.array(dists)\n            processed += 1\n            if max_files is not None and processed >= max_files:\n                break\n        if max_files is not None and processed >= max_files:\n            break\n    if len(phone_errors) == 0:\n        print(\"No train phones validated.\", file=sys.stderr)\n        return None\n    per_phone_scores = []\n    for phone, dists in phone_errors.items():\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        per_phone_scores.append((p50 + p95) / 2.0)\n    metric = float(np.mean(per_phone_scores))\n    return metric, phone_errors\n\n\n# --- Build submission predictions by matching test device_gnss to sample_submission times ---\ndef build_submission(sample_sub_path):\n    sub = pd.read_csv(sample_sub_path)\n    # ensure required columns exist\n    if not all(c in sub.columns for c in [\"phone\", \"UnixTimeMillis\"]):\n        raise ValueError(\"Sample submission missing required columns\")\n    n = len(sub)\n    out_lats = np.full(n, np.nan, dtype=float)\n    out_lons = np.full(n, np.nan, dtype=float)\n    # group by phone using boolean masks for safe indexing\n    phones = sub[\"phone\"].unique()\n    # Cache tmaps\n    tmap_cache = {}\n    global_entries = []\n    # pre-scan test dir to build at least some tmap cache (speeds fallback)\n    for drive in sorted(os.listdir(TEST_DIR)):\n        drive_path = os.path.join(TEST_DIR, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone_name in sorted(os.listdir(drive_path)):\n            gnss_path = os.path.join(drive_path, phone_name, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            if gnss_path in tmap_cache:\n                continue\n            tmap = build_wls_time_latlon_map(gnss_path)\n            tmap_cache[gnss_path] = tmap\n            if tmap is not None:\n                mid = len(tmap[\"times\"]) // 2\n                global_entries.append(\n                    (tmap[\"times\"][mid], tmap[\"lats\"][mid], tmap[\"lons\"][mid])\n                )\n    if len(global_entries) > 0:\n        global_times = np.array([e[0] for e in global_entries]).astype(np.int64)\n        global_lats = np.array([e[1] for e in global_entries])\n        global_lons = np.array([e[2] for e in global_entries])\n    else:\n        global_times = np.array([], dtype=np.int64)\n        global_lats = np.array([])\n        global_lons = np.array([])\n\n    for phone_val in phones:\n        mask = sub[\"phone\"] == phone_val\n        indices = np.where(mask)[0]\n        # parse drive and phone name: sample uses format drive_phoneName (drive may contain underscores)\n        if \"_\" not in phone_val:\n            print(f\"Unexpected phone id format: {phone_val}\", file=sys.stderr)\n            continue\n        # split from right once: drive part may contain many underscores\n        drive = phone_val.rsplit(\"_\", 1)[0]\n        phone_name = phone_val.rsplit(\"_\", 1)[1]\n        gnss_path = os.path.join(TEST_DIR, drive, phone_name, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            # try candidate matching under drive\n            found = False\n            drive_path = os.path.join(TEST_DIR, drive)\n            if os.path.isdir(drive_path):\n                for candidate in os.listdir(drive_path):\n                    if candidate.lower() == phone_name.lower():\n                        cand_path = os.path.join(\n                            drive_path, candidate, \"device_gnss.csv\"\n                        )\n                        if os.path.exists(cand_path):\n                            gnss_path = cand_path\n                            found = True\n                            break\n            if not os.path.exists(gnss_path):\n                print(\n                    f\"Missing device_gnss for {phone_val} expected at {gnss_path}\",\n                    file=sys.stderr,\n                )\n                # leave as NaN for now, will fill later\n                continue\n        # load or get from cache\n        if gnss_path in tmap_cache:\n            tmap = tmap_cache[gnss_path]\n        else:\n            tmap = build_wls_time_latlon_map(gnss_path)\n            tmap_cache[gnss_path] = tmap\n        query_times = sub.loc[mask, \"UnixTimeMillis\"].astype(np.int64).values\n        if tmap is None:\n            # try to find another map under same drive (different phone) in tmap_cache\n            alt_found = False\n            drive_path = os.path.join(TEST_DIR, drive)\n            if os.path.isdir(drive_path):\n                for candidate in os.listdir(drive_path):\n                    cand_path = os.path.join(drive_path, candidate, \"device_gnss.csv\")\n                    if cand_path in tmap_cache and tmap_cache[cand_path] is not None:\n                        tmap_alt = tmap_cache[cand_path]\n                        latp, lonp = predict_from_map(tmap_alt, query_times)\n                        out_lats[indices] = latp\n                        out_lons[indices] = lonp\n                        alt_found = True\n                        break\n            if alt_found:\n                continue\n            # else we will leave NaNs for now\n            continue\n        latp, lonp = predict_from_map(tmap, query_times)\n        out_lats[indices] = latp\n        out_lons[indices] = lonp\n\n    # fill NaNs using global medians from global_entries\n    nan_idx = np.where((~np.isfinite(out_lats)) | (~np.isfinite(out_lons)))[0]\n    if len(nan_idx) > 0:\n        print(\n            f\"Filling {len(nan_idx)} missing entries with nearest available mapping...\",\n            file=sys.stderr,\n        )\n        if len(global_times) > 0:\n            for idx in nan_idx:\n                qt = int(sub.loc[idx, \"UnixTimeMillis\"])\n                j = np.argmin(np.abs(global_times - qt))\n                out_lats[idx] = global_lats[j]\n                out_lons[idx] = global_lons[j]\n        else:\n            # absolute fallback: set to zeros (shouldn't happen)\n            out_lats[nan_idx] = 0.0\n            out_lons[nan_idx] = 0.0\n\n    submission_df = pd.DataFrame(\n        {\n            \"phone\": sub[\"phone\"],\n            \"UnixTimeMillis\": sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": out_lats,\n            \"LongitudeDegrees\": out_lons,\n        }\n    )\n    submission_df.to_csv(OUTPUT_FILE, index=False)\n    return submission_df\n\n\ndef main():\n    print(\"Starting run...\", file=sys.stderr)\n    print(\n        \"Building validation metric on train set (matching WLS -> ground truth)...\",\n        file=sys.stderr,\n    )\n    # limit number of train phones processed for speed; increase if desired\n    val_res = validate_on_train(max_files=60)\n    if val_res is None:\n        print(\n            \"Validation failed or no data available. Proceeding to build submission.\",\n            file=sys.stderr,\n        )\n    else:\n        metric_value, phone_errors = val_res\n        print(\n            f\"Validation metric (mean of per-phone (50th+95th)/2): {metric_value:.3f} meters\"\n        )\n        # print summary counts\n        print(f\"Validated {len(phone_errors)} phones (sample).\", file=sys.stderr)\n    print(\"Building submission predictions...\", file=sys.stderr)\n    if not os.path.exists(SAMPLE_SUB):\n        print(f\"Sample submission not found at {SAMPLE_SUB}\", file=sys.stderr)\n        return\n    submission_df = build_submission(SAMPLE_SUB)\n    if submission_df is not None:\n        print(f\"Saved submission to {OUTPUT_FILE}\", file=sys.stderr)\n        # print preview\n        print(\"Submission preview (first 5 rows):\")\n        print(submission_df.head(5).to_string(index=False))\n    else:\n        print(\"No submission dataframe produced.\", file=sys.stderr)\n\n\nif __name__ == \"__main__\":\n    main()\n",
                "exec_time": 0.0001862049102783203,
                "metric": null,
                "is_buggy": true,
                "exc_type": null,
                "exc_value": null,
                "analysis": "The run produced no visible script logs or outputs besides the execution time, and no submission.csv was created in ./submission. Validation metric could not be computed because the script did not process/train on any data (no evidence of reading train/test files).",
                "alignment": {
                        "plan_code_similarity": 0.0056197646060863115,
                        "has_changes": true,
                        "change_magnitude": 0.9497512751086203
                }
        },
        {
                "index": 16,
                "step": 16,
                "id": "5d9156a01dfe437b855e72aa95b44227",
                "parent_id": null,
                "plan": "The bug was that no test rows were matched to aggregated test epochs, so every prediction fell back to zeros. I'll replace the fragile matching logic with a robust normalization-based matcher: normalize strings (lowercase, remove non-alphanumerics), precompute normalized keys for test data, and match by exact normalized key, substring, suffix, and token overlap. I keep the same training/CV pipeline and still compute the CV metric, then use the improved matching to produce real predictions and save them to submission/submission.csv. The script prints the CV metric and a short summary of how many sample rows were matched vs fallback.",
                "code": "#!/usr/bin/env python3\nimport os\nimport sys\nimport glob\nimport math\nimport re\nfrom collections import defaultdict, Counter\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GroupKFold\n\n\n# Utilities\ndef ensure_dir(p):\n    if not os.path.exists(p):\n        os.makedirs(p)\n\n\ndef ecef_to_latlon(x, y, z):\n    # WGS84 conversion from ECEF to lat/lon (approx)\n    a = 6378137.0\n    e = 8.1819190842622e-2\n    asq = a * a\n    esq = e * e\n    b = math.sqrt(asq * (1 - esq))\n    ep = math.sqrt((asq - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3, p - esq * a * math.cos(th) ** 3\n    )\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine_meters(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return R * 2 * np.arcsin(np.sqrt(a))\n\n\ndef normalize_key(s):\n    if not isinstance(s, str):\n        return \"\"\n    s = s.strip().lower()\n    # replace separators with underscore\n    s = re.sub(r\"[\\s\\-]+\", \"_\", s)\n    # remove anything not alnum or underscore\n    s = re.sub(r\"[^0-9a-z_]\", \"\", s)\n    return s\n\n\n# Paths\nINPUT = \"./input\"\nTRAIN_DIR = os.path.join(INPUT, \"train\")\nTEST_DIR = os.path.join(INPUT, \"test\")\nSAMPLE_SUB = os.path.join(INPUT, \"sample_submission.csv\")\nSUBMISSION_OUT = os.path.join(\"submission\")\nensure_dir(SUBMISSION_OUT)\nSUBMISSION_PATH = os.path.join(SUBMISSION_OUT, \"submission.csv\")\n\n\n# Helper to find column by keywords (case-insensitive)\ndef find_col(cols, keywords):\n    cols_l = [c.lower() for c in cols]\n    for kw in keywords:\n        for i, c in enumerate(cols_l):\n            if kw in c:\n                return cols[i]\n    return None\n\n\nprint(\"Starting aggregation and training...\")\n\n# Step 1: Aggregate train GNSS epochs and join to ground truth\ntrain_agg_rows = []\nprint(\"Scanning train folders...\")\nfor drive_path in sorted(glob.glob(os.path.join(TRAIN_DIR, \"*\"))):\n    if not os.path.isdir(drive_path):\n        continue\n    drive = os.path.basename(drive_path)\n    for phone in sorted(os.listdir(drive_path)):\n        phone_dir = os.path.join(drive_path, phone)\n        if not os.path.isdir(phone_dir):\n            continue\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n        if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n            continue\n        try:\n            usecols = [\n                \"utcTimeMillis\",\n                \"Svid\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateMetersPerSecond\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            df = pd.read_csv(gnss_path, usecols=lambda c: c in usecols)\n        except Exception:\n            df = pd.read_csv(gnss_path)\n            df = df[[c for c in usecols if c in df.columns]]\n        wls_cols = [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        missing_wls = any(c not in df.columns for c in wls_cols)\n        if missing_wls or df.shape[0] == 0:\n            continue\n        # Group by epoch\n        agg = (\n            df.groupby(\"utcTimeMillis\")\n            .agg(\n                WlsX=(\"WlsPositionXEcefMeters\", \"first\"),\n                WlsY=(\"WlsPositionYEcefMeters\", \"first\"),\n                WlsZ=(\"WlsPositionZEcefMeters\", \"first\"),\n                sat_count=(\"Svid\", lambda s: s.nunique()),\n                mean_cn0=(\"Cn0DbHz\", \"mean\"),\n                std_cn0=(\"Cn0DbHz\", \"std\"),\n                mean_prr=(\"PseudorangeRateMetersPerSecond\", \"mean\"),\n            )\n            .reset_index()\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        )\n        # Convert to lat/lon\n        lat_list = []\n        lon_list = []\n        for xi, yi, zi in zip(agg.WlsX.values, agg.WlsY.values, agg.WlsZ.values):\n            try:\n                lat, lon = ecef_to_latlon(float(xi), float(yi), float(zi))\n            except Exception:\n                lat, lon = np.nan, np.nan\n            lat_list.append(lat)\n            lon_list.append(lon)\n        agg[\"WlsLat\"] = lat_list\n        agg[\"WlsLon\"] = lon_list\n        # read ground truth\n        gt = pd.read_csv(gt_path)\n        gt_cols = gt.columns.tolist()\n        time_col = find_col(gt_cols, [\"unixtimemillis\", \"unixtime\", \"time\"])\n        lat_col = find_col(gt_cols, [\"latitude\", \"lat\"])\n        lon_col = find_col(gt_cols, [\"longitude\", \"lon\", \"long\"])\n        if time_col is None or lat_col is None or lon_col is None:\n            continue\n        gt = gt[[time_col, lat_col, lon_col]].dropna()\n        gt = gt.rename(\n            columns={\n                time_col: \"UnixTimeMillis\",\n                lat_col: \"LatitudeDegrees\",\n                lon_col: \"LongitudeDegrees\",\n            }\n        )\n        gt = gt.sort_values(\"UnixTimeMillis\").reset_index(drop=True)\n        agg = agg.sort_values(\"UnixTimeMillis\").reset_index(drop=True)\n        if agg.shape[0] == 0 or gt.shape[0] == 0:\n            continue\n        # For each agg epoch, find nearest gt time using searchsorted\n        gt_times = gt.UnixTimeMillis.values\n        agg_times = agg.UnixTimeMillis.values\n        idxs = np.searchsorted(gt_times, agg_times)\n        chosen_lat = []\n        chosen_lon = []\n        chosen_dt = []\n        for i, t in enumerate(agg_times):\n            best_dt = None\n            lat_val = np.nan\n            lon_val = np.nan\n            for cand in (idxs[i] - 1, idxs[i]):\n                if 0 <= cand < len(gt_times):\n                    dt = abs(gt_times[cand] - t)\n                    if best_dt is None or dt < best_dt:\n                        best_dt = dt\n                        lat_val = float(gt.LatitudeDegrees.values[cand])\n                        lon_val = float(gt.LongitudeDegrees.values[cand])\n            chosen_lat.append(lat_val)\n            chosen_lon.append(lon_val)\n            chosen_dt.append(best_dt if best_dt is not None else np.nan)\n        agg[\"GtLat\"] = chosen_lat\n        agg[\"GtLon\"] = chosen_lon\n        agg[\"time_diff_ms\"] = chosen_dt\n        # keep only fairly close matches (<=500ms)\n        agg = agg[agg.time_diff_ms <= 500].copy()\n        if agg.shape[0] == 0:\n            continue\n        agg[\"drive\"] = drive\n        agg[\"phone\"] = phone\n        # compute deltas (degrees)\n        agg[\"dLat\"] = agg[\"GtLat\"] - agg[\"WlsLat\"]\n        agg[\"dLon\"] = agg[\"GtLon\"] - agg[\"WlsLon\"]\n        train_agg_rows.append(\n            agg[\n                [\n                    \"drive\",\n                    \"phone\",\n                    \"UnixTimeMillis\",\n                    \"WlsX\",\n                    \"WlsY\",\n                    \"WlsZ\",\n                    \"WlsLat\",\n                    \"WlsLon\",\n                    \"sat_count\",\n                    \"mean_cn0\",\n                    \"std_cn0\",\n                    \"mean_prr\",\n                    \"GtLat\",\n                    \"GtLon\",\n                    \"dLat\",\n                    \"dLon\",\n                ]\n            ]\n        )\n    print(\n        \"Processed drive:\",\n        drive,\n        \"current total epochs:\",\n        sum([len(x) for x in train_agg_rows]) if len(train_agg_rows) > 0 else 0,\n    )\n\nif len(train_agg_rows) == 0:\n    print(\"No train data found or no matching WLS/ground truth pairs. Exiting.\")\n    sys.exit(1)\n\ntrain_df = pd.concat(train_agg_rows, ignore_index=True)\nprint(\"Total training epochs collected:\", len(train_df))\n\n# Prepare features and targets\nfeat_cols = [\"WlsLat\", \"WlsLon\", \"sat_count\", \"mean_cn0\", \"std_cn0\", \"mean_prr\"]\nfor c in feat_cols:\n    if c not in train_df.columns:\n        train_df[c] = 0.0\ntrain_df = train_df.dropna(subset=[\"dLat\", \"dLon\", \"WlsLat\", \"WlsLon\"])\nX = train_df[feat_cols].fillna(0.0).values\ny_lat = train_df[\"dLat\"].values\ny_lon = train_df[\"dLon\"].values\ngroups = train_df[\"drive\"].values\n\n# 5-fold GroupKFold by drive\ngkf = GroupKFold(n_splits=5)\nlat_preds = np.zeros_like(y_lat)\nlon_preds = np.zeros_like(y_lon)\n\nprint(\"Running 5-fold CV with Ridge...\")\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X[train_idx], X[val_idx]\n    ylat_tr, ylat_val = y_lat[train_idx], y_lat[val_idx]\n    ylon_tr, ylon_val = y_lon[train_idx], y_lon[val_idx]\n    model_lat = Ridge(alpha=1.0)\n    model_lon = Ridge(alpha=1.0)\n    model_lat.fit(X_tr, ylat_tr)\n    model_lon.fit(X_tr, ylon_tr)\n    lat_preds[val_idx] = model_lat.predict(X_val)\n    lon_preds[val_idx] = model_lon.predict(X_val)\n    print(f\" Fold {fold+1} done.\")\n\n# Evaluate on CV predictions using competition metric\ntrain_df[\"pred_dLat\"] = lat_preds\ntrain_df[\"pred_dLon\"] = lon_preds\ntrain_df[\"pred_lat\"] = train_df[\"WlsLat\"] + train_df[\"pred_dLat\"]\ntrain_df[\"pred_lon\"] = train_df[\"WlsLon\"] + train_df[\"pred_dLon\"]\ntrain_df[\"err_m\"] = haversine_meters(\n    train_df[\"pred_lat\"].values,\n    train_df[\"pred_lon\"].values,\n    train_df[\"GtLat\"].values,\n    train_df[\"GtLon\"].values,\n)\nphone_groups = train_df.groupby([\"drive\", \"phone\"])\nphone_scores = []\nfor (drive, phone), gp in phone_groups:\n    errs = gp[\"err_m\"].values\n    if len(errs) < 5:\n        continue\n    p50 = np.percentile(errs, 50)\n    p95 = np.percentile(errs, 95)\n    phone_scores.append((p50 + p95) / 2.0)\nif len(phone_scores) == 0:\n    mean_metric = float(\"nan\")\nelse:\n    mean_metric = float(np.mean(phone_scores))\nprint(\n    \"CV competition metric (mean of per-phone (50th+95th)/2): {:.6f} meters\".format(\n        mean_metric\n    )\n)\n\n# Train final models on all train data\nfinal_lat = Ridge(alpha=1.0).fit(X, y_lat)\nfinal_lon = Ridge(alpha=1.0).fit(X, y_lon)\nprint(\"Final models trained on all training data.\")\n\n# Build test predictions by reading device_gnss per test phone and matching sample_submission times\nprint(\"Loading sample submission to build test predictions...\")\nsample = pd.read_csv(SAMPLE_SUB)\nscols = sample.columns.tolist()\ns_phone_col = find_col(scols, [\"phone\", \"device\", \"drive\"])\ns_time_col = find_col(scols, [\"unixtimemillis\", \"unixtime\", \"time\"])\ns_lat_col = find_col(scols, [\"latitude\", \"lat\"])\ns_lon_col = find_col(scols, [\"longitude\", \"lon\", \"long\"])\nif s_phone_col is None:\n    s_phone_col = scols[0]\nif s_time_col is None:\n    s_time_col = scols[1] if len(scols) > 1 else scols[0]\n\nout_rows = []\ntest_cache = {}\nprint(\"Scanning test folders and aggregating GNSS epochs...\")\nfor drive_path in sorted(glob.glob(os.path.join(TEST_DIR, \"*\"))):\n    if not os.path.isdir(drive_path):\n        continue\n    drive = os.path.basename(drive_path)\n    for phone in sorted(os.listdir(drive_path)):\n        phone_dir = os.path.join(drive_path, phone)\n        if not os.path.isdir(phone_dir):\n            continue\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            continue\n        try:\n            df = pd.read_csv(\n                gnss_path,\n                usecols=lambda c: c\n                in [\n                    \"utcTimeMillis\",\n                    \"Svid\",\n                    \"Cn0DbHz\",\n                    \"PseudorangeRateMetersPerSecond\",\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ],\n            )\n        except Exception:\n            df = pd.read_csv(gnss_path)\n            df = df[\n                [\n                    c\n                    for c in [\n                        \"utcTimeMillis\",\n                        \"Svid\",\n                        \"Cn0DbHz\",\n                        \"PseudorangeRateMetersPerSecond\",\n                        \"WlsPositionXEcefMeters\",\n                        \"WlsPositionYEcefMeters\",\n                        \"WlsPositionZEcefMeters\",\n                    ]\n                    if c in df.columns\n                ]\n            ]\n        if df.shape[0] == 0:\n            continue\n        if not all(\n            c in df.columns\n            for c in [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        ):\n            continue\n        agg = (\n            df.groupby(\"utcTimeMillis\")\n            .agg(\n                WlsX=(\"WlsPositionXEcefMeters\", \"first\"),\n                WlsY=(\"WlsPositionYEcefMeters\", \"first\"),\n                WlsZ=(\"WlsPositionZEcefMeters\", \"first\"),\n                sat_count=(\"Svid\", lambda s: s.nunique()),\n                mean_cn0=(\"Cn0DbHz\", \"mean\"),\n                std_cn0=(\"Cn0DbHz\", \"std\"),\n                mean_prr=(\"PseudorangeRateMetersPerSecond\", \"mean\"),\n            )\n            .reset_index()\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        )\n        lat_list = []\n        lon_list = []\n        for xi, yi, zi in zip(agg.WlsX.values, agg.WlsY.values, agg.WlsZ.values):\n            try:\n                lat, lon = ecef_to_latlon(float(xi), float(yi), float(zi))\n            except Exception:\n                lat, lon = np.nan, np.nan\n            lat_list.append(lat)\n            lon_list.append(lon)\n        agg[\"WlsLat\"] = lat_list\n        agg[\"WlsLon\"] = lon_list\n        agg = agg.sort_values(\"UnixTimeMillis\").reset_index(drop=True)\n        key = drive + \"_\" + phone\n        test_cache[key] = agg\n        print(\"Aggregated test:\", key, \"epochs:\", len(agg))\n\n# Build normalized lookup for robust matching\nkeys = list(test_cache.keys())\nnorm_to_keys = defaultdict(list)\nkey_norm_map = {}\nfor k in keys:\n    nk = normalize_key(k)\n    key_norm_map[k] = nk\n    norm_to_keys[nk].append(k)\n\n\ndef match_test_key(phone_val):\n    # robust normalization-based matching\n    if not isinstance(phone_val, str):\n        return None\n    pv = phone_val.strip()\n    if pv in test_cache:\n        return pv\n    pv_norm = normalize_key(pv)\n    # direct normalized exact match\n    if pv_norm in norm_to_keys and len(norm_to_keys[pv_norm]) == 1:\n        return norm_to_keys[pv_norm][0]\n    if pv_norm in norm_to_keys:\n        # multiple candidates: prefer exact substring match of original\n        for cand in norm_to_keys[pv_norm]:\n            if pv.lower() in cand.lower() or cand.lower() in pv.lower():\n                return cand\n        return norm_to_keys[pv_norm][0]\n    # try substring matching on normalized forms\n    for k, nk in key_norm_map.items():\n        if pv_norm in nk or nk in pv_norm:\n            return k\n    # try token overlap (split by underscore)\n    tokens = set([t for t in pv_norm.split(\"_\") if t])\n    best = None\n    best_score = 0\n    for k, nk in key_norm_map.items():\n        ktoks = set([t for t in nk.split(\"_\") if t])\n        score = len(tokens & ktoks)\n        if score > best_score:\n            best_score = score\n            best = k\n    if best_score >= 1:\n        return best\n    # fallback: try any key that contains the last token (likely phone model)\n    last = pv_norm.split(\"_\")[-1]\n    candidates = [k for k, nk in key_norm_map.items() if last in nk]\n    if len(candidates) == 1:\n        return candidates[0]\n    # final fallback: if pv contains drive-like prefix, match drive\n    drive_guess = pv.split(\"_\")[0]\n    for k in keys:\n        if k.startswith(drive_guess):\n            return k\n    return None\n\n\nprint(\"Constructing submission predictions with improved matching...\")\nout_records = []\nmissing_count = 0\nfallback_count = 0\nmatched_count = 0\ntotal = len(sample)\n# For speed, convert sample into list\nfor i, row in sample.iterrows():\n    phone_val = row[s_phone_col]\n    # parse time\n    tval = row[s_time_col]\n    try:\n        t = int(tval)\n    except Exception:\n        try:\n            t = int(float(tval))\n        except Exception:\n            t = None\n    key = None\n    if isinstance(phone_val, str):\n        key = match_test_key(phone_val)\n    if key is None or key not in test_cache or t is None:\n        missing_count += 1\n        # fallback to sample lat/lon if available, else zeros\n        if (\n            s_lat_col is not None\n            and s_lon_col is not None\n            and (not pd.isna(row[s_lat_col]))\n            and (not pd.isna(row[s_lon_col]))\n        ):\n            pred_lat = row[s_lat_col]\n            pred_lon = row[s_lon_col]\n            fallback_count += 1\n        else:\n            pred_lat = 0.0\n            pred_lon = 0.0\n    else:\n        agg = test_cache[key]\n        times = agg.UnixTimeMillis.values\n        if len(times) == 0:\n            missing_count += 1\n            if (\n                s_lat_col is not None\n                and s_lon_col is not None\n                and (not pd.isna(row[s_lat_col]))\n                and (not pd.isna(row[s_lon_col]))\n            ):\n                pred_lat = row[s_lat_col]\n                pred_lon = row[s_lon_col]\n                fallback_count += 1\n            else:\n                pred_lat = 0.0\n                pred_lon = 0.0\n        else:\n            pos = np.searchsorted(times, t)\n            cand_idx = []\n            for cand in (pos - 1, pos):\n                if 0 <= cand < len(times):\n                    cand_idx.append(cand)\n            # choose nearest\n            best_idx = None\n            best_dt = None\n            for ci in cand_idx:\n                dt = abs(int(times[ci]) - t)\n                if best_dt is None or dt < best_dt:\n                    best_dt = dt\n                    best_idx = ci\n            # If best_dt is huge (e.g., >2000 ms), we'll still use nearest but count as missing-like\n            row_agg = agg.iloc[best_idx]\n            wls_lat = row_agg[\"WlsLat\"]\n            wls_lon = row_agg[\"WlsLon\"]\n            feat = np.array(\n                [\n                    wls_lat,\n                    wls_lon,\n                    (\n                        row_agg.get(\"sat_count\", 0)\n                        if not pd.isna(row_agg.get(\"sat_count\", np.nan))\n                        else 0.0\n                    ),\n                    (\n                        row_agg.get(\"mean_cn0\", 0.0)\n                        if not pd.isna(row_agg.get(\"mean_cn0\", np.nan))\n                        else 0.0\n                    ),\n                    (\n                        row_agg.get(\"std_cn0\", 0.0)\n                        if not pd.isna(row_agg.get(\"std_cn0\", np.nan))\n                        else 0.0\n                    ),\n                    (\n                        row_agg.get(\"mean_prr\", 0.0)\n                        if not pd.isna(row_agg.get(\"mean_prr\", np.nan))\n                        else 0.0\n                    ),\n                ]\n            ).reshape(1, -1)\n            pred_dlat = final_lat.predict(feat)[0]\n            pred_dlon = final_lon.predict(feat)[0]\n            pred_lat = wls_lat + pred_dlat\n            pred_lon = wls_lon + pred_dlon\n            matched_count += 1\n    # Ensure numeric values\n    try:\n        pred_lat = float(pred_lat)\n    except Exception:\n        pred_lat = 0.0\n    try:\n        pred_lon = float(pred_lon)\n    except Exception:\n        pred_lon = 0.0\n    out_records.append((phone_val, int(t) if t is not None else -1, pred_lat, pred_lon))\n\nprint(\n    \"Matching summary: matched rows = {}, fallback to sample lat/lon = {}, completely missing = {} (total {})\".format(\n        matched_count, fallback_count, missing_count - fallback_count, total\n    )\n)\n\n# Write submission with required header names\nout_df = pd.DataFrame(\n    out_records,\n    columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"],\n)\nout_df.to_csv(SUBMISSION_PATH, index=False)\nprint(\"Saved submission to\", SUBMISSION_PATH)\nprint(\"Done.\")\n",
                "exec_time": 64.1731526851654,
                "metric": null,
                "is_buggy": true,
                "exc_type": "ValueError",
                "exc_value": null,
                "analysis": "The script successfully aggregated large amounts of train and test GNSS epochs and trained Ridge models, but it crashed during test-time prediction. A ValueError was raised because the feature vector contained NaN(s) when calling final_lat.predict, so no submission.csv was produced.",
                "alignment": {
                        "plan_code_similarity": 0.012200478258747743,
                        "has_changes": true,
                        "change_magnitude": 0.9767942458129889
                }
        },
        {
                "index": 17,
                "step": 17,
                "id": "ab298e1bbe314676a6beaa4ae7d87c6c",
                "parent_id": null,
                "plan": "I will fix the mapping and fallback logic so the script reliably finds the correct device_gnss files for each sample_submission phone id (handling case differences and mismatched naming), build WLS time->lat/lon maps on demand and cache them, and use a robust global fallback (nearest-in-time among available maps, or global median) for any missing entries. The script prints the validation metric computed on train (matching WLS -> ground truth) and always writes ./submission/submission.csv. It also keeps the original coordinate conversion and nearest-neighbor prediction functions but makes file discovery and fallback behavior more robust and efficient.",
                "code": "#!/usr/bin/env python3\nimport os\nimport sys\nimport math\nimport re\nimport numpy as np\nimport pandas as pd\n\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSAMPLE_SUB = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nOUTPUT_DIR = \"./submission\"\nOUTPUT_FILE = os.path.join(OUTPUT_DIR, \"submission.csv\")\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# ECEF to geodetic (WGS84) conversion (iterative)\ndef ecef_to_latlon(x, y, z, tol=1e-12):\n    scalar_input = False\n    if np.isscalar(x):\n        scalar_input = True\n        x = np.array([x], dtype=float)\n        y = np.array([y], dtype=float)\n        z = np.array([z], dtype=float)\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    a = 6378137.0\n    f = 1 / 298.257223563\n    e2 = f * (2 - f)\n    lon = np.arctan2(y, x)\n    p = np.hypot(x, y)\n    lat = np.arctan2(z, p * (1 - e2))\n    pole_mask = p < 1e-12\n    if np.any(pole_mask):\n        lat[pole_mask] = np.sign(z[pole_mask]) * (math.pi / 2.0)\n    nonpole = ~pole_mask\n    for _ in range(50):\n        sin_lat = np.sin(lat[nonpole])\n        N = a / np.sqrt(1 - e2 * sin_lat * sin_lat)\n        new_lat = np.arctan2(z[nonpole] + e2 * N * sin_lat, p[nonpole])\n        if np.all(np.abs(new_lat - lat[nonpole]) < tol):\n            lat[nonpole] = new_lat\n            break\n        lat[nonpole] = new_lat\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    if scalar_input:\n        return float(lat_deg[0]), float(lon_deg[0])\n    return lat_deg, lon_deg\n\n\n# Haversine distance (meters)\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = math.radians(lat1)\n    phi2 = math.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = math.radians(lon2 - lon1)\n    a = (\n        math.sin(dphi / 2.0) ** 2\n        + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * math.asin(math.sqrt(max(0.0, a)))\n\n\n# Robust WLS column finder using multiple patterns\ndef find_wls_cols(df):\n    cols = df.columns.tolist()\n    lc = [c.lower() for c in cols]\n    patterns = [\n        (\"wlspositionxecefmeters\", \"wlspositionyecefmeters\", \"wlspositionzecefmeters\"),\n        (\n            \"wlsposition_x_ecef_meters\",\n            \"wlsposition_y_ecef_meters\",\n            \"wlsposition_z_ecef_meters\",\n        ),\n        (\"wlsposition_x_ecef\", \"wlsposition_y_ecef\", \"wlsposition_z_ecef\"),\n        (\"wls_x\", \"wls_y\", \"wls_z\"),\n    ]\n    for px, py, pz in patterns:\n        try:\n            xi = lc.index(px)\n            yi = lc.index(py)\n            zi = lc.index(pz)\n            return cols[xi], cols[yi], cols[zi]\n        except Exception:\n            continue\n    xcol = ycol = zcol = None\n    for c, c_l in zip(cols, lc):\n        if re.search(r\"wls.*x.*ecef.*meter\", c_l):\n            xcol = c\n        if re.search(r\"wls.*y.*ecef.*meter\", c_l):\n            ycol = c\n        if re.search(r\"wls.*z.*ecef.*meter\", c_l):\n            zcol = c\n    if xcol and ycol and zcol:\n        return xcol, ycol, zcol\n    for c, c_l in zip(cols, lc):\n        if \"wls\" in c_l:\n            if (\"x\" in c_l and xcol is None) or re.search(r\"\\bx\\b\", c_l):\n                xcol = xcol or c\n            if (\"y\" in c_l and ycol is None) or re.search(r\"\\by\\b\", c_l):\n                ycol = ycol or c\n            if (\"z\" in c_l and zcol is None) or re.search(r\"\\bz\\b\", c_l):\n                zcol = zcol or c\n    if xcol and ycol and zcol:\n        return xcol, ycol, zcol\n    for c, c_l in zip(cols, lc):\n        if \"ecef\" in c_l:\n            if \"x\" in c_l and xcol is None:\n                xcol = c\n            if \"y\" in c_l and ycol is None:\n                ycol = c\n            if \"z\" in c_l and zcol is None:\n                zcol = c\n    if xcol and ycol and zcol:\n        return xcol, ycol, zcol\n    return None, None, None\n\n\n# Build time->latlon mapping for a device_gnss csv\ndef build_wls_time_latlon_map(gnss_path):\n    try:\n        df = pd.read_csv(gnss_path)\n    except Exception as e:\n        print(f\"Failed to read {gnss_path}: {e}\", file=sys.stderr)\n        return None\n    if df.shape[0] == 0:\n        return None\n    # detect time column robustly\n    time_col = None\n    for c in df.columns:\n        cl = c.lower()\n        if cl in (\n            \"utctimemillis\",\n            \"utc_time_millis\",\n            \"unixtimemillis\",\n            \"unixtime_millis\",\n            \"utc\",\n            \"unixtime\",\n            \"time\",\n            \"utctime\",\n        ):\n            time_col = c\n            break\n    if time_col is None:\n        for c in df.columns:\n            if np.issubdtype(df[c].dtype, np.integer) or np.issubdtype(\n                df[c].dtype, np.floating\n            ):\n                s = df[c].dropna()\n                if s.size > 0:\n                    try:\n                        v = float(s.iloc[0])\n                    except Exception:\n                        continue\n                    if abs(v) > 1e9:\n                        time_col = c\n                        break\n    if time_col is None:\n        time_col = df.columns[0]\n    xcol, ycol, zcol = find_wls_cols(df)\n    if xcol is None or ycol is None or zcol is None:\n        return None\n    keep = df[[time_col, xcol, ycol, zcol]].copy()\n    keep = keep.dropna(subset=[xcol, ycol, zcol])\n    if keep.shape[0] == 0:\n        return None\n    try:\n        keep[time_col] = pd.to_numeric(keep[time_col], errors=\"coerce\")\n    except Exception:\n        keep[time_col] = keep[time_col]\n    keep = keep.dropna(subset=[time_col])\n    if keep.shape[0] == 0:\n        return None\n    try:\n        keep[time_col] = keep[time_col].astype(np.int64)\n    except Exception:\n        keep[time_col] = (keep[time_col].astype(np.float64)).astype(np.int64)\n    keep = keep.sort_values(by=time_col).drop_duplicates(\n        subset=[time_col], keep=\"first\"\n    )\n    times = keep[time_col].values.astype(np.int64)\n    xs = keep[xcol].astype(float).values\n    ys = keep[ycol].astype(float).values\n    zs = keep[zcol].astype(float).values\n    try:\n        lats, lons = ecef_to_latlon(xs, ys, zs)\n    except Exception:\n        lats = np.empty_like(xs)\n        lons = np.empty_like(xs)\n        for i, (xx, yy, zz) in enumerate(zip(xs, ys, zs)):\n            try:\n                lat, lon = ecef_to_latlon(xx, yy, zz)\n            except Exception:\n                lat, lon = np.nan, np.nan\n            lats[i] = lat\n            lons[i] = lon\n    mask = np.isfinite(lats) & np.isfinite(lons)\n    if not np.any(mask):\n        return None\n    order = np.argsort(times[mask])\n    return {\n        \"times\": times[mask][order],\n        \"lats\": lats[mask][order],\n        \"lons\": lons[mask][order],\n    }\n\n\n# Predict lat/lon for requested times by nearest neighbor in times array\ndef predict_from_map(tmap, query_times):\n    times = tmap[\"times\"]\n    lats = tmap[\"lats\"]\n    lons = tmap[\"lons\"]\n    if len(times) == 0:\n        return np.full(len(query_times), np.nan), np.full(len(query_times), np.nan)\n    qt = np.asarray(query_times, dtype=np.int64)\n    idxs = np.searchsorted(times, qt, side=\"left\")\n    preds_lat = np.empty(len(qt))\n    preds_lon = np.empty(len(qt))\n    for i, (q, idx) in enumerate(zip(qt, idxs)):\n        cand = []\n        if idx < len(times):\n            cand.append((abs(int(times[idx]) - int(q)), idx))\n        if idx - 1 >= 0:\n            cand.append((abs(int(times[idx - 1]) - int(q)), idx - 1))\n        if len(cand) == 0:\n            preds_lat[i], preds_lon[i] = np.nan, np.nan\n        else:\n            _, chosen = min(cand, key=lambda x: x[0])\n            preds_lat[i] = lats[chosen]\n            preds_lon[i] = lons[chosen]\n    return preds_lat, preds_lon\n\n\n# --- Validation on train using ground_truth ---\ndef validate_on_train(max_files=None):\n    phone_errors = {}\n    processed = 0\n    if not os.path.isdir(TRAIN_DIR):\n        print(\"No train directory found, skipping validation.\", file=sys.stderr)\n        return None\n    for drive in sorted(os.listdir(TRAIN_DIR)):\n        drive_path = os.path.join(TRAIN_DIR, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n            tmap = build_wls_time_latlon_map(gnss_path)\n            if tmap is None:\n                continue\n            try:\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"failed read gt {gt_path}: {e}\", file=sys.stderr)\n                continue\n            if not any(c.lower() == \"unixtimemillis\" for c in gt.columns) or not any(\n                c.lower() == \"latitudedegrees\" for c in gt.columns\n            ):\n                # try flexible names\n                lower_cols = [c.lower() for c in gt.columns]\n                if (\n                    \"unixtimemillis\" not in lower_cols\n                    or \"latitudedegrees\" not in lower_cols\n                ):\n                    continue\n            time_col = [c for c in gt.columns if c.lower() == \"unixtimemillis\"][0]\n            lat_col = [c for c in gt.columns if c.lower() == \"latitudedegrees\"][0]\n            lon_col = [c for c in gt.columns if c.lower() == \"longitudedegrees\"][0]\n            q_times = gt[time_col].astype(np.int64).values\n            q_lats = gt[lat_col].astype(float).values\n            q_lons = gt[lon_col].astype(float).values\n            pred_lats, pred_lons = predict_from_map(tmap, q_times)\n            dists = []\n            for a, b, pa, pb in zip(q_lats, q_lons, pred_lats, pred_lons):\n                if np.isfinite(pa) and np.isfinite(pb):\n                    dists.append(haversine(a, b, pa, pb))\n            if len(dists) == 0:\n                continue\n            phone_key = f\"{drive}_{phone}\"\n            phone_errors[phone_key] = np.array(dists)\n            processed += 1\n            if max_files is not None and processed >= max_files:\n                break\n        if max_files is not None and processed >= max_files:\n            break\n    if len(phone_errors) == 0:\n        print(\"No train phones validated.\", file=sys.stderr)\n        return None\n    per_phone_scores = []\n    for phone, dists in phone_errors.items():\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        per_phone_scores.append((p50 + p95) / 2.0)\n    metric = float(np.mean(per_phone_scores))\n    return metric, phone_errors\n\n\n# --- Build submission predictions by matching test device_gnss to sample_submission times ---\ndef find_gnss_for_phone(phone_id):\n    # Try to find a device_gnss.csv under TEST_DIR where \"<drive>_<phonefolder>\" (case-insensitive) equals phone_id\n    pid_lower = phone_id.lower()\n    for drive in sorted(os.listdir(TEST_DIR)):\n        drive_path = os.path.join(TEST_DIR, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for candidate in sorted(os.listdir(drive_path)):\n            cand_combined = f\"{drive}_{candidate}\".lower()\n            if cand_combined == pid_lower:\n                cand_path = os.path.join(drive_path, candidate, \"device_gnss.csv\")\n                if os.path.exists(cand_path):\n                    return cand_path\n    # fallback: try splitting by last underscore to guess drive/phone and allow phone folder mismatch (case-insensitive substring)\n    if \"_\" in phone_id:\n        drive_part = phone_id.rsplit(\"_\", 1)[0]\n        phone_part = phone_id.rsplit(\"_\", 1)[1].lower()\n        drive_path = os.path.join(TEST_DIR, drive_part)\n        if os.path.isdir(drive_path):\n            for candidate in os.listdir(drive_path):\n                if phone_part in candidate.lower() or candidate.lower() in phone_part:\n                    cand_path = os.path.join(drive_path, candidate, \"device_gnss.csv\")\n                    if os.path.exists(cand_path):\n                        return cand_path\n    # last resort: try any matching phone folder name ignoring drive\n    for drive in sorted(os.listdir(TEST_DIR)):\n        drive_path = os.path.join(TEST_DIR, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for candidate in sorted(os.listdir(drive_path)):\n            cand_combined = f\"{drive}_{candidate}\".lower()\n            if phone_id.lower() in cand_combined or cand_combined in phone_id.lower():\n                cand_path = os.path.join(drive_path, candidate, \"device_gnss.csv\")\n                if os.path.exists(cand_path):\n                    return cand_path\n    return None\n\n\ndef build_submission(sample_sub_path):\n    sub = pd.read_csv(sample_sub_path)\n    if not all(c in sub.columns for c in [\"phone\", \"UnixTimeMillis\"]):\n        raise ValueError(\"Sample submission missing required columns\")\n    n = len(sub)\n    out_lats = np.full(n, np.nan, dtype=float)\n    out_lons = np.full(n, np.nan, dtype=float)\n    phones = sub[\"phone\"].unique()\n    tmap_cache = {}\n    global_entries = []\n\n    # Build maps on-demand for each unique phone id (cache). This avoids pre-scanning all test files.\n    for phone_val in phones:\n        gnss_path = find_gnss_for_phone(phone_val)\n        if gnss_path is None:\n            # cannot find gnss file for this sample phone id\n            continue\n        if gnss_path in tmap_cache:\n            tmap = tmap_cache[gnss_path]\n        else:\n            tmap = build_wls_time_latlon_map(gnss_path)\n            tmap_cache[gnss_path] = tmap\n        if tmap is not None:\n            mid = len(tmap[\"times\"]) // 2\n            global_entries.append(\n                (tmap[\"times\"][mid], tmap[\"lats\"][mid], tmap[\"lons\"][mid])\n            )\n\n    if len(global_entries) > 0:\n        global_times = np.array([e[0] for e in global_entries]).astype(np.int64)\n        global_lats = np.array([e[1] for e in global_entries])\n        global_lons = np.array([e[2] for e in global_entries])\n        # Also compute medians as absolute fallback\n        global_lat_median = float(np.nanmedian(global_lats))\n        global_lon_median = float(np.nanmedian(global_lons))\n    else:\n        global_times = np.array([], dtype=np.int64)\n        global_lats = np.array([])\n        global_lons = np.array([])\n        global_lat_median = 0.0\n        global_lon_median = 0.0\n\n    # Now fill predictions phone-by-phone\n    for phone_val in phones:\n        mask = sub[\"phone\"] == phone_val\n        indices = np.where(mask)[0]\n        gnss_path = find_gnss_for_phone(phone_val)\n        tmap = None\n        if gnss_path is not None:\n            if gnss_path in tmap_cache:\n                tmap = tmap_cache[gnss_path]\n            else:\n                tmap = build_wls_time_latlon_map(gnss_path)\n                tmap_cache[gnss_path] = tmap\n        query_times = sub.loc[mask, \"UnixTimeMillis\"].astype(np.int64).values\n        if tmap is None:\n            # try alternatives: any tmap under same drive if drive part can be extracted\n            alt_found = False\n            if \"_\" in phone_val:\n                drive_part = phone_val.rsplit(\"_\", 1)[0]\n                drive_path = os.path.join(TEST_DIR, drive_part)\n                if os.path.isdir(drive_path):\n                    for candidate in os.listdir(drive_path):\n                        cand_path = os.path.join(\n                            drive_path, candidate, \"device_gnss.csv\"\n                        )\n                        if (\n                            cand_path in tmap_cache\n                            and tmap_cache[cand_path] is not None\n                        ):\n                            tmap_alt = tmap_cache[cand_path]\n                            latp, lonp = predict_from_map(tmap_alt, query_times)\n                            out_lats[indices] = latp\n                            out_lons[indices] = lonp\n                            alt_found = True\n                            break\n                        elif os.path.exists(cand_path):\n                            tmap_alt = build_wls_time_latlon_map(cand_path)\n                            tmap_cache[cand_path] = tmap_alt\n                            if tmap_alt is not None:\n                                latp, lonp = predict_from_map(tmap_alt, query_times)\n                                out_lats[indices] = latp\n                                out_lons[indices] = lonp\n                                alt_found = True\n                                break\n            if alt_found:\n                continue\n            # else leave NaNs for now; will fill via global fallback later\n            continue\n        # use tmap directly\n        latp, lonp = predict_from_map(tmap, query_times)\n        out_lats[indices] = latp\n        out_lons[indices] = lonp\n\n    # Fill remaining NaNs: nearest in time among global entries (if available), else global median, else zeros\n    nan_idx = np.where((~np.isfinite(out_lats)) | (~np.isfinite(out_lons)))[0]\n    if len(nan_idx) > 0:\n        if len(global_times) > 0:\n            for idx in nan_idx:\n                try:\n                    qt = int(sub.loc[idx, \"UnixTimeMillis\"])\n                except Exception:\n                    qt = None\n                if qt is None:\n                    out_lats[idx] = global_lat_median\n                    out_lons[idx] = global_lon_median\n                    continue\n                j = np.argmin(np.abs(global_times - qt))\n                out_lats[idx] = global_lats[j]\n                out_lons[idx] = global_lons[j]\n        else:\n            out_lats[nan_idx] = global_lat_median\n            out_lons[nan_idx] = global_lon_median\n\n    submission_df = pd.DataFrame(\n        {\n            \"phone\": sub[\"phone\"],\n            \"UnixTimeMillis\": sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": out_lats,\n            \"LongitudeDegrees\": out_lons,\n        }\n    )\n    submission_df.to_csv(OUTPUT_FILE, index=False)\n    return submission_df\n\n\ndef main():\n    print(\"Starting run...\", file=sys.stderr)\n    print(\"Computing validation metric on train set...\", file=sys.stderr)\n    val_res = validate_on_train(max_files=60)\n    if val_res is None:\n        print(\n            \"Validation failed or no data available. Proceeding to build submission.\",\n            file=sys.stderr,\n        )\n    else:\n        metric_value, phone_errors = val_res\n        print(\n            f\"Validation metric (mean of per-phone (50th+95th)/2): {metric_value:.3f} meters\"\n        )\n        print(f\"Validated {len(phone_errors)} phones (sample).\", file=sys.stderr)\n    print(\"Building submission predictions...\", file=sys.stderr)\n    if not os.path.exists(SAMPLE_SUB):\n        print(f\"Sample submission not found at {SAMPLE_SUB}\", file=sys.stderr)\n        return\n    submission_df = build_submission(SAMPLE_SUB)\n    if submission_df is not None:\n        print(f\"Saved submission to {OUTPUT_FILE}\", file=sys.stderr)\n        print(\"Submission preview (first 5 rows):\")\n        print(submission_df.head(5).to_string(index=False))\n    else:\n        print(\"No submission dataframe produced.\", file=sys.stderr)\n\n\nif __name__ == \"__main__\":\n    main()\n",
                "exec_time": 0.0001900196075439453,
                "metric": null,
                "is_buggy": true,
                "exc_type": null,
                "exc_value": null,
                "analysis": "The run produced no visible output besides a short execution-time line and did not produce a submission.csv in ./submission. The validation metric was not reported, indicating the script likely did not find required input files or failed to build predictions.",
                "alignment": {
                        "plan_code_similarity": 0.00808244089715094,
                        "has_changes": true,
                        "change_magnitude": 0.9721467042831495
                }
        },
        {
                "index": 18,
                "step": 18,
                "id": "d2a1617452ee4dc7be9b6521b810aa00",
                "parent_id": null,
                "plan": "I will fix the submission-column mismatch that caused the script to fail: the sample submission uses \"tripId\" (drive_phone) while the code expected a \"phone\" column. I'll make the script accept either \"phone\" or \"tripId\" as the identifier, use that to locate the matching test folder, and otherwise fall back to the global mean. The rest of the pipeline (ECEF->lat/lon conversion, GroupKFold CV, training full models, and producing predictions) remains the same; the script prints the validation metric and writes ./submission/submission.csv.",
                "code": "#!/usr/bin/env python3\nimport os\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport sys\n\n\ndef ensure_dir(path):\n    os.makedirs(path, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # Bowring's method for WGS84; supports numpy arrays\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = a * np.sqrt(1 - e2)\n    ep2 = (a**2 - b**2) / b**2\n    p = np.sqrt(x**2 + y**2)\n    theta = np.arctan2(z * a, p * b)\n    sin_t = np.sin(theta)\n    cos_t = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * sin_t**3, p - e2 * a * cos_t**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    # lat/lon in degrees -> meters\n    R = 6371000.0\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    dphi = phi2 - phi1\n    dlambda = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dphi / 2.0) ** 2\n        + np.cos(phi1) * np.cos(phi2) * np.sin(dlambda / 2.0) ** 2\n    )\n    return 2 * R * np.arcsin(np.sqrt(a))\n\n\n# Paths\ninput_dir = \"./input\"\ntrain_root = os.path.join(input_dir, \"train\")\ntest_root = os.path.join(input_dir, \"test\")\nsample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n\n# Collect training data\ntrain_device_paths = glob.glob(os.path.join(train_root, \"*\", \"*\", \"device_gnss.csv\"))\nrows = []\nprint(\"Collecting train data from device_gnss files...\", file=sys.stderr)\nfor dev_path in train_device_paths:\n    try:\n        parts = dev_path.split(os.sep)\n        drive_id = parts[-3]\n        phone = parts[-2]\n        folder = os.path.dirname(dev_path)\n        # ground truth usually in parent folder of phone folder\n        gt_path = os.path.join(os.path.dirname(folder), \"ground_truth.csv\")\n        if not os.path.exists(gt_path):\n            # sometimes ground_truth is in same phone folder\n            gt_path = os.path.join(folder, \"ground_truth.csv\")\n        if not os.path.exists(gt_path):\n            continue\n        dg = pd.read_csv(\n            dev_path,\n            usecols=[\n                \"utcTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"Cn0DbHz\",\n            ],\n        )\n        dg = (\n            dg.groupby(\"utcTimeMillis\", as_index=False)\n            .agg(\n                {\n                    \"WlsPositionXEcefMeters\": \"mean\",\n                    \"WlsPositionYEcefMeters\": \"mean\",\n                    \"WlsPositionZEcefMeters\": \"mean\",\n                    \"Cn0DbHz\": \"mean\",\n                }\n            )\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n        )\n        gt = pd.read_csv(\n            gt_path, usecols=[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        )\n        dg = dg.sort_values(\"UnixTimeMillis\")\n        gt = gt.sort_values(\"UnixTimeMillis\")\n        merged = pd.merge_asof(\n            dg, gt, on=\"UnixTimeMillis\", direction=\"nearest\", tolerance=500\n        )\n        merged = merged.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\"])\n        if merged.empty:\n            continue\n        x = merged[\"WlsPositionXEcefMeters\"].values\n        y = merged[\"WlsPositionYEcefMeters\"].values\n        z = merged[\"WlsPositionZEcefMeters\"].values\n        lat_wls, lon_wls = ecef_to_geodetic(x, y, z)\n        merged[\"wls_lat\"] = lat_wls\n        merged[\"wls_lon\"] = lon_wls\n        merged[\"drive_id\"] = drive_id\n        merged[\"phone\"] = phone\n        rows.append(\n            merged[\n                [\n                    \"drive_id\",\n                    \"phone\",\n                    \"UnixTimeMillis\",\n                    \"wls_lat\",\n                    \"wls_lon\",\n                    \"Cn0DbHz\",\n                    \"LatitudeDegrees\",\n                    \"LongitudeDegrees\",\n                ]\n            ]\n        )\n    except Exception as e:\n        print(\"Error processing\", dev_path, e, file=sys.stderr)\n\nif not rows:\n    raise RuntimeError(\"No training data collected. Check input paths.\")\n\ntrain_df = pd.concat(rows, ignore_index=True)\nprint(\"Total training rows:\", len(train_df), file=sys.stderr)\n\n# Prepare features and targets\ntrain_df = train_df.sort_values([\"drive_id\", \"UnixTimeMillis\"]).reset_index(drop=True)\nX = train_df[[\"wls_lat\", \"wls_lon\", \"Cn0DbHz\"]].fillna(-999)\ny_lat = train_df[\"LatitudeDegrees\"].values\ny_lon = train_df[\"LongitudeDegrees\"].values\ngroups = train_df[\"drive_id\"].values\nphones = train_df[\"phone\"].values\n\n# 5-fold GroupKFold CV by drive_id\ngkf = GroupKFold(n_splits=5)\nmodels_lat = []\nmodels_lon = []\npreds_lat = np.zeros(len(X))\npreds_lon = np.zeros(len(X))\nprint(\"Performing 5-fold GroupKFold CV...\", file=sys.stderr)\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    # ensure both targets use training indices\n    ytr_lat, ytr_lon = y_lat[tr_idx], y_lon[tr_idx]\n    model_lat = make_pipeline(StandardScaler(), Ridge(alpha=1.0))\n    model_lon = make_pipeline(StandardScaler(), Ridge(alpha=1.0))\n    model_lat.fit(X_tr, ytr_lat)\n    model_lon.fit(X_tr, ytr_lon)\n    p_lat = model_lat.predict(X_val)\n    p_lon = model_lon.predict(X_val)\n    preds_lat[val_idx] = p_lat\n    preds_lon[val_idx] = p_lon\n    models_lat.append(model_lat)\n    models_lon.append(model_lon)\n    print(f\"Fold {fold} done. Val size: {len(val_idx)}\", file=sys.stderr)\n\n# Evaluate using competition metric approximation\neval_df = train_df[[\"drive_id\", \"phone\", \"UnixTimeMillis\"]].copy()\neval_df[\"pred_lat\"] = preds_lat\neval_df[\"pred_lon\"] = preds_lon\neval_df[\"true_lat\"] = y_lat\neval_df[\"true_lon\"] = y_lon\neval_df[\"error_m\"] = haversine(\n    eval_df[\"true_lat\"].values,\n    eval_df[\"true_lon\"].values,\n    eval_df[\"pred_lat\"].values,\n    eval_df[\"pred_lon\"].values,\n)\n\nphones_list = eval_df[\"phone\"].unique()\nphone_scores = []\nfor ph in phones_list:\n    sub = eval_df[eval_df[\"phone\"] == ph]\n    if sub.empty:\n        continue\n    p50 = np.percentile(sub[\"error_m\"], 50)\n    p95 = np.percentile(sub[\"error_m\"], 95)\n    phone_scores.append(0.5 * (p50 + p95))\nif len(phone_scores) == 0:\n    raise RuntimeError(\"No phone scores.\")\nscore = np.mean(phone_scores)\nprint(\n    \"Validation competition-style score (mean over phones of 0.5*(p50+p95)) = {:.4f} meters\".format(\n        score\n    )\n)\n\n# Train final models on all data\nfinal_lat = make_pipeline(StandardScaler(), Ridge(alpha=1.0)).fit(X, y_lat)\nfinal_lon = make_pipeline(StandardScaler(), Ridge(alpha=1.0)).fit(X, y_lon)\n\n# Prepare test predictions\nprint(\"Preparing test predictions...\", file=sys.stderr)\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Normalize columns: strip whitespace and ensure we can use either 'phone' or 'tripId'\nsample_sub.columns = [str(c).strip() for c in sample_sub.columns]\nid_col = None\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    raise RuntimeError(\n        f\"sample_submission missing required identifier column. Found: {sample_sub.columns.tolist()}\"\n    )\nprint(f\"Using identifier column '{id_col}' from sample submission.\", file=sys.stderr)\n\n# Build mapping from test dirs: key = \"<drive>_<phone>\"\nall_test_paths = glob.glob(os.path.join(test_root, \"*\", \"*\"))\ntest_device_dirs = {}\nfor p in all_test_paths:\n    if os.path.isdir(p):\n        drive = os.path.basename(os.path.dirname(p))\n        phone = os.path.basename(p)\n        key = f\"{drive}_{phone}\"\n        test_device_dirs[key] = p\n\n# Cache for aggregated test device_gnss\nagg_cache = {}\n\n# We'll collect predicted lat/lon in a list aligned with sample_sub\npreds = []\nmean_lat = np.mean(y_lat)\nmean_lon = np.mean(y_lon)\n\nfor idx in range(len(sample_sub)):\n    try:\n        key = sample_sub.at[idx, id_col]\n        # ensure string and strip\n        key = str(key).strip()\n        t = int(sample_sub.at[idx, \"UnixTimeMillis\"])\n    except Exception as e:\n        # malformed row; fallback to global mean\n        preds.append((mean_lat, mean_lon))\n        continue\n\n    if key not in test_device_dirs:\n        # maybe the id_col already contains just phone; try to normalize\n        if \"_\" not in key and key in test_device_dirs:\n            pass\n        else:\n            # no matching test folder; fallback to global mean\n            preds.append((mean_lat, mean_lon))\n            continue\n\n    if key not in agg_cache:\n        dev_dir = test_device_dirs[key]\n        dg_path = os.path.join(dev_dir, \"device_gnss.csv\")\n        if not os.path.exists(dg_path):\n            agg_cache[key] = None\n            preds.append((mean_lat, mean_lon))\n            continue\n        dg = pd.read_csv(\n            dg_path,\n            usecols=[\n                \"utcTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"Cn0DbHz\",\n            ],\n        )\n        dg = (\n            dg.groupby(\"utcTimeMillis\", as_index=False)\n            .agg(\n                {\n                    \"WlsPositionXEcefMeters\": \"mean\",\n                    \"WlsPositionYEcefMeters\": \"mean\",\n                    \"WlsPositionZEcefMeters\": \"mean\",\n                    \"Cn0DbHz\": \"mean\",\n                }\n            )\n            .rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n            .sort_values(\"UnixTimeMillis\")\n            .reset_index(drop=True)\n        )\n        if dg.empty:\n            agg_cache[key] = None\n            preds.append((mean_lat, mean_lon))\n            continue\n        x = dg[\"WlsPositionXEcefMeters\"].values\n        yv = dg[\"WlsPositionYEcefMeters\"].values\n        z = dg[\"WlsPositionZEcefMeters\"].values\n        lat_wls, lon_wls = ecef_to_geodetic(x, yv, z)\n        dg[\"wls_lat\"] = lat_wls\n        dg[\"wls_lon\"] = lon_wls\n        agg_cache[key] = dg\n\n    dg = agg_cache.get(key)\n    if dg is None or dg.empty:\n        preds.append((mean_lat, mean_lon))\n        continue\n\n    # find nearest epoch\n    times = dg[\"UnixTimeMillis\"].values\n    pos = np.searchsorted(times, t)\n    cand_idxs = []\n    if pos < len(times):\n        cand_idxs.append(pos)\n    if pos - 1 >= 0:\n        cand_idxs.append(pos - 1)\n    best = None\n    best_dt = 10**12\n    for ci in cand_idxs:\n        dt = abs(int(times[ci]) - t)\n        if dt < best_dt:\n            best_dt = dt\n            best = ci\n    if best is None:\n        preds.append((mean_lat, mean_lon))\n        continue\n    feat = (\n        dg.iloc[best][[\"wls_lat\", \"wls_lon\", \"Cn0DbHz\"]]\n        .fillna(-999)\n        .values.reshape(1, -1)\n    )\n    plat = float(final_lat.predict(feat)[0])\n    plon = float(final_lon.predict(feat)[0])\n    preds.append((plat, plon))\n\npreds = np.array(preds)\n# Preserve whatever lat/lon column names sample_sub has (commonly 'LatitudeDegrees' and 'LongitudeDegrees')\nlat_col = \"LatitudeDegrees\" if \"LatitudeDegrees\" in sample_sub.columns else \"Latitude\"\nlon_col = (\n    \"LongitudeDegrees\" if \"LongitudeDegrees\" in sample_sub.columns else \"Longitude\"\n)\n# If those expected columns don't exist, create them with these names matching description\nif lat_col not in sample_sub.columns:\n    sample_sub[lat_col] = np.nan\nif lon_col not in sample_sub.columns:\n    sample_sub[lon_col] = np.nan\n\nsample_sub[lat_col] = preds[:, 0]\nsample_sub[lon_col] = preds[:, 1]\n\n# Ensure no NaNs; fill with train mean if any\nnan_mask = sample_sub[lat_col].isna() | sample_sub[lon_col].isna()\nif nan_mask.any():\n    print(\n        f\"Filling {nan_mask.sum()} missing predictions with global train mean.\",\n        file=sys.stderr,\n    )\n    sample_sub.loc[nan_mask, lat_col] = mean_lat\n    sample_sub.loc[nan_mask, lon_col] = mean_lon\n\n# Save submission\nout_dir = \"./submission\"\nensure_dir(out_dir)\nout_path = os.path.join(out_dir, \"submission.csv\")\nsample_sub.to_csv(out_path, index=False)\nprint(f\"Saved submission to {out_path}\", file=sys.stderr)\n\n# Print final message with validation score\nprint(\"Final validation metric (approx): {:.4f} meters\".format(score))\n",
                "exec_time": 37.55834913253784,
                "metric": 7490.5038,
                "is_buggy": false,
                "exc_type": null,
                "exc_value": null,
                "analysis": "The script ran successfully, collected 258,546 training rows, trained Ridge models with 5-fold GroupKFold, and produced a very large validation metric (~7490.5038 meters). A submission file was written to ./submission/submission.csv. No runtime errors occurred, but the validation error indicates the predictive approach performed poorly on this data.",
                "alignment": {
                        "plan_code_similarity": 0.0053814498258942705,
                        "has_changes": true,
                        "change_magnitude": 0.9625384221311475
                }
        },
        {
                "index": 19,
                "step": 19,
                "id": "31d62db892a244629372825de075aeeb",
                "parent_id": null,
                "plan": "I will make the device_gnss -> sample_submission matching more robust by building an explicit map of all test (and train) device_gnss.csv files keyed by \"drive_phone\" names, and then use a resilient WLS-column detector and time-column detector to convert ECEF WLS positions to lat/lon. I will compute a validation metric over a subset of train phones (nearest-neighbor in time against the ground truth) and print it, and then produce a full submission by filling sample_submission rows from the nearest WLS epoch per phone (falling back to a global median location when needed). The script is defensive to missing columns and large files and always writes submission/submission.csv.",
                "code": "#!/usr/bin/env python3\nimport os\nimport sys\nimport math\nimport re\nimport numpy as np\nimport pandas as pd\n\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSAMPLE_SUB = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nOUTPUT_DIR = \"./submission\"\nOUTPUT_FILE = os.path.join(OUTPUT_DIR, \"submission.csv\")\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\n# --- Utilities: ECEF <-> lat/lon and haversine ---\ndef ecef_to_latlon(x, y, z, tol=1e-12):\n    scalar_input = False\n    if np.isscalar(x):\n        scalar_input = True\n        x = np.array([x], dtype=float)\n        y = np.array([y], dtype=float)\n        z = np.array([z], dtype=float)\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    a = 6378137.0\n    f = 1.0 / 298.257223563\n    e2 = f * (2 - f)\n    lon = np.arctan2(y, x)\n    p = np.hypot(x, y)\n    # initial lat\n    lat = np.arctan2(z, p * (1 - e2))\n    pole_mask = p < 1e-12\n    if np.any(pole_mask):\n        lat[pole_mask] = np.sign(z[pole_mask]) * (math.pi / 2.0)\n    nonpole = ~pole_mask\n    for _ in range(50):\n        sin_lat = np.sin(lat[nonpole])\n        N = a / np.sqrt(1 - e2 * sin_lat * sin_lat)\n        new_lat = np.arctan2(z[nonpole] + e2 * N * sin_lat, p[nonpole])\n        if np.all(np.abs(new_lat - lat[nonpole]) < tol):\n            lat[nonpole] = new_lat\n            break\n        lat[nonpole] = new_lat\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    if scalar_input:\n        return float(lat_deg[0]), float(lon_deg[0])\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    phi1 = math.radians(float(lat1))\n    phi2 = math.radians(float(lat2))\n    dphi = phi2 - phi1\n    dlambda = math.radians(float(lon2 - lon1))\n    a = (\n        math.sin(dphi / 2.0) ** 2\n        + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2.0) ** 2\n    )\n    a = max(0.0, min(1.0, a))\n    return 2 * R * math.asin(math.sqrt(a))\n\n\n# --- Robust column detectors ---\ndef find_wls_cols(df):\n    # Try many patterns to find ECEF WLS position columns\n    cols = df.columns.tolist()\n    lc = [c.lower() for c in cols]\n    # Common exact names\n    patterns = [\n        (\"wlspositionxecefmeters\", \"wlspositionyecefmeters\", \"wlspositionzecefmeters\"),\n        (\n            \"wlsposition_x_ecef_meters\",\n            \"wlsposition_y_ecef_meters\",\n            \"wlsposition_z_ecef_meters\",\n        ),\n        (\"wlsposition_x_ecef\", \"wlsposition_y_ecef\", \"wlsposition_z_ecef\"),\n        (\"wls_x\", \"wls_y\", \"wls_z\"),\n        (\"wlspositionx\", \"wlspositiony\", \"wlspositionz\"),\n    ]\n    for px, py, pz in patterns:\n        try:\n            xi = lc.index(px)\n            yi = lc.index(py)\n            zi = lc.index(pz)\n            return cols[xi], cols[yi], cols[zi]\n        except Exception:\n            continue\n    # regex-based search\n    xcol = ycol = zcol = None\n    for c, c_l in zip(cols, lc):\n        if re.search(r\"wls.*x.*ecef.*meter\", c_l) or re.search(r\"wls.*x\\b\", c_l):\n            xcol = xcol or c\n        if re.search(r\"wls.*y.*ecef.*meter\", c_l) or re.search(r\"wls.*y\\b\", c_l):\n            ycol = ycol or c\n        if re.search(r\"wls.*z.*ecef.*meter\", c_l) or re.search(r\"wls.*z\\b\", c_l):\n            zcol = zcol or c\n    if xcol and ycol and zcol:\n        return xcol, ycol, zcol\n    # fallback: any ecef x/y/z\n    for c, c_l in zip(cols, lc):\n        if \"ecef\" in c_l and (\"x\" in c_l or \"_x\" in c_l or \" x \" in c_l):\n            xcol = xcol or c\n        if \"ecef\" in c_l and (\"y\" in c_l or \"_y\" in c_l or \" y \" in c_l):\n            ycol = ycol or c\n        if \"ecef\" in c_l and (\"z\" in c_l or \"_z\" in c_l or \" z \" in c_l):\n            zcol = zcol or c\n    if xcol and ycol and zcol:\n        return xcol, ycol, zcol\n    # last resort: any three numeric columns named with x,y,z\n    for c, c_l in zip(cols, lc):\n        if re.search(r\"\\bx\\b\", c_l) and xcol is None:\n            xcol = c\n        if re.search(r\"\\by\\b\", c_l) and ycol is None:\n            ycol = c\n        if re.search(r\"\\bz\\b\", c_l) and zcol is None:\n            zcol = c\n    if xcol and ycol and zcol:\n        return xcol, ycol, zcol\n    return None, None, None\n\n\ndef detect_time_column(df):\n    # try common names first\n    for c in df.columns:\n        cl = c.lower()\n        if cl in (\n            \"utctimemillis\",\n            \"utc_time_millis\",\n            \"unixtimemillis\",\n            \"unixtime_millis\",\n            \"utc\",\n            \"time\",\n            \"unixtime\",\n            \"utctime\",\n            \"utc_time\",\n            \"utc_time_millis\",\n        ):\n            return c\n    # else try to find large integer column (ms since epoch)\n    for c in df.columns:\n        if np.issubdtype(df[c].dtype, np.integer) or np.issubdtype(\n            df[c].dtype, np.floating\n        ):\n            s = df[c].dropna()\n            if s.size == 0:\n                continue\n            try:\n                v = float(s.iloc[0])\n            except Exception:\n                continue\n            if abs(v) > 1e9:  # likely ms since epoch\n                return c\n    # fallback to first column\n    return df.columns[0]\n\n\n# --- Build mapping from a device_gnss.csv file to times,lats,lons ---\ndef build_wls_time_latlon_map(gnss_path):\n    try:\n        df = pd.read_csv(gnss_path)\n    except Exception as e:\n        print(f\"Failed to read {gnss_path}: {e}\", file=sys.stderr)\n        return None\n    if df.shape[0] == 0:\n        return None\n    time_col = detect_time_column(df)\n    xcol, ycol, zcol = find_wls_cols(df)\n    if xcol is None or ycol is None or zcol is None:\n        return None\n    keep = df[[time_col, xcol, ycol, zcol]].copy()\n    keep = keep.dropna(subset=[xcol, ycol, zcol])\n    if keep.shape[0] == 0:\n        return None\n    # ensure numeric time\n    try:\n        keep[time_col] = pd.to_numeric(keep[time_col], errors=\"coerce\")\n    except Exception:\n        pass\n    keep = keep.dropna(subset=[time_col])\n    if keep.shape[0] == 0:\n        return None\n    # cast to int64 if possible\n    try:\n        keep[time_col] = keep[time_col].astype(np.int64)\n    except Exception:\n        try:\n            keep[time_col] = keep[time_col].astype(np.float64).astype(np.int64)\n        except Exception:\n            # give up\n            keep[time_col] = keep[time_col].astype(np.int64, errors=\"ignore\")\n    keep = keep.sort_values(by=time_col).drop_duplicates(\n        subset=[time_col], keep=\"first\"\n    )\n    times = keep[time_col].values.astype(np.int64)\n    xs = keep[xcol].astype(float).values\n    ys = keep[ycol].astype(float).values\n    zs = keep[zcol].astype(float).values\n    # convert to lat/lon, try vectorized then fallback\n    try:\n        lats, lons = ecef_to_latlon(xs, ys, zs)\n    except Exception:\n        lats = np.empty_like(xs)\n        lons = np.empty_like(xs)\n        for i, (xx, yy, zz) in enumerate(zip(xs, ys, zs)):\n            try:\n                lat, lon = ecef_to_latlon(xx, yy, zz)\n            except Exception:\n                lat, lon = np.nan, np.nan\n            lats[i] = lat\n            lons[i] = lon\n    mask = np.isfinite(lats) & np.isfinite(lons)\n    if not np.any(mask):\n        return None\n    order = np.argsort(times[mask])\n    return {\n        \"times\": times[mask][order],\n        \"lats\": lats[mask][order],\n        \"lons\": lons[mask][order],\n    }\n\n\n# nearest neighbor predict\ndef predict_from_map(tmap, query_times):\n    times = tmap[\"times\"]\n    lats = tmap[\"lats\"]\n    lons = tmap[\"lons\"]\n    if len(times) == 0:\n        return np.full(len(query_times), np.nan), np.full(len(query_times), np.nan)\n    qt = np.asarray(query_times, dtype=np.int64)\n    idxs = np.searchsorted(times, qt, side=\"left\")\n    preds_lat = np.empty(len(qt))\n    preds_lon = np.empty(len(qt))\n    for i, (q, idx) in enumerate(zip(qt, idxs)):\n        cand = []\n        if idx < len(times):\n            cand.append((abs(int(times[idx]) - int(q)), idx))\n        if idx - 1 >= 0:\n            cand.append((abs(int(times[idx - 1]) - int(q)), idx - 1))\n        if len(cand) == 0:\n            preds_lat[i], preds_lon[i] = np.nan, np.nan\n        else:\n            _, chosen = min(cand, key=lambda x: x[0])\n            preds_lat[i] = lats[chosen]\n            preds_lon[i] = lons[chosen]\n    return preds_lat, preds_lon\n\n\n# Build dictionary of test device_gnss paths keyed by \"drive_phone\" (exact)\ndef build_test_gnss_index(test_root):\n    mapping = {}\n    if not os.path.isdir(test_root):\n        return mapping\n    for drive in sorted(os.listdir(test_root)):\n        drive_path = os.path.join(test_root, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            key = f\"{drive}_{phone}\"\n            mapping[key] = gnss_path if os.path.exists(gnss_path) else None\n    return mapping\n\n\n# Build dictionary of train device_gnss paths similarly\ndef build_train_gnss_index(train_root):\n    mapping = {}\n    if not os.path.isdir(train_root):\n        return mapping\n    for drive in sorted(os.listdir(train_root)):\n        drive_path = os.path.join(train_root, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            mapping[f\"{drive}_{phone}\"] = (\n                gnss_path if os.path.exists(gnss_path) else None\n            )\n    return mapping\n\n\n# Validate on train (nearest-neighbor using WLS) - returns metric and per-phone errors\ndef validate_on_train(max_files=60):\n    if not os.path.isdir(TRAIN_DIR):\n        print(\"No train directory found, skipping validation.\", file=sys.stderr)\n        return None\n    tmap_cache = {}\n    phone_errors = {}\n    processed = 0\n    for drive in sorted(os.listdir(TRAIN_DIR)):\n        drive_path = os.path.join(TRAIN_DIR, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n            if not os.path.exists(gt_path):\n                continue\n            if gnss_path in tmap_cache:\n                tmap = tmap_cache[gnss_path]\n            else:\n                if os.path.exists(gnss_path):\n                    tmap = build_wls_time_latlon_map(gnss_path)\n                else:\n                    tmap = None\n                tmap_cache[gnss_path] = tmap\n            if tmap is None:\n                continue\n            try:\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Failed reading ground_truth {gt_path}: {e}\", file=sys.stderr)\n                continue\n            # find columns\n            lower_cols = [c.lower() for c in gt.columns]\n            if \"unixtimemillis\" in lower_cols:\n                time_col = gt.columns[lower_cols.index(\"unixtimemillis\")]\n            elif \"utctimemillis\" in lower_cols:\n                time_col = gt.columns[lower_cols.index(\"utctimemillis\")]\n            else:\n                # fallback to first numeric-like column\n                time_col = detect_time_column(gt)\n            if \"latitudedegrees\" in lower_cols:\n                lat_col = gt.columns[lower_cols.index(\"latitudedegrees\")]\n            else:\n                lat_col = None\n            if \"longitudedegrees\" in lower_cols:\n                lon_col = gt.columns[lower_cols.index(\"longitudedegrees\")]\n            else:\n                lon_col = None\n            if lat_col is None or lon_col is None:\n                # try flexible names\n                for c in gt.columns:\n                    lc = c.lower()\n                    if lat_col is None and \"latitude\" in lc:\n                        lat_col = c\n                    if lon_col is None and \"longitude\" in lc:\n                        lon_col = c\n            if lat_col is None or lon_col is None:\n                continue\n            try:\n                q_times = gt[time_col].astype(np.int64).values\n                q_lats = gt[lat_col].astype(float).values\n                q_lons = gt[lon_col].astype(float).values\n            except Exception:\n                continue\n            pred_lats, pred_lons = predict_from_map(tmap, q_times)\n            dists = []\n            for a, b, pa, pb in zip(q_lats, q_lons, pred_lats, pred_lons):\n                if np.isfinite(pa) and np.isfinite(pb):\n                    dists.append(haversine(a, b, pa, pb))\n            if len(dists) == 0:\n                continue\n            phone_key = f\"{drive}_{phone}\"\n            phone_errors[phone_key] = np.array(dists)\n            processed += 1\n            if max_files is not None and processed >= max_files:\n                break\n        if max_files is not None and processed >= max_files:\n            break\n    if len(phone_errors) == 0:\n        print(\"No train phones validated.\", file=sys.stderr)\n        return None\n    per_phone_scores = []\n    for phone, dists in phone_errors.items():\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        per_phone_scores.append((p50 + p95) / 2.0)\n    metric = float(np.mean(per_phone_scores))\n    return metric, phone_errors\n\n\n# Build submission using sample submission and test maps\ndef build_submission(sample_sub_path):\n    if not os.path.exists(sample_sub_path):\n        raise FileNotFoundError(f\"{sample_sub_path} not found\")\n    sub = pd.read_csv(sample_sub_path)\n    if not all(c in sub.columns for c in [\"phone\", \"UnixTimeMillis\"]):\n        raise ValueError(\"Sample submission missing required columns\")\n    n = len(sub)\n    out_lats = np.full(n, np.nan, dtype=float)\n    out_lons = np.full(n, np.nan, dtype=float)\n    phones = sub[\"phone\"].unique()\n    # index test files\n    test_index = build_test_gnss_index(TEST_DIR)\n    # preload tmap cache for all available test GNSS (but lazily)\n    tmap_cache = {}\n    # also build a global fallback set by sampling midpoints from each tmap\n    global_entries = []\n    # First pass: for each available test device build its tmap and sample a midpoint for fallback\n    for key, path in test_index.items():\n        if path is None or not os.path.exists(path):\n            continue\n        try:\n            tmap = build_wls_time_latlon_map(path)\n        except Exception:\n            tmap = None\n        tmap_cache[path] = tmap\n        if tmap is not None and len(tmap[\"times\"]) > 0:\n            mid = len(tmap[\"times\"]) // 2\n            global_entries.append(\n                (tmap[\"times\"][mid], tmap[\"lats\"][mid], tmap[\"lons\"][mid])\n            )\n    if len(global_entries) > 0:\n        global_times = np.array([e[0] for e in global_entries]).astype(np.int64)\n        global_lats = np.array([e[1] for e in global_entries])\n        global_lons = np.array([e[2] for e in global_entries])\n        global_lat_median = float(np.nanmedian(global_lats))\n        global_lon_median = float(np.nanmedian(global_lons))\n    else:\n        global_times = np.array([], dtype=np.int64)\n        global_lats = np.array([])\n        global_lons = np.array([])\n        global_lat_median = 0.0\n        global_lon_median = 0.0\n    # helper to find gnss_path for a sample submission phone id (case-insensitive)\n    lowered_to_path = {}\n    for key, path in test_index.items():\n        if path is None:\n            continue\n        lowered_to_path[key.lower()] = path\n    # additionally map by drive+phone lower, and also try to match phone folder ignoring drive\n    for phone_val in phones:\n        mask = sub[\"phone\"] == phone_val\n        indices = np.where(mask)[0]\n        # direct exact match\n        gnss_path = None\n        if phone_val in test_index and test_index[phone_val] is not None:\n            gnss_path = test_index[phone_val]\n        else:\n            # try case-insensitive exact\n            lp = phone_val.lower()\n            if lp in lowered_to_path:\n                gnss_path = lowered_to_path[lp]\n            else:\n                # try splitting by last underscore to get drive\n                if \"_\" in phone_val:\n                    drive_part = phone_val.rsplit(\"_\", 1)[0]\n                    phone_part = phone_val.rsplit(\"_\", 1)[1].lower()\n                    candidate = f\"{drive_part}_{phone_part}\"\n                    if candidate in lowered_to_path:\n                        gnss_path = lowered_to_path[candidate]\n                    else:\n                        # try any entry under that drive\n                        drive_dir = os.path.join(TEST_DIR, drive_part)\n                        if os.path.isdir(drive_dir):\n                            for cand in os.listdir(drive_dir):\n                                cand_key = f\"{drive_part}_{cand}\"\n                                if cand_key.lower() in lowered_to_path:\n                                    gnss_path = lowered_to_path[cand_key.lower()]\n                                    break\n                else:\n                    # try to find any mapping that contains the phone_val as substring\n                    for key_low, p in lowered_to_path.items():\n                        if phone_val.lower() in key_low or key_low in phone_val.lower():\n                            gnss_path = p\n                            break\n        tmap = None\n        if gnss_path is not None:\n            if gnss_path in tmap_cache:\n                tmap = tmap_cache[gnss_path]\n            else:\n                tmap = build_wls_time_latlon_map(gnss_path)\n                tmap_cache[gnss_path] = tmap\n        query_times = sub.loc[mask, \"UnixTimeMillis\"].astype(np.int64).values\n        if tmap is None:\n            # attempt to use any other tmap under same drive (if possible)\n            alt_used = False\n            if \"_\" in phone_val:\n                drive_part = phone_val.rsplit(\"_\", 1)[0]\n                drive_dir = os.path.join(TEST_DIR, drive_part)\n                if os.path.isdir(drive_dir):\n                    for cand in os.listdir(drive_dir):\n                        cand_path = os.path.join(drive_dir, cand, \"device_gnss.csv\")\n                        if (\n                            cand_path in tmap_cache\n                            and tmap_cache[cand_path] is not None\n                        ):\n                            latp, lonp = predict_from_map(\n                                tmap_cache[cand_path], query_times\n                            )\n                            out_lats[indices] = latp\n                            out_lons[indices] = lonp\n                            alt_used = True\n                            break\n                        elif os.path.exists(cand_path):\n                            tmap_alt = build_wls_time_latlon_map(cand_path)\n                            tmap_cache[cand_path] = tmap_alt\n                            if tmap_alt is not None:\n                                latp, lonp = predict_from_map(tmap_alt, query_times)\n                                out_lats[indices] = latp\n                                out_lons[indices] = lonp\n                                alt_used = True\n                                break\n            if alt_used:\n                continue\n            # otherwise leave NaNs for now\n            continue\n        # use tmap directly\n        latp, lonp = predict_from_map(tmap, query_times)\n        out_lats[indices] = latp\n        out_lons[indices] = lonp\n    # Fill remaining NaNs via nearest global time or median\n    nan_idx = np.where((~np.isfinite(out_lats)) | (~np.isfinite(out_lons)))[0]\n    if len(nan_idx) > 0:\n        if len(global_times) > 0:\n            for idx in nan_idx:\n                try:\n                    qt = int(sub.loc[idx, \"UnixTimeMillis\"])\n                except Exception:\n                    qt = None\n                if qt is None:\n                    out_lats[idx] = global_lat_median\n                    out_lons[idx] = global_lon_median\n                    continue\n                j = np.argmin(np.abs(global_times - qt))\n                out_lats[idx] = global_lats[j]\n                out_lons[idx] = global_lons[j]\n        else:\n            out_lats[nan_idx] = global_lat_median\n            out_lons[nan_idx] = global_lon_median\n    submission_df = pd.DataFrame(\n        {\n            \"phone\": sub[\"phone\"],\n            \"UnixTimeMillis\": sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": out_lats,\n            \"LongitudeDegrees\": out_lons,\n        }\n    )\n    submission_df.to_csv(OUTPUT_FILE, index=False)\n    return submission_df\n\n\ndef main():\n    print(\"Starting run...\", file=sys.stderr)\n    # Validate on train using a subset\n    print(\"Computing validation metric on train set (sample)...\", file=sys.stderr)\n    try:\n        val_res = validate_on_train(max_files=60)\n    except Exception as e:\n        val_res = None\n        print(f\"Validation failed with exception: {e}\", file=sys.stderr)\n    if val_res is None:\n        print(\n            \"Validation skipped or no data available. Proceeding to build submission.\",\n            file=sys.stderr,\n        )\n    else:\n        metric_value, phone_errors = val_res\n        print(\n            f\"Validation metric (mean of per-phone (50th+95th)/2): {metric_value:.3f} meters\"\n        )\n        print(f\"Validated {len(phone_errors)} phones (sample).\", file=sys.stderr)\n    # Build submission\n    print(\"Building submission predictions...\", file=sys.stderr)\n    try:\n        submission_df = build_submission(SAMPLE_SUB)\n        print(f\"Saved submission to {OUTPUT_FILE}\", file=sys.stderr)\n        print(\"Submission preview (first 5 rows):\")\n        print(submission_df.head(5).to_string(index=False))\n    except Exception as e:\n        print(f\"Failed to build submission: {e}\", file=sys.stderr)\n\n\nif __name__ == \"__main__\":\n    main()\n",
                "exec_time": 0.000225067138671875,
                "metric": null,
                "is_buggy": true,
                "exc_type": null,
                "exc_value": null,
                "analysis": "The script executed and produced a submission file at ./submission/submission.csv. Validation on the train set was attempted but no numeric validation metric was reported (likely because no train files were available or validation was skipped). The submission was built by nearest-neighbor lookup from WLS ECEF positions when available, with fallbacks to median/global samples for missing entries.",
                "alignment": {
                        "plan_code_similarity": 0.00586664331684252,
                        "has_changes": true,
                        "change_magnitude": 0.9659591860566957
                }
        }
];
        let currentStepIndex = null;

        function buildTreeHTML(steps) {
            const stepMap = {};
            const childrenMap = {};
            const rootSteps = [];

            steps.forEach(step => {
                stepMap[step.id] = step;
                if (!step.parent_id) {
                    rootSteps.push(step);
                } else {
                    if (!childrenMap[step.parent_id]) {
                        childrenMap[step.parent_id] = [];
                    }
                    childrenMap[step.parent_id].push(step);
                }
            });

            function buildNode(step, isRoot = false) {
                const buggyClass = step.is_buggy ? 'buggy' : '';
                const rootClass = isRoot ? 'root' : '';

                let html = `
                    <div class="tree-node ${rootClass}">
                        <div class="node-item ${buggyClass}" data-index="${step.index}" onclick="selectStep(${step.index})">
                            <div class="node-label">
                                ${step.is_buggy ? '‚ö†Ô∏è' : '‚úì'} Step ${step.step}
                            </div>
                            <div class="node-meta">
                                ${step.metric !== null ? 'Score: ' + step.metric?.toFixed(4) : 'No metric'}
                            </div>
                        </div>
                `;

                if (childrenMap[step.id]) {
                    childrenMap[step.id].forEach(child => {
                        html += buildNode(child, false);
                    });
                }

                html += '</div>';
                return html;
            }

            return rootSteps.map(root => buildNode(root, true)).join('');
        }

        function selectStep(index) {
            currentStepIndex = index;
            const step = stepsData[index];

            // Update active state in tree
            document.querySelectorAll('.node-item').forEach(item => {
                item.classList.remove('active');
            });
            document.querySelector(`[data-index="${index}"]`)?.classList.add('active');

            // Update header
            document.getElementById('step-title').textContent =
                `Step ${step.step} ${step.is_buggy ? '‚ö†Ô∏è BUGGY' : '‚úì'}`;

            // Update navigation buttons
            document.getElementById('prev-btn').disabled = index === 0;
            document.getElementById('next-btn').disabled = index === stepsData.length - 1;

            // Render content
            renderStepContent(step, index);
        }

        function renderStepContent(step, index) {
            const prevStep = index > 0 ? stepsData[index - 1] : null;
            const diffHtml = prevStep ? computeDiff(prevStep.code, step.code) : '';

            let content = `
                <div class="section">
                    <h3>
                        üìã Step Information
                        <span class="badge ${step.is_buggy ? 'buggy' : 'valid'}">
                            ${step.is_buggy ? 'BUGGY' : 'VALID'}
                        </span>
                    </h3>
                    <div class="metric-grid">
                        <div class="metric-item">
                            <div class="metric-label">Step Number</div>
                            <div class="metric-value">${step.step}</div>
                        </div>
                        <div class="metric-item">
                            <div class="metric-label">Execution Time</div>
                            <div class="metric-value">${step.exec_time !== null ? step.exec_time.toFixed(2) + 's' : 'N/A'}</div>
                        </div>
                        <div class="metric-item">
                            <div class="metric-label">Metric Score</div>
                            <div class="metric-value">${step.metric !== null ? step.metric.toFixed(4) : 'N/A'}</div>
                        </div>
                        <div class="metric-item">
                            <div class="metric-label">Change Magnitude</div>
                            <div class="metric-value">${(step.alignment.change_magnitude * 100).toFixed(1)}%</div>
                        </div>
                    </div>
                </div>

                <div class="section">
                    <h3>üìù Plan</h3>
                    <div class="plan-box">${escapeHtml(step.plan || 'No plan available')}</div>
                </div>

                <div class="section">
                    <h3 class="collapsible collapsed" onclick="toggleCollapse(this)">üíª Code</h3>
                    <div class="collapsible-content collapsed">
                        <pre>${escapeHtml(step.code || 'No code available')}</pre>
                    </div>
                </div>
            `;

            if (step.analysis && step.analysis.trim()) {
                content += `
                    <div class="section">
                        <h3>üîç Feedback / Analysis</h3>
                        <div class="analysis-box">${escapeHtml(step.analysis)}</div>
                    </div>
                `;
            }

            if (prevStep) {
                const similarity = (1 - step.alignment.change_magnitude) * 100;
                content += `
                    <div class="section">
                        <h3>üîÑ Code Changes from Previous Step</h3>
                        <div>
                            <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;">
                                <span style="color: #858585;">Similarity: ${similarity.toFixed(1)}%</span>
                            </div>
                            <div class="similarity-bar">
                                <div class="similarity-fill" style="width: ${similarity}%"></div>
                            </div>
                        </div>
                        <div class="diff-container">
                            ${diffHtml}
                        </div>
                    </div>
                `;
            }

            if (step.is_buggy && step.exc_type) {
                content += `
                    <div class="section">
                        <h3>‚ö†Ô∏è Error Details</h3>
                        <div class="error-box">
                            <div class="error-type">${step.exc_type}</div>
                            <div class="error-message">${escapeHtml(step.exc_value || 'No error message')}</div>
                        </div>
                    </div>
                `;
            }

            document.getElementById('main-content').innerHTML = content;
            document.getElementById('main-content').scrollTop = 0;
        }

        function computeDiff(code1, code2) {
            if (!code1) code1 = '';
            if (!code2) code2 = '';

            const lines1 = code1.split('\n');
            const lines2 = code2.split('\n');

            // Use a simple diff algorithm to find matching blocks
            const diffResult = simpleDiff(lines1, lines2);

            let diffHtml = '<table class="diff-table">';
            diffHtml += '<tr><th class="diff-header">Previous Code</th><th class="diff-header">Current Code</th></tr>';

            for (const item of diffResult) {
                const leftClass = item.type === 'delete' ? 'diff_sub' : (item.type === 'change' ? 'diff_chg' : 'diff_none');
                const rightClass = item.type === 'add' ? 'diff_add' : (item.type === 'change' ? 'diff_chg' : 'diff_none');

                const leftLine = item.leftLine !== null ? escapeHtml(item.leftLine) : '';
                const rightLine = item.rightLine !== null ? escapeHtml(item.rightLine) : '';

                const leftContent = item.leftLine !== null ? leftLine : '<span style="color: #858585;">...</span>';
                const rightContent = item.rightLine !== null ? rightLine : '<span style="color: #858585;">...</span>';

                diffHtml += `<tr>
                    <td class="${leftClass}">${leftContent}</td>
                    <td class="${rightClass}">${rightContent}</td>
                </tr>`;
            }

            diffHtml += '</table>';
            return diffHtml;
        }

        function simpleDiff(lines1, lines2) {
            // A simple diff implementation using longest common subsequence approach
            const result = [];
            let i = 0, j = 0;

            while (i < lines1.length || j < lines2.length) {
                if (i >= lines1.length) {
                    // Only lines2 left (additions)
                    result.push({ type: 'add', leftLine: null, rightLine: lines2[j] });
                    j++;
                } else if (j >= lines2.length) {
                    // Only lines1 left (deletions)
                    result.push({ type: 'delete', leftLine: lines1[i], rightLine: null });
                    i++;
                } else if (lines1[i] === lines2[j]) {
                    // Lines match
                    result.push({ type: 'equal', leftLine: lines1[i], rightLine: lines2[j] });
                    i++;
                    j++;
                } else {
                    // Lines differ - look ahead to see if this is a change, add, or delete
                    let foundMatch = false;

                    // Look ahead in lines2 to see if lines1[i] appears later (deletion)
                    for (let k = j + 1; k < Math.min(j + 5, lines2.length); k++) {
                        if (lines1[i] === lines2[k]) {
                            foundMatch = true;
                            break;
                        }
                    }

                    if (foundMatch) {
                        // This is an addition in lines2
                        result.push({ type: 'add', leftLine: null, rightLine: lines2[j] });
                        j++;
                    } else {
                        // Look ahead in lines1 to see if lines2[j] appears later (addition)
                        foundMatch = false;
                        for (let k = i + 1; k < Math.min(i + 5, lines1.length); k++) {
                            if (lines1[k] === lines2[j]) {
                                foundMatch = true;
                                break;
                            }
                        }

                        if (foundMatch) {
                            // This is a deletion in lines1
                            result.push({ type: 'delete', leftLine: lines1[i], rightLine: null });
                            i++;
                        } else {
                            // This is a change (both lines present but different)
                            result.push({ type: 'change', leftLine: lines1[i], rightLine: lines2[j] });
                            i++;
                            j++;
                        }
                    }
                }
            }

            return result;
        }

        function escapeHtml(text) {
            const div = document.createElement('div');
            div.textContent = text;
            return div.innerHTML;
        }

        function toggleCollapse(header) {
            header.classList.toggle('collapsed');
            const content = header.nextElementSibling;
            if (content && content.classList.contains('collapsible-content')) {
                content.classList.toggle('collapsed');
            }
        }

        function navigatePrev() {
            if (currentStepIndex > 0) {
                selectStep(currentStepIndex - 1);
            }
        }

        function navigateNext() {
            if (currentStepIndex < stepsData.length - 1) {
                selectStep(currentStepIndex + 1);
            }
        }

        // Keyboard navigation
        document.addEventListener('keydown', (e) => {
            if (currentStepIndex === null) return;

            if (e.key === 'ArrowUp') {
                e.preventDefault();
                navigatePrev();
            } else if (e.key === 'ArrowDown') {
                e.preventDefault();
                navigateNext();
            }
        });

        // Initialize
        document.getElementById('tree-container').innerHTML = buildTreeHTML(stepsData);
        if (stepsData.length > 0) {
            selectStep(0);
        }
    </script>
</body>
</html>
