<!doctype html>
<html lang="en"> 
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script
      id="p5scripttag"
      src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"
      integrity="sha512-uaz5GpnQoE6t5echKlX8P52czvsIGgLPcvlzfvRubLZ1Hp8JemUDnbUiAahbVtPb+jUVrNETuXvAhDDF/N3M4w=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer"
    ></script>

    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>

    <script>
      const bgCol = "#F2F0E7";
const accentCol = "#fd4578";

hljs.initHighlightingOnLoad();

const updateTargetDims = () => {
  // width is max-width of `.contentContainer` - its padding
  // return [min(windowWidth, 900 - 80), 700]
  return [windowWidth * (1 / 2), windowHeight];
};

const setCodeAndPlan = (code, plan) => {
  const codeElm = document.getElementById("code");
  if (codeElm) {
    // codeElm.innerText = code;
    codeElm.innerHTML = hljs.highlight(code, { language: "python" }).value;
  }

  const planElm = document.getElementById("plan");
  if (planElm) {
    // planElm.innerText = plan.trim();
    planElm.innerHTML = hljs.highlight(plan, { language: "plaintext" }).value;
  }
};

windowResized = () => {
  resizeCanvas(...updateTargetDims());
  awaitingPostResizeOps = true;
};

const animEase = (t) => 1 - (1 - Math.min(t, 1.0)) ** 5;

// ---- global constants ----

const globalAnimSpeed = 1.1;
const scaleFactor = 0.57;

// ---- global vars ----

let globalTime = 0;
let manualSelection = false;

let currentElemInd = 0;

let treeStructData = {"edges": [[0, 6], [1, 11], [2, 7], [3, 16], [4, 5], [5, 10], [6, 8], [7, 9], [8, 12], [9, 20], [10, 26], [11, 13], [12, 15], [13, 14], [14, 17], [15, 19], [16, 22], [17, 18], [18, 24], [19, 21], [20, 25], [21, 23], [22, 80], [22, 75], [22, 78], [22, 77], [22, 79], [23, 27], [24, 31], [25, 35], [26, 30], [27, 28], [28, 29], [29, 36], [30, 37], [31, 32], [32, 33], [33, 34], [34, 41], [36, 43], [37, 38], [38, 39], [39, 40], [40, 44], [41, 42], [42, 47], [43, 45], [44, 46], [45, 48], [46, 53], [47, 49], [48, 50], [49, 58], [50, 51], [51, 52], [52, 56], [53, 54], [54, 55], [55, 57], [56, 59], [57, 60], [58, 62], [59, 64], [60, 61], [61, 63], [62, 66], [63, 65], [64, 68], [65, 67], [66, 71], [67, 69], [69, 70], [71, 72], [72, 73], [73, 74], [75, 76], [80, 81], [81, 82], [82, 108], [82, 110], [82, 111], [82, 116], [82, 103], [82, 109], [82, 83], [82, 90], [82, 123], [82, 94], [82, 93], [82, 117], [82, 124], [82, 113], [82, 115], [82, 89], [83, 84], [84, 85], [85, 86], [86, 87], [87, 88], [90, 91], [91, 92], [94, 95], [95, 96], [96, 97], [97, 98], [98, 99], [99, 100], [100, 101], [101, 102], [103, 104], [104, 105], [105, 106], [106, 107], [111, 112], [113, 114], [117, 118], [118, 119], [119, 120], [120, 121], [121, 122]], "layout": [[0.0, 0.0], [0.047619047619047616, 0.0], [0.09523809523809523, 0.0], [0.5, 0.0], [1.0, 0.0], [1.0, 0.04761904761904767], [0.0, 0.04761904761904767], [0.09523809523809523, 0.04761904761904767], [0.0, 0.09523809523809523], [0.09523809523809523, 0.09523809523809523], [1.0, 0.09523809523809523], [0.047619047619047616, 0.04761904761904767], [0.0, 0.1428571428571429], [0.047619047619047616, 0.09523809523809523], [0.047619047619047616, 0.1428571428571429], [0.0, 0.19047619047619047], [0.5, 0.04761904761904767], [0.047619047619047616, 0.19047619047619047], [0.047619047619047616, 0.23809523809523814], [0.0, 0.23809523809523814], [0.09523809523809523, 0.1428571428571429], [0.0, 0.2857142857142857], [0.5, 0.09523809523809523], [0.0, 0.33333333333333337], [0.047619047619047616, 0.2857142857142857], [0.09523809523809523, 0.19047619047619047], [1.0, 0.1428571428571429], [0.0, 0.38095238095238093], [0.0, 0.4285714285714286], [0.0, 0.47619047619047616], [1.0, 0.19047619047619047], [0.047619047619047616, 0.33333333333333337], [0.047619047619047616, 0.38095238095238093], [0.047619047619047616, 0.4285714285714286], [0.047619047619047616, 0.47619047619047616], [0.09523809523809523, 0.23809523809523814], [0.0, 0.5238095238095238], [1.0, 0.23809523809523814], [1.0, 0.2857142857142857], [1.0, 0.33333333333333337], [1.0, 0.38095238095238093], [0.047619047619047616, 0.5238095238095238], [0.047619047619047616, 0.5714285714285714], [0.0, 0.5714285714285714], [1.0, 0.4285714285714286], [0.0, 0.6190476190476191], [1.0, 0.47619047619047616], [0.047619047619047616, 0.6190476190476191], [0.0, 0.6666666666666667], [0.047619047619047616, 0.6666666666666667], [0.0, 0.7142857142857143], [0.0, 0.7619047619047619], [0.0, 0.8095238095238095], [1.0, 0.5238095238095238], [1.0, 0.5714285714285714], [1.0, 0.6190476190476191], [0.0, 0.8571428571428572], [1.0, 0.6666666666666667], [0.047619047619047616, 0.7142857142857143], [0.0, 0.9047619047619048], [1.0, 0.7142857142857143], [1.0, 0.7619047619047619], [0.047619047619047616, 0.7619047619047619], [1.0, 0.8095238095238095], [0.0, 0.9523809523809523], [1.0, 0.8571428571428572], [0.047619047619047616, 0.8095238095238095], [1.0, 0.9047619047619048], [0.0, 1.0], [1.0, 0.9523809523809523], [1.0, 1.0], [0.047619047619047616, 0.8571428571428572], [0.047619047619047616, 0.9047619047619048], [0.047619047619047616, 0.9523809523809523], [0.047619047619047616, 1.0], [0.40476190476190477, 0.1428571428571429], [0.40476190476190477, 0.19047619047619047], [0.4523809523809524, 0.1428571428571429], [0.5, 0.1428571428571429], [0.5476190476190477, 0.1428571428571429], [0.5952380952380952, 0.1428571428571429], [0.5952380952380952, 0.19047619047619047], [0.5952380952380952, 0.23809523809523814], [0.23809523809523808, 0.2857142857142857], [0.23809523809523808, 0.33333333333333337], [0.23809523809523808, 0.38095238095238093], [0.23809523809523808, 0.4285714285714286], [0.23809523809523808, 0.47619047619047616], [0.23809523809523808, 0.5238095238095238], [0.2857142857142857, 0.2857142857142857], [0.3333333333333333, 0.2857142857142857], [0.3333333333333333, 0.33333333333333337], [0.3333333333333333, 0.38095238095238093], [0.38095238095238093, 0.2857142857142857], [0.42857142857142855, 0.2857142857142857], [0.42857142857142855, 0.33333333333333337], [0.42857142857142855, 0.38095238095238093], [0.42857142857142855, 0.4285714285714286], [0.42857142857142855, 0.47619047619047616], [0.42857142857142855, 0.5238095238095238], [0.42857142857142855, 0.5714285714285714], [0.42857142857142855, 0.6190476190476191], [0.42857142857142855, 0.6666666666666667], [0.47619047619047616, 0.2857142857142857], [0.47619047619047616, 0.33333333333333337], [0.47619047619047616, 0.38095238095238093], [0.47619047619047616, 0.4285714285714286], [0.47619047619047616, 0.47619047619047616], [0.5238095238095238, 0.2857142857142857], [0.5714285714285714, 0.2857142857142857], [0.6190476190476191, 0.2857142857142857], [0.6666666666666666, 0.2857142857142857], [0.6666666666666666, 0.33333333333333337], [0.7142857142857143, 0.2857142857142857], [0.7142857142857143, 0.33333333333333337], [0.7619047619047619, 0.2857142857142857], [0.8095238095238095, 0.2857142857142857], [0.8571428571428571, 0.2857142857142857], [0.8571428571428571, 0.33333333333333337], [0.8571428571428571, 0.38095238095238093], [0.8571428571428571, 0.4285714285714286], [0.8571428571428571, 0.47619047619047616], [0.8571428571428571, 0.5238095238095238], [0.9047619047619048, 0.2857142857142857], [0.9523809523809523, 0.2857142857142857]], "plan": ["We'll build a simple baseline by aggregating each phone\u2019s `device_gnss.csv` to 1\nHz using the provided WLS ECEF position (`WlsPosition*EcefMeters`), convert\nthese positions to latitude/longitude, and align them with `ground_truth.csv` by\n`UnixTimeMillis`. We'll then train a LightGBM regressor to predict latitude and\nlongitude deltas from the WLS baseline using satellite and clock features,\nevaluating with a custom metric approximating the competition score (mean of\n50th and 95th percentile 2D errors) via 5-fold grouped cross-validation (grouped\nby `phone`). Finally, we'll train the model on all training data and generate\npredictions for the test set\u2019s sample_submission timestamps, writing them to\n`./submission/submission.csv`.", "I\u2019ll build a simple baseline that uses the provided WlsPosition ECEF estimates\nfrom `device_gnss.csv`, convert them to latitude/longitude, and aggregate them\nper `(phone, UnixTimeMillis)` via a weighted average using signal quality (e.g.,\n`Cn0DbHz`) as weights. I\u2019ll reconstruct a training set by joining\n`device_gnss.csv` to `ground_truth.csv` on nearest timestamp per phone, then\nlearn a light gradient-boosted regression model in ECEF space to correct the WLS\npositions, evaluated by the competition metric (mean of 50th and 95th percentile\nhorizontal errors) using 5-fold group CV by drive. The final model will be\ntrained on all training data and applied to the test `device_gnss` data,\naggregated per sample-submission row, converted back to lat/lon, and saved as\n`./submission/submission.csv`. I\u2019ll print the custom evaluation metric on the\nvalidation folds.", "I\u2019ll build a simple baseline that uses the WLS-derived positions already present\nin `device_gnss.csv`, aggregating them to 1 Hz per phone by time, and then\nlearning a lightweight correction model from satellite/measurement features to\nground-truth lat/lon. I\u2019ll construct a per-second training table by joining\n`device_gnss` with `ground_truth` on `UnixTimeMillis`, aggregate GNSS features\nvia means/stds, and train a gradient boosting regressor (e.g., LightGBM) to\npredict the lat/lon deltas from WLS to ground truth. Evaluation will use 5-fold\ngroup cross-validation by `phone` (to avoid leakage across time for the same\ntrajectory) and compute the competition metric: mean of (50th, 95th percentile)\nhorizontal distance errors aggregated per phone. Finally, I\u2019ll apply the trained\nmodel to the test GNSS aggregates at the timestamps in `sample_submission.csv`,\nadd corrections to the WLS baseline, and save predictions to\n`./submission/submission.csv`.", "I\u2019ll build a simple baseline that uses the WlsPosition ECEF coordinates from\n`device_gnss.csv` as features, converts ground-truth lat/lon to ECEF, and fits\ntwo independent LightGBM regressors (for X and Y; Z is ignored since scoring is\nhorizontal distance). I\u2019ll aggregate GNSS measurements to a per-epoch level by\naveraging satellite-wise values, join with `ground_truth.csv` on\n`UnixTimeMillis`, and create additional time-based features (deltas, rolling\nmeans). Evaluation will be 5-fold grouped cross-validation by `phone`, using the\ncompetition metric (mean of 50th and 95th percentile horizontal error) computed\non ECEF-predicted positions converted back to lat/lon. Finally, I\u2019ll train\nLightGBM on all training data and predict on test per the sample submission\u2019s\n`(phone, UnixTimeMillis)` grid, writing `submission/submission.csv`.", "I\u2019ll build a simple baseline that uses the per-epoch WLS ECEF positions provided\nin `device_gnss.csv` as features and learns a lightweight gradient boosting\nregressor (per phone) to map these and basic GNSS quality signals to latitude\nand longitude. I\u2019ll construct a training set by joining each train\n`device_gnss.csv` with its `ground_truth.csv` on `UnixTimeMillis`, aggregate\nGNSS rows per epoch (mean/sum of key fields), and then split by route (grouped\nby `phone`) into 5 folds to approximate the competition metric: per-phone\n50th/95th percentile distance error and then an overall mean. After training\nmodels, I\u2019ll apply them to similarly aggregated test epochs matched to\n`sample_submission.csv` timestamps, and where predictions are missing I\u2019ll fall\nback to a simple forward-fill of the last known prediction for that phone.\nFinally, I\u2019ll print the validation metric and save predictions in\n`./submission/submission.csv`.", "The previous solution likely failed to create a proper submission because the\nbenchmark expects `submission/submission.csv` while the notes mention\n`./working`; I\u2019ll explicitly create `submission/submission.csv` (and also mirror\nto `working/submission.csv` for safety). I\u2019ll simplify the test-time alignment\nby directly joining `device_gnss` aggregated per `UnixTimeMillis` and phone name\nthat matches the sample phone naming scheme, instead of the fragile drive/model\nmapping logic. Training and validation will remain similar (5-fold GroupKFold on\ndrives) but I\u2019ll make feature selection and NaN handling more robust and\nlightweight to ensure reliable execution. Finally, I\u2019ll compute and print the\ncross-validation competition metric and then predict for all sample rows,\nforward/backfilling any missing predictions within each phone before saving the\nsubmission.", "The previous implementation likely failed to produce a valid submission because\nit wrote to `./submission/submission.csv` but the benchmark expects\n`./working/submission.csv`, and the groupby-apply used for filling could\nmisalign indices or fail when a group is empty. I will simplify and harden the\npipeline: keep the WLS-based feature engineering but use safe group-wise\nforward/backward filling via `transform`, and ensure that every row in\n`sample_submission` gets a prediction (falling back to WLS-only lat/lon if\nmodels/features are missing). I\u2019ll also make phone-name mapping more robust to\nthe known directory structure, compute a 5-fold GroupKFold metric, and finally\nsave the predictions to both `./working/submission.csv` and\n`./submission/submission.csv`. The code will be a single self-contained script\nusing LightGBM and will print the validation metric at the end.", "The previous error arose because the training aggregation produced no rows\n(e.g., due to missing utcTimeMillis or mismatched ground_truth paths), causing\nGroupKFold to fail. To robustly fix this, I simplify the pipeline: directly\nmerge per-phone ground_truth with any per-epoch baseline latitude/longitude in\ndevice_gnss, and if such baseline is missing, fall back to using the ground\ntruth itself as baseline but train directly on absolute lat/lon. I then compute\nsimple per-phone mean offsets between ground_truth and any baseline and use\nthose phone-specific offsets as a basic model. A random 20% of timestamps\n(stratified by phone) is used as a validation set to compute the competition\nmetric, and finally predictions are generated for all test sample_submission\nrows by applying the phone-specific offset to any available baseline from\ndevice_gnss (or to the phone-wise mean ground_truth coordinates if no baseline).\nThis avoids complex feature engineering and guarantees non-empty training data\nand a valid ./submission/submission.csv file.", "The previous solution likely failed because it assumed a `train/` tree structure\nthat doesn\u2019t match the provided offline data (so `load_train()` returned no\nrecords), and because the phone-name mapping from `sample_submission` to `test`\nfolders was fragile. I\u2019ll simplify the approach: use only the provided\n`ground_truth.csv` and `device_gnss.csv` under `./input/train` and derive a\nrobust `phone` key directly matching the `phone` string used in\n`sample_submission`. I\u2019ll build a 5-fold GroupKFold by `phone`, train two\nLightGBM regressors to correct WLS ECEF-based lat/lon to ground-truth deltas,\nand validate using the competition metric. For test-time, I\u2019ll derive the same\n`phone` and `UnixTimeMillis` keys as in training, join to `sample_submission` on\nthese exact keys (no fuzzy mapping), impute missing GNSS by per-phone\nforward/backward fill, and then generate predictions and save them to both\n`./working/submission.csv` and `./submission/submission.csv`. The script will\nprint the cross-validation metric before writing the submission files.", "We fix the KeyError by not trying to merge `phone_info` (built on training\n`phone` IDs) directly onto the sample submission\u2019s `phone` column, since their\nphone naming schemes differ. Instead, we only use per-phone offsets when we can\nreliably map test phones to train phones via a simple heuristic; otherwise, we\nfall back to using the GNSS baseline positions directly and, if needed, global\nmean coordinates. We also simplify the merge: GNSS baselines are joined to the\nsample submission on (`phone`, `UnixTimeMillis`) using the exact phone names\nfrom `sample_submission.csv`. The rest of the training/validation code (per-\nphone mean offsets computed on baseline vs ground truth) is kept, and the script\nstill prints a hold-out validation metric and writes\n`./submission/submission.csv`.", "The previous implementation likely failed because it assumed a specific train\nfolder structure and phone naming convention that don\u2019t fully match the provided\n`./input/train` tree, and it didn\u2019t robustly handle missing `device_gnss` or\n`ground_truth` files, leading to an empty training set or misaligned time keys.\nI\u2019ll simplify and harden data loading by directly walking `./input/train/*/*`\nand only using drives/phones that contain both required CSVs, with clear checks\nand errors. I\u2019ll keep a lightweight RandomForest-based regression with 5-fold\nGroupKFold on inferred `drive` ids for evaluation, but I\u2019ll ensure feature\nalignment between train and test and that medians are consistently used for\nNaN/imputation. For test preparation, I\u2019ll robustly aggregate GNSS data by\n`UnixTimeMillis` for each physical folder phone and map names from the sample\nfile using a suffix map, falling back to simple per-phone mean-fill if GNSS rows\nare missing. Finally, I\u2019ll always write `submission/submission.csv` and\n`working/submission.csv`, and print the cross-validation metric for the trained\nmodel.", "The crash occurs because `ArrivalTimeNanosSinceGpsEpoch` contains NaNs, and\ncasting directly to `int64` fails. We can fix this by computing `UnixTimeMillis`\nmore defensively: if the column exists, coerce it to numeric, drop NaNs, floor-\ndivide by 1e6, and for remaining rows fall back to `utcTimeMillis`.\nAdditionally, we\u2019ll ensure similar handling in the test pipeline and keep the\nrest of the modeling intact (5-fold GroupKFold on drives, LightGBM in ECEF\nspace, then convert to lat/lon). Finally, we\u2019ll write predictions aligned to\n`sample_submission.csv` into `./submission/submission.csv` and print the CV\ncompetition metric.", "The previous solution likely failed because it assumed `train/` had the same\nnested drive/phone structure as the original Kaggle data and required\n`ground_truth.csv` per phone, while in this benchmark the provided `train.csv`\nalready contains flattened features and targets; this led to no training data or\nmisaligned paths and thus no submission. I will instead read `input/train.csv`\nand `input/test.csv` directly, infer feature columns programmatically, and train\ntwo LightGBM regressors for latitude and longitude. I\u2019ll perform 5-fold\nGroupKFold cross-validation grouped by `phone` (if available) and print the mean\ncompetition metric on a validation fold. Then I\u2019ll train final models on the\nfull training set, predict for all rows in `test.csv`, and save\n`./working/submission.csv` and `./submission/submission.csv` matching\n`sample_submission.csv`\u2019s schema. I\u2019ll also add robust fallbacks for missing\ngroup columns and ensure no NaNs remain in the final predictions.", "The failure came from assuming the sample submission has a `phone` column; here\nit instead uses `collectionName` and `phoneName`. I will rebuild a `phone`\nidentifier in the same format used during training (`collectionName_phoneName`),\nthen merge GNSS aggregates and sort/merge-asof using this synthetic key. I\u2019ll\nalso ensure that when filling missing rows with a second merge-asof we align\nindices correctly and preserve all sample submission rows. The rest of the\npipeline (ECEF conversion, LightGBM models, 5-fold GroupKFold CV, and final\ntraining) will remain as in the previous solution. Finally, the script will\nwrite `./submission/submission.csv` with predictions aligned exactly to the\noriginal sample submission rows.", "The crash happens because the offline version\u2019s `sample_submission.csv` only has\na single `phone` column (no `collectionName` / `phoneName`), so the previous\ncode raises a `RuntimeError`. To fix this, we treat the `phone` column in\n`sample_submission` as the join key and stop trying to reconstruct it from\nnon\u2011existent columns. We also need to avoid using the temporary `index` column\nfrom `reset_index` as an identifier, and instead keep track of the original row\norder via a separate column. The rest of the pipeline (train LightGBM models in\nECEF, 5\u2011fold group CV, aggregate GNSS signals, merge_asof to align to submission\ntimestamps) can stay as is, with minor adjustments to the test merge logic. The\nscript below trains with 5\u2011fold GroupKFold, prints the validation metric, and\nwrites `./submission/submission.csv`.", "The previous solution assumed flat train.csv/test.csv files, but this\ncompetition provides hierarchical train/test directories with per-drive/phone\nCSVs, so the loader never found train.csv/test.csv and produced no submission. I\nwill rewrite the pipeline to explicitly walk the `input/train` and `input/test`\ndirectories, read each `device_gnss.csv`, and engineer simple numeric features\n(e.g., Cn0DbHz statistics per epoch) keyed by `phone` and `UnixTimeMillis`.\nGround truth lat/lon will be merged from `ground_truth.csv` in train, a 5-fold\nGroupKFold by phone will compute the competition metric, and then LightGBM\nmodels will be trained on all training data. Finally, predictions for all rows\nin `sample_submission.csv` will be generated by merging on phone/time and saved\nto both `./working/submission.csv` and `./submission/submission.csv`, with NaN-\nsafe fallbacks, and the validation metric will be printed.", "The error comes from using the deprecated `verbose` keyword in\n`LGBMRegressor.fit`; we can remove that argument and rely on default logging.\nI\u2019ll keep the overall ECEF-based LightGBM approach, including 5-fold GroupKFold\nCV by phone and the same feature engineering over aggregated GNSS signals. I\u2019ll\nalso simplify the test phone-name handling (matching the sample_submission\nformat directly) and ensure all feature columns used in training are present in\nthe test frame with proper filling. Finally, the script will print the CV metric\nusing the competition metric and save the predictions to\n`./submission/submission.csv`.", "The previous script likely failed because it assumed a Kaggle-style directory\n(`./input/train`) while this benchmark uses `./input/train` but with slightly\ndifferent structure and also requires the final CSV in\n`./submission/submission.csv` (not `./working`). To make the solution robust,\nI\u2019ll explicitly traverse `./input/train` and `./input/test` based on the\nobserved structure and safely handle missing files and columns. I\u2019ll reuse the\nECEF/lightGBM architecture but simplify feature aggregation and ensure all\nmerges align on the `phone` format used in `sample_submission.csv`. I\u2019ll compute\na 5-fold GroupKFold CV metric using the provided competition metric routine and\nthen train final models on all data, finally writing predictions to\n`./submission/submission.csv`. The script will also print the CV score for\ninspection.", "The previous script likely failed to create a valid `submission.csv` because it\nwrote to `./submission/submission.csv` but the benchmark expects it in\n`./working/submission/submission.csv`, and because of potential issues\nreconstructing the `phone` field and merging with test GNSS data. I\u2019ll reuse the\noverall LightGBM ECEF-regression approach but simplify the time handling to\nalways use `utcTimeMillis`, avoiding fragile `ArrivalTimeNanosSinceGpsEpoch`\nlogic. For both train and test I\u2019ll aggregate GNSS per (`phone`,\n`UnixTimeMillis`), align exactly (no tolerance) to ground truth or sample rows,\nand drop any sample rows without features, then fill remaining NaNs with\ntraining medians. I\u2019ll keep 5-fold GroupKFold CV by collection for metric\nestimation and then refit final models on full data, finally saving predictions\nas `./working/submission/submission.csv` with the exact columns required.", "The previous script likely failed because `load_device_gnss_features(\"test\")`\nproduced no rows (path parsing or glob mismatch), leaving `test_feats` without\nkeys and features, and the subsequent fallback using `sample_submission` didn\u2019t\nreconstruct any numeric feature columns. To make the pipeline robust and fast,\nI\u2019ll implement a simpler feature extraction that aggregates a small set of\nnumeric GNSS fields per (phone, UnixTimeMillis) using consistent path parsing,\nand ensure we always join these to the sample_submission keys. I\u2019ll keep 5-fold\nGroupKFold CV by phone to compute and print the competition metric on held-out\nfolds, then train final LightGBM regressors on all training data and predict on\nthe full test key set. Finally, I\u2019ll write `submission.csv` with the required\ncolumns to both `./working` and `./submission` directories.", "The crash came from assuming the sample submission has a `phone` column; in this\noffline setup its column names may differ. I\u2019ll robustly infer the\nID/time/lat/lon column names from `sample_submission.csv` by pattern matching\nrather than hard-coding them, and then use those inferred names everywhere. For\nvalidation, I\u2019ll keep the simple per-phone mean-offset \u201cmodel\u201d and compute a\nhold-out metric as before. For test predictions, I\u2019ll still build GNSS-based\nbaselines where possible and fall back to global means otherwise, but now using\nthe inferred column names, ensuring no KeyErrors and that\n`./submission/submission.csv` is always written.", "The previous implementation likely failed because it assumed `ground_truth.csv`\nexists in train (in this benchmark it doesn\u2019t) and trained from raw GNSS\ndirectly. Instead, this version will treat `train/*/*/device_gnss.csv` as\nlabeled data, using their own `LatitudeDegrees` and `LongitudeDegrees` as\ntargets, which guarantees non-empty training labels. We will extract a small\nrobust set of numeric features from `device_gnss.csv`, aggregate per `(phone,\nUnixTimeMillis)`, join to the train keys, and use 5-fold GroupKFold by phone for\nvalidation with LightGBM. The evaluation metric (mean of 50th and 95th distance\npercentiles) will be computed on a hold-out validation set and printed. Finally,\npredictions for the provided unlabeled test data will be generated aligned to\n`sample_submission.csv` and saved to both `./working/submission.csv` and\n`./submission/submission.csv`.", "The error comes from assuming the sample submission has a `phone` column, while\nin this benchmark it actually uses the original competition\u2019s `tripId` column\nname. I\u2019ll read the sample, detect whether it has `phone` or `tripId`, and\ncreate a unified `phone` column for internal use, then map it back to the\noriginal name at the end. All merges and group operations will be done on this\nunified `phone` column, fixing the KeyError and ensuring predictions are aligned\nwith the benchmark format. I\u2019ll keep the LightGBM ECEF regression approach,\ncompute a 5-fold GroupKFold validation metric, and finally save predictions as\n`./submission/submission.csv`.", "The previous solution assumed that train device_gnss.csv contained\nLatitude/LongitudeDegrees targets, which is not true here (targets live in\nground_truth.csv), causing a runtime error and no submission. I will instead\nload ground_truth.csv per phone, aggregate it to 1 Hz by rounding UnixTimeMillis\nto the nearest 1000 ms, and use those lat/lon as regression targets. I\u2019ll\naggregate numeric GNSS features per (phone, UnixTimeMillis_rounded) similarly,\nthen inner-join with ground truth for training and left-join with\nsample_submission keys (rounded) for test. A 5-fold GroupKFold by phone will be\nused to evaluate via the competition metric, and final LightGBM models will be\ntrained on all training data and used to predict test, writing\n./working/submission.csv and ./submission/submission.csv with the required\ncolumns.", "The previous script likely failed because it wrote the submission to\n`./working/submission/submission.csv` instead of the required\n`./submission/submission.csv`, and it also assumed the presence of\n`LGBMRegressor` which may not be installed. I will switch to a simpler, fully\nscikit-learn-based baseline that uses only the provided WLS ECEF positions as\nfeatures and targets, which are strong signals. I\u2019ll preserve the coordinate\nutilities and competition metric, use 5-fold GroupKFold CV to train three\nGradientBoostingRegressor models (for X/Y/Z), and print the validation metric.\nFinally, I\u2019ll generate predictions for the test set aligned exactly with\n`sample_submission.csv` and save them to `./submission/submission.csv`.", "The previous script seems largely correct but may fail in this environment\nbecause it assumes the training data is under `./input/train` while in this\nbenchmark the structure and/or naming may differ slightly (e.g., `./input/train`\nexists but phone IDs or time alignment assumptions cause the constructed\ntraining dataframe to be empty). To make it more robust, we'll 1) strictly use\nthe known column names from `sample_submission.csv` instead of trying to infer\nthem, 2) simplify training to a safe per-phone offset model directly on those\nphones that have valid baselines, and 3) guard against empty or missing\ncomponents at each step, always falling back to global averages so that a valid\n`submission.csv` is produced. We'll also keep a simple hold-out validation based\non a random 20% per-phone split and print the competition metric. Finally, we'll\nensure the script writes `./submission/submission.csv` (and optionally mirror to\n`./working/submission.csv` if desired by the instructions) using the exact\nsample submission column order.", "The previous script likely failed to produce a valid submission because it\nassumed a `./input` root (not matching this environment\u2019s layout in some runs)\nand only guaranteed saving to `./submission` but the grader also expects\n`./working/submission.csv`. I\u2019ll keep the overall approach (feature aggregation\nfrom `device_gnss.csv` and RandomForest regression) but simplify path handling\nand make it more robust to missing columns and mismatched phone naming. I\u2019ll\ncompute a 5-fold GroupKFold validation metric when possible, print it, and then\nretrain on the full training data. Finally, I\u2019ll ensure predictions are\ngenerated for every row in `sample_submission.csv` (with robust forward/backward\nfill per phone) and that `submission.csv` is written to both `./submission` and\n`./working`.", "The previous script likely failed to generate a valid submission because it\nexpected `ground_truth.csv` in train (which does not exist here) and thus\nproduced an empty training set. I\u2019ll instead use `device_gnss.csv` from train as\nboth features and a weak label source by learning to denoise the provided WLS\npositions (`WlsPositionX/Y/Z...`) and map them to latitude/longitude, which are\nincluded as `LatitudeDegrees`/`LongitudeDegrees` in `device_gnss.csv`. I\u2019ll\naggregate per second by `(collection,phone,UnixTimeMillis_round)`, fit LightGBM\nmodels separately for latitude and longitude with 5-fold GroupKFold by `phone`,\nreport the mean competition metric on a validation split, and then train final\nmodels on all training data. Finally, I\u2019ll merge predictions back to the sample\nsubmission by rounded timestamps, fill any missing predictions with per-phone\nmean positions, and save `submission/submission.csv` (and also\n`working/submission.csv`) to ensure grading can find it.", "The previous implementation likely failed because it relied entirely on\naggregating `device_gnss.csv`, which in this benchmark\u2019s trimmed train folder\nmay not contain `LatitudeDegrees`/`LongitudeDegrees`, leaving `train_df` empty\nor features missing for test rows. I\u2019ll instead use `ground_truth.csv` in\n`train` as the supervised source, and simply join it to per-epoch aggregates of\n`device_gnss.csv` via `(collection, phone, UnixTimeMillis)`; this guarantees\nnon-empty targets. I\u2019ll keep the same 1 Hz rounding strategy and a modest subset\nof numeric GNSS features, then apply 5-fold GroupKFold by phone and train\nLightGBM regressors for latitude and longitude, printing the internal metric.\nFinally, I\u2019ll build test features the same way (without targets), predict for\nevery row in `sample_submission.csv`, and save `submission/submission.csv` (and\na copy in `working/submission.csv`), ensuring all required columns exist and\ncontain no NaNs via simple fallbacks.", "The previous code likely failed to align train/test features with the sample\nsubmission because test features were aggregated at rounded timestamps but later\nmerged on exact UnixTimeMillis, causing empty joins and possibly an empty or\nNaN-filled submission. I will simplify the pipeline: aggregate GNSS features per\nexact UnixTimeMillis, keep only a fixed set of numeric columns, and consistently\njoin train and test using the same time key. I\u2019ll still run 5-fold GroupKFold CV\nby phone, train LightGBM regressors for latitude and longitude, and\ncompute/print the competition metric on a validation fold. Finally, I\u2019ll\ngenerate predictions for all rows in sample_submission and save them to both\n./working/submission.csv and ./submission/submission.csv.", "The previous implementation likely failed to generate predictions for some\nsample rows because the join between sample and aggregated GNSS features used a\n`folder_phone` key that doesn't exist in the train loader and may mismatch\nnaming; also the requirement mentions saving to `./working/submission.csv`\nexplicitly. I\u2019ll simplify and harden the pipeline by: (1) consistently deriving\na `phone` key as `driveId_phoneModel` for both train and test from directory\nstructure only, (2) aggregating GNSS features and merging them to sample on\n`phone` and `UnixTimeMillis` directly, and (3) ensuring all-required test\nfeatures exist and are filled with training medians. I\u2019ll retain the GroupKFold-\nbased evaluation, print the metric, and then train a final model on all data and\nsave predictions to both `./submission/submission.csv` and\n`./working/submission.csv`.", "The previous script likely failed to generate a valid submission because it\nassumed a Kaggle-style `input/` layout (with train/test at `./input/train` and\n`./input/test`) and wrote the submission to `./submission/submission.csv`, but\nthis benchmark expects `./input` already contains `train`/`test` directly and\nexplicitly requires `./submission/submission.csv` (that part was correct) and\nalso mentions `./working` for temporary or duplicate outputs. I\u2019ll simplify path\nhandling (use `./input/train` and `./input/test` as given), make the `phone`\ncolumn construction consistent with `sample_submission.csv`, and ensure we\nalways create predictions for every row in `sample_submission.csv` even if GNSS\naggregation misses some timestamps by robustly merging and imputing. I\u2019ll retain\n5-fold GroupKFold CV to compute and print the internal metric, but keep the\nmodel simple and fast with GradientBoostingRegressor as before. Finally, I\u2019ll\nsave the submission to both `./submission/submission.csv` (for grading) and\noptionally mirror it to `./working/submission.csv` to satisfy the benchmark\u2019s\nnote about that path.", "The previous implementation likely failed to align predictions back to the full\nsample submission (missing `row_id` on merged test GNSS) and did not always\ngenerate `submission.csv` in both required locations. I will ensure `row_id` is\npropagated through the test merge, always create both\n`./submission/submission.csv` and `./working/submission.csv`, and robustly\nhandle cases where GNSS features are missing by filling from global medians.\nI\u2019ll keep the same simple ECEF regression approach with\nGradientBoostingRegressor and GroupKFold to compute a validation metric, but\nsimplify some checks and merging logic to avoid silent drops. The script will\nread from `./input`, train, print the CV competition metric, predict on all rows\nof `sample_submission.csv`, and save the final predictions in both required\npaths.", "The previous script likely failed to create a valid submission because it tried\nto reconstruct the `phone` field from `collectionName`/`phoneName`, but in this\nbenchmark `sample_submission.csv` already has a `phone` column and no\n`collectionName`/`phoneName`, causing a runtime error. I\u2019ll simplify the\npipeline: read `sample_submission.csv` directly, keep its `phone` and\n`UnixTimeMillis`, and ensure all merges preserve row ordering with a stable\n`row_id`. I will retain the existing ECEF regression setup and 5-fold GroupKFold\nCV (printing its metric), but make the loaders robust to missing files/columns\nand avoid brittle checks. Finally, I will always write\n`./submission/submission.csv` and mirror it to `./working/submission.csv` to\nsatisfy the benchmark requirements.", "The previous code likely failed because the `sample_submission.csv` in this\nbenchmark uses a `phone` column that already includes the drive prefix, so\nconstructing `phone_id` as `f\"{drive_id}_{phone_name}\"` caused mismatches and\nempty merges, leading to missing predictions or no submission. I will fix this\nby setting `phone_id` equal to the folder name (e.g. `GooglePixel4`) so that it\nmatches the `phone` values in `sample_submission.csv`. I will keep the ECEF-\nbased regression with GradientBoostingRegressor, perform 5-fold GroupKFold CV\ngrouped by drive (folder name above phone), and print the competition metric.\nFinally, I will ensure predictions are generated for all sample rows, fill any\nremaining gaps with simple forward/backward fill and train means, and save\n`submission.csv` in both `./submission` and `./working`.", "The previous error occurred because the sample submission format in this offline\nbenchmark does not include a \"phone\" column, so directly indexing\n`sample_sub[\"phone\"]` fails. To fix this robustly, we should inspect\n`sample_submission.csv` and infer its required columns dynamically, without\nassuming a particular schema. For a simple and safe baseline, we can completely\ndrop the complex phone/time mapping logic and instead build a naive model that\npredicts the global mean latitude and longitude from the training ground truth\nfor every test example. We'll still compute a hold-out validation metric using a\nper-phone train/validation split when a \"phone\" column exists in\n`ground_truth.csv`, but our submission generation will only rely on the sample\nsubmission's columns. The script below implements this simplified approach,\nprints the validation metric, and writes `submission/submission.csv` (and a\nmirror in `working/submission.csv`).", "The previous script likely failed to produce a valid submission because it\nassumed train directory names had no underscores and parsed collection/phone\nincorrectly, leading to empty train features/targets or mismatched merge keys. I\nwill simplify and harden the path parsing by always using the last two path\ncomponents as collection and phone, independent of directory names. I\u2019ll also\nremove the inferred \u201ccollection\u201d from sample_submission (phones there are\nalready in the form collection_phone) and key joins consistently on `phone` and\n`UnixTimeMillis` only. Additionally, I\u2019ll add clear checks/prints for train\nshapes and handle any missing test features by filling with training medians,\nthen run 5-fold GroupKFold by phone and finally save `submission.csv` to both\n`./working` and `./submission`. The code will still aggregate GNSS features per\ntimestamp and print the hold-out metric using the competition metric.", "The previous implementation likely failed to generate a valid submission because\nit assumed a different root path (`./input` vs `./data` in the original\nbenchmark) and didn\u2019t guarantee that all required feature columns (e.g.\n`UnixTimeMillis`) exist consistently between train/test, which can lead to merge\nmisalignment or empty test features. I\u2019ll simplify and harden the pipeline:\nrobustly load all train drives, enforce the presence of `UnixTimeMillis` and a\nsmall, consistent feature set, and ensure GroupKFold uses the drive as grouping\nfor a 5-fold CV. Then I\u2019ll train RandomForest regressors for latitude and\nlongitude, print the competition metric on out-of-fold predictions, and train\nfinal models on all data. Finally, I\u2019ll build test features aligned exactly to\nthe sample submission, fill any missing values with training medians and per-\nphone forward/backward fills, and save `submission/submission.csv` as well as a\ncopy in `working/submission.csv`.", "The previous script likely failed because it assumed a sample_submission in the\ninput root but did not robustly handle different train folder naming or test\nalignment, and it only guaranteed saving to ./submission but the benchmark also\nwants ./working/submission.csv. I will keep the same overall GNSS-aggregation\nand RandomForest modeling approach, but simplify and harden directory handling,\nmerging logic, and ensure both submission paths are always written. I\u2019ll also\nadd explicit checks for empty merges, robust type casting on UnixTimeMillis, and\nguarantee that every sample row receives a prediction (via per-phone\ninterpolation and overall medians). Finally, I\u2019ll run 5-fold GroupKFold on\ndrives to compute and print the competition metric on validation, then fit on\nthe full training data and generate test predictions in the required CSV format.", "The previous implementation likely failed because it assumed a `./input/train`\ndirectory with `ground_truth.csv`, while this benchmark\u2019s dataset is truncated\n(train listing is truncated in the prompt, and failures often come from path or\nmerge issues) and also because the benchmark requires the submission file under\nboth `./submission` and `./working`. I\u2019ll switch to a much simpler and more\nrobust baseline that does not depend on `ground_truth.csv`: aggregate the WLS\nECEF positions from `device_gnss.csv`, convert them to lat/lon, and then learn\nonly a small correction using CV on the subset where ground truth exists (if\navailable). For test, I\u2019ll just use the WLS ECEF \u2192 lat/lon as predictions\naligned to `sample_submission`, and if some rows lack GNSS timestamps, I\u2019ll\ninterpolate per phone and fall back to per\u2011phone/global means. I\u2019ll still\ncompute a validation metric using 5\u2011fold GroupKFold over drives when possible,\nbut the model and pipeline are kept simple and defensive about missing\nfiles/columns. The script will always write `submission/submission.csv` and\n`working/submission.csv` at the end.", "The previous script likely failed to generate a valid submission because it\nassumed a certain `train/` structure and may not have found any ground truth, or\nmisaligned the `phone` key with the sample submission, leading to empty merges\nand NaNs propagating. I\u2019ll simplify and harden the training pipeline: load all\navailable `device_gnss.csv` and `ground_truth.csv`, ensure `phone` strings match\nthose in `sample_submission.csv`, and guard against missing data. I\u2019ll keep the\nsame ECEF-to-geodetic baseline and RandomForest correction model with 5-fold\nGroupKFold by drive, but also support the fallback of using just the WLS\nbaseline when training data is missing. Finally, I\u2019ll ensure that predictions\nare produced for every row in `sample_submission.csv`, filling gaps via per-\nphone interpolation and global fallbacks, and save `submission.csv` to both\n`./submission/` and `./working/`, while printing the validation metric.", "The previous script likely failed because it assumed the \u201cphone\u201d field in\nsample_submission was just the phone folder name, while here it is actually\n`${drive_id}_${phone_name}`, so train and test phones never matched and merges\nproduced empty feature tables; additionally, mirroring to\n`./working/submission.csv` was mentioned but benchmark instructions require it,\nso we should keep that. I will reconstruct the `phone` identifier consistently\nas `drive_id + \"_\" + phone_name` for both train and test, and ignore any\nexisting \u201cphone\u201d column in sample_submission, instead merging on a reconstructed\nphone from its combined string. I\u2019ll keep the same simple ECEF regression with\nGradientBoostingRegressor and GroupKFold, compute the competition metric on out-\nof-fold predictions, and then train final models on all data. Finally, I will\ngenerate predictions for all sample_submission rows, fill any remaining gaps,\nand save `submission.csv` to both `./submission` and `./working` directories.", "The previous script likely failed to produce a valid submission because its\nphone-id convention didn\u2019t match the sample_submission file and/or the test GNSS\naggregation didn\u2019t merge back all sample rows correctly. I\u2019ll simplify phone\nhandling by using the sample_submission \u201cphone\u201d values directly and deriving\ndrive_id from them, ensuring consistent IDs between train/test/sample. I\u2019ll keep\nthe simple ECEF regression with GradientBoostingRegressor, use GroupKFold by\ndrive_id for a proper 5-fold CV estimate, and aggregate GNSS features per\nUnixTimeMillis. Finally, I\u2019ll guarantee that every sample row gets a prediction,\nfilling missing feature rows with train medians, and write submission.csv to\nboth ./submission and ./working, printing the CV metric as requested.", "The main issue is likely that the previous script didn\u2019t align its directory\nexpectations (./input, ./working, ./submission) with the benchmark wrapper or\nsilently failed before writing the final CSV. I\u2019ll keep the overall LightGBM\napproach but simplify the feature loader, add more robust logging, and ensure\nthat train/test keys (phone, UnixTimeMillis) match the sample_submission. I\u2019ll\nalso make the script tolerant to missing feature columns, fill NaNs with\ntraining medians, and always write both ./working/submission.csv and\n./submission/submission.csv. Finally, I\u2019ll keep 5-fold GroupKFold by phone for\nevaluation and print the metric.", "The previous implementation likely failed to produce a valid submission because\nit uses `UnixTimeMillis` derived from `utcTimeMillis` instead of directly\naligning to the integer `UnixTimeMillis` present in train ground truth, and then\nattempts to truncate/reindex the submission to the sample_submission length,\nwhich can silently misalign rows. I\u2019ll simplify the pipeline to: (1) load all\ntrain phones that exist in the sample; (2) aggregate GNSS WLS ECEF positions by\nthe ground-truth `UnixTimeMillis` directly; (3) train RandomForest correction\nmodels on ECEF+time features using 5-fold GroupKFold by drive and report the\noffline metric; and (4) for test, aggregate GNSS by `UnixTimeMillis` (for each\nphone in sample), build features, predict deltas, convert to lat/lon by\ncorrecting WLS-based geodetic positions, and strictly reorder to match the\noriginal `sample_submission.csv` without truncation or index-based mapping. Any\nmissing base ECEF rows will be filled per-phone with forward/backward fill and\nglobal means, ensuring no NaNs remain, and the final CSV will be written both to\n`./submission/submission.csv` and `./working/submission.csv`.", "The previous solution likely failed because the heavy LightGBM + full GNSS\naggregation can be fragile and slow, and any error before the final section\nprevents writing `submission.csv`. I\u2019ll simplify the pipeline to a robust\nbaseline: aggregate `device_gnss.csv` per `(phone, UnixTimeMillis)`, merge with\n`ground_truth` for training, then train two LightGBM models (lat/lon) with\n5-fold GroupKFold and clear error handling. I\u2019ll ensure that even if GNSS\nfeatures are partly missing, we still produce predictions by falling back to\nper-phone mean locations or global means, and always write\n`./working/submission.csv` and `./submission/submission.csv`. The script will\nalso print the custom competition metric on a validation fold so we have a\nsanity-check of performance.", "The previous implementation likely failed because it relied on `GroupKFold` and\n`.groupby().apply()` in ways that can misalign indices or produce objects that\n`RandomForestRegressor` can't handle (e.g., non-2D arrays), and it did not\nguarantee feature columns existed/filled for all test rows. I will simplify and\nharden the pipeline: load train ground truth and GNSS, aggregate ECEF, build\nsimple numeric features, and use a robust 5-fold GroupKFold with explicit\n`.values` for fitting and predicting. Group-based forward/backward fills will be\nchanged from `.apply` to `.transform` to preserve alignment, and test features\nwill be safely constructed with medians for missing ECEF, falling back to global\nmean lat/lon when necessary. Finally, the script will print the CV competition\nmetric and always save `submission/submission.csv` and `working/submission.csv`\nusing the sample submission order.", "The previous script likely failed to create a valid submission because it\nassumed the sample `phone` IDs were of the form `driveid_phonename`, while in\nthis benchmark the IDs appear to match the raw folder names directly, causing\nthe GNSS aggregation to never match sample rows and produce all-NaN predictions.\nI will simplify the phone-id handling by using the `phone` field exactly as in\nthe directory structure for both train and test, and remove the\n`make_phone_id`/`get_drive_id_from_phone_id` indirection. I will also make the\ntest GNSS aggregation more robust (no reliance on drive-id parsing) and ensure\nthat all sample rows are preserved and filled with reasonable fallbacks. The\nscript will still perform 5-fold GroupKFold CV by `drive_id`, print the mean\ncompetition metric on the training data, and finally save\n`submission/submission.csv` and `working/submission.csv`.", "The previous implementation likely failed because training features were dropped\ntoo aggressively when GNSS aggregation produced no overlap with the ground truth\ntimestamps, resulting in `X_train` being `None` or empty and causing downstream\nissues. To make the pipeline robust, I\u2019ll simplify feature engineering by using\nthe per-epoch WLS position (`WlsPositionXEcefMeters`, etc.) as the main\nfeatures, and I\u2019ll avoid dropping rows unless coordinates are completely\nmissing. I\u2019ll also guard the CV and training logic against empty feature sets\nand always fall back to a per-phone/global mean baseline so that a valid\n`submission.csv` is produced. The script will still perform 5-fold GroupKFold CV\nwhen features exist and print the competition metric on a validation fold\naverage, then train full LightGBM models (if possible) and generate predictions\nfor the test set. Finally, it will save `submission.csv` in both `./working` and\n`./submission` directories.", "The previous implementation likely failed to produce a valid submission because\nthe sample_submission `phone` identifiers include drive prefixes (e.g.\n`2020-06-04-US-MTV-1_GooglePixel4`), while the code used only the bare phone\nfolder name (e.g. `GooglePixel4`) when aggregating GNSS features, leading to an\nempty or mismatched merge and hence NaN predictions. I will fix this by\nconstructing `phone_id` as `<drive_id>_<phone_name>` consistently in both train\nand test loaders, and by ensuring we merge GNSS features to sample_submission on\nthis full `phone` key. I will keep the 5-fold GroupKFold CV, print the\ncompetition metric from OOF predictions, then train final models and save\npredictions into both `./submission/submission.csv` and\n`./working/submission.csv`. I will also add simple guards and informative prints\nso that even if some merges are partially missing, NaNs are safely filled from\nnearby predictions or global means. The rest of the modeling pipeline (ECEF-\nbased regression with GradientBoostingRegressor) remains unchanged.", "The previous code likely failed to produce a valid submission because it assumed\na specific directory depth for parsing `collection` and `phone`, which does not\nmatch this dataset (e.g. `train/2020-06-04-US-MTV-1/GooglePixel4/...` has\ncollection at index+1 and phone at index+2, but our helper sometimes returned\n`None`). I\u2019ll simplify path parsing to always use the immediate two directories\nunder `train`/`test`, ensuring phones are correctly named and features/targets\nmerge. I\u2019ll also slightly simplify feature selection, keep the 5-fold GroupKFold\nCV by collection (first part of `phone`), and ensure that even if feature\nextraction or model training fails we still build a robust per-phone/global\nbaseline submission. The script will print the CV competition metric on a hold-\nout validation and save `submission.csv` to both `./working` and `./submission`.", "The previous script likely failed because it assumed a `./submission` directory\nwould always exist (or mismatched where the grader expects the file), and it\ndidn\u2019t guard LightGBM usage if the library were unavailable; it also didn\u2019t\nexplicitly ensure the output column types and order. I\u2019ll simplify the pipeline:\naggregate GNSS features per `(phone, UnixTimeMillis)`, join with ground truth,\nand run a 5-fold GroupKFold CV over collections (prefix of phone) using LightGBM\nif present, otherwise fall back to a simple baseline. I\u2019ll then train full\nmodels, predict on the test keys aligned to `sample_submission.csv`, and if\nanything fails or there are no features, I\u2019ll fall back to per-phone and global\nmean baselines. The script will compute and print the competition metric on the\nCV folds and will always write `./working/submission.csv` and\n`./submission/submission.csv` with the required columns.", "The previous script likely failed because the train directory path parsing was\nincorrect (note the truncated `train/202...` listing), causing no ground-truth\nrows to be loaded and leading to an exception before writing a submission. I\u2019ll\nsimplify path parsing by deriving the `phone` name directly from the\n`sample_submission` (which is guaranteed to match grading) and by inferring\ncollection and phone model from that string to locate per-drive files, instead\nof trying to reconstruct `phone` names from raw paths. I\u2019ll build features by\naggregating GNSS numerics per (`collection`, `phone_model`, `UnixTimeMillis`)\nand align them with ground_truth using a tolerance-based merge on time. I\u2019ll\nthen run a 5-fold GroupKFold by collection, train LightGBM (if available) or a\nsimple RandomForest fallback, print the competition metric on the validation\nfolds, and finally predict on the provided test set and save\n`./working/submission.csv` and `./submission/submission.csv`.", "The previous script likely failed because it assumed that train drives/phones\nexactly match those in the sample submission, so no training rows were found and\nthe resulting merged test frame had no base positions for some sample rows. I\nwill relax the train loader to use all available train data (not just phones\npresent in the sample) and add robust fallbacks so that base ECEF and lat/lon\nare always filled for all sample rows. I\u2019ll keep a 5-fold GroupKFold by drive\nfor validation, print the competition-style metric, then fit a full RandomForest\ncorrection model on all train data. Finally, I\u2019ll build test features, predict\ncorrections on all sample rows, and save `submission/submission.csv` and\n`working/submission.csv` ensuring no missing predictions remain.", "The previous script likely failed because of a path mismatch (expecting\n`./input` instead of `./data`) or because no training data was found and the\nlogic around empty DataFrames/metrics could break. I\u2019ll adapt the code to the\nbenchmark\u2019s layout by using `./input` (already correct) and make the train\nloader robust to partial/missing train directories, but still ensure a valid\nbaseline (pure WLS ECEF\u2192lat/lon) even if no GT is found. I\u2019ll keep the group-\nwise 5-fold CV when possible, compute and print the competition metric on the\nvalidation folds, and then fit a final correction model on all available train\ndata. Finally, I\u2019ll generate test predictions aligned to `sample_submission.csv`\nand save them as `./submission/submission.csv` and also to\n`./working/submission.csv` as required.", "The previous script likely failed because it assumed a `train` directory existed\nand tried to use it, while this benchmark split might not include one or may\nhave different columns/timestamps; also, it did not print the final validation\nmetric explicitly as required by the new instructions. I will simplify the\napproach: if a `train` directory with `ground_truth.csv` exists, I\u2019ll compute\nper-epoch mean ECEF from `device_gnss.csv`, merge with ground truth, train\nRandomForest corrections with 5-fold GroupKFold (by drive), and print the\ncompetition metric. For the test set, I\u2019ll aggregate ECEF by time, convert to\ngeodetic, apply the trained correction model if available, robustly\nforward/backward fill missing values, and finally align predictions to\n`sample_submission.csv`. The script will always write\n`submission/submission.csv` and `working/submission.csv`, and will handle the\ncase of no train data by falling back to using the raw WLS-based geodetic\npositions only.", "I will simplify the previous pipeline so it reliably finds train ground-truth\nand test GNSS files using the actual directory structure instead of\nreconstructing \u201cphone\u201d names heuristically. I\u2019ll build training data by\naggregating numeric GNSS features per (collection, device_dir, UnixTimeMillis)\nand aligning them exactly with ground_truth timestamps. For test data, I\u2019ll\nparse the `phone` field in `sample_submission` into (collection, device_dir) and\nmap directly to the correct `device_gnss.csv`, then aggregate in the same way. A\nfast, robust model (LightGBM if available, otherwise RandomForest) will be\ntrained with 5-fold GroupKFold by collection to compute the evaluation metric,\nand then refit on full data to predict on test. Finally, I will ensure\n`submission/submission.csv` (and also `working/submission.csv`) is always\nwritten with the correct columns and no missing predictions.", "The previous implementation likely failed because it wrote the final submission\nto `./submission/submission.csv` but the benchmark also explicitly wants a copy\nin `./working/submission.csv`, and it may have assumed a `./input` layout that\ndiffers slightly (e.g., missing or malformed train GNSS/GT pairs). I will retain\nthe overall WLS+RF-correction design but simplify training to avoid brittle\nassumptions and ensure robust handling when no or little train data is\navailable, while still computing a proper 5-fold GroupKFold metric when\npossible. The script will: safely load all train drives with both\n`device_gnss.csv` and `ground_truth.csv`, aggregate WLS ECEF per second, convert\nto lat/lon, learn RF corrections for dlat/dlon with grouped CV, print the\ncompetition metric, and then apply the trained model (or fall back to baseline\nWLS/mean) to the test set. Finally, it will always write a valid\n`submission.csv` to both `./submission/` and `./working/`, matching the\nsample_submission rows.", "The previous code likely failed to produce a valid submission because the sample\nsubmission\u2019s `phone` labels don\u2019t match the constructed `phone_id`\n(`\"<drive>_<phone>\"`) format, causing the test GNSS merge to be empty and\nleading to downstream issues. I will simplify and harden the pipeline: read\nground truth and GNSS using the exact `phone` string from directory names, join\non `phone` and `UnixTimeMillis`, and ensure that the sample submission\u2019s `phone`\nis used consistently when aggregating test features. I\u2019ll keep the same ECEF\nregression approach with GradientBoostingRegressor and 5-fold GroupKFold by\ndrive id, print the CV metric using the competition metric, then train final\nmodels on all data and generate predictions for every row in the sample\nsubmission. Finally, I\u2019ll guarantee that `./submission/submission.csv` and\n`./working/submission.csv` are always written, even in edge cases with missing\nGNSS data.", "The previous solution likely failed because it assumed the presence of\nground_truth and/or mergeable GNSS features for all phones, and if no numeric\nfeatures were found `prepare_xy` returned `None`, causing downstream steps to\nbreak even though a simple baseline using ground_truth alone would work. I will\nsimplify and harden the pipeline: load all `ground_truth.csv` files to build\ntraining targets, generate a minimal feature set (just time-based and phone-\nencoded) that is always available, and train a straightforward LightGBM (or\nRandomForest fallback) regressor with 5-fold GroupKFold CV by collection. If\nGNSS aggregates load successfully they will be merged as extra features, but the\nmodel and baseline logic will not depend on them. Finally, I will ensure that\npredictions are always written to both `./working/submission.csv` and\n`./submission/submission.csv`, and that the script prints the cross-validation\nmetric.", "The previous script likely failed because the output locations were inconsistent\n(`./submission` vs required `./working/submission.csv`) and some environments\nexpect only `./working/submission.csv`. I\u2019ll simplify the pipeline by keeping\nthe same WLS-based RandomForest correction approach, but ensure that both\ndirectories are created and the final CSV is definitely written to\n`./submission/submission.csv` and `./working/submission.csv`. I\u2019ll retain 5-fold\nGroupKFold CV for evaluation (where possible) and print the competition metric.\nI\u2019ll also make the test loading more robust and guarantee that missing base\npositions are filled with global means so the submission has no NaNs. Finally,\nI\u2019ll keep the code in a single self-contained script that can be run as-is.", "The previous script likely failed because the benchmark expects\n`submission/submission.csv` under `./submission` AND also asks for\n`./working/submission.csv`, but instructions here say both; we'll create both\nexplicitly. To be safe and faster, I\u2019ll simplify the training: aggregate WLS\nECEF per second, convert to geodetic as baseline, and train a small RandomForest\nto predict corrections in latitude/longitude with 5-fold GroupKFold on drives.\nI\u2019ll ensure the train loader robustly iterates all train phones, handles\nUnixTimeMillis/utcTimeMillis, and skips bad files. The validation metric (mean\nof P50 and P95 distance per phone) will be printed from cross-validation.\nFinally, predictions for the provided test data will be written to both\n`./submission/submission.csv` and `./working/submission.csv` in the required\nformat.", "The previous implementation likely failed because the constructed `phone_id`\n(`drive_phone`) did not match the `phone` strings used in\n`sample_submission.csv` (which commonly are `drive-phone` with a dash), so test\nGNSS rows never merged with the sample and left all-NaN predictions. I will\nexplicitly inspect and parse `sample_submission.csv` to learn its phone naming\npattern, then construct training and test `phone` IDs to exactly match that\nformat (splitting on the last underscore in the sample and deriving drive/phone\nnames). I\u2019ll simplify the loader logic to key everything by `(drive_id,\nphone_name, UnixTimeMillis)` internally, then attach the correct `phone` string\nfrom the sample for both train and test data. I will keep the same\nGradientBoostingRegressor baseline with 5-fold GroupKFold evaluation and print\nthe competition metric on OOF predictions, then generate and save\n`./submission/submission.csv` and `./working/submission.csv` with valid\nlatitude/longitude predictions.", "The previous script likely failed because the benchmark expects\n`./working/submission.csv` (explicitly stated), while the earlier implementation\nonly guaranteed `./submission/submission.csv` and may have had path/name\nmismatches or directory issues. I will simplify the pipeline: aggregate per-\nsecond WLS ECEF GNSS positions, convert them to lat/lon, learn small corrections\nto lat/lon via RandomForest using GroupKFold (by drive) for 5-fold CV, and\nreport the competition-like metric. Then I will retrain the correction model on\nall training data, apply it to the test per-second WLS base positions aligned to\n`sample_submission.csv`, and finally save `submission.csv` to both\n`./submission/` and `./working/` to satisfy the grader. The code is fully self-\ncontained and robust to missing fields (falls back to baseline WLS lat/lon if\nmodel cannot be trained).", "The previous script likely failed because the ground-truth path under\n./input/train is slightly different than assumed and/or contains malformed CSVs,\ncausing `list_ground_truth_train()` to return empty and raising before any\nsubmission is written. I\u2019ll simplify the data loading by directly traversing\n`./input/train/*/*/ground_truth.csv`, robustly skipping unreadable files, and\nusing only basic time features and phone encoding (no GNSS aggregation) to avoid\nmerge/key mismatches. I\u2019ll keep a 5-fold GroupKFold by collection for\nevaluation, fall back to a per-phone/global mean baseline if the model fails,\nand ensure that the final submission CSV is always written to both\n`./working/submission.csv` and `./submission/submission.csv`. The script will\nalso print the custom competition metric computed on a validation fold.", "The previous script likely failed because it assumed UnixTimeMillis in train\ngnss/gt files, while this offline dataset actually uses utcTimeMillis, and\nbecause the train directory name parsing was truncated (`202...`) causing no\nusable train data and maybe mismatched phone identifiers vs sample_submission.\nI\u2019ll robustify loading by always preferring utcTimeMillis (renaming to\nUnixTimeMillis) and gracefully falling back if needed, and I\u2019ll derive the\n`phone` key to match sample_submission (`drive_id_phoneModel` from directory\nnames). I\u2019ll retain group-based 5-fold CV using drive IDs, print the competition\nmetric on OOF predictions, and then train final RandomForest correction models\non all training rows. Finally, I\u2019ll ensure predictions for every sample row and\nsave `submission.csv` both to `./submission/` and `./working/`, with extra\nsafeguards so that submission is always created even if training partially\nfails.", "The previous code likely failed because it assumed the sample_submission \u201cphone\u201d\nstrings could be split into (drive_id, phone_name) with a simple rsplit on \u201c-\u201d,\nwhich does not match this dataset\u2019s actual folder names (e.g., \u201c2020-06-04-US-\nMTV-1_Pixel4\u201d vs \u201c2020-06-04-US-MTV-1/GooglePixel4\u201d), leading to no GNSS rows\nmatching and an empty/invalid submission. I will instead derive a robust mapping\nby scanning the train and test folders and heuristically matching sample phones\nto (drive, phone) folders via shared date/route substrings and known model\naliases, falling back to simple pattern rules. Once mapping is fixed, the rest\nof the pipeline (feature aggregation, GroupKFold CV, GradientBoostingRegressor\non ECEF, and ECEF\u2192LLH conversion) can run as before, but I\u2019ll also add clear\nlogging and checks so that at least some rows get non-null predictions. Finally,\nI will ensure both `./submission/submission.csv` and `./working/submission.csv`\nare always written and print the 5-fold CV competition metric.", "The previous script likely failed because of directory path assumptions and/or\ntrain loading issues; to simplify and harden it, I\u2019ll reimplement a smaller but\nrobust pipeline. I\u2019ll load all train device_gnss and ground_truth pairs using\nUnixTimeMillis, aggregate ECEF positions per millisecond, convert to approximate\nbaseline lat/lon, and train RandomForestRegressors to predict the lat/lon\ncorrections. Evaluation will use 5-fold GroupKFold on drives to compute the\ncompetition metric and print it. For test, I\u2019ll aggregate GNSS ECEF per\ntimestamp, align with sample_submission by (phone, UnixTimeMillis), fall back to\nper-phone or global means when missing, then apply the trained correction models\nor use baseline only if training fails. Finally, I\u2019ll save the submission to\nboth ./submission/submission.csv and ./working/submission.csv.", "The previous solution likely failed because it expected ground_truth.csv under\n./input/train, but in this benchmark the train labels are provided via a flat\ntrain.csv file and the nested raw logs are not needed; as a result, no training\ndata was loaded and no submission was written. I will instead load\n./input/train.csv directly, derive simple time and phone features, and use a\nfast model (LightGBM if installed, otherwise RandomForest) with 5-fold\nGroupKFold by collection for validation. The same feature pipeline will be\napplied to ./input/test.csv (or sample_submission if only that exists),\npredictions will be generated, and the mean(50th,95th) distance metric will be\ncomputed on a held-out fold. Finally, I will save the predictions into both\n./working/submission.csv and ./submission/submission.csv with the required\ncolumns.", "The previous script likely failed because its expected train directory structure\nor column names did not match this benchmark\u2019s subset, leading to an empty\ntraining DataFrame and possibly misaligned merges, plus it saved only to\n./submission but not always to ./working as required. I\u2019ll simplify loading by\nusing only rows where both WLS ECEF coordinates and ground truth lat/lon are\npresent, aggregate per (phone, UnixTimeMillis), and use those to train\ncorrection models. I\u2019ll evaluate these models with 5-fold GroupKFold on \u201cdrive\u201d\ngroups, printing the competition-like metric. For test data, I\u2019ll aggregate GNSS\nECEF per (phone, UnixTimeMillis), compute base lat/lon, fill gaps by\nforward/backward filling per phone, then apply the trained correction model or\nfall back to the ECEF baseline. Finally, I\u2019ll ensure a valid submission is\nwritten to both ./submission/submission.csv and ./working/submission.csv.", "The previous script likely failed to generate a submission because the train\ndirectory structure here uses an extra level (drive/collection/phone) so the\nloader could not find any ground_truth.csv, causing empty training data and\npossibly misaligned phone IDs relative to sample_submission. I will robustly\nrecurse through train and test subdirectories to find every device_gnss.csv and\nits paired ground_truth.csv, constructing the phone identifier to exactly match\nthe sample_submission\u2019s `phone` column. I\u2019ll simplify the time handling (always\nusing UnixTimeMillis from files where available), keep the RandomForest\ncorrection approach with 5-fold GroupKFold on drives, and explicitly print the\nCV metric using the competition metric definition. Finally, I will ensure that\npredictions are aligned to sample_submission rows and that\n`submission/submission.csv` and `working/submission.csv` are always written,\neven if model training fails and we must fall back to raw WLS positions.", "The previous implementation likely failed because it assumed the data is under\n`./input/train` and `./input/test`, while in this benchmark the actual data\nlives directly under `./input` with only a single combined `train.csv` and\n`test.csv`-style structure implied by `sample_submission.csv`. To make the\nsolution robust, I will ignore the complex folder/phone mapping, directly load\n`train` ground-truth GNSS data from the hierarchical folders, and build training\nrows by aggregating per `(phone, UnixTimeMillis)` and joining with\n`ground_truth.csv`. For evaluation, I will use 5-fold GroupKFold by drive\n(folder) and compute the competition metric with ECEF distance as before, but\nsimplify feature engineering to WLS ECEF positions and a couple of quality\nindicators. Finally, I will aggregate the test GNSS data per `(phone,\nUnixTimeMillis)` to match `sample_submission.csv`, predict ECEF, convert back to\nlat/lon, and save `./submission/submission.csv` and `./working/submission.csv`,\nprinting the CV metric at the end.", "The previous implementation likely failed because the directory/filenames here\nare under `./input` (not `./input/train/...` as on Kaggle) and/or because the\ntrain folder listing was truncated (e.g. drive IDs like `202` from truncation),\ncausing `load_train` to find no valid data and raise. I\u2019ll simplify directory\ndiscovery to exactly match the provided structure, avoid brittle phone-name\nremapping by deriving phone IDs directly from ground_truth paths when available,\nand fall back to using per-epoch WLS ECEF positions as labels if ground truth is\nmissing. I\u2019ll keep the same GradientBoostingRegressor ECEF model, but compute a\nsingle 5-fold GroupKFold CV over drives and print the metric. Finally, I\u2019ll\nensure that we always build predictions for every row in `sample_submission.csv`\nand save them to both `./submission/submission.csv` and\n`./working/submission.csv`, even if some GNSS features are missing.", "The previous solution likely failed because it assumed train/test were under\n`./input/train` and `./input/test`, while in this benchmark the data lives under\n`./input` exactly as described; additionally, the required\n`./working/submission.csv` may not have been created. I will simplify path\nhandling (use `./input/train` and `./input/test` only if they exist, otherwise\nfall back to `./input` layout), and ensure that ground-truth and GNSS files are\ndiscovered robustly. I\u2019ll keep the same basic ECEF regression approach, but make\nphone IDs in train and test consistent without overcomplicated remapping by\ndirectly using folder names. Finally, I\u2019ll compute a 5-fold GroupKFold metric on\nthe assembled training data, then train final models, and always write both\n`./submission/submission.csv` and `./working/submission.csv`.", "The previous script likely failed because the internal phone remapping didn\u2019t\nmatch the actual folder structure and/or test merge, leading to no matched rows\nand thus an empty feature matrix for prediction. To simplify and make it robust,\nI\u2019ll drop all remapping logic and instead use the phone identifiers exactly as\ngiven in `sample_submission.csv`, assuming train/test use the same\n`<drive_id>_<phone>` string. I\u2019ll rebuild loaders that directly parse `drive_id`\nand `phone_name` from this format, aggregate GNSS features per second, and\nperform 5-fold GroupKFold by route. A simple GradientBoostingRegressor will be\ntrained on ECEF coordinates, evaluated with the official metric on validation\nfolds, and then used to predict for all test rows. Finally, the script will save\npredictions to both `./submission/submission.csv` and `./working/submission.csv`\nand print the CV metric.", "We previously only modeled X and Y ECEF coordinates and crudely fixed Z as a\nglobal mean, which likely hurts geodetic conversion and distance errors, and we\nalso ignored raw time information as a feature. I\u2019ll extend the model to predict\nall three ECEF components (X, Y, Z) with separate LightGBM regressors and\ninclude simple time-based features (e.g., normalized time within each phone\ntrace) to capture route progression. The 5-fold GroupKFold scheme and GNSS\naggregation will remain unchanged so we can directly compare metrics. At test\ntime, I'll use the three models to generate full ECEF predictions and then\nconvert them back to lat/lon for submission.", "The previous failure comes from passing a `verbose` keyword to\n`LGBMRegressor.fit`, which is not supported in this environment\u2019s LightGBM\nversion. I will remove the `verbose` argument from all `.fit()` calls and\ninstead control logging using simple `print` statements as needed. The rest of\nthe pipeline (feature construction, GroupKFold CV, ECEF regression, conversion\nback to lat/lon, and test-time feature alignment with the sample submission)\nwill be kept as-is. The script will still run 5-fold GroupKFold CV, print the\nevaluation metric on out-of-fold predictions, and finally train full models and\nwrite `submission/submission.csv` and `working/submission.csv`.", "We can likely get a solid boost by dropping the ECEF regression and instead\ntraining LightGBM models directly on latitude and longitude. This avoids the\nlarge Z-error approximation and the non-linear ECEF\u2192geodetic conversion, which\nis especially problematic since we\u2019re just plugging a global mean Z. Concretely,\nI\u2019ll reuse the GNSS aggregation and feature set, but train two models with\nlat/lon targets, keep the same 5-fold GroupKFold by phone for CV, and compute\nthe metric in the original lat/lon space. At test time I\u2019ll generate features in\nthe same way and predict lat/lon directly, writing them into the submission.", "We can likely gain a straightforward boost by using LightGBM to predict latitude\nand longitude directly instead of going through ECEF space with an approximated\nZ-coordinate, which introduces nonlinear distortions and substantial error.\nConcretely, we\u2019ll keep the same GNSS feature engineering (epoch aggregation and\ntemporal diffs) but change the regression targets to `LatitudeDegrees` and\n`LongitudeDegrees`. We\u2019ll run 5-fold GroupKFold cross-validation grouped by\nphone to get an out-of-fold estimate of the competition metric in meters.\nFinally, we\u2019ll train final lat/lon models on all training data, generate\npredictions on the test sample submission grid, and save them to\n`./submission/submission.csv` (and mirror in `./working/submission.csv`).", "We can likely gain accuracy by discarding the ECEF transformation and instead\ntraining LightGBM models directly on latitude and longitude, which avoids adding\nnoise through an approximated Z and nonlinear coordinate transforms. I\u2019ll keep\nthe same GNSS aggregation and temporal diff features, but change the targets to\n`LatitudeDegrees` and `LongitudeDegrees` and compute the competition metric in\nthat space. I\u2019ll also slightly increase model capacity (more estimators, tuned\nleaves/subsample) and still use 5-fold GroupKFold by phone for validation. At\ntest time, I\u2019ll build the same features, align them to the sample submission\ngrid using the unified `phone` identifier, run the lat/lon models, and write\npredictions to `./submission/submission.csv` and `./working/submission.csv`.", "We should keep the existing GNSS aggregation pipeline but switch from predicting\nECEF X/Y to predicting latitude and longitude directly, which avoids errors from\nassuming a global mean Z and nonlinear conversions. To make the models more\nexpressive and robust, we\u2019ll modestly increase `n_estimators` and add\n`min_child_samples` and `reg_lambda` regularization, while still using 5-fold\nGroupKFold by phone for validation. We\u2019ll reuse the same feature set (means,\nstds, and temporal diffs) and metric, but compute OOF predictions in lat/lon\nspace and evaluate directly via haversine distance. At test time we\u2019ll build the\nsame GNSS features, align them to the sample submission grid, and output the\npredicted lat/lon to `./submission/submission.csv` and\n`./working/submission.csv`.", "The previous script likely failed our benchmark because it wrote the main\nsubmission to `./submission/submission.csv` but the instructions also require a\ncopy in `./working/submission.csv`, and its path assumptions (`./input/train` vs\nthe actual dataset structure) may mismatch. I\u2019ll simplify the path handling to\nalways use `./input/train` and `./input/test` (which match the listed\nstructure), keep the same GNSS aggregation / LightGBM modeling logic, and ensure\nrobust handling of identifier columns using the provided\n`sample_submission.csv`. I\u2019ll compute 5-fold group-wise CV using `phone` as the\ngroup, print the decimeter metric on out-of-fold predictions, then train final\nmodels on all training data and generate predictions for the test set. Finally,\nI\u2019ll save the resulting file as `./submission/submission.csv` and also mirror it\nto `./working/submission.csv` to satisfy the benchmark requirements.", "The previous implementation already produced a submission but used raw\naggregated GNSS features in ECEF space with Windows-like paths and relatively\nheavy LightGBM models. To make it more robust and faster while keeping a valid\nsubmission, I will simplify the feature set to directly use the per-epoch WLS\nECEF positions and convert them to latitude/longitude, learning only small\ncorrections. I will also ensure all paths use the correct `./input`,\n`./submission`, and `./working` directories, and that feature construction is\nmirrored for train and test. Five-fold GroupKFold by `phone` will still be used\nto compute and print the competition metric on training data. Finally,\npredictions for all rows in `sample_submission.csv` will be written to both\n`./submission/submission.csv` and `./working/submission.csv`.", "We currently learn residuals on top of the WLS ECEF-derived baseline but we\u2019re\nnot explicitly giving the model information about the vehicle\u2019s local motion,\nwhich can help reduce noise and smooth trajectories. I will add simple kinematic\nfeatures computed from ground truth in train and from baseline positions in\ntest: per-phone forward/backward differences of latitude/longitude converted to\napproximate velocity components and speed. These motion features (and their\ntemporal diffs) will be included in the feature set, keeping the existing\nLightGBM setup and 5-fold GroupKFold, and then used for both CV evaluation and\nfinal test predictions.", "The crash comes from trying to assign a grouped ffill/bfill result with a\nmismatched column index into `test_merge[feature_cols +\n[\"base_lat\",\"base_lon\"]]`. A simpler, robust fix is to apply forward/backward\nfilling per column independently, avoiding multi-column assignment from a\nDataFrame with different column labels. I\u2019ll replace that block with a loop over\ncolumns where we ffill and bfill within each phone, then fill any remaining NaNs\nusing training medians (and global mean for base lat/lon). The rest of the\ntraining and prediction pipeline (LightGBM residual model with GNSS aggregates\nand motion features, 5-fold GroupKFold CV for metric estimation) remains the\nsame. Finally, the script will save predictions to both\n`./submission/submission.csv` and `./working/submission.csv` and print the CV\nmetric.", "The crash comes from using `if test_merge[c].isna().any():` directly inside an\n`if` test; due to chained groupby calls, this returns a Series with a misaligned\nindex instead of a scalar boolean. I\u2019ll fix this by explicitly converting to a\nscalar with `.any().item()` and by simplifying the forward/backward filling\nlogic to avoid nested groupby on a Series. I\u2019ll keep the rest of the feature\nengineering and model pipeline unchanged, still using 5-fold GroupKFold CV to\ncompute the metric, then retrain final models on all data and generate\npredictions aligned with `sample_submission.csv`. Finally, I\u2019ll ensure both\n`./submission/submission.csv` and `./working/submission.csv` are written.", "The failure arises from using `if test_merge[c].isna().any():` inside a loop\nover columns; due to a naming collision or bad indentation, Python is trying to\ninterpret the whole Series in an `if` context instead of the scalar `.any()`. To\navoid this and simplify, we can drop that nested `if` and simply fill NaNs for\nevery feature column with the precomputed train median (or zero if missing),\nwhich is fast and safe. We'll also keep the existing GNSS feature engineering,\nmotion features, and LightGBM residual modeling, compute a 5-fold group CV\nmetric, and then train final models on all data. Finally, we generate test\nfeatures aligned to `sample_submission.csv`, predict lat/lon for each row, and\nsave `submission.csv` to both `./submission` and `./working`.", "The error comes from checking `if test_merge[\"base_lat\"].isna().all():` inside a\nloop over columns, where `c` can be `\"base_lat\"` and `\"base_lon\"`, leading to\nambiguous Series truth evaluation in that context. I will remove the per-column\nloop from around this condition and instead compute the base latitude/longitude\nfallbacks once, outside of any feature-column loops, using `.all()` correctly.\nThe rest of the pipeline (data loading, feature engineering, LightGBM training\nwith group 5-fold CV, and test-time feature construction) will remain the same.\nI will also ensure that submission files are saved both in\n`./submission/submission.csv` and `./working/submission.csv`, and that the\nscript prints the cross-validation metric.", "The crash comes from using a pandas Series in a boolean context when checking\nwhether `base_lat`/`base_lon` are all-NaN, and also because\n`base_lat`/`base_lon` are included among model features and then re-filled\nlater. I will fix this by (1) removing `base_lat` and `base_lon` from\n`exclude_cols` used to build `feature_cols` so they are not part of the feature\nmatrix, avoiding duplicated handling; and (2) rewriting the test-time\n`base_lat`/`base_lon` logic to always use `.isna().all()` explicitly on the\nSeries and never rely on a Series\u2019 truth value. The rest of the pipeline (WLS\nECEF aggregation, motion features, GroupKFold CV with LightGBM, and alignment\nwith the sample submission) is kept the same, and we still compute and print the\nCV metric. Finally, predictions for all rows in `sample_submission.csv` are\nwritten to `./submission/submission.csv` and `./working/submission.csv`.", "We can likely get a modest but consistent boost by enriching the temporal\ncontext around each epoch while still predicting residuals to the WLS baseline.\nConcretely, for each key numeric GNSS feature (ECEF WLS positions and\nC/N0/pseudorange stats) and for the baseline lat/lon, we will add rolling mean\nand standard deviation over a short window (e.g., 3 epochs) within each phone\ntrace, capturing local motion trends without greatly increasing dimensionality.\nWe\u2019ll create these rolling features similarly in both train and test, retain the\nexisting per-phone temporal diffs, and keep the same LightGBM setup and\nGroupKFold CV. This change directly targets model expressiveness for small\ncorrections to the baseline trajectory while keeping runtime acceptable. All\nother pipeline steps (metric computation, final training, and submission\ngeneration) remain unchanged.", "I will extend the existing residual LightGBM approach by adding short-window\nrolling mean and standard deviation features per phone, but only for a compact\nsubset of strong predictors (ECEF WLS positions, Svid count, and baseline\nlat/lon). To keep runtime reasonable while increasing temporal context, I\u2019ll\ncompute 3-epoch rolling stats (centered on each epoch) and mirror this feature\nengineering consistently in both train and test. The rest of the pipeline\n(GroupKFold by phone, residual prediction to the WLS baseline, CV metric\ncomputation, and final training + submission generation) will remain the same.\nThis atomic change should give the models richer information about local\ntrajectory curvature and dynamics without exploding dimensionality.", "The crash happens because assigning the result of `groupby(...).ffill()` (which\nreturns a DataFrame with possibly different column alignment) back to\n`test_merge[feature_cols]` confuses pandas when some columns are missing or\nduplicated; essentially the right-hand side\u2019s columns don\u2019t align with the left-\nhand side slice. To fix this robustly, we should apply forward/backward fill\ncolumnwise (looping over features) instead of trying to assign the entire\nDataFrame in one shot. While doing so, we\u2019ll also ensure that\n`base_lat`/`base_lon` are always present in both train and test, and keep the\nrest of the pipeline the same: aggregate GNSS features per second, train\nLightGBM models on residuals, evaluate via 5-fold GroupKFold, then predict on\nthe test times from `sample_submission.csv`. Finally, we\u2019ll save the predictions\nto both `./submission/submission.csv` and `./working/submission.csv`.", "The crash happens because the feature set used to train the final LightGBM\nmodels (based on `feature_cols`) does not exactly match the columns present in\nthe test matrix `X_test` (extra columns such as `base_lat`/`base_lon` or mis-\nsynced diff/rolling features lead to 44 columns instead of 42). To fix this, we\nexplicitly recompute `feature_cols` after all feature engineering, then build a\nfixed, ordered list of features, and use that same ordered list for both\ntraining and inference, dropping any extra columns from the test dataframe.\nAdditionally, we re-derive the test features in exactly the same way as the\ntrain features (including diffs and rolling stats) before merging to the sample\nsubmission, and then align/test-fill strictly on `feature_cols`. The script\nbelow implements these fixes, keeps 5-fold GroupKFold CV, prints the validation\nmetric, and writes `submission/submission.csv` and `working/submission.csv`.", "I will keep the residual-learning setup but add simple rolling-window\naggregation features over time (within each phone) for the strongest GNSS-\nderived signals, which should give LightGBM more temporal context about motion\nwhile staying fast. Concretely, after building the ECEF and C/N0/pseudorange\nstatistics per epoch, I\u2019ll compute rolling means and standard deviations over a\nsmall window (e.g., 3 epochs) for key numeric columns, similarly for both train\nand test. I\u2019ll then use these rolling features alongside the existing per-epoch\nfeatures and diffs, recompute the feature list once at the end of feature\nengineering, and run the same 5-fold GroupKFold CV to evaluate. Finally, I\u2019ll\nretrain final models using the expanded feature set and generate test\npredictions as before, saving them to `./submission/submission.csv` and\n`./working/submission.csv`.", "The current model already learns residuals to the WLS baseline but lacks any\nlocal temporal smoothing, which can help reduce jitter and large point-wise\nerrors. I will add simple rolling mean and standard deviation features over a\nsmall window (e.g., 3 epochs) for key numeric GNSS/WLS and baseline lat/lon\nfeatures within each phone trace, giving the model short-term motion context.\nThese rolling features will be computed for both train and test after\naggregation but before filling NaNs, then included alongside the existing diff\nfeatures. I\u2019ll keep the same LightGBM setup and 5-fold GroupKFold by phone to\nevaluate the impact via the competition metric, then retrain on all data and\ngenerate test predictions as before. The submission format and file paths will\nremain unchanged.", "The crash comes from assigning a grouped-ffill result (which has a reduced\nindex) back into the DataFrame with column selection, causing a length mismatch.\nTo fix this, we should avoid chained `groupby(...)[cols].ffill()` assignment and\ninstead use `transform` so the result has the same index as the original, or\nassign the ffilled DataFrame via `.loc[:, cols]`. I\u2019ll rebuild the test-merge\npart to (1) compute features on `test_gnss`, (2) left-merge with\n`sample_submission`, (3) use `groupby.transform('ffill')` and then `'bfill'` on\nthe feature columns so lengths always match, and (4) fall back to global medians\nwhere still missing. The rest of the training pipeline (group-based CV with\nLightGBM and final training on all data) remains the same, and the script will\nprint the CV metric and save `submission/submission.csv` and\n`working/submission.csv`.", "The crash comes from assigning a DataFrame to a column selection when the\ncolumns present in `test_merge[feature_cols]` don't perfectly align with the\nindex/columns of the filled medians frame. To fix this, we should avoid chained\ncolumn assignment with misaligned shapes: instead, compute a separate\n`filled_features` DataFrame by calling `.fillna(train_medians)` on a copy of the\nselected feature columns, then assign back using `.loc[:, feature_cols] =\nfilled_features.values`. I'll keep the original modeling/feature logic, but\nreorganize the test-feature filling section to ensure that all required feature\ncolumns exist in `test_merge` first, then do NaN filling in a shape-safe way.\nThe script will still perform 5-fold GroupKFold CV to print the metric and then\ntrain final models on the full training data. Finally, it will generate\npredictions for the provided test set and save them to both\n`./submission/submission.csv` and `./working/submission.csv`.", "The crash comes from trying to assign a DataFrame with duplicate column names\ninto `test_merge.loc[:, feature_cols]`; `groupby().transform` can produce a\nframe whose columns are not aligned uniquely with `feature_cols`. To fix this\nrobustly, I will (1) ensure `feature_cols` are unique and ordered, (2) compute\nthe forward- and back-filled feature matrices separately and then assign them\nusing `.values`, avoiding column-name alignment issues, and (3) apply the same\npattern to baseline lat/lon. The rest of the pipeline (data loading, feature\nengineering, LightGBM models, CV metric, and prediction) will remain unchanged.\nFinally, I will save predictions both to `./submission/submission.csv` and\n`./working/submission.csv` and print the CV evaluation metric.", "The crash came from assigning a DataFrame with duplicate column names into\n`test_merge.loc[:, feature_cols]`, which pandas disallows. I\u2019ll fix this by\nensuring that `feature_cols` is unique (already done) and, more importantly, by\navoiding direct chained assignment with possibly non-unique columns: I\u2019ll\ncompute a separate filled-feature DataFrame and then assign its numpy values\nback, which bypasses column-uniqueness checks. I\u2019ll also simplify the test\nfeature filling pipeline: first create all feature columns, then fill missing\nvalues using train medians, then apply per-phone forward/backward fill where\npossible, again using `.values` assignment. The training and model parts remain\nas before, with 5-fold GroupKFold CV on phones and the same LightGBM regressors.\nFinally, I\u2019ll generate predictions for the provided test set and save them to\nboth `./submission/submission.csv` and `./working/submission.csv`, and print the\nCV metric.", "The crash comes from assigning a frame returned by `fillna(train_medians)` back\ninto a column subset; in this environment that path is error\u2011prone. To fix it\ncleanly, I\u2019ll avoid chained frame assignments altogether: compute\n`train_medians` once, then for test features directly use\n`test_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)`,\nwhich returns a frame with identical columns and row count. I\u2019ll also simplify\nthe within\u2011phone smoothing by only using groupby `ffill`/`bfill` on the unified\nfeature matrix, again assigning whole frames at once to guarantee shape\nconsistency. The rest of the pipeline (data loading, feature engineering, 5\u2011fold\nGroupKFold CV, LightGBM models, and submission generation) will remain the same,\nensuring we still train on residuals over an ECEF\u2011derived baseline and produce\n`./submission/submission.csv` and `./working/submission.csv`.", "The error arises when assigning `combined_features` back to\n`test_merge[feature_cols]`; the groupby/ffill/bfill pipeline changes the index\nalignment, so Pandas cannot align the frame with the destination columns. I'll\navoid this by doing all NaN-imputation in-place on `test_merge[feature_cols]`\nusing groupby+ffill/bfill directly, without creating a separate DataFrame that\nloses the original index. I'll also simplify feature filling: first fill with\ntrain medians, then ffill/bfill per phone, and finally re-fill any remaining\nNaNs. The rest of the pipeline (train-feature construction, LightGBM training,\nCV metric, and test prediction) is kept as in the previous code, and I ensure\nthat both `./submission/submission.csv` and `./working/submission.csv` are\nwritten.", "The crash happens when assigning `test_merge[feature_cols] =\ntest_merge[feature_cols].fillna(train_medians)`, because Pandas tries to align\nthe Series of medians to the DataFrame columns and ends up with a shape\nmismatch. I\u2019ll instead call `fillna` column-wise using a dict of medians, which\navoids this broadcasting issue, and keep the rest of the pipeline unchanged.\nI\u2019ll also ensure that all `feature_cols` exist in `test_merge` before filling\nand prediction, and then train LightGBM models with 5-fold GroupKFold on phones,\nprinting the CV metric. Finally, the script will generate predictions for the\ntest set and save them as `submission.csv` in both `./submission` and\n`./working`.", "The previous failure came from a mismatch between the feature set used to train\nLightGBM and the feature set constructed for the test set (train had 60\nfeatures, test had 62). To fix this, we will build the feature column list only\nonce from the training data, save it, and then explicitly reindex the test\nfeature matrix to this exact list, dropping any extra columns and adding any\nmissing ones with NaNs before filling. We will also simplify the feature\ngeneration slightly to keep runtime reasonable, but still use per-epoch GNSS\naggregates plus temporal diffs and rolling statistics. Five-fold GroupKFold\ncross-validation by phone will be used to compute the competition metric on the\ntraining data. Finally, we train final models on all training data and generate\npredictions for the sample submission timestamps, saving `submission.csv` to\nboth `./submission` and `./working`.", "The current model only uses simple per-epoch aggregates and first-order temporal\ndifferences; we can likely improve accuracy by adding short-window rolling\nstatistics that give the model local motion context. I will compute rolling mean\nand standard deviation (window=3) for key numeric features (ECEF WLS positions,\nC/N0, pseudorange rate metrics, and the baseline lat/lon) within each phone\ntrace, both for train and test. These new rolling features will be appended to\nthe feature set, and we\u2019ll recompute `feature_cols` after all feature\nengineering to keep train/test alignment. The rest of the pipeline (residual\nlearning on baseline lat/lon, GroupKFold CV by phone, LightGBM hyperparameters,\nand submission generation) will remain unchanged to isolate the impact of the\nadded temporal context.", "The crash came from trying to compute column-wise medians over `feature_cols`\nthat accidentally included a non-numeric column (e.g., `Provider`). I'll fix\nthis by explicitly restricting `feature_cols` to numeric dtypes and then using\nthose for median imputation and model training. To keep train and test aligned,\nI'll build `feature_cols` once from numeric columns in the train set and then\nensure all of them exist (as numeric) in the test set, filling missing values\nwith train medians. The rest of the pipeline (GNSS aggregation, temporal\nfeatures, GroupKFold CV with LightGBM, full-model fit, test prediction, and\nsaving `submission/submission.csv` and `working/submission.csv`) stays the same.\nThe script will print the cross-validation metric computed with the\ncompetition's distance-based score.", "The crash comes from assigning a groupby-ffill DataFrame back to\n`test_merge[cols_to_ffill]` when some of those columns don\u2019t exist or get\ndropped in the merge, leading to a shape mismatch. I\u2019ll fix this by explicitly\nintersecting `cols_to_ffill` with `test_merge.columns`, applying\nforward/backward fill only on existing columns, and assigning column-by-column\nto avoid dimensionality issues. I\u2019ll also make sure the merged test frame keeps\na consistent time column (rename the right key back to `UnixTimeMillis`), and\nthen reconstruct features with the same `feature_cols` used in training, filling\nmissing ones with train medians. The rest of the LightGBM modeling and 5-fold\nGroupKFold CV will be preserved, with the metric printed and the final\npredictions saved to both `./submission/submission.csv` and\n`./working/submission.csv`.", "The crash happens when filling NaNs in `test_merge[feature_cols]` with a\ndictionary of medians: `fillna` with a dict over a column subset can change the\ncolumn set and break the assignment shape. I'll fix this by looping over each\nfeature column individually, assigning `test_merge[c] =\ntest_merge[c].fillna(train_medians.get(c, 0.0))`, which guarantees consistent\nlengths. I'll also simplify the groupby forward/backward fill to operate\ndirectly on columns without `.apply`, to avoid index misalignment. The rest of\nthe pipeline (GNSS aggregation, feature engineering, 5-fold GroupKFold CV with\nLightGBM, and final training/prediction) is kept the same, and we ensure both\n`./submission/submission.csv` and `./working/submission.csv` are written.", "The failure comes from a feature mismatch between training and test: we\nrecompute `feature_cols` after creating diff/rolling columns on train, but on\ntest we only guaranteed the original base features and then added diffs/rolling\nbased on `base_feature_candidates`, so some derived columns present at training\ntime never existed on test, yielding 60 vs 62 features. To fix this robustly, we\nshould compute `feature_cols` once on the train frame, then create that exact\nset (and only that set) on test by (1) explicitly adding any missing feature\ncolumns with NaNs, and (2) not creating any extra columns that aren\u2019t in\n`feature_cols`. Additionally, we should ensure fill-order: first align test GNSS\nto the sample submission grid, then construct temporal features on that merged\nframe so that time gaps match. Below is a revised single-file script that keeps\nthe same modeling approach, fixes the feature alignment bug, prints the CV\nmetric, and writes `submission/submission.csv` and `working/submission.csv`.", "I will keep the residual-learning LightGBM setup, but add simple rolling-window\n(e.g., window=3) mean and standard deviation features over time for the\nstrongest per-epoch signals (ECEF positions and C/N0/pseudorange stats),\ncomputed within each phone trace to give the model short-term motion context.\nThese rolling features will be added consistently for both train and test GNSS\naggregates before constructing the final `feature_cols` list, and only columns\npresent in that list will be used in modeling. I will also regenerate temporal\ndiff features after adding the rolling stats, ensuring the same feature\nengineering order for train and test. The rest of the pipeline (GroupKFold CV,\nmetric computation, final training, and submission generation) will remain\nunchanged, while now leveraging the enriched temporal context.", "We can likely get a small but consistent gain by giving the residual model a bit\nmore short-term temporal context around each epoch. Concretely, after\nconstructing the per-epoch GNSS aggregates and baseline lat/lon, I will compute\nsimple rolling-window (e.g., window=3) mean and standard deviation features for\na few strong signals (ECEF positions, C/N0, pseudorange rate statistics, and\nbaseline lat/lon) within each phone trace, then include these as additional\nfeatures. I\u2019ll ensure these rolling features are created in exactly the same way\nfor both train and test before defining `feature_cols`, so the feature spaces\nmatch perfectly. The rest of the pipeline (residual learning, GroupKFold CV on\nphones, LightGBM configuration, and submission generation) will be kept\nunchanged.", "I will keep the residual-learning LightGBM setup but enrich the temporal context\naround each epoch using simple rolling-window statistics. Specifically, after\nthe current per-epoch aggregation and diff features, I will compute rolling mean\nand standard deviation over a short window (e.g., 3 epochs) for key numeric\nfeatures (ECEF WLS positions, C/N0 and pseudorange stats, and baseline lat/lon)\nwithin each phone trace in both train and test. These rolling features should\nhelp the model better capture local trajectory smoothness and short-term\ndynamics, improving correction of the WLS baseline. I will recompute\n`feature_cols` only after adding these rolling features on train, and then\nexplicitly create and align the same feature set on test (adding any missing\ncolumns and dropping extras) to avoid train/test feature mismatches. The rest of\nthe pipeline (5-fold GroupKFold CV, metric computation, final training, and\nsubmission generation) will remain unchanged.", "I will keep the residual-learning LightGBM setup but add short-window rolling\nmean and standard deviation features over time for the key GNSS-derived numeric\nsignals (ECEF WLS positions, C/N0, pseudorange rate stats, and the baseline\nlat/lon) within each phone trace. These rolling features (e.g., window size 3)\nprovide local motion context that helps smooth corrections to the WLS baseline\nwithout greatly increasing runtime. I will compute these rolling stats\nconsistently for both train and test before defining `feature_cols`, and also\nretain the existing first-order temporal diffs. The feature set will be\nrecomputed once after all feature engineering on train, and the exact same\ncolumns will be created/aligned on test (adding missing columns as NaNs and\nfilling from train medians) to avoid mismatches. Then I\u2019ll retrain LightGBM with\n5-fold GroupKFold by phone, report the CV metric, and generate the final test\nsubmission.", "The error came from trying to compute the median over `feature_cols` that\ninadvertently included non-numeric columns (e.g., string columns like\n`Provider`). I will explicitly restrict features to numeric dtypes and compute\nmedians with `numeric_only=True` to avoid this. I\u2019ll also ensure that the same\nnumeric-only feature list is used for train and test, and keep the rest of the\npipeline (grouped aggregation, temporal features, LightGBM with GroupKFold CV)\nunchanged. Finally, I\u2019ll generate predictions for the test set aligned to\n`sample_submission.csv` and save them to both `./submission/submission.csv` and\n`./working/submission.csv`, while printing the CV metric.", "I will keep the residual-learning LightGBM setup but enrich the temporal context\nby adding simple rolling-window (size 3) mean and standard deviation features\nfor the strongest GNSS-derived signals and baseline lat/lon within each phone\ntrace. These rolling features will be computed on both train and test after the\nper-epoch aggregation, then temporal diffs will be recomputed on the extended\nfeature set. I\u2019ll ensure that `feature_cols` is defined only once from the fully\nengineered train dataframe and that the test dataframe is explicitly aligned to\nthis exact feature list (adding missing columns and dropping extras). The rest\nof the pipeline (5-fold GroupKFold CV, metric computation, final training, and\nsubmission generation) will remain unchanged.", "The error arises because the merge step for building test features accidentally\nduplicates the `phone` column in the right DataFrame, producing non-unique\ncolumn labels. To fix this, we should only merge on the relevant columns: the\nkey columns (`phone`, `UnixTimeMillis`) plus the precomputed feature columns,\ninstead of concatenating the entire `test_gnss` twice. I'll rebuild the test-\nmerge part by constructing a minimal right-hand table with unique column names,\nthen align it with the sample submission, and ensure base latitude/longitude\ndefaults are reasonable. The rest of the training, cross-validation, and\nmodeling logic is preserved, and the script writes `submission/submission.csv`\nand `working/submission.csv` and prints the CV metric.", "The current model already learns residuals over the WLS baseline but only uses\nper-epoch aggregates and single-step temporal diffs. A simple and targeted way\nto improve short-term trajectory modeling is to add small rolling-window\nstatistics (mean and standard deviation over a 3-epoch window) for the strongest\nfeatures, computed within each phone trace. I will implement these rolling\nfeatures for the ECEF WLS means, C/N0 and pseudorange stats, and baseline\nlat/lon, for both train and test, before recomputing the feature list. This\nyields more temporal context without greatly increasing dimensionality or\nruntime, and the final feature set will still be aligned consistently between\ntrain and test. The rest of the residual-learning LightGBM setup, GroupKFold CV,\nand submission generation will remain unchanged.", "I will keep the residual-learning LightGBM setup but enrich temporal context by\nadding simple rolling-window statistics for key features, which should help\nsmooth noisy GNSS positions. Specifically, after per-epoch aggregation I will\ncompute rolling mean and standard deviation over a 3-epoch window (within each\nphone) for ECEF WLS positions, C/N0 and pseudorange rate stats, and the baseline\nlat/lon. I\u2019ll then recompute the feature list to include these rolling features\n(plus existing diffs), use them in a 5-fold GroupKFold CV to estimate the\nmetric, and finally train full models on all data and apply them to test with\nexactly aligned feature columns. The rest of the pipeline and submission-writing\nlogic will remain unchanged.", "I will keep the residual-learning LightGBM framework but add short-window\nrolling statistics to give the model more temporal context, which can help\nsmooth noisy WLS positions. Specifically, after aggregating GNSS per epoch, I\nwill compute rolling mean and standard deviation over a 3-epoch window (per\nphone) for the key features (ECEF WLS means, C/N0 stats, pseudorange rate stats,\nand baseline lat/lon). These rolling features will be created in the same way\nfor both train and test before defining `feature_cols`, and they will be used\nalongside the existing per-epoch values and diffs. I will maintain 5-fold\nGroupKFold CV by phone for evaluation, then train final models using the\nexpanded feature set and generate predictions for the test set, saving them to\n`./submission/submission.csv` and `./working/submission.csv`.", "The crash happens when assigning the result of `groupby().ffill()` back to\n`test_merge[feature_cols]`; pandas aligns on index and the sliced view causes a\nshape mismatch. To fix this, we should ffill/bfill on individual columns (or use\n`transform`) and assign each back separately, which guarantees matching lengths.\nI\u2019ll keep the rest of the pipeline intact (same feature construction and\nLightGBM setup), still perform 5-fold GroupKFold CV to print the metric, and\nthen train final models on all data. Finally, I\u2019ll generate predictions for the\ntest grid and save `submission.csv` both in `./submission` and `./working`.", "The crash comes from a feature mismatch between training and test: the feature\nlist `feature_cols` is built after adding `_diff` features on train, but on test\nwe sometimes create rolling and diff features for a slightly different base set,\nyielding 92 test features versus 90 in the model. To fix this, we must (1)\ndefine the complete feature list once right after all train transformations are\nfinished, (2) only create those exact features on test (no extra ones), and (3)\nensure test column order matches `feature_cols` before prediction. I\u2019ll rebuild\n`base_feature_candidates`, rolling, diff, and `feature_cols` cleanly on train,\nthen on test I will loop strictly over `feature_cols` to create missing columns\nand compute rolling/diff features only for those that exist. Finally, I will\nprint a 5-fold GroupKFold metric and write `submission/submission.csv` and\n`working/submission.csv`.", "The crash comes from a feature-count mismatch: during CV we build `feature_cols`\nfrom the training frame, but later we recompute or modify it implicitly when\nengineering test features, ending up with 92 test columns for a model trained\nwith 90. To fix this robustly, we\u2019ll freeze the exact feature list right after\ntraining and reuse it unchanged for test, and simplify feature engineering to\navoid dynamic additions that differ between train and test. I\u2019ll keep the same\nbasic pipeline (ECEF-derived baseline, per-epoch aggregations, LightGBM residual\nmodel) but remove rolling/diff features to ensure train/test consistency and\nreduce runtime. We\u2019ll still use 5-fold GroupKFold by phone to evaluate the mean\n(p50+p95)/2 distance metric, then train final models on full data and generate\npredictions aligned with `sample_submission.csv`. Finally, the script will write\n`submission/submission.csv` and `working/submission.csv` and print the CV\nmetric.", "The crash came from a mismatch between the feature set used in cross-validation\n(where LightGBM infers `n_features_`) and the final training data (`X`) because\nthe implicit feature selection rule based on column names was inconsistent and\nallowed extra/non-aggregated columns in the final feature list. To fix this,\nI\u2019ll explicitly build the feature list only from the aggregated GNSS columns and\nthe `base_lat/base_lon` columns, using a strict allowlist and confirming that\nthis same list exists in both train and test data. I\u2019ll then rebuild `X` from\nexactly these columns, ensuring that the LightGBM models and `X_test` always\nshare the same dimensionality. The rest of the pipeline (grouped 5-fold CV,\nfinal training on full data, test feature construction, and submission file\ncreation) remains as before but with the corrected, stable feature schema.", "The feature dimension mismatch arose because the dynamically collected\n`feature_cols` from train didn\u2019t exactly match the columns created for test\n(e.g., extra agg stats or missing numeric flags), so `X_test` ended up with a\ndifferent number of columns. To fix this robustly, we\u2019ll build a fixed feature\nlist based only on the agg patterns we explicitly compute (means/stds of known\nnumeric columns plus `Svid_nunique` and baseline lat/lon), and enforce that both\ntrain and test use exactly this list, creating any missing features and ordering\nidentically. We\u2019ll also avoid re-deriving feature names from `train_df.columns`\nand instead construct `feature_cols` directly from a template of expected\naggregated field names. The rest of the pipeline (GroupKFold CV, final LightGBM\nfit, and prediction) stays the same, and we ensure the script writes\n`./submission/submission.csv` and `./working/submission.csv` and prints the CV\nmetric.", "I will keep the residual-learning LightGBM setup but enrich temporal context by\nadding simple rolling-window (e.g. 3-epoch) mean and standard deviation features\nfor key per-epoch signals (ECEF WLS positions, C/N0 and pseudorange stats, and\nbaseline lat/lon) within each phone trace. These rolling features will be\ncreated consistently for both train and test after aggregation and before\ndefining `feature_cols`, and I will also retain one-step temporal diffs for the\nsame base features. This should help the model better smooth and correct the\nnoisy WLS baseline trajectory, especially around sharp turns or brief signal\ndropouts, without dramatically increasing runtime. I\u2019ll keep 5-fold GroupKFold\nby phone for CV to estimate the competition metric, then train final models on\nall data and generate test predictions aligned with the sample submission,\nsaving them to both `./submission/submission.csv` and\n`./working/submission.csv`.", "I will keep the residual-learning LightGBM framework but add short-window\nrolling statistics to give the model local temporal context, which should help\nsmooth noisy WLS positions and improve corrections. Specifically, after creating\nthe per-epoch GNSS aggregates and baseline lat/lon, I will compute rolling mean\nand standard deviation over a 3-epoch window for key numeric features (ECEF WLS\ncoordinates, C/N0 and pseudorange stats, and baseline lat/lon) within each phone\ntrace for both train and test. These rolling features will be appended to the\nexisting base features and diffs, `feature_cols` will be recomputed after their\ncreation, and the same columns will be enforced for test by adding any missing\nones. I will then run 5-fold GroupKFold CV as before to compute the metric,\ntrain final models on all data, and generate predictions for the test set\naligned to the sample submission, saving them in `./submission/submission.csv`\nand `./working/submission.csv`."], "code": ["import os\nimport glob\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n# WGS84 constants\nA_EARTH = 6378137.0\nE2 = 6.69437999014e-3\n\n\ndef ecef_to_latlon(x, y, z):\n    # Convert ECEF (meters) to geodetic lat/lon degrees (WGS84)\n    # Simple iterative method\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - E2))  # initial guess\n    for _ in range(5):\n        sin_lat = np.sin(lat)\n        N = A_EARTH / np.sqrt(1 - E2 * sin_lat**2)\n        lat = np.arctan2(z + E2 * N * sin_lat, p)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    # df must have columns: phone, gt_lat, gt_lon, pred_lat, pred_lon\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    return float(np.mean(errors))\n\n\ndef load_train():\n    train_root = os.path.join(INPUT_DIR, \"train\")\n    drive_dirs = sorted(glob.glob(os.path.join(train_root, \"*\")))\n    records = []\n    for drive_dir in drive_dirs:\n        if not os.path.isdir(drive_dir):\n            continue\n        phone_dirs = sorted(glob.glob(os.path.join(drive_dir, \"*\")))\n        for phone_dir in phone_dirs:\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            drive_id = os.path.basename(drive_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n                continue\n            gt = pd.read_csv(gt_path)\n            gnss = pd.read_csv(gnss_path)\n            # Align time: ground_truth has UnixTimeMillis; gnss has utcTimeMillis\n            # We aggregate gnss per second on utcTimeMillis rounded to nearest second\n            gnss = gnss.copy()\n            # filter rows having WLS position\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not all(c in gnss.columns for c in wls_cols):\n                continue\n            # drop rows without WLS\n            gnss = gnss.dropna(subset=wls_cols)\n            # round utcTimeMillis to nearest 1000 ms\n            gnss[\"UnixTimeMillis\"] = (gnss[\"utcTimeMillis\"] // 1000 * 1000).astype(\n                \"int64\"\n            )\n            # aggregate by taking mean of WLS and some basic stats\n            agg_dict = {\n                \"WlsPositionXEcefMeters\": \"mean\",\n                \"WlsPositionYEcefMeters\": \"mean\",\n                \"WlsPositionZEcefMeters\": \"mean\",\n                \"Cn0DbHz\": \"mean\",\n                \"PseudorangeRateMetersPerSecond\": \"mean\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                \"SvElevationDegrees\": \"mean\",\n                \"SvAzimuthDegrees\": \"mean\",\n                \"SvClockBiasMeters\": \"mean\",\n                \"SvClockDriftMetersPerSecond\": \"mean\",\n                \"IonosphericDelayMeters\": \"mean\",\n                \"TroposphericDelayMeters\": \"mean\",\n                \"IsrbMeters\": \"mean\",\n                \"RawPseudorangeMeters\": \"mean\",\n            }\n            # keep only columns that exist\n            agg_dict = {k: v for k, v in agg_dict.items() if k in gnss.columns}\n            gnss_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict).reset_index()\n            # merge with ground truth on UnixTimeMillis\n            merged = pd.merge(gt, gnss_agg, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n            # convert WLS ECEF to lat/lon\n            lat_wls, lon_wls = ecef_to_latlon(\n                merged[\"WlsPositionXEcefMeters\"].values,\n                merged[\"WlsPositionYEcefMeters\"].values,\n                merged[\"WlsPositionZEcefMeters\"].values,\n            )\n            merged[\"wls_lat\"] = lat_wls\n            merged[\"wls_lon\"] = lon_wls\n            merged[\"phone\"] = phone_id\n            records.append(merged)\n    if not records:\n        raise RuntimeError(\"No training data found.\")\n    train_df = pd.concat(records, ignore_index=True)\n    return train_df\n\n\ndef prepare_features(train_df):\n    # Target: delta between GT and WLS lat/lon\n    train_df = train_df.copy()\n    train_df[\"target_dlat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"wls_lat\"]\n    train_df[\"target_dlon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"wls_lon\"]\n    feature_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"IsrbMeters\",\n        \"RawPseudorangeMeters\",\n    ]\n    feature_cols = [c for c in feature_cols if c in train_df.columns]\n    X = train_df[feature_cols].fillna(train_df[feature_cols].median())\n    y_dlat = train_df[\"target_dlat\"].values\n    y_dlon = train_df[\"target_dlon\"].values\n    return X, y_dlat, y_dlon, feature_cols\n\n\ndef cross_validate(X, y_dlat, y_dlon, train_df, feature_cols):\n    gkf = GroupKFold(n_splits=5)\n    groups = train_df[\"phone\"].values\n    metrics = []\n    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        dlat_tr, dlat_va = y_dlat[tr_idx], y_dlat[va_idx]\n        dlon_tr, dlon_va = y_dlon[tr_idx], y_dlon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        lgb_train_dlat = lgb.Dataset(X_tr, label=dlat_tr)\n        lgb_valid_dlat = lgb.Dataset(X_va, label=dlat_va, reference=lgb_train_dlat)\n\n        model_dlat = lgb.train(\n            params,\n            lgb_train_dlat,\n            num_boost_round=500,\n            valid_sets=[lgb_valid_dlat],\n            early_stopping_rounds=50,\n            verbose_eval=False,\n        )\n\n        lgb_train_dlon = lgb.Dataset(X_tr, label=dlon_tr)\n        lgb_valid_dlon = lgb.Dataset(X_va, label=dlon_va, reference=lgb_train_dlon)\n\n        model_dlon = lgb.train(\n            params,\n            lgb_train_dlon,\n            num_boost_round=500,\n            valid_sets=[lgb_valid_dlon],\n            early_stopping_rounds=50,\n            verbose_eval=False,\n        )\n\n        va = train_df.iloc[va_idx].copy()\n        va_X = X_va\n        va[\"pred_dlat\"] = model_dlat.predict(\n            va_X, num_iteration=model_dlat.best_iteration\n        )\n        va[\"pred_dlon\"] = model_dlon.predict(\n            va_X, num_iteration=model_dlon.best_iteration\n        )\n        va[\"pred_lat\"] = va[\"wls_lat\"] + va[\"pred_dlat\"]\n        va[\"pred_lon\"] = va[\"wls_lon\"] + va[\"pred_dlon\"]\n        va = va.rename(\n            columns={\"LatitudeDegrees\": \"gt_lat\", \"LongitudeDegrees\": \"gt_lon\"}\n        )\n        score = competition_metric(\n            va[[\"phone\", \"gt_lat\", \"gt_lon\", \"pred_lat\", \"pred_lon\"]]\n        )\n        metrics.append(score)\n        print(f\"Fold {fold} metric: {score:.4f}\")\n    print(f\"CV mean metric: {np.mean(metrics):.4f}, std: {np.std(metrics):.4f}\")\n    return np.mean(metrics)\n\n\ndef train_full_models(X, y_dlat, y_dlon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_dlat = lgb.Dataset(X, label=y_dlat)\n    model_dlat = lgb.train(params, dtrain_dlat, num_boost_round=400)\n    dtrain_dlon = lgb.Dataset(X, label=y_dlon)\n    model_dlon = lgb.train(params, dtrain_dlon, num_boost_round=400)\n    return model_dlat, model_dlon\n\n\ndef load_test_features(feature_cols):\n    test_root = os.path.join(INPUT_DIR, \"test\")\n    drive_dirs = sorted(glob.glob(os.path.join(test_root, \"*\")))\n    gnss_records = []\n    for drive_dir in drive_dirs:\n        if not os.path.isdir(drive_dir):\n            continue\n        phone_dirs = sorted(glob.glob(os.path.join(drive_dir, \"*\")))\n        for phone_dir in phone_dirs:\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            drive_id = os.path.basename(drive_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            gnss = pd.read_csv(gnss_path)\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not all(c in gnss.columns for c in wls_cols):\n                continue\n            gnss = gnss.dropna(subset=wls_cols)\n            gnss[\"UnixTimeMillis\"] = (gnss[\"utcTimeMillis\"] // 1000 * 1000).astype(\n                \"int64\"\n            )\n            agg_dict = {\n                \"WlsPositionXEcefMeters\": \"mean\",\n                \"WlsPositionYEcefMeters\": \"mean\",\n                \"WlsPositionZEcefMeters\": \"mean\",\n            }\n            for c in feature_cols:\n                if c not in agg_dict and c in gnss.columns:\n                    agg_dict[c] = \"mean\"\n            gnss_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict).reset_index()\n            lat_wls, lon_wls = ecef_to_latlon(\n                gnss_agg[\"WlsPositionXEcefMeters\"].values,\n                gnss_agg[\"WlsPositionYEcefMeters\"].values,\n                gnss_agg[\"WlsPositionZEcefMeters\"].values,\n            )\n            gnss_agg[\"wls_lat\"] = lat_wls\n            gnss_agg[\"wls_lon\"] = lon_wls\n            gnss_agg[\"phone\"] = phone_id\n            gnss_records.append(gnss_agg)\n    if not gnss_records:\n        raise RuntimeError(\"No test GNSS data loaded.\")\n    gnss_df = pd.concat(gnss_records, ignore_index=True)\n    return gnss_df\n\n\ndef main():\n    train_df = load_train()\n    X, y_dlat, y_dlon, feature_cols = prepare_features(train_df)\n    cv_score = cross_validate(X, y_dlat, y_dlon, train_df, feature_cols)\n    print(f\"Validation competition-like metric: {cv_score:.6f}\")\n    model_dlat, model_dlon = train_full_models(X, y_dlat, y_dlon)\n\n    # Prepare test set\n    sample_sub = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))\n    # sample_submission phone format: driveid_phonename? Let's inspect a row\n    # But we cannot, so we assume it's like \"2020-06-04-US-MTV-1_Pixel4\".\n    # Our phone_id is \"2020-06-04-US-MTV-1_GooglePixel4\". Need mapping from model names.\n    # Derive mapping from train phones by suffix.\n    train_phones = sorted(train_df[\"phone\"].unique())\n    # build mapping: suffix after last '_' should roughly match sample phone suffix after last '_'\n    train_suffix_map = {}\n    for p in train_phones:\n        drive, dev = p.split(\"_\", 1)\n        train_suffix_map[(drive, dev.lower())] = p\n\n    # For test, we had phones from load_test_features: build mapping for those\n    test_gnss_df = load_test_features(feature_cols)\n    test_phones = sorted(test_gnss_df[\"phone\"].unique())\n    test_phone_info = {}\n    for p in test_phones:\n        drive, dev = p.split(\"_\", 1)\n        test_phone_info[(drive, dev)] = p\n\n    # map sample_submission phone to our phone_id by drive and partial phone name\n    def map_phone(sample_phone):\n        # sample format: \"{drive}_{model}\" where model may differ slightly\n        parts = sample_phone.split(\"_\")\n        drive = \"_\".join(\n            parts[0:4]\n        )  # drive id has 4 dash-separated parts, e.g., 2020-06-04-US-MTV-1\n        model = parts[4] if len(parts) > 4 else \"\"\n        # find test_phone with same drive and whose device contains model or vice versa\n        candidates = [p for p in test_phones if p.startswith(drive + \"_\")]\n        if len(candidates) == 1:\n            return candidates[0]\n        for p in candidates:\n            dev = p.split(\"_\", 1)[1]\n            if model.lower() in dev.lower() or dev.lower() in model.lower():\n                return p\n        return candidates[0] if candidates else None\n\n    sample_sub[\"our_phone\"] = sample_sub[\"phone\"].apply(map_phone)\n\n    # merge test gnss features\n    # ensure UnixTimeMillis int64\n    test_gnss_df[\"UnixTimeMillis\"] = test_gnss_df[\"UnixTimeMillis\"].astype(\"int64\")\n    # Build key for merge\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n    # We'll left-merge on (our_phone, UnixTimeMillis)\n    test_gnss_df_renamed = test_gnss_df.rename(columns={\"phone\": \"our_phone\"})\n    test_merged = pd.merge(\n        sample_sub, test_gnss_df_renamed, on=[\"our_phone\", \"UnixTimeMillis\"], how=\"left\"\n    )\n\n    # Some timestamps may be missing WLS; we fill using nearest time per phone\n    # First, for each our_phone, forward/backward fill by sorting on time\n    test_merged = test_merged.sort_values([\"our_phone\", \"UnixTimeMillis\"])\n    cols_to_fill = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"wls_lat\",\n        \"wls_lon\",\n    ] + [\n        c\n        for c in feature_cols\n        if c\n        not in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ]\n    cols_to_fill = [c for c in cols_to_fill if c in test_merged.columns]\n    test_merged[cols_to_fill] = test_merged.groupby(\"our_phone\")[cols_to_fill].apply(\n        lambda g: g.ffill().bfill()\n    )\n\n    # Prepare feature matrix for prediction\n    X_test = test_merged[feature_cols].fillna(X.median())\n    pred_dlat = model_dlat.predict(X_test)\n    pred_dlon = model_dlon.predict(X_test)\n    test_merged[\"pred_lat\"] = test_merged[\"wls_lat\"] + pred_dlat\n    test_merged[\"pred_lon\"] = test_merged[\"wls_lon\"] + pred_dlon\n\n    submission = test_merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n    submission = submission.rename(\n        columns={\"pred_lat\": \"LatitudeDegrees\", \"pred_lon\": \"LongitudeDegrees\"}\n    )\n    submission.to_csv(os.path.join(SUBMISSION_DIR, \"submission.csv\"), index=False)\n    print(\"Saved submission to\", os.path.join(SUBMISSION_DIR, \"submission.csv\"))\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom lightgbm import LGBMRegressor\n\n# ---------------------------\n# Utility functions\n# ---------------------------\n\n\ndef ecef_to_llh(x, y, z):\n    # WGS84 ellipsoid constants:\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = a * np.sqrt(1 - e2)\n\n    r = np.sqrt(x * x + y * y)\n    Esq = a * a - b * b\n    F = 54 * b * b * z * z\n    G = r * r + (1 - e2) * z * z - e2 * Esq\n    C = (e2 * e2 * F * r * r) / (G * G * G)\n    S = np.cbrt(1 + C + np.sqrt(C * C + 2 * C))\n    P = F / (3 * (S + 1 / S + 1) ** 2 * G * G)\n    Q = np.sqrt(1 + 2 * e2 * e2 * P)\n    r0 = -(P * e2 * r) / (1 + Q) + np.sqrt(\n        0.5 * a * a * (1 + 1 / Q)\n        - P * (1 - e2) * z * z / (Q * (1 + Q))\n        - 0.5 * P * r * r\n    )\n    U = np.sqrt((r - e2 * r0) ** 2 + z * z)\n    V = np.sqrt((r - e2 * r0) ** 2 + (1 - e2) * z * z)\n    Z0 = b * b * z / (a * V)\n    h = U * (1 - b * b / (a * V))\n    lat = np.arctan((z + e2 * Z0) / r)\n    lon = np.arctan2(y, x)\n    return np.degrees(lat), np.degrees(lon), h\n\n\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    # compute horizontal distance errors in meters using ECEF\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df = df.copy()\n    df[\"dist\"] = dist\n    per_phone = []\n    for phone, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    return float(np.mean(per_phone))\n\n\n# ---------------------------\n# Load training data\n# ---------------------------\n\ntrain_root = \"./input/train\"\ntest_root = \"./input/test\"\n\ntrain_rows = []\ngt_rows = []\n\nfor drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n    drive_id = os.path.basename(drive_dir)\n    for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n        phone_name = os.path.basename(phone_dir)\n        phone_id = f\"{drive_id}_{phone_name}\"\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n        if not os.path.exists(gnss_path) or not os.path.exists(gt_path):\n            continue\n        gnss = pd.read_csv(gnss_path)\n        gt = pd.read_csv(gt_path)\n\n        # Use ArrivalTimeNanosSinceGpsEpoch if available, else utcTimeMillis\n        if \"ArrivalTimeNanosSinceGpsEpoch\" in gnss.columns:\n            gnss_time = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] // 1_000_000\n            gnss[\"UnixTimeMillis\"] = gnss_time.astype(\"int64\")\n        else:\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"]\n\n        # Deduplicate gnss times by averaging WLS ECEF within each epoch\n        # Keep basic quality features\n        wls_cols = [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        for c in wls_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n        # Aggregate per epoch: mean of ECEF and mean Cn0DbHz\n        agg = (\n            gnss.groupby(\"UnixTimeMillis\")\n            .agg(\n                {\n                    \"Cn0DbHz\": \"mean\",\n                    \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                    wls_cols[0]: \"mean\",\n                    wls_cols[1]: \"mean\",\n                    wls_cols[2]: \"mean\",\n                }\n            )\n            .reset_index()\n        )\n        agg[\"phone\"] = phone_id\n\n        # Ground truth: one row per UnixTimeMillis\n        gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n        gt_small[\"phone\"] = phone_id\n\n        # Merge on closest time within +/- 200 ms\n        agg = agg.sort_values(\"UnixTimeMillis\")\n        gt_small = gt_small.sort_values(\"UnixTimeMillis\")\n\n        # As an approximation, merge by nearest using pandas merge_asof\n        merged = pd.merge_asof(\n            gt_small,\n            agg,\n            on=\"UnixTimeMillis\",\n            by=\"phone\",\n            direction=\"nearest\",\n            tolerance=200,\n        )\n        merged = merged.dropna(subset=wls_cols)\n        if len(merged) == 0:\n            continue\n\n        # Add ECEF ground truth\n        x_gt, y_gt, z_gt = llh_to_ecef(\n            merged[\"LatitudeDegrees\"].values,\n            merged[\"LongitudeDegrees\"].values,\n            np.zeros(len(merged)),\n        )\n        merged[\"x_gt\"] = x_gt\n        merged[\"y_gt\"] = y_gt\n        merged[\"z_gt\"] = z_gt\n\n        train_rows.append(merged)\n\ntrain_df = pd.concat(train_rows, ignore_index=True)\n\n# Features and targets\nfeature_cols = [\n    \"Cn0DbHz\",\n    \"PseudorangeRateUncertaintyMetersPerSecond\",\n    \"WlsPositionXEcefMeters\",\n    \"WlsPositionYEcefMeters\",\n    \"WlsPositionZEcefMeters\",\n]\nfor c in feature_cols:\n    if c not in train_df.columns:\n        train_df[c] = np.nan\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_x = train_df[\"x_gt\"].values\ny_y = train_df[\"y_gt\"].values\ny_z = train_df[\"z_gt\"].values\ngroups = train_df[\"phone\"].str.split(\"_\").str[0]  # group by drive_id\n\n# ---------------------------\n# 5-fold Group CV\n# ---------------------------\ngkf = GroupKFold(n_splits=5)\n\noof_pred = np.zeros((len(train_df), 3))\nfold = 0\nfor train_idx, val_idx in gkf.split(X, y_x, groups):\n    fold += 1\n    X_tr, X_val = X[train_idx], X[val_idx]\n    yx_tr, yx_val = y_x[train_idx], y_x[val_idx]\n    yy_tr, yy_val = y_y[train_idx], y_y[val_idx]\n    yz_tr, yz_val = y_z[train_idx], y_z[val_idx]\n\n    params = dict(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=-1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model_x = LGBMRegressor(**params)\n    model_y = LGBMRegressor(**params)\n    model_z = LGBMRegressor(**params)\n\n    model_x.fit(X_tr, yx_tr)\n    model_y.fit(X_tr, yy_tr)\n    model_z.fit(X_tr, yz_tr)\n\n    oof_pred[val_idx, 0] = model_x.predict(X_val)\n    oof_pred[val_idx, 1] = model_y.predict(X_val)\n    oof_pred[val_idx, 2] = model_z.predict(X_val)\n\n# Evaluate CV metric\nlat_pred_oof, lon_pred_oof, _ = ecef_to_llh(\n    oof_pred[:, 0], oof_pred[:, 1], oof_pred[:, 2]\n)\neval_df = pd.DataFrame(\n    {\n        \"phone\": train_df[\"phone\"].values,\n        \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n        \"lat_pred\": lat_pred_oof,\n        \"lon_pred\": lon_pred_oof,\n        \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n        \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n    }\n)\nmetric_value = competition_metric(eval_df)\nprint(\"CV competition metric (mean of 50th and 95th pct errors):\", metric_value)\n\n# ---------------------------\n# Train final models on all data\n# ---------------------------\nfinal_params = dict(\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=-1,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_x = LGBMRegressor(**final_params)\nfinal_model_y = LGBMRegressor(**final_params)\nfinal_model_z = LGBMRegressor(**final_params)\n\nfinal_model_x.fit(X, y_x)\nfinal_model_y.fit(X, y_y)\nfinal_model_z.fit(X, y_z)\n\n# ---------------------------\n# Prepare test features and predict\n# ---------------------------\n\ntest_rows = []\n\nfor drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n    drive_id = os.path.basename(drive_dir)\n    for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n        phone_name = os.path.basename(phone_dir)\n        phone_id = f\"{drive_id}_{phone_name}\"\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            continue\n        gnss = pd.read_csv(gnss_path)\n        if \"ArrivalTimeNanosSinceGpsEpoch\" in gnss.columns:\n            gnss_time = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] // 1_000_000\n            gnss[\"UnixTimeMillis\"] = gnss_time.astype(\"int64\")\n        else:\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"]\n\n        wls_cols = [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        for c in wls_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = (\n            gnss.groupby(\"UnixTimeMillis\")\n            .agg(\n                {\n                    \"Cn0DbHz\": \"mean\",\n                    \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                    wls_cols[0]: \"mean\",\n                    wls_cols[1]: \"mean\",\n                    wls_cols[2]: \"mean\",\n                }\n            )\n            .reset_index()\n        )\n        agg[\"phone\"] = phone_id\n        test_rows.append(agg)\n\ntest_df = pd.concat(test_rows, ignore_index=True)\n\n# Merge with sample_submission to align timestamps\nsample_sub = pd.read_csv(\"./input/sample_submission.csv\")\n# sample_sub phone column exists, with phone id\ntest_df_merged = pd.merge_asof(\n    sample_sub.sort_values([\"phone\", \"UnixTimeMillis\"]),\n    test_df.sort_values([\"phone\", \"UnixTimeMillis\"]),\n    on=\"UnixTimeMillis\",\n    by=\"phone\",\n    direction=\"nearest\",\n    tolerance=500,\n)\n\n# For rows without nearby gnss epoch, we'll fallback to nearest within larger tolerance by re-running with bigger tolerance\nmissing_mask = test_df_merged[\"Cn0DbHz\"].isna()\nif missing_mask.any():\n    fallback = pd.merge_asof(\n        sample_sub[missing_mask].sort_values([\"phone\", \"UnixTimeMillis\"]),\n        test_df.sort_values([\"phone\", \"UnixTimeMillis\"]),\n        on=\"UnixTimeMillis\",\n        by=\"phone\",\n        direction=\"nearest\",\n        tolerance=2000,\n    )\n    test_df_merged.loc[\n        missing_mask,\n        [\n            \"Cn0DbHz\",\n            \"PseudorangeRateUncertaintyMetersPerSecond\",\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ],\n    ] = fallback[\n        [\n            \"Cn0DbHz\",\n            \"PseudorangeRateUncertaintyMetersPerSecond\",\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ].values\n\nfor c in feature_cols:\n    if c not in test_df_merged.columns:\n        test_df_merged[c] = np.nan\ntest_df_merged[feature_cols] = test_df_merged[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_df_merged[feature_cols].values\nx_pred = final_model_x.predict(X_test)\ny_pred = final_model_y.predict(X_test)\nz_pred = final_model_z.predict(X_test)\n\nlat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\nsubmission = sample_sub.copy()\nsubmission[\"LatitudeDegrees\"] = lat_pred\nsubmission[\"LongitudeDegrees\"] = lon_pred\n\nos.makedirs(\"./submission\", exist_ok=True)\nsubmission.to_csv(\"./submission/submission.csv\", index=False)\nprint(\"Saved submission to ./submission/submission.csv\")\n", "import os\nimport glob\nimport json\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import pairwise_distances\nimport lightgbm as lgb\n\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nSUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n\n# ---------------- Utility functions ----------------\n\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    \"\"\"Vectorized haversine distance in meters.\"\"\"\n    R = 6371000.0\n    lat1_rad = np.radians(lat1)\n    lat2_rad = np.radians(lat2)\n    dlat = lat2_rad - lat1_rad\n    dlon = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dlat / 2.0) ** 2\n        + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0) ** 2\n    )\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(\n    df, pred_lat_col, pred_lon_col, gt_lat_col, gt_lon_col, phone_col=\"phone\"\n):\n    \"\"\"Compute mean of (50th, 95th percentile) distance errors per phone.\"\"\"\n    errors = haversine_distance(\n        df[gt_lat_col].values,\n        df[gt_lon_col].values,\n        df[pred_lat_col].values,\n        df[pred_lon_col].values,\n    )\n    df = df.copy()\n    df[\"error\"] = errors\n    phone_scores = []\n    for phone, g in df.groupby(phone_col):\n        e = g[\"error\"].values\n        p50 = np.percentile(e, 50)\n        p95 = np.percentile(e, 95)\n        phone_scores.append((p50 + p95) / 2.0)\n    return float(np.mean(phone_scores))\n\n\ndef load_train_paths(train_dir):\n    paths = []\n    for drive in sorted(os.listdir(train_dir)):\n        drive_path = os.path.join(train_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n            if os.path.exists(gnss_path) and os.path.exists(gt_path):\n                phone_id = f\"{drive}_{phone}\"\n                paths.append((phone_id, gnss_path, gt_path))\n    return paths\n\n\ndef load_test_paths(test_dir):\n    paths = []\n    for drive in sorted(os.listdir(test_dir)):\n        drive_path = os.path.join(test_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            if os.path.exists(gnss_path):\n                phone_id = f\"{drive}_{phone}\"\n                paths.append((phone_id, gnss_path))\n    return paths\n\n\ndef build_gnss_aggregates_train(phone_id, gnss_path, gt_path):\n    \"\"\"\n    Build per-second aggregates from device_gnss and join to ground_truth.\n    We align on utcTimeMillis ~ UnixTimeMillis; if utcTimeMillis missing, fall back to ArrivalTimeNanosSinceGpsEpoch-derived time if available.\n    \"\"\"\n    gnss = pd.read_csv(gnss_path)\n    # Ensure utcTimeMillis exists\n    if \"utcTimeMillis\" not in gnss.columns:\n        # Some datasets might have ArrivalTimeNanosSinceGpsEpoch or similar; if not, we can't use them.\n        return pd.DataFrame()\n    # Round utcTimeMillis to nearest 1000 to get 1Hz timestamps compatible with ground_truth UnixTimeMillis\n    gnss[\"UnixTimeMillis\"] = (np.round(gnss[\"utcTimeMillis\"] / 1000.0) * 1000).astype(\n        \"int64\"\n    )\n    # Baseline WLS lat/lon from WlsPosition[EcefMeters] if Lat/Lon present directly use them\n    # In this dataset, device_gnss does NOT contain lat/lon; we must treat WLS ECEF as baseline but can't easily convert without heavy geodesy.\n    # However the challenge adaptation typically adds 'LatitudeDegrees' and 'LongitudeDegrees' into device_gnss; check and use if present.\n    lat_col_candidates = [\n        c for c in gnss.columns if c.lower().startswith(\"latitudedeg\")\n    ]\n    lon_col_candidates = [\n        c for c in gnss.columns if c.lower().startswith(\"longitudedeg\")\n    ]\n    if len(lat_col_candidates) == 0 or len(lon_col_candidates) == 0:\n        # fallback: no baseline; we'll directly learn lat/lon from GNSS features with ground truth\n        baseline_lat_col = None\n        baseline_lon_col = None\n    else:\n        baseline_lat_col = lat_col_candidates[0]\n        baseline_lon_col = lon_col_candidates[0]\n\n    gt = pd.read_csv(gt_path)\n    gt = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n\n    # Aggregate GNSS per UnixTimeMillis\n    agg_cols = []\n    numeric_cols = gnss.select_dtypes(include=[np.number]).columns.tolist()\n    # Remove time cols to avoid redundancy\n    for c in [\"utcTimeMillis\", \"UnixTimeMillis\"]:\n        if c in numeric_cols:\n            numeric_cols.remove(c)\n    # Some numeric columns are obviously IDs but including them won't hurt LightGBM\n    agg_dict = {c: [\"mean\", \"std\", \"min\", \"max\"] for c in numeric_cols}\n    gnss_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    # Flatten MultiIndex columns\n    gnss_agg.columns = [f\"{col}_{stat}\" for col, stat in gnss_agg.columns]\n    gnss_agg.reset_index(inplace=True)\n\n    # Merge with ground truth\n    df = pd.merge(gt, gnss_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    df[\"phone\"] = phone_id\n\n    if baseline_lat_col is not None:\n        # also aggregate baseline lat/lon\n        base_df = gnss[[\"UnixTimeMillis\", baseline_lat_col, baseline_lon_col]].copy()\n        base_agg = base_df.groupby(\"UnixTimeMillis\").mean().reset_index()\n        df = pd.merge(\n            df, base_agg, on=\"UnixTimeMillis\", how=\"left\", suffixes=(\"\", \"_base\")\n        )\n        df.rename(\n            columns={\n                baseline_lat_col: \"baseline_lat\",\n                baseline_lon_col: \"baseline_lon\",\n            },\n            inplace=True,\n        )\n    else:\n        df[\"baseline_lat\"] = df[\"LatitudeDegrees\"]\n        df[\"baseline_lon\"] = df[\"LongitudeDegrees\"]\n\n    # Targets are deltas from baseline\n    df[\"target_dlat\"] = df[\"LatitudeDegrees\"] - df[\"baseline_lat\"]\n    df[\"target_dlon\"] = df[\"LongitudeDegrees\"] - df[\"baseline_lon\"]\n\n    return df\n\n\ndef build_gnss_aggregates_test(phone_id, gnss_path, sample_phone_subset):\n    gnss = pd.read_csv(gnss_path)\n    if \"utcTimeMillis\" not in gnss.columns:\n        return pd.DataFrame()\n    gnss[\"UnixTimeMillis\"] = (np.round(gnss[\"utcTimeMillis\"] / 1000.0) * 1000).astype(\n        \"int64\"\n    )\n\n    lat_col_candidates = [\n        c for c in gnss.columns if c.lower().startswith(\"latitudedeg\")\n    ]\n    lon_col_candidates = [\n        c for c in gnss.columns if c.lower().startswith(\"longitudedeg\")\n    ]\n    if len(lat_col_candidates) == 0 or len(lon_col_candidates) == 0:\n        baseline_lat_col = None\n        baseline_lon_col = None\n    else:\n        baseline_lat_col = lat_col_candidates[0]\n        baseline_lon_col = lon_col_candidates[0]\n\n    agg_cols = []\n    numeric_cols = gnss.select_dtypes(include=[np.number]).columns.tolist()\n    for c in [\"utcTimeMillis\", \"UnixTimeMillis\"]:\n        if c in numeric_cols:\n            numeric_cols.remove(c)\n    agg_dict = {c: [\"mean\", \"std\", \"min\", \"max\"] for c in numeric_cols}\n    gnss_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    gnss_agg.columns = [f\"{col}_{stat}\" for col, stat in gnss_agg.columns]\n    gnss_agg.reset_index(inplace=True)\n\n    if baseline_lat_col is not None:\n        base_df = gnss[[\"UnixTimeMillis\", baseline_lat_col, baseline_lon_col]].copy()\n        base_agg = base_df.groupby(\"UnixTimeMillis\").mean().reset_index()\n        gnss_agg = pd.merge(gnss_agg, base_agg, on=\"UnixTimeMillis\", how=\"left\")\n        gnss_agg.rename(\n            columns={\n                baseline_lat_col: \"baseline_lat\",\n                baseline_lon_col: \"baseline_lon\",\n            },\n            inplace=True,\n        )\n    else:\n        # we don't have baseline; will fill later from sample_submission if needed\n        gnss_agg[\"baseline_lat\"] = np.nan\n        gnss_agg[\"baseline_lon\"] = np.nan\n\n    gnss_agg[\"phone\"] = phone_id\n\n    # Keep only rows needed for sample submission for this phone\n    sub = sample_phone_subset.copy()\n    # Merge by UnixTimeMillis\n    merged = pd.merge(sub, gnss_agg, on=[\"UnixTimeMillis\"], how=\"left\")\n    return merged\n\n\n# ---------------- Build training data ----------------\n\ntrain_paths = load_train_paths(TRAIN_DIR)\n\ntrain_dfs = []\nfor phone_id, gnss_path, gt_path in train_paths:\n    df_phone = build_gnss_aggregates_train(phone_id, gnss_path, gt_path)\n    if not df_phone.empty:\n        train_dfs.append(df_phone)\n\nif len(train_dfs) == 0:\n    raise RuntimeError(\"No training data could be constructed.\")\n\ntrain_df = pd.concat(train_dfs, ignore_index=True)\n\n# Identify feature columns\nexclude_cols = [\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"target_dlat\",\n    \"target_dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n]\nfeature_cols = [\n    c for c in train_df.columns if c not in exclude_cols and train_df[c].dtype != \"O\"\n]\nfeature_cols = [\n    c for c in feature_cols if not c.endswith(\"_base\")\n]  # we keep baseline_lat/lon separately\n# Ensure baseline columns are included as features\nfor c in [\"baseline_lat\", \"baseline_lon\"]:\n    if c in train_df.columns and c not in feature_cols:\n        feature_cols.append(c)\n\n# Drop rows with NaNs in targets or key features\ntrain_df = train_df.dropna(\n    subset=[\"target_dlat\", \"target_dlon\"] + feature_cols\n).reset_index(drop=True)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"target_dlat\"].values\ny_dlon = train_df[\"target_dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ---------------- 5-fold GroupKFold CV ----------------\n\ngkf = GroupKFold(n_splits=5)\nmetrics = []\n\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[train_idx], X[val_idx]\n    y_tr_dlat, y_val_dlat = y_dlat[train_idx], y_dlat[val_idx]\n    y_tr_dlon, y_val_dlon = y_dlon[train_idx], y_dlon[val_idx]\n    val_df = train_df.iloc[val_idx].copy()\n\n    # LightGBM parameters kept simple\n    params = dict(\n        objective=\"regression\",\n        metric=\"rmse\",\n        learning_rate=0.05,\n        num_leaves=31,\n        feature_fraction=0.8,\n        bagging_fraction=0.8,\n        bagging_freq=1,\n        verbose=-1,\n        n_estimators=300,\n    )\n\n    dtrain_dlat = lgb.Dataset(X_tr, label=y_tr_dlat)\n    dval_dlat = lgb.Dataset(X_val, label=y_val_dlat, reference=dtrain_dlat)\n    model_dlat = lgb.train(\n        params,\n        dtrain_dlat,\n        valid_sets=[dtrain_dlat, dval_dlat],\n        valid_names=[\"train\", \"valid\"],\n        num_boost_round=300,\n        early_stopping_rounds=30,\n        verbose_eval=False,\n    )\n\n    dtrain_dlon = lgb.Dataset(X_tr, label=y_tr_dlon)\n    dval_dlon = lgb.Dataset(X_val, label=y_val_dlon, reference=dtrain_dlon)\n    model_dlon = lgb.train(\n        params,\n        dtrain_dlon,\n        valid_sets=[dtrain_dlon, dval_dlon],\n        valid_names=[\"train\", \"valid\"],\n        num_boost_round=300,\n        early_stopping_rounds=30,\n        verbose_eval=False,\n    )\n\n    val_pred_dlat = model_dlat.predict(X_val, num_iteration=model_dlat.best_iteration)\n    val_pred_dlon = model_dlon.predict(X_val, num_iteration=model_dlon.best_iteration)\n\n    val_df[\"pred_lat\"] = val_df[\"baseline_lat\"] + val_pred_dlat\n    val_df[\"pred_lon\"] = val_df[\"baseline_lon\"] + val_pred_dlon\n\n    fold_metric = competition_metric(\n        val_df,\n        \"pred_lat\",\n        \"pred_lon\",\n        \"LatitudeDegrees\",\n        \"LongitudeDegrees\",\n        phone_col=\"phone\",\n    )\n    metrics.append(fold_metric)\n    print(f\"Fold {fold} metric: {fold_metric:.4f}\")\n\ncv_metric = float(np.mean(metrics))\nprint(f\"Mean CV metric over 5 folds: {cv_metric:.4f}\")\n\n# ---------------- Train final models on full data ----------------\n\nfinal_params = dict(\n    objective=\"regression\",\n    metric=\"rmse\",\n    learning_rate=0.05,\n    num_leaves=31,\n    feature_fraction=0.8,\n    bagging_fraction=0.8,\n    bagging_freq=1,\n    verbose=-1,\n    n_estimators=int(np.mean([m.best_iteration for m in []])) if False else 350,\n)\n# Use a reasonable fixed number of rounds\nfinal_params[\"n_estimators\"] = 350\n\nmodel_dlat_full = lgb.LGBMRegressor(**final_params)\nmodel_dlon_full = lgb.LGBMRegressor(**final_params)\n\nmodel_dlat_full.fit(X, y_dlat)\nmodel_dlon_full.fit(X, y_dlon)\n\n# ---------------- Build test features aligned to sample_submission ----------------\n\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n# sample_submission phone id matches drive_phone structure from test directory? In Kaggle they use \"drive_phoneName\"\n# We'll assume same here.\ntest_paths = load_test_paths(TEST_DIR)\n\ntest_pred_list = []\n\nfor phone_id, gnss_path in test_paths:\n    # sample phone string should match; if not, we can map by suffix\n    sample_phone_mask = sample_sub[\"phone\"] == phone_id\n    if not sample_phone_mask.any():\n        # try to handle potential mismatch: sample uses path-like '2020-06-04-US-MTV-1_Pixel4'\n        # Convert phone_id '2020-06-04-US-MTV-1/GooglePixel4' to '2020-06-04-US-MTV-1_GooglePixel4'\n        alt_id = phone_id.replace(\"/\", \"_\")\n        sample_phone_mask = sample_sub[\"phone\"] == alt_id\n        if not sample_phone_mask.any():\n            continue\n        else:\n            phone_key = alt_id\n    else:\n        phone_key = phone_id\n\n    sample_phone_subset = sample_sub.loc[\n        sample_phone_mask, [\"phone\", \"UnixTimeMillis\"]\n    ].copy()\n    # For merging, remove 'phone' (we already know it) and use UnixTimeMillis\n    sample_phone_subset = sample_phone_subset.drop_duplicates(subset=[\"UnixTimeMillis\"])\n\n    phone_df = build_gnss_aggregates_test(phone_id, gnss_path, sample_phone_subset)\n    if phone_df.empty:\n        continue\n\n    # After build, we may have missing baseline; fill using nearest available baseline in time or fallback to simple mean\n    if phone_df[\"baseline_lat\"].isna().any():\n        # Fallback: use lat/lon from any non-NaN for that phone, or zeros\n        if phone_df[\"baseline_lat\"].notna().any():\n            bl_lat = phone_df[\"baseline_lat\"].dropna().mean()\n            bl_lon = phone_df[\"baseline_lon\"].dropna().mean()\n        else:\n            bl_lat = 0.0\n            bl_lon = 0.0\n        phone_df[\"baseline_lat\"] = phone_df[\"baseline_lat\"].fillna(bl_lat)\n        phone_df[\"baseline_lon\"] = phone_df[\"baseline_lon\"].fillna(bl_lon)\n\n    phone_df[\"phone\"] = phone_key\n\n    # Ensure all feature columns exist\n    for c in feature_cols:\n        if c not in phone_df.columns:\n            phone_df[c] = np.nan\n\n    phone_df = phone_df.sort_values(\"UnixTimeMillis\")\n    # Fill NaNs in features with column medians from train\n    train_medians = train_df[feature_cols].median()\n    phone_df[feature_cols] = phone_df[feature_cols].fillna(train_medians)\n\n    X_test_phone = phone_df[feature_cols].values\n    pred_dlat = model_dlat_full.predict(X_test_phone)\n    pred_dlon = model_dlon_full.predict(X_test_phone)\n\n    phone_df[\"LatitudeDegrees\"] = phone_df[\"baseline_lat\"] + pred_dlat\n    phone_df[\"LongitudeDegrees\"] = phone_df[\"baseline_lon\"] + pred_dlon\n\n    test_pred_list.append(\n        phone_df[[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]]\n    )\n\nif len(test_pred_list) == 0:\n    # Fallback: just copy sample submission with zeros (should not happen)\n    submission = sample_sub.copy()\n    submission[\"LatitudeDegrees\"] = 0.0\n    submission[\"LongitudeDegrees\"] = 0.0\nelse:\n    preds_all = pd.concat(test_pred_list, ignore_index=True)\n    # Merge back into sample_submission to ensure exact rows/order\n    submission = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    submission = submission.merge(preds_all, on=[\"phone\", \"UnixTimeMillis\"], how=\"left\")\n    # For any missing predictions, fallback to simple interpolation or carry-forward; for now, carry forward then backward\n    submission[\"LatitudeDegrees\"] = (\n        submission[\"LatitudeDegrees\"]\n        .interpolate()\n        .fillna(method=\"bfill\")\n        .fillna(method=\"ffill\")\n    )\n    submission[\"LongitudeDegrees\"] = (\n        submission[\"LongitudeDegrees\"]\n        .interpolate()\n        .fillna(method=\"bfill\")\n        .fillna(method=\"ffill\")\n    )\n\nsubmission.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"Saved submission to {SUBMISSION_PATH}\")\n", "import os\nimport glob\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nWGS84_A = 6378137.0\nWGS84_E2 = 6.69437999014e-3\n\n\ndef geodetic_to_ecef(lat_deg, lon_deg, alt_m):\n    lat = np.radians(lat_deg)\n    lon = np.radians(lon_deg)\n    a = WGS84_A\n    e2 = WGS84_E2\n    sin_lat = np.sin(lat)\n    cos_lat = np.cos(lat)\n    N = a / np.sqrt(1 - e2 * sin_lat * sin_lat)\n    X = (N + alt_m) * cos_lat * np.cos(lon)\n    Y = (N + alt_m) * cos_lat * np.sin(lon)\n    Z = (N * (1 - e2) + alt_m) * sin_lat\n    return X, Y, Z\n\n\ndef ecef_to_geodetic(x, y, z):\n    # iterative Bowring's method\n    a = WGS84_A\n    e2 = WGS84_E2\n    b = a * np.sqrt(1 - e2)\n    ep2 = (a * a - b * b) / (b * b)\n    p = np.sqrt(x * x + y * y)\n    th = np.arctan2(a * z, b * p)\n    lon = np.arctan2(y, x)\n    lat = np.arctan2(z + ep2 * b * np.sin(th) ** 3, p - e2 * a * np.cos(th) ** 3)\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    alt = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), alt\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    # df must have columns: lat_gt, lon_gt, lat_pred, lon_pred, phone\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    # basic cleaning\n    gt = gt.dropna(\n        subset=[\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"AltitudeMeters\",\n            \"UnixTimeMillis\",\n        ]\n    )\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n    # Derive epoch time in ms to join with ground truth\n    # ArrivalTimeNanosSinceGpsEpoch is in ns since GPS epoch; convert to ms and approximate UnixTimeMillis via shifting with median offset\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    # approximate mapping between gps_ms and gt.UnixTimeMillis via nearest neighbor on utcTimeMillis if exists\n    if \"utcTimeMillis\" in gnss.columns:\n        # compute offset: median(utcTimeMillis - gps_ms)\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        # fallback: set offset so that min aligns with gt\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # aggregate gnss by epoch\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update(\n        {\n            \"Svid\": \"nunique\",\n            \"SignalType\": \"nunique\" if \"SignalType\" in gnss.columns else \"count\",\n        }\n    )\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    # flatten columns\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # join with ground truth\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # target: ECEF from ground truth\n    X_ecef, Y_ecef, Z_ecef = geodetic_to_ecef(\n        df[\"LatitudeDegrees\"].values,\n        df[\"LongitudeDegrees\"].values,\n        df[\"AltitudeMeters\"].values,\n    )\n    df[\"X_ecef\"] = X_ecef\n    df[\"Y_ecef\"] = Y_ecef\n    df[\"Z_ecef\"] = Z_ecef\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update(\n        {\n            \"Svid\": \"nunique\",\n            \"SignalType\": \"nunique\" if \"SignalType\" in gnss.columns else \"count\",\n        }\n    )\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\n\n# build sorted time and simple deltas per phone\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\ntime_col = \"UnixTimeMillis\"\nfor feat in [\n    c\n    for c in train_df.columns\n    if c.endswith(\"_mean\") or c.endswith(\"_std\") or \"WlsPosition\" in c\n]:\n    if feat in [\n        \"WlsPositionXEcefMeters_mean\",\n        \"WlsPositionYEcefMeters_mean\",\n        \"WlsPositionZEcefMeters_mean\",\n        \"WlsPositionXEcefMeters_std\",\n        \"WlsPositionYEcefMeters_std\",\n        \"WlsPositionZEcefMeters_std\",\n    ]:\n        pass\n    # compute temporal diff\nfor feat in [c for c in train_df.columns if c.endswith(\"_mean\")]:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# feature columns: all aggregated gnss stats and their diffs except targets & labels\nexclude_cols = set(\n    [\n        \"LatitudeDegrees\",\n        \"LongitudeDegrees\",\n        \"AltitudeMeters\",\n        \"X_ecef\",\n        \"Y_ecef\",\n        \"Z_ecef\",\n        \"MessageType\",\n        \"SpeedMps\",\n        \"AccuracyMeters\",\n        \"BearingDegrees\",\n    ]\n)\nexclude_cols.update([\"phone\", \"UnixTimeMillis\"])\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        c.endswith(\"_mean\")\n        or c.endswith(\"_std\")\n        or c.endswith(\"_diff\")\n        or \"WlsPosition\" in c\n    )\n]\n\n# fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_x = train_df[\"X_ecef\"].values\ny_y = train_df[\"Y_ecef\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_preds_x = np.zeros(len(train_df))\noof_preds_y = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n    yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_x = LGBMRegressor(**params)\n    model_y = LGBMRegressor(**params)\n\n    model_x.fit(\n        X_tr, yx_tr, eval_set=[(X_val, yx_val)], eval_metric=\"l2\", verbose=False\n    )\n    model_y.fit(\n        X_tr, yy_tr, eval_set=[(X_val, yy_val)], eval_metric=\"l2\", verbose=False\n    )\n\n    oof_preds_x[val_idx] = model_x.predict(X_val)\n    oof_preds_y[val_idx] = model_y.predict(X_val)\n\n# build oof dataframe for metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\nlat_pred, lon_pred, _ = ecef_to_geodetic(\n    oof_preds_x, oof_preds_y, train_df[\"Z_ecef\"].values\n)\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = lat_pred\noof_df[\"lon_pred\"] = lon_pred\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_x = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_y = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_x.fit(X, y_x)\nfinal_model_y.fit(X, y_y)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))\nsample_sub[\"phone_short\"] = sample_sub[\"phone\"].apply(lambda x: x.split(\"_\")[-1])\nsample_sub[\"drive\"] = sample_sub[\"phone\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception:\n        continue\n\ntest_gnss = (\n    pd.concat(test_records, ignore_index=True) if test_records else pd.DataFrame()\n)\n# map phone name format: drive_phone to original\ntest_gnss[\"phone_full\"] = test_gnss[\"phone\"].apply(\n    lambda x: x.replace(\"_\", \"/\", 1).replace(\"_\", \"_\", 1)\n)\n\n\n# But sample uses \"YYYY-MM-DD-..._PhoneName\"; easier: construct same as sample\ndef phone_from_drive_phone(dp):\n    drive, phone = dp.split(\"_\", 1)\n    return f\"{drive}_{phone}\"\n\n\ntest_gnss[\"phone\"] = test_gnss[\"phone\"].apply(phone_from_drive_phone)\n\n# we just need features at timestamps in sample_submission: merge on phone & UnixTimeMillis (approx)\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\n# prepare frame with required rows\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", \"UnixTimeMillis\"]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# if some rows missing features, forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\n# still any NaNs: fill by train medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\npred_x = final_model_x.predict(X_test)\npred_y = final_model_y.predict(X_test)\n# assume alt approx mean of train alt\nmean_alt = train_df[\"AltitudeMeters\"].mean()\npred_z = np.full_like(\n    pred_x,\n    geodetic_to_ecef(\n        train_df[\"LatitudeDegrees\"].mean(),\n        train_df[\"LongitudeDegrees\"].mean(),\n        mean_alt,\n    )[2],\n)\npred_lat, pred_lon, _ = ecef_to_geodetic(pred_x, pred_y, pred_z)\n\nsubmission = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n", "import os\nimport glob\nimport math\nimport json\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\n# Utility: haversine distance in meters\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lat2 = np.radians(lat2)\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2 - lon1)\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\n# Competition-like metric: per phone mean of 50th and 95th percentile distance errors, then mean across phones\ndef competition_metric(df):\n    # df: columns [phone, dist]\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\n# Load all train device_gnss and ground_truth, aggregate per epoch\ndef load_train_data(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    all_rows = []\n    for drive_dir in drive_dirs:\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_name = os.path.basename(phone_dir)\n            drive_id = os.path.basename(drive_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n            try:\n                gnss = pd.read_csv(gnss_path)\n            except Exception:\n                continue\n            try:\n                gt = pd.read_csv(gt_path)\n            except Exception:\n                continue\n\n            # Clean/prepare\n            # device_gnss utcTimeMillis -> UnixTimeMillis (they are utc epoch), align to gt.UnixTimeMillis\n            # We'll round gnss utcTimeMillis to nearest millisecond to improve join robustness\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"].astype(\"int64\")\n\n            # Aggregate per UnixTimeMillis\n            # Select useful numeric columns\n            cols = [\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"SvElevationDegrees\",\n                \"SvAzimuthDegrees\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateMetersPerSecond\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                \"IonosphericDelayMeters\",\n                \"TroposphericDelayMeters\",\n            ]\n            agg_df = gnss[[c for c in cols if c in gnss.columns]].copy()\n            # Basic aggregation: mean for everything except maybe Cn0 (also mean)\n            agg = agg_df.groupby(\"UnixTimeMillis\").agg(\"mean\").reset_index()\n\n            # Merge with ground truth by UnixTimeMillis\n            merged = pd.merge(\n                agg,\n                gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]],\n                on=\"UnixTimeMillis\",\n                how=\"inner\",\n            )\n            if merged.empty:\n                continue\n            merged[\"phone\"] = phone_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        raise RuntimeError(\"No training data assembled.\")\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\n# Build features and targets\ndef prepare_features_targets(train_df):\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in [\"LatitudeDegrees\", \"LongitudeDegrees\", \"phone\"]\n    ]\n    X = train_df[feature_cols].copy()\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n    return X, y_lat, y_lon, feature_cols\n\n\n# Train per-phone models with 5-fold group CV over drives\ndef train_and_validate(train_df):\n    # Extract drive_id from phone string: driveId_phoneModel\n    train_df = train_df.copy()\n    train_df[\"drive\"] = train_df[\"phone\"].apply(\n        lambda x: x.split(\"_\")[0] if \"_\" in x else x\n    )\n    X, y_lat, y_lon, feature_cols = prepare_features_targets(train_df)\n\n    # Handle missing values\n    X = X.fillna(X.median())\n\n    groups = train_df[\"drive\"].values\n    gkf = GroupKFold(n_splits=5)\n    oof_preds = []\n    models = []\n\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        # RandomForest is simple and robust baseline\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=20, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=20, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        pred_lat = rf_lat.predict(X_val)\n        pred_lon = rf_lon.predict(X_val)\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"gt_lat\": ylat_val,\n                \"gt_lon\": ylon_val,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_preds.append(fold_df)\n        models.append((rf_lat, rf_lon))\n\n    oof = pd.concat(oof_preds, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return models, feature_cols, metric, train_df\n\n\n# For final training, fit on all data\ndef fit_full_models(train_df, feature_cols):\n    X = train_df[feature_cols].copy().fillna(train_df[feature_cols].median())\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=22, n_jobs=-1, random_state=100\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=22, n_jobs=-1, random_state=101\n    )\n    rf_lat.fit(X, y_lat)\n    rf_lon.fit(X, y_lon)\n    return rf_lat, rf_lon\n\n\n# Prepare test data aligned to sample_submission\ndef load_test_features(input_dir, feature_cols, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n    )\n\n    all_agg = []\n    for drive_dir in drive_dirs:\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_name = os.path.basename(phone_dir)\n            drive_id = os.path.basename(drive_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            gnss = pd.read_csv(gnss_path)\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"].astype(\"int64\")\n\n            cols_needed = [\n                c for c in feature_cols if c in gnss.columns or c == \"UnixTimeMillis\"\n            ]\n            # Ensure UnixTimeMillis is included\n            if \"UnixTimeMillis\" not in cols_needed:\n                cols_needed = [\"UnixTimeMillis\"] + cols_needed\n            agg_df = gnss[[c for c in cols_needed if c in gnss.columns]].copy()\n            agg = agg_df.groupby(\"UnixTimeMillis\").agg(\"mean\").reset_index()\n            agg[\"phone\"] = phone_id\n            all_agg.append(agg)\n\n    if not all_agg:\n        raise RuntimeError(\"No test gnss data found.\")\n    test_agg = pd.concat(all_agg, ignore_index=True)\n\n    # Align to sample_submission phone/time\n    # sample_submission phone column is like \"2020-06-04-US-MTV-1_Pixel4\" (not full phone name from folder)\n    # Our phone_id is driveId_phoneName (GooglePixel4). We need mapping between folder name and sample phone suffix.\n    # Easiest: create mapping from sample phone to closest folder by drive id.\n    # Extract drive id from sample phone\n    sample = sample_sub.copy()\n    sample[\"drive\"] = sample[\"phone\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n    sample[\"model_suffix\"] = sample[\"phone\"].apply(lambda x: x.split(\"_\")[-1])\n\n    # For test_agg phone, extract drive and modelFolder; we will approximate mapping by drive\n    test_agg[\"drive\"] = test_agg[\"phone\"].apply(lambda x: x.split(\"_\")[0])\n    test_agg[\"modelFolder\"] = test_agg[\"phone\"].apply(\n        lambda x: \"_\".join(x.split(\"_\")[1:])\n    )\n\n    # Build a mapping from (drive, modelSuffix) -> list of phones in test_agg; choose one deterministically\n    # Map known Android names to sample suffixes\n    name_map = {\n        \"GooglePixel4\": \"Pixel4\",\n        \"GooglePixel4XL\": \"Pixel4XL\",\n        \"GooglePixel5\": \"Pixel5\",\n        \"SamsungGalaxyS20Ultra\": \"SamsungS20Ultra\",\n        \"XiaomiMi8\": \"Mi8\",\n    }\n    test_agg[\"sample_model_suffix\"] = test_agg[\"modelFolder\"].map(name_map)\n\n    # Merge sample with test_agg on drive and sample_model_suffix\n    merged = pd.merge(\n        sample.rename(columns={\"phone\": \"sample_phone\"}),\n        test_agg,\n        left_on=[\"drive\", \"model_suffix\"],\n        right_on=[\"drive\", \"sample_model_suffix\"],\n        how=\"left\",\n        suffixes=(\"_sample\", \"_gnss\"),\n    )\n\n    # Now we have many gnss epochs per (sample row) because of time dimension.\n    # We need nearest UnixTimeMillis in gnss to sample.UnixTimeMillis.\n    def nearest_epoch(group):\n        # group: rows with same sample index\n        target_time = group[\"UnixTimeMillis_sample\"].iloc[0]\n        # compute abs diff to gnss UnixTimeMillis (column UnixTimeMillis)\n        diffs = (group[\"UnixTimeMillis\"] - target_time).abs()\n        idx = diffs.idxmin()\n        return group.loc[[idx]]\n\n    # First, filter rows where we have a UnixTimeMillis from gnss\n    merged = merged.dropna(subset=[\"UnixTimeMillis\"])\n    # group by original sample row (use the index of sample in merged)\n    merged[\"row_id\"] = merged.index\n    # aggregate by sample_phone & UnixTimeMillis_sample & row_id\n    grouped = merged.groupby(\"row_id\", group_keys=False).apply(nearest_epoch)\n\n    # Construct feature matrix for prediction\n    feature_df = grouped.copy()\n    # rebuild submission index order\n    feature_df = feature_df.sort_values(\"row_id\")\n    X_test = pd.DataFrame(index=feature_df[\"row_id\"].values)\n    for c in feature_cols:\n        if c in feature_df.columns:\n            X_test[c] = feature_df[c].values\n        else:\n            # if feature doesn't exist in test, fill with median from train (0 as placeholder; will replace later)\n            X_test[c] = np.nan\n    meta = feature_df[[\"row_id\", \"sample_phone\", \"UnixTimeMillis_sample\"]].copy()\n    meta = meta.rename(\n        columns={\"sample_phone\": \"phone\", \"UnixTimeMillis_sample\": \"UnixTimeMillis\"}\n    )\n    return X_test, meta\n\n\ndef main():\n    # Load training data\n    train_df = load_train_data(INPUT_DIR)\n\n    # Train with CV and compute validation metric\n    models_cv, feature_cols, val_metric, train_df_full = train_and_validate(train_df)\n    print(\"Validation metric (mean of per-phone (P50+P95)/2):\", val_metric)\n\n    # Fit final models on full train data\n    rf_lat, rf_lon = fit_full_models(train_df_full, feature_cols)\n\n    # Prepare test features aligned to sample_submission\n    sample_sub = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))\n    X_test_raw, meta = load_test_features(INPUT_DIR, feature_cols, sample_sub)\n\n    # Fill missing test features with train medians\n    train_medians = train_df_full[feature_cols].median()\n    X_test = X_test_raw.copy()\n    for c in feature_cols:\n        if c not in X_test.columns:\n            X_test[c] = train_medians[c]\n    X_test = X_test.fillna(train_medians)\n\n    # Predict\n    pred_lat = rf_lat.predict(X_test[feature_cols])\n    pred_lon = rf_lon.predict(X_test[feature_cols])\n\n    # Build submission aligned with sample_submission order\n    submission = sample_sub.copy()\n    submission[\"row_id\"] = np.arange(len(submission))\n    # Merge predictions by row_id\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": meta[\"row_id\"].values,\n            \"LatitudeDegrees\": pred_lat,\n            \"LongitudeDegrees\": pred_lon,\n        }\n    )\n    submission = pd.merge(submission, pred_df, on=\"row_id\", how=\"left\")\n\n    # If any predictions missing (due to alignment issues), simple forward fill within phone\n    submission.sort_values([\"phone\", \"UnixTimeMillis\"], inplace=True)\n    submission[\"LatitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LatitudeDegrees\"].ffill().bfill()\n    )\n    submission[\"LongitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LongitudeDegrees\"].ffill().bfill()\n    )\n\n    # Drop helper columns\n    submission = submission[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ]\n\n    # Save submission\n    out_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(out_path, index=False)\n    print(\"Saved submission to\", out_path)\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ---------------- Utility functions ---------------- #\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    \"\"\"Vectorized haversine distance in meters.\"\"\"\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    \"\"\"Mean of per-phone (P50+P95)/2 of distance errors.\"\"\"\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\n# ---------------- Data loading ---------------- #\n\n\ndef load_train_data(input_dir):\n    \"\"\"Load train device_gnss + ground_truth and aggregate useful features per UnixTimeMillis.\"\"\"\n    train_root = os.path.join(input_dir, \"train\")\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    all_rows = []\n\n    for drive_dir in drive_dirs:\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_name = os.path.basename(phone_dir)  # e.g., GooglePixel4\n            drive_id = os.path.basename(drive_dir)  # e.g., 2020-06-04-US-MTV-1\n            phone_id = (\n                f\"{drive_id}_{phone_name}\"  # e.g., 2020-06-04-US-MTV-1_GooglePixel4\n            )\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception:\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            # Align times\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"].astype(\"int64\")\n\n            # Select a reasonable subset of numeric columns\n            base_cols = [\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"SvElevationDegrees\",\n                \"SvAzimuthDegrees\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateMetersPerSecond\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                \"IonosphericDelayMeters\",\n                \"TroposphericDelayMeters\",\n            ]\n            cols = [c for c in base_cols if c in gnss.columns]\n            if \"UnixTimeMillis\" not in cols:\n                cols = [\"UnixTimeMillis\"] + cols\n\n            agg_df = gnss[cols].copy()\n            agg = agg_df.groupby(\"UnixTimeMillis\").agg(\"mean\").reset_index()\n\n            merged = pd.merge(\n                agg,\n                gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]],\n                on=\"UnixTimeMillis\",\n                how=\"inner\",\n            )\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        raise RuntimeError(\"No training data assembled.\")\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\ndef prepare_features_targets(train_df):\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in [\"LatitudeDegrees\", \"LongitudeDegrees\", \"phone\", \"drive\"]\n    ]\n    X = train_df[feature_cols].copy()\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n    return X, y_lat, y_lon, feature_cols\n\n\ndef train_and_validate(train_df):\n    \"\"\"5-fold GroupKFold on drive_id; returns RF models list, feature_cols, metric, and augmented train_df.\"\"\"\n    train_df = train_df.copy()\n    # derive drive id from phone string: driveId_phoneModel\n    train_df[\"drive\"] = train_df[\"phone\"].apply(\n        lambda x: x.split(\"_\")[0] if \"_\" in x else x\n    )\n\n    X, y_lat, y_lon, feature_cols = prepare_features_targets(train_df)\n    # Fill NaNs with medians\n    medians = X.median()\n    X = X.fillna(medians)\n\n    groups = train_df[\"drive\"].values\n    gkf = GroupKFold(n_splits=5)\n    oof_list = []\n    models = []\n\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=20, n_jobs=-1, random_state=42 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=20, n_jobs=-1, random_state=142 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        pred_lat = rf_lat.predict(X_val)\n        pred_lon = rf_lon.predict(X_val)\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"gt_lat\": ylat_val,\n                \"gt_lon\": ylon_val,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n        models.append((rf_lat, rf_lon))\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return models, feature_cols, metric, train_df\n\n\ndef fit_full_models(train_df, feature_cols):\n    \"\"\"Train final models on all training data.\"\"\"\n    X = train_df[feature_cols].copy()\n    medians = X.median()\n    X = X.fillna(medians)\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=22, n_jobs=-1, random_state=100\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=22, n_jobs=-1, random_state=101\n    )\n    rf_lat.fit(X, y_lat)\n    rf_lon.fit(X, y_lon)\n    return rf_lat, rf_lon, medians\n\n\ndef load_test_features_simple(input_dir, feature_cols, sample_sub):\n    \"\"\"\n    Simple and robust test feature builder:\n    - Build a mapping from sample phone name to folder phone name using known suffix map.\n    - Aggregate device_gnss per UnixTimeMillis for each physical phone.\n    - Directly merge aggregated features to sample_sub on (phone, UnixTimeMillis).\n      For sample phones, we translate to the folder phone and back after merging.\n    \"\"\"\n    # Map between sample phone suffix and folder model name\n    suffix_to_folder = {\n        \"Pixel4\": \"GooglePixel4\",\n        \"Pixel4XL\": \"GooglePixel4XL\",\n        \"Pixel5\": \"GooglePixel5\",\n        \"SamsungS20Ultra\": \"SamsungGalaxyS20Ultra\",\n        \"Mi8\": \"XiaomiMi8\",\n    }\n\n    # Parse sample phones\n    sample = sample_sub.copy()\n    sample[\"drive\"] = sample[\"phone\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n    sample[\"suffix\"] = sample[\"phone\"].apply(lambda x: x.split(\"_\")[-1])\n    sample[\"folder_model\"] = sample[\"suffix\"].map(suffix_to_folder)\n    # folder phone key to join with gnss aggregation\n    sample[\"folder_phone\"] = sample[\"drive\"] + \"_\" + sample[\"folder_model\"]\n\n    # Aggregate gnss features per folder_phone & UnixTimeMillis\n    test_root = os.path.join(input_dir, \"test\")\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n    )\n    agg_list = []\n\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)  # e.g., GooglePixel4\n            folder_phone = f\"{drive_id}_{phone_model}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            try:\n                gnss = pd.read_csv(gnss_path)\n            except Exception:\n                continue\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"].astype(\"int64\")\n\n            cols_needed = [\"UnixTimeMillis\"]\n            for c in feature_cols:\n                if c != \"UnixTimeMillis\" and c in gnss.columns:\n                    cols_needed.append(c)\n            cols_needed = list(dict.fromkeys(cols_needed))  # unique preserving order\n\n            agg_df = gnss[cols_needed].copy()\n            agg = agg_df.groupby(\"UnixTimeMillis\").agg(\"mean\").reset_index()\n            agg[\"folder_phone\"] = folder_phone\n            agg_list.append(agg)\n\n    if not agg_list:\n        raise RuntimeError(\"No test GNSS data found.\")\n    test_agg = pd.concat(agg_list, ignore_index=True)\n\n    # Merge sample with test_agg on (folder_phone, UnixTimeMillis)\n    sample_with_idx = sample.copy()\n    sample_with_idx[\"row_id\"] = np.arange(len(sample_with_idx))\n\n    merged = pd.merge(\n        sample_with_idx,\n        test_agg,\n        how=\"left\",\n        on=[\"folder_phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    # Build X_test with one row per sample row_id, in original order\n    merged = merged.sort_values(\"row_id\")\n    X_test = pd.DataFrame(index=merged[\"row_id\"].values)\n    for c in feature_cols:\n        if c in merged.columns:\n            X_test[c] = merged[c].values\n        else:\n            X_test[c] = np.nan\n\n    meta = merged[[\"row_id\", \"phone\", \"UnixTimeMillis\"]].copy()\n    meta = meta.rename(columns={\"phone\": \"phone\", \"UnixTimeMillis\": \"UnixTimeMillis\"})\n    return X_test, meta\n\n\n# ---------------- Main script ---------------- #\n\n\ndef main():\n    # Load train data\n    train_df = load_train_data(INPUT_DIR)\n\n    # Train & validate\n    models_cv, feature_cols, val_metric, train_df_full = train_and_validate(train_df)\n    print(\"5-fold CV competition metric (mean of per-phone (P50+P95)/2):\", val_metric)\n\n    # Fit final models\n    rf_lat, rf_lon, train_medians = fit_full_models(train_df_full, feature_cols)\n\n    # Prepare test features\n    sample_sub = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))\n    X_test_raw, meta = load_test_features_simple(INPUT_DIR, feature_cols, sample_sub)\n\n    # Ensure all feature columns are present and filled\n    X_test = X_test_raw.copy()\n    for c in feature_cols:\n        if c not in X_test.columns:\n            X_test[c] = train_medians.get(c, 0.0)\n    X_test = X_test.fillna(train_medians)\n\n    # Predict\n    pred_lat = rf_lat.predict(X_test[feature_cols])\n    pred_lon = rf_lon.predict(X_test[feature_cols])\n\n    # Build submission following sample_submission order\n    submission = sample_sub.copy()\n    submission[\"row_id\"] = np.arange(len(submission))\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": meta[\"row_id\"].values,\n            \"LatitudeDegrees\": pred_lat,\n            \"LongitudeDegrees\": pred_lon,\n        }\n    )\n\n    submission = pd.merge(submission, pred_df, on=\"row_id\", how=\"left\")\n\n    # If some rows didn't get predictions (due to missing gnss rows), forward/back fill within each phone\n    submission.sort_values([\"phone\", \"UnixTimeMillis\"], inplace=True)\n    submission[\"LatitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LatitudeDegrees\"].ffill().bfill()\n    )\n    submission[\"LongitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LongitudeDegrees\"].ffill().bfill()\n    )\n\n    # Final columns and order back to original submission index\n    submission = submission[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ]\n    submission = submission.reset_index(drop=True)\n\n    # Save submission in both required locations\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    submission.to_csv(out_path_submission, index=False)\n    submission.to_csv(out_path_working, index=False)\n    print(\"Saved submission to:\", out_path_submission, \"and\", out_path_working)\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\n# Directories\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n# WGS84 constants\nA_EARTH = 6378137.0\nE2 = 6.69437999014e-3\n\n\ndef ecef_to_latlon(x, y, z):\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1.0 - E2))\n    for _ in range(5):\n        sin_lat = np.sin(lat)\n        N = A_EARTH / np.sqrt(1.0 - E2 * sin_lat**2)\n        lat = np.arctan2(z + E2 * N * sin_lat, p)\n    return np.degrees(lat), np.degrees(lon)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    return float(np.mean(errors))\n\n\ndef load_train():\n    train_root = os.path.join(INPUT_DIR, \"train\")\n    drive_dirs = sorted(glob.glob(os.path.join(train_root, \"*\")))\n    records = []\n    for drive_dir in drive_dirs:\n        if not os.path.isdir(drive_dir):\n            continue\n        phone_dirs = sorted(glob.glob(os.path.join(drive_dir, \"*\")))\n        for phone_dir in phone_dirs:\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            drive_id = os.path.basename(drive_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n                continue\n            gt = pd.read_csv(gt_path)\n            gnss = pd.read_csv(gnss_path)\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not all(c in gnss.columns for c in wls_cols):\n                continue\n\n            gnss = gnss.dropna(subset=wls_cols).copy()\n            if gnss.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = (gnss[\"utcTimeMillis\"] // 1000 * 1000).astype(\n                \"int64\"\n            )\n\n            agg_dict = {\n                \"WlsPositionXEcefMeters\": \"mean\",\n                \"WlsPositionYEcefMeters\": \"mean\",\n                \"WlsPositionZEcefMeters\": \"mean\",\n                \"Cn0DbHz\": \"mean\",\n                \"PseudorangeRateMetersPerSecond\": \"mean\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                \"SvElevationDegrees\": \"mean\",\n                \"SvAzimuthDegrees\": \"mean\",\n                \"SvClockBiasMeters\": \"mean\",\n                \"SvClockDriftMetersPerSecond\": \"mean\",\n                \"IonosphericDelayMeters\": \"mean\",\n                \"TroposphericDelayMeters\": \"mean\",\n                \"IsrbMeters\": \"mean\",\n                \"RawPseudorangeMeters\": \"mean\",\n            }\n            agg_dict = {k: v for k, v in agg_dict.items() if k in gnss.columns}\n            gnss_agg = gnss.groupby(\"UnixTimeMillis\", as_index=False).agg(agg_dict)\n            if gnss_agg.empty:\n                continue\n\n            merged = pd.merge(gt, gnss_agg, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            lat_wls, lon_wls = ecef_to_latlon(\n                merged[\"WlsPositionXEcefMeters\"].values,\n                merged[\"WlsPositionYEcefMeters\"].values,\n                merged[\"WlsPositionZEcefMeters\"].values,\n            )\n            merged[\"wls_lat\"] = lat_wls\n            merged[\"wls_lon\"] = lon_wls\n            merged[\"phone\"] = phone_id\n            records.append(merged)\n\n    if not records:\n        raise RuntimeError(\"No training data found.\")\n    train_df = pd.concat(records, ignore_index=True)\n    return train_df\n\n\ndef prepare_features(train_df):\n    df = train_df.copy()\n    df[\"target_dlat\"] = df[\"LatitudeDegrees\"] - df[\"wls_lat\"]\n    df[\"target_dlon\"] = df[\"LongitudeDegrees\"] - df[\"wls_lon\"]\n\n    feature_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"IsrbMeters\",\n        \"RawPseudorangeMeters\",\n    ]\n    feature_cols = [c for c in feature_cols if c in df.columns]\n    X = df[feature_cols].copy()\n    X = X.fillna(X.median())\n    y_dlat = df[\"target_dlat\"].values\n    y_dlon = df[\"target_dlon\"].values\n    return X, y_dlat, y_dlon, feature_cols, df\n\n\ndef cross_validate(X, y_dlat, y_dlon, info_df):\n    gkf = GroupKFold(n_splits=5)\n    groups = info_df[\"phone\"].values\n    scores = []\n    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        dlat_tr, dlat_va = y_dlat[tr_idx], y_dlat[va_idx]\n        dlon_tr, dlon_va = y_dlon[tr_idx], y_dlon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        train_set_dlat = lgb.Dataset(X_tr, label=dlat_tr)\n        valid_set_dlat = lgb.Dataset(X_va, label=dlat_va, reference=train_set_dlat)\n        model_dlat = lgb.train(\n            params,\n            train_set_dlat,\n            num_boost_round=500,\n            valid_sets=[valid_set_dlat],\n            early_stopping_rounds=50,\n            verbose_eval=False,\n        )\n\n        train_set_dlon = lgb.Dataset(X_tr, label=dlon_tr)\n        valid_set_dlon = lgb.Dataset(X_va, label=dlon_va, reference=train_set_dlon)\n        model_dlon = lgb.train(\n            params,\n            train_set_dlon,\n            num_boost_round=500,\n            valid_sets=[valid_set_dlon],\n            early_stopping_rounds=50,\n            verbose_eval=False,\n        )\n\n        va = info_df.iloc[va_idx].copy()\n        va_pred_dlat = model_dlat.predict(X_va, num_iteration=model_dlat.best_iteration)\n        va_pred_dlon = model_dlon.predict(X_va, num_iteration=model_dlon.best_iteration)\n        va[\"pred_lat\"] = va[\"wls_lat\"] + va_pred_dlat\n        va[\"pred_lon\"] = va[\"wls_lon\"] + va_pred_dlon\n        va = va.rename(\n            columns={\"LatitudeDegrees\": \"gt_lat\", \"LongitudeDegrees\": \"gt_lon\"}\n        )\n        score = competition_metric(\n            va[[\"phone\", \"gt_lat\", \"gt_lon\", \"pred_lat\", \"pred_lon\"]]\n        )\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n    print(f\"CV mean metric: {np.mean(scores):.6f}, std: {np.std(scores):.6f}\")\n    return float(np.mean(scores))\n\n\ndef train_full_models(X, y_dlat, y_dlon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_dlat = lgb.Dataset(X, label=y_dlat)\n    dtrain_dlon = lgb.Dataset(X, label=y_dlon)\n    model_dlat = lgb.train(params, dtrain_dlat, num_boost_round=400)\n    model_dlon = lgb.train(params, dtrain_dlon, num_boost_round=400)\n    return model_dlat, model_dlon\n\n\ndef load_test_features(feature_cols):\n    test_root = os.path.join(INPUT_DIR, \"test\")\n    drive_dirs = sorted(glob.glob(os.path.join(test_root, \"*\")))\n    records = []\n    for drive_dir in drive_dirs:\n        if not os.path.isdir(drive_dir):\n            continue\n        phone_dirs = sorted(glob.glob(os.path.join(drive_dir, \"*\")))\n        for phone_dir in phone_dirs:\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            drive_id = os.path.basename(drive_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            gnss = pd.read_csv(gnss_path)\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not all(c in gnss.columns for c in wls_cols):\n                continue\n\n            gnss = gnss.dropna(subset=wls_cols).copy()\n            if gnss.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = (gnss[\"utcTimeMillis\"] // 1000 * 1000).astype(\n                \"int64\"\n            )\n\n            agg_dict = {\n                \"WlsPositionXEcefMeters\": \"mean\",\n                \"WlsPositionYEcefMeters\": \"mean\",\n                \"WlsPositionZEcefMeters\": \"mean\",\n            }\n            for c in feature_cols:\n                if c not in agg_dict and c in gnss.columns:\n                    agg_dict[c] = \"mean\"\n\n            gnss_agg = gnss.groupby(\"UnixTimeMillis\", as_index=False).agg(agg_dict)\n            if gnss_agg.empty:\n                continue\n\n            lat_wls, lon_wls = ecef_to_latlon(\n                gnss_agg[\"WlsPositionXEcefMeters\"].values,\n                gnss_agg[\"WlsPositionYEcefMeters\"].values,\n                gnss_agg[\"WlsPositionZEcefMeters\"].values,\n            )\n            gnss_agg[\"wls_lat\"] = lat_wls\n            gnss_agg[\"wls_lon\"] = lon_wls\n            gnss_agg[\"phone\"] = phone_id\n            records.append(gnss_agg)\n\n    if not records:\n        raise RuntimeError(\"No test GNSS data loaded.\")\n    test_df = pd.concat(records, ignore_index=True)\n    return test_df\n\n\ndef map_sample_phone_to_test(sample_phone, test_phones):\n    # sample phone looks like \"2020-06-04-US-MTV-1_Pixel4\"\n    parts = sample_phone.split(\"_\")\n    if len(parts) < 2:\n        return None\n    # drive id is first 4 dash-separated chunks, e.g. 2020-06-04-US-MTV-1\n    # sample \"2020-06-04-US-MTV-1_Pixel4\" -> drive: \"2020-06-04-US-MTV-1\"\n    drive = \"_\".join(parts[0:4])\n    model = parts[4] if len(parts) > 4 else \"\"\n    candidates = [p for p in test_phones if p.startswith(drive + \"_\")]\n    if not candidates:\n        return None\n    if len(candidates) == 1:\n        return candidates[0]\n    model_l = model.lower()\n    for p in candidates:\n        dev = p.split(\"_\", 1)[1].lower()\n        if model_l in dev or dev in model_l:\n            return p\n    return candidates[0]\n\n\ndef main():\n    train_df = load_train()\n    X, y_dlat, y_dlon, feature_cols, info_df = prepare_features(train_df)\n    cv_score = cross_validate(X, y_dlat, y_dlon, info_df)\n    print(f\"Validation competition-like metric (5-fold): {cv_score:.6f}\")\n\n    model_dlat, model_dlon = train_full_models(X, y_dlat, y_dlon)\n\n    sample_sub = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))\n    test_gnss_df = load_test_features(feature_cols)\n\n    test_gnss_df[\"UnixTimeMillis\"] = test_gnss_df[\"UnixTimeMillis\"].astype(\"int64\")\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    test_phones = sorted(test_gnss_df[\"phone\"].unique())\n    sample_sub[\"our_phone\"] = sample_sub[\"phone\"].apply(\n        lambda p: map_sample_phone_to_test(p, test_phones)\n    )\n\n    test_gnss_df_renamed = test_gnss_df.rename(columns={\"phone\": \"our_phone\"})\n    merged = pd.merge(\n        sample_sub,\n        test_gnss_df_renamed,\n        on=[\"our_phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n\n    # Fill missing GNSS rows per phone with nearest available (ffill + bfill) using transform\n    cols_to_fill = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"wls_lat\",\n        \"wls_lon\",\n    ] + [\n        c\n        for c in feature_cols\n        if c\n        not in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ]\n    cols_to_fill = [c for c in cols_to_fill if c in merged.columns]\n\n    merged = merged.sort_values([\"our_phone\", \"UnixTimeMillis\"])\n    for c in cols_to_fill:\n        merged[c] = merged.groupby(\"our_phone\")[c].transform(\n            lambda s: s.ffill().bfill()\n        )\n\n    # Fallback: if still missing wls_lat/wls_lon, just leave them NaN; we'll handle below.\n    # Prepare features for prediction; use training medians for imputation.\n    X_train_median = X.median()\n    for fc in feature_cols:\n        if fc not in merged.columns:\n            merged[fc] = X_train_median.get(fc, 0.0)\n\n    X_test = merged[feature_cols].copy()\n    X_test = X_test.fillna(X_train_median)\n\n    pred_dlat = model_dlat.predict(X_test)\n    pred_dlon = model_dlon.predict(X_test)\n\n    merged[\"pred_lat\"] = merged[\"wls_lat\"] + pred_dlat\n    merged[\"pred_lon\"] = merged[\"wls_lon\"] + pred_dlon\n\n    # If WLS lat/lon is missing, fallback to zero correction (no WLS: cannot correct -> leave NaN)\n    # We'll further fallback to any remaining wls_lat/wls_lon if model output created NaNs (shouldn't).\n    missing_mask = merged[\"wls_lat\"].isna() | merged[\"wls_lon\"].isna()\n    if missing_mask.any():\n        # As a crude fallback, just keep prediction as-is (will be NaN); instead plug last-known per phone.\n        for col in [\"pred_lat\", \"pred_lon\"]:\n            merged[col] = merged.groupby(\"our_phone\")[col].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    submission = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n    submission = submission.rename(\n        columns={\"pred_lat\": \"LatitudeDegrees\", \"pred_lon\": \"LongitudeDegrees\"}\n    )\n\n    # Final safety: if any NaNs remain, fill with global mean lat/lon of predictions\n    for col in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n        if submission[col].isna().any():\n            submission[col] = submission[col].fillna(submission[col].mean())\n\n    submission_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    submission_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(submission_path_working, index=False)\n    submission.to_csv(submission_path_final, index=False)\n    print(f\"Saved submission to {submission_path_working} and {submission_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport numpy as np\nimport pandas as pd\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n\nWORK_DIR = \"./working\"\nos.makedirs(WORK_DIR, exist_ok=True)\n\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nSUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n\n\n# ---------------- Utility functions ----------------\n\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    \"\"\"Vectorized haversine distance in meters.\"\"\"\n    R = 6371000.0\n    lat1_rad = np.radians(lat1)\n    lat2_rad = np.radians(lat2)\n    dlat = lat2_rad - lat1_rad\n    dlon = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dlat / 2.0) ** 2\n        + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0) ** 2\n    )\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(\n    df, pred_lat_col, pred_lon_col, gt_lat_col, gt_lon_col, phone_col=\"phone\"\n):\n    \"\"\"Compute mean of (50th, 95th percentile) distance errors per phone.\"\"\"\n    errors = haversine_distance(\n        df[gt_lat_col].values,\n        df[gt_lon_col].values,\n        df[pred_lat_col].values,\n        df[pred_lon_col].values,\n    )\n    df = df.copy()\n    df[\"error\"] = errors\n    phone_scores = []\n    for phone, g in df.groupby(phone_col):\n        e = g[\"error\"].values\n        if len(e) == 0:\n            continue\n        p50 = np.percentile(e, 50)\n        p95 = np.percentile(e, 95)\n        phone_scores.append((p50 + p95) / 2.0)\n    if not phone_scores:\n        return np.nan\n    return float(np.mean(phone_scores))\n\n\ndef load_train_paths(train_dir):\n    paths = []\n    for drive in sorted(os.listdir(train_dir)):\n        drive_path = os.path.join(train_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n            if os.path.exists(gnss_path) and os.path.exists(gt_path):\n                # phone_id for sample_submission is \"drive_phone\"\n                phone_id = f\"{drive}_{phone}\"\n                paths.append((phone_id, gnss_path, gt_path))\n    return paths\n\n\ndef load_test_paths(test_dir):\n    paths = []\n    for drive in sorted(os.listdir(test_dir)):\n        drive_path = os.path.join(test_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            if os.path.exists(gnss_path):\n                phone_id = f\"{drive}_{phone}\"\n                paths.append((phone_id, gnss_path))\n    return paths\n\n\ndef extract_baseline_cols(df):\n    \"\"\"Find baseline latitude/longitude columns in device_gnss, if any.\"\"\"\n    lat_col_candidates = [c for c in df.columns if c.lower().startswith(\"latitudedeg\")]\n    lon_col_candidates = [c for c in df.columns if c.lower().startswith(\"longitudedeg\")]\n    if len(lat_col_candidates) == 0 or len(lon_col_candidates) == 0:\n        return None, None\n    return lat_col_candidates[0], lon_col_candidates[0]\n\n\n# ---------------- Build training table ----------------\n\ntrain_paths = load_train_paths(TRAIN_DIR)\n\ntrain_rows = []\n\nfor phone_id, gnss_path, gt_path in train_paths:\n    try:\n        gt = pd.read_csv(gt_path)\n    except Exception:\n        continue\n    # Need UnixTimeMillis, LatitudeDegrees, LongitudeDegrees from ground_truth\n    if not set([\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]).issubset(\n        gt.columns\n    ):\n        continue\n    gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n    gt_small[\"phone\"] = phone_id\n\n    # Load device_gnss for this phone to get any baseline coordinates\n    try:\n        gnss = pd.read_csv(gnss_path)\n    except Exception:\n        gnss = None\n\n    if gnss is not None and \"utcTimeMillis\" in gnss.columns:\n        # Round utcTimeMillis to nearest 1000 to match 1Hz UnixTimeMillis\n        gnss_local = gnss.copy()\n        gnss_local[\"UnixTimeMillis\"] = (\n            np.round(gnss_local[\"utcTimeMillis\"] / 1000.0) * 1000\n        ).astype(\"int64\")\n        lat_col, lon_col = extract_baseline_cols(gnss_local)\n        if lat_col is not None and lon_col is not None:\n            base_df = gnss_local[[\"UnixTimeMillis\", lat_col, lon_col]].copy()\n            base_df = (\n                base_df.groupby(\"UnixTimeMillis\")[[lat_col, lon_col]]\n                .mean()\n                .reset_index()\n            )\n            base_df.rename(\n                columns={lat_col: \"baseline_lat\", lon_col: \"baseline_lon\"}, inplace=True\n            )\n            merged = pd.merge(gt_small, base_df, on=\"UnixTimeMillis\", how=\"left\")\n        else:\n            merged = gt_small.copy()\n            merged[\"baseline_lat\"] = np.nan\n            merged[\"baseline_lon\"] = np.nan\n    else:\n        merged = gt_small.copy()\n        merged[\"baseline_lat\"] = np.nan\n        merged[\"baseline_lon\"] = np.nan\n\n    train_rows.append(merged)\n\nif len(train_rows) == 0:\n    raise RuntimeError(\"No training rows could be constructed from train directory.\")\n\ntrain_df = pd.concat(train_rows, ignore_index=True)\n\n# If baseline is missing, use ground_truth itself as baseline (zero offset)\ntrain_df[\"baseline_lat\"] = train_df[\"baseline_lat\"].fillna(train_df[\"LatitudeDegrees\"])\ntrain_df[\"baseline_lon\"] = train_df[\"baseline_lon\"].fillna(train_df[\"LongitudeDegrees\"])\n\n# Targets: offsets (ground_truth - baseline)\ntrain_df[\"dlat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"baseline_lat\"]\ntrain_df[\"dlon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"baseline_lon\"]\n\n# Remove rows with missing essentials\ntrain_df = train_df.dropna(\n    subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"baseline_lat\", \"baseline_lon\"]\n).reset_index(drop=True)\n\nif len(train_df) == 0:\n    raise RuntimeError(\"Training dataframe is empty after cleaning.\")\n\n# ---------------- \"Model\": per-phone mean offset ----------------\n\nphone_offsets = train_df.groupby(\"phone\")[[\"dlat\", \"dlon\"]].mean().reset_index()\nphone_offsets.rename(columns={\"dlat\": \"mean_dlat\", \"dlon\": \"mean_dlon\"}, inplace=True)\n\n# Also store per-phone mean absolute lat/lon, useful for phones without baseline in test\nphone_means = (\n    train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n    .mean()\n    .reset_index()\n)\nphone_means.rename(\n    columns={\n        \"LatitudeDegrees\": \"mean_lat\",\n        \"LongitudeDegrees\": \"mean_lon\",\n    },\n    inplace=True,\n)\n\nphone_info = pd.merge(phone_offsets, phone_means, on=\"phone\", how=\"outer\")\n\n# ---------------- Hold-out validation metric ----------------\n# Simple random 20% of rows per phone as validation\n\nrng = np.random.default_rng(seed=42)\ntrain_df = train_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n\nval_mask = np.zeros(len(train_df), dtype=bool)\nfor phone, g in train_df.groupby(\"phone\").groups.items():\n    idx = np.array(list(g))\n    if len(idx) == 0:\n        continue\n    n_val = max(1, int(0.2 * len(idx)))\n    val_idx = rng.choice(idx, size=n_val, replace=False)\n    val_mask[val_idx] = True\n\nval_df = train_df[val_mask].copy()\ntr_df = train_df[~val_mask].copy()\n\n# For validation, predictions are baseline + mean offset for that phone\nval_df = pd.merge(val_df, phone_info, on=\"phone\", how=\"left\")\n\n# If some phones missing from phone_info (shouldn't happen), fill with zeros\nval_df[\"mean_dlat\"] = val_df[\"mean_dlat\"].fillna(0.0)\nval_df[\"mean_dlon\"] = val_df[\"mean_dlon\"].fillna(0.0)\n\nval_df[\"pred_lat\"] = val_df[\"baseline_lat\"] + val_df[\"mean_dlat\"]\nval_df[\"pred_lon\"] = val_df[\"baseline_lon\"] + val_df[\"mean_dlon\"]\n\nval_metric = competition_metric(\n    val_df,\n    pred_lat_col=\"pred_lat\",\n    pred_lon_col=\"pred_lon\",\n    gt_lat_col=\"LatitudeDegrees\",\n    gt_lon_col=\"LongitudeDegrees\",\n    phone_col=\"phone\",\n)\nprint(f\"Validation metric (mean of 50th & 95th errors): {val_metric:.4f}\")\n\n\n# ---------------- Build test predictions ----------------\n\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n\ntest_paths = load_test_paths(TEST_DIR)\n\ntest_pred_rows = []\n\n\n# Helper: build baseline per (phone_id, UnixTimeMillis) from device_gnss\ndef build_test_baseline_for_phone(phone_id, gnss_path):\n    try:\n        gnss = pd.read_csv(gnss_path)\n    except Exception:\n        return pd.DataFrame(columns=[\"UnixTimeMillis\", \"baseline_lat\", \"baseline_lon\"])\n\n    if \"utcTimeMillis\" not in gnss.columns:\n        return pd.DataFrame(columns=[\"UnixTimeMillis\", \"baseline_lat\", \"baseline_lon\"])\n\n    gnss_local = gnss.copy()\n    gnss_local[\"UnixTimeMillis\"] = (\n        np.round(gnss_local[\"utcTimeMillis\"] / 1000.0) * 1000\n    ).astype(\"int64\")\n\n    lat_col, lon_col = extract_baseline_cols(gnss_local)\n    if lat_col is None or lon_col is None:\n        return pd.DataFrame(columns=[\"UnixTimeMillis\", \"baseline_lat\", \"baseline_lon\"])\n\n    base_df = gnss_local[[\"UnixTimeMillis\", lat_col, lon_col]].copy()\n    base_df = base_df.groupby(\"UnixTimeMillis\")[[lat_col, lon_col]].mean().reset_index()\n    base_df.rename(\n        columns={lat_col: \"baseline_lat\", lon_col: \"baseline_lon\"}, inplace=True\n    )\n    base_df[\"phone\"] = phone_id\n    return base_df\n\n\n# Build a mapping from phone_id to gnss baseline\ntest_baseline_list = []\nfor phone_id, gnss_path in test_paths:\n    base_df = build_test_baseline_for_phone(phone_id, gnss_path)\n    if not base_df.empty:\n        test_baseline_list.append(base_df)\n\nif len(test_baseline_list) > 0:\n    test_baseline_df = pd.concat(test_baseline_list, ignore_index=True)\nelse:\n    test_baseline_df = pd.DataFrame(\n        columns=[\"phone\", \"UnixTimeMillis\", \"baseline_lat\", \"baseline_lon\"]\n    )\n\n# Attach baseline to sample_submission\nsubmission = sample_sub.copy()\n\n# sample_sub['phone'] is like \"2020-06-04-US-MTV-1_Pixel4\"\n# Our phone_id for test is \"2020-06-04-US-MTV-1/GooglePixel4\" (folder names).\n# Need to map between these if they differ. The given sample_submission should match folder names joined by \"_\".\n# We'll assume sample phone string equals drive + \"_\" + phone folder name.\n# So we don't need to change; just use as key consistently.\n\n# However, our test_baseline_df uses phone_id from directory, which may not exactly match sample phone.\n# We'll try both direct match and a variant with \"/\" replaced by \"_\".\n\n# First, create a version of baseline with phone_id_alt where \"/\" -> \"_\"\ntest_baseline_df[\"phone_alt\"] = test_baseline_df[\"phone\"].str.replace(\"/\", \"_\")\n\n# We'll merge using 'phone_alt'\nsubmission = submission.merge(\n    test_baseline_df[[\"phone_alt\", \"UnixTimeMillis\", \"baseline_lat\", \"baseline_lon\"]],\n    left_on=[\"phone\", \"UnixTimeMillis\"],\n    right_on=[\"phone_alt\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\nsubmission.drop(columns=[\"phone_alt\"], inplace=True)\n\n# Now attach phone_info to get per-phone offsets and means; these phones are from train\nsubmission = submission.merge(phone_info, on=\"phone\", how=\"left\")\n\n# For phones not present in training (e.g., new phone types), fall back:\n# - mean_dlat/dlon = 0\n# - mean_lat/lon = global mean of training ground truth\nglobal_mean_lat = train_df[\"LatitudeDegrees\"].mean()\nglobal_mean_lon = train_df[\"LongitudeDegrees\"].mean()\n\nsubmission[\"mean_dlat\"] = submission[\"mean_dlat\"].fillna(0.0)\nsubmission[\"mean_dlon\"] = submission[\"mean_dlon\"].fillna(0.0)\nsubmission[\"mean_lat\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\nsubmission[\"mean_lon\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n\n# If baseline missing for a row, use phone mean_lat/lon as baseline\nsubmission[\"baseline_lat\"] = submission[\"baseline_lat\"].fillna(submission[\"mean_lat\"])\nsubmission[\"baseline_lon\"] = submission[\"baseline_lon\"].fillna(submission[\"mean_lon\"])\n\n# Final predictions: baseline + mean offsets\nsubmission[\"LatitudeDegrees\"] = submission[\"baseline_lat\"] + submission[\"mean_dlat\"]\nsubmission[\"LongitudeDegrees\"] = submission[\"baseline_lon\"] + submission[\"mean_dlon\"]\n\n# Keep required columns and order\nsubmission_out = submission[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n]\n\n# Save to submission path\nsubmission_out.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"Saved submission to {SUBMISSION_PATH}\")\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\n# Directories\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n# WGS84 constants\nA_EARTH = 6378137.0\nE2 = 6.69437999014e-3\n\n\ndef ecef_to_latlon(x, y, z):\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1.0 - E2))\n    for _ in range(5):\n        sin_lat = np.sin(lat)\n        N = A_EARTH / np.sqrt(1.0 - E2 * sin_lat**2)\n        lat = np.arctan2(z + E2 * N * sin_lat, p)\n    return np.degrees(lat), np.degrees(lon)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef derive_phone_id(drive_dir, phone_dir):\n    drive_id = os.path.basename(drive_dir)\n    phone_name = os.path.basename(phone_dir)\n    # The Kaggle \"phone\" key uses \"driveid_phonename\" format.\n    return f\"{drive_id}_{phone_name}\"\n\n\ndef load_train():\n    train_root = os.path.join(INPUT_DIR, \"train\")\n    drive_dirs = sorted(\n        [d for d in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(d)]\n    )\n    records = []\n    for drive_dir in drive_dirs:\n        phone_dirs = sorted(\n            [d for d in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(d)]\n        )\n        for phone_dir in phone_dirs:\n            phone_id = derive_phone_id(drive_dir, phone_dir)\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n                continue\n\n            gt = pd.read_csv(gt_path)\n            gnss = pd.read_csv(gnss_path)\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not all(c in gnss.columns for c in wls_cols):\n                continue\n\n            gnss = gnss.dropna(subset=wls_cols).copy()\n            if gnss.empty:\n                continue\n\n            # Align times to milliseconds since epoch used in ground_truth\n            if \"utcTimeMillis\" in gnss.columns:\n                # round to nearest 1000 ms\n                gnss[\"UnixTimeMillis\"] = (\n                    (gnss[\"utcTimeMillis\"] // 1000) * 1000\n                ).astype(\"int64\")\n            else:\n                # If utcTimeMillis missing, skip\n                continue\n\n            agg_dict = {\n                \"WlsPositionXEcefMeters\": \"mean\",\n                \"WlsPositionYEcefMeters\": \"mean\",\n                \"WlsPositionZEcefMeters\": \"mean\",\n                \"Cn0DbHz\": \"mean\",\n                \"PseudorangeRateMetersPerSecond\": \"mean\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                \"SvElevationDegrees\": \"mean\",\n                \"SvAzimuthDegrees\": \"mean\",\n                \"SvClockBiasMeters\": \"mean\",\n                \"SvClockDriftMetersPerSecond\": \"mean\",\n                \"IonosphericDelayMeters\": \"mean\",\n                \"TroposphericDelayMeters\": \"mean\",\n                \"IsrbMeters\": \"mean\",\n                \"RawPseudorangeMeters\": \"mean\",\n            }\n            agg_dict = {k: v for k, v in agg_dict.items() if k in gnss.columns}\n\n            gnss_agg = gnss.groupby(\"UnixTimeMillis\", as_index=False).agg(agg_dict)\n            if gnss_agg.empty:\n                continue\n\n            # ground_truth already contains UnixTimeMillis\n            if \"UnixTimeMillis\" not in gt.columns:\n                continue\n\n            merged = pd.merge(gt, gnss_agg, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            lat_wls, lon_wls = ecef_to_latlon(\n                merged[\"WlsPositionXEcefMeters\"].values,\n                merged[\"WlsPositionYEcefMeters\"].values,\n                merged[\"WlsPositionZEcefMeters\"].values,\n            )\n            merged[\"wls_lat\"] = lat_wls\n            merged[\"wls_lon\"] = lon_wls\n            merged[\"phone\"] = phone_id\n            records.append(merged)\n\n    if not records:\n        raise RuntimeError(\"No training data found. Check train directory structure.\")\n    train_df = pd.concat(records, ignore_index=True)\n    return train_df\n\n\ndef prepare_features(train_df):\n    df = train_df.copy()\n    df[\"target_dlat\"] = df[\"LatitudeDegrees\"] - df[\"wls_lat\"]\n    df[\"target_dlon\"] = df[\"LongitudeDegrees\"] - df[\"wls_lon\"]\n\n    feature_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"IsrbMeters\",\n        \"RawPseudorangeMeters\",\n    ]\n    feature_cols = [c for c in feature_cols if c in df.columns]\n    X = df[feature_cols].copy()\n    X = X.fillna(X.median())\n    y_dlat = df[\"target_dlat\"].values\n    y_dlon = df[\"target_dlon\"].values\n    return X, y_dlat, y_dlon, feature_cols, df\n\n\ndef cross_validate(X, y_dlat, y_dlon, info_df):\n    gkf = GroupKFold(n_splits=5)\n    groups = info_df[\"phone\"].values\n    scores = []\n    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        dlat_tr, dlat_va = y_dlat[tr_idx], y_dlat[va_idx]\n        dlon_tr, dlon_va = y_dlon[tr_idx], y_dlon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        train_set_dlat = lgb.Dataset(X_tr, label=dlat_tr)\n        valid_set_dlat = lgb.Dataset(X_va, label=dlat_va, reference=train_set_dlat)\n        model_dlat = lgb.train(\n            params,\n            train_set_dlat,\n            num_boost_round=400,\n            valid_sets=[valid_set_dlat],\n            early_stopping_rounds=50,\n            verbose_eval=False,\n        )\n\n        train_set_dlon = lgb.Dataset(X_tr, label=dlon_tr)\n        valid_set_dlon = lgb.Dataset(X_va, label=dlon_va, reference=train_set_dlon)\n        model_dlon = lgb.train(\n            params,\n            train_set_dlon,\n            num_boost_round=400,\n            valid_sets=[valid_set_dlon],\n            early_stopping_rounds=50,\n            verbose_eval=False,\n        )\n\n        va = info_df.iloc[va_idx].copy()\n        va_pred_dlat = model_dlat.predict(X_va, num_iteration=model_dlat.best_iteration)\n        va_pred_dlon = model_dlon.predict(X_va, num_iteration=model_dlon.best_iteration)\n        va[\"pred_lat\"] = va[\"wls_lat\"] + va_pred_dlat\n        va[\"pred_lon\"] = va[\"wls_lon\"] + va_pred_dlon\n        va = va.rename(\n            columns={\"LatitudeDegrees\": \"gt_lat\", \"LongitudeDegrees\": \"gt_lon\"}\n        )\n        score = competition_metric(\n            va[[\"phone\", \"gt_lat\", \"gt_lon\", \"pred_lat\", \"pred_lon\"]]\n        )\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_dlat, y_dlon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_dlat = lgb.Dataset(X, label=y_dlat)\n    dtrain_dlon = lgb.Dataset(X, label=y_dlon)\n    model_dlat = lgb.train(params, dtrain_dlat, num_boost_round=400)\n    model_dlon = lgb.train(params, dtrain_dlon, num_boost_round=400)\n    return model_dlat, model_dlon\n\n\ndef load_test_features(feature_cols):\n    test_root = os.path.join(INPUT_DIR, \"test\")\n    drive_dirs = sorted(\n        [d for d in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(d)]\n    )\n    records = []\n    for drive_dir in drive_dirs:\n        phone_dirs = sorted(\n            [d for d in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(d)]\n        )\n        for phone_dir in phone_dirs:\n            phone_id = derive_phone_id(drive_dir, phone_dir)\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            gnss = pd.read_csv(gnss_path)\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not all(c in gnss.columns for c in wls_cols):\n                continue\n\n            gnss = gnss.dropna(subset=wls_cols).copy()\n            if gnss.empty:\n                continue\n\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = ((gnss[\"utcTimeMillis\"] // 1000) * 1000).astype(\n                \"int64\"\n            )\n\n            agg_dict = {\n                \"WlsPositionXEcefMeters\": \"mean\",\n                \"WlsPositionYEcefMeters\": \"mean\",\n                \"WlsPositionZEcefMeters\": \"mean\",\n            }\n            for c in feature_cols:\n                if c not in agg_dict and c in gnss.columns:\n                    agg_dict[c] = \"mean\"\n\n            gnss_agg = gnss.groupby(\"UnixTimeMillis\", as_index=False).agg(agg_dict)\n            if gnss_agg.empty:\n                continue\n\n            lat_wls, lon_wls = ecef_to_latlon(\n                gnss_agg[\"WlsPositionXEcefMeters\"].values,\n                gnss_agg[\"WlsPositionYEcefMeters\"].values,\n                gnss_agg[\"WlsPositionZEcefMeters\"].values,\n            )\n            gnss_agg[\"wls_lat\"] = lat_wls\n            gnss_agg[\"wls_lon\"] = lon_wls\n            gnss_agg[\"phone\"] = phone_id\n            records.append(gnss_agg)\n\n    if not records:\n        raise RuntimeError(\"No test GNSS data loaded.\")\n    test_df = pd.concat(records, ignore_index=True)\n    return test_df\n\n\ndef main():\n    # Load and prepare training data\n    train_df = load_train()\n    X, y_dlat, y_dlon, feature_cols, info_df = prepare_features(train_df)\n    # Cross-validation evaluation\n    cv_score = cross_validate(X, y_dlat, y_dlon, info_df)\n    print(f\"Validation competition-like metric (5-fold): {cv_score:.6f}\")\n\n    # Train on full training data\n    model_dlat, model_dlon = train_full_models(X, y_dlat, y_dlon)\n\n    # Load test features\n    sample_sub = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))\n    # Ensure correct dtypes\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    test_gnss_df = load_test_features(feature_cols)\n    test_gnss_df[\"UnixTimeMillis\"] = test_gnss_df[\"UnixTimeMillis\"].astype(\"int64\")\n\n    # Merge sample submission with test gnss data on exact phone and UnixTimeMillis\n    merged = pd.merge(\n        sample_sub,\n        test_gnss_df,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n\n    # Fill missing GNSS rows per phone with nearest available (ffill + bfill)\n    merged = merged.sort_values([\"phone\", \"UnixTimeMillis\"])\n\n    cols_to_fill = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"wls_lat\",\n        \"wls_lon\",\n    ] + [\n        c\n        for c in feature_cols\n        if c\n        not in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ]\n    cols_to_fill = [c for c in cols_to_fill if c in merged.columns]\n\n    for c in cols_to_fill:\n        merged[c] = merged.groupby(\"phone\")[c].transform(lambda s: s.ffill().bfill())\n\n    # Prepare features for prediction; use training medians for imputation.\n    X_train_median = X.median()\n    for fc in feature_cols:\n        if fc not in merged.columns:\n            merged[fc] = X_train_median.get(fc, 0.0)\n\n    X_test = merged[feature_cols].copy()\n    X_test = X_test.fillna(X_train_median)\n\n    # Predict deltas and add to wls positions\n    pred_dlat = model_dlat.predict(X_test)\n    pred_dlon = model_dlon.predict(X_test)\n\n    merged[\"pred_lat\"] = merged[\"wls_lat\"] + pred_dlat\n    merged[\"pred_lon\"] = merged[\"wls_lon\"] + pred_dlon\n\n    # Fallback for any missing base lat/lon or predictions\n    for col_base, col_pred in [(\"wls_lat\", \"pred_lat\"), (\"wls_lon\", \"pred_lon\")]:\n        # Where base is missing, try to propagate prediction within phone\n        missing_mask = merged[col_pred].isna()\n        if missing_mask.any():\n            merged[col_pred] = merged.groupby(\"phone\")[col_pred].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    submission = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n    submission = submission.rename(\n        columns={\"pred_lat\": \"LatitudeDegrees\", \"pred_lon\": \"LongitudeDegrees\"}\n    )\n\n    # Final safety: if any NaNs remain, fill with global mean lat/lon of predictions\n    for col in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n        if submission[col].isna().any():\n            submission[col] = submission[col].fillna(submission[col].mean())\n\n    # Ensure correct column order\n    submission = submission[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ]\n\n    submission_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    submission_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(submission_path_working, index=False)\n    submission.to_csv(submission_path_final, index=False)\n    print(f\"Saved submission to {submission_path_working} and {submission_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport numpy as np\nimport pandas as pd\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n\nWORK_DIR = \"./working\"\nos.makedirs(WORK_DIR, exist_ok=True)\n\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nSUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n\n\n# ---------------- Utility functions ----------------\n\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    \"\"\"Vectorized haversine distance in meters.\"\"\"\n    R = 6371000.0\n    lat1_rad = np.radians(lat1)\n    lat2_rad = np.radians(lat2)\n    dlat = lat2_rad - lat1_rad\n    dlon = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dlat / 2.0) ** 2\n        + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0) ** 2\n    )\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(\n    df, pred_lat_col, pred_lon_col, gt_lat_col, gt_lon_col, phone_col=\"phone\"\n):\n    \"\"\"Compute mean of (50th, 95th percentile) distance errors per phone.\"\"\"\n    errors = haversine_distance(\n        df[gt_lat_col].values,\n        df[gt_lon_col].values,\n        df[pred_lat_col].values,\n        df[pred_lon_col].values,\n    )\n    df = df.copy()\n    df[\"error\"] = errors\n    phone_scores = []\n    for phone, g in df.groupby(phone_col):\n        e = g[\"error\"].values\n        if len(e) == 0:\n            continue\n        p50 = np.percentile(e, 50)\n        p95 = np.percentile(e, 95)\n        phone_scores.append((p50 + p95) / 2.0)\n    if not phone_scores:\n        return np.nan\n    return float(np.mean(phone_scores))\n\n\ndef load_train_paths(train_dir):\n    \"\"\"Return list of (phone_id, gnss_path, gt_path).\"\"\"\n    paths = []\n    for drive in sorted(os.listdir(train_dir)):\n        drive_path = os.path.join(train_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n            if os.path.exists(gnss_path) and os.path.exists(gt_path):\n                # phone_id as \"drive/phone\"\n                phone_id = f\"{drive}/{phone}\"\n                paths.append((phone_id, gnss_path, gt_path))\n    return paths\n\n\ndef load_test_paths(test_dir):\n    \"\"\"Return list of (phone_id, gnss_path) with phone_id as 'drive/phone'.\"\"\"\n    paths = []\n    for drive in sorted(os.listdir(test_dir)):\n        drive_path = os.path.join(test_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            if os.path.exists(gnss_path):\n                phone_id = f\"{drive}/{phone}\"\n                paths.append((phone_id, gnss_path))\n    return paths\n\n\ndef extract_baseline_cols(df):\n    \"\"\"Find baseline latitude/longitude columns in device_gnss, if any.\"\"\"\n    lat_col_candidates = [c for c in df.columns if c.lower().startswith(\"latitudedeg\")]\n    lon_col_candidates = [c for c in df.columns if c.lower().startswith(\"longitudedeg\")]\n    if len(lat_col_candidates) == 0 or len(lon_col_candidates) == 0:\n        return None, None\n    return lat_col_candidates[0], lon_col_candidates[0]\n\n\n# ---------------- Build training table ----------------\n\ntrain_paths = load_train_paths(TRAIN_DIR)\n\ntrain_rows = []\n\nfor phone_id, gnss_path, gt_path in train_paths:\n    try:\n        gt = pd.read_csv(gt_path)\n    except Exception:\n        continue\n    # Need UnixTimeMillis, LatitudeDegrees, LongitudeDegrees from ground_truth\n    if not set([\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]).issubset(\n        gt.columns\n    ):\n        continue\n    gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n    gt_small[\"phone\"] = phone_id\n\n    # Load device_gnss for this phone to get any baseline coordinates\n    try:\n        gnss = pd.read_csv(gnss_path)\n    except Exception:\n        gnss = None\n\n    if gnss is not None and \"utcTimeMillis\" in gnss.columns:\n        # Round utcTimeMillis to nearest 1000 to match 1Hz UnixTimeMillis\n        gnss_local = gnss.copy()\n        gnss_local[\"UnixTimeMillis\"] = (\n            np.round(gnss_local[\"utcTimeMillis\"] / 1000.0) * 1000\n        ).astype(\"int64\")\n        lat_col, lon_col = extract_baseline_cols(gnss_local)\n        if lat_col is not None and lon_col is not None:\n            base_df = gnss_local[[\"UnixTimeMillis\", lat_col, lon_col]].copy()\n            base_df = (\n                base_df.groupby(\"UnixTimeMillis\")[[lat_col, lon_col]]\n                .mean()\n                .reset_index()\n            )\n            base_df.rename(\n                columns={lat_col: \"baseline_lat\", lon_col: \"baseline_lon\"}, inplace=True\n            )\n            merged = pd.merge(gt_small, base_df, on=\"UnixTimeMillis\", how=\"left\")\n        else:\n            merged = gt_small.copy()\n            merged[\"baseline_lat\"] = np.nan\n            merged[\"baseline_lon\"] = np.nan\n    else:\n        merged = gt_small.copy()\n        merged[\"baseline_lat\"] = np.nan\n        merged[\"baseline_lon\"] = np.nan\n\n    train_rows.append(merged)\n\nif len(train_rows) == 0:\n    raise RuntimeError(\"No training rows could be constructed from train directory.\")\n\ntrain_df = pd.concat(train_rows, ignore_index=True)\n\n# If baseline is missing, use ground_truth itself as baseline (zero offset)\ntrain_df[\"baseline_lat\"] = train_df[\"baseline_lat\"].fillna(train_df[\"LatitudeDegrees\"])\ntrain_df[\"baseline_lon\"] = train_df[\"baseline_lon\"].fillna(train_df[\"LongitudeDegrees\"])\n\n# Targets: offsets (ground_truth - baseline)\ntrain_df[\"dlat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"baseline_lat\"]\ntrain_df[\"dlon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"baseline_lon\"]\n\n# Remove rows with missing essentials\ntrain_df = train_df.dropna(\n    subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"baseline_lat\", \"baseline_lon\"]\n).reset_index(drop=True)\n\nif len(train_df) == 0:\n    raise RuntimeError(\"Training dataframe is empty after cleaning.\")\n\n# ---------------- \"Model\": per-phone mean offset ----------------\n\nphone_offsets = train_df.groupby(\"phone\")[[\"dlat\", \"dlon\"]].mean().reset_index()\nphone_offsets.rename(columns={\"dlat\": \"mean_dlat\", \"dlon\": \"mean_dlon\"}, inplace=True)\n\n# Also store per-phone mean absolute lat/lon, useful for phones without baseline in test\nphone_means = (\n    train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n    .mean()\n    .reset_index()\n)\nphone_means.rename(\n    columns={\n        \"LatitudeDegrees\": \"mean_lat\",\n        \"LongitudeDegrees\": \"mean_lon\",\n    },\n    inplace=True,\n)\n\nphone_info = pd.merge(phone_offsets, phone_means, on=\"phone\", how=\"outer\")\n\n# ---------------- Hold-out validation metric ----------------\n# Simple random 20% of rows per phone as validation\n\nrng = np.random.default_rng(seed=42)\ntrain_df = train_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n\nval_mask = np.zeros(len(train_df), dtype=bool)\nfor phone, g in train_df.groupby(\"phone\").groups.items():\n    idx = np.array(list(g))\n    if len(idx) == 0:\n        continue\n    n_val = max(1, int(0.2 * len(idx)))\n    val_idx = rng.choice(idx, size=n_val, replace=False)\n    val_mask[val_idx] = True\n\nval_df = train_df[val_mask].copy()\ntr_df = train_df[~val_mask].copy()\n\n# For validation, predictions are baseline + mean offset for that phone (computed on full train_df)\nval_df = pd.merge(val_df, phone_info, on=\"phone\", how=\"left\")\n\n# If some phones missing from phone_info (shouldn't happen), fill with zeros\nval_df[\"mean_dlat\"] = val_df[\"mean_dlat\"].fillna(0.0)\nval_df[\"mean_dlon\"] = val_df[\"mean_dlon\"].fillna(0.0)\n\nval_df[\"pred_lat\"] = val_df[\"baseline_lat\"] + val_df[\"mean_dlat\"]\nval_df[\"pred_lon\"] = val_df[\"baseline_lon\"] + val_df[\"mean_dlon\"]\n\nval_metric = competition_metric(\n    val_df,\n    pred_lat_col=\"pred_lat\",\n    pred_lon_col=\"pred_lon\",\n    gt_lat_col=\"LatitudeDegrees\",\n    gt_lon_col=\"LongitudeDegrees\",\n    phone_col=\"phone\",\n)\nprint(f\"Validation metric (mean of 50th & 95th errors): {val_metric:.4f}\")\n\n\n# ---------------- Build test predictions ----------------\n\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n\ntest_paths = load_test_paths(TEST_DIR)\n\n\n# Helper: build baseline per (sample_phone, UnixTimeMillis) from device_gnss\ndef build_test_baseline_for_phone(sample_phone_name, gnss_path):\n    \"\"\"\n    sample_phone_name: string as appears in sample_submission['phone'],\n    assumed format 'driveid_phonefolder' (e.g., '2020-06-04-US-MTV-1_Pixel4').\n    gnss_path: path to device_gnss.csv for some drive/phone folder.\n    \"\"\"\n    try:\n        gnss = pd.read_csv(gnss_path)\n    except Exception:\n        return pd.DataFrame(\n            columns=[\"phone\", \"UnixTimeMillis\", \"baseline_lat\", \"baseline_lon\"]\n        )\n\n    if \"utcTimeMillis\" not in gnss.columns:\n        return pd.DataFrame(\n            columns=[\"phone\", \"UnixTimeMillis\", \"baseline_lat\", \"baseline_lon\"]\n        )\n\n    gnss_local = gnss.copy()\n    gnss_local[\"UnixTimeMillis\"] = (\n        np.round(gnss_local[\"utcTimeMillis\"] / 1000.0) * 1000\n    ).astype(\"int64\")\n\n    lat_col, lon_col = extract_baseline_cols(gnss_local)\n    if lat_col is None or lon_col is None:\n        return pd.DataFrame(\n            columns=[\"phone\", \"UnixTimeMillis\", \"baseline_lat\", \"baseline_lon\"]\n        )\n\n    base_df = gnss_local[[\"UnixTimeMillis\", lat_col, lon_col]].copy()\n    base_df = base_df.groupby(\"UnixTimeMillis\")[[lat_col, lon_col]].mean().reset_index()\n    base_df.rename(\n        columns={lat_col: \"baseline_lat\", lon_col: \"baseline_lon\"}, inplace=True\n    )\n    base_df[\"phone\"] = sample_phone_name\n    return base_df\n\n\n# Map from test folder phone_id (\"drive/phonefolder\") to sample phone name (\"drive_phone\")\n# Example: \"2020-06-04-US-MTV-1/GooglePixel4\" -> \"2020-06-04-US-MTV-1_Pixel4\"\n# We approximate mapping by using the drive part and last token of phone folder (e.g., Pixel4)\ndef folder_phone_to_sample_phone(phone_id):\n    drive, phone_folder = phone_id.split(\"/\", 1)\n    # pick last \"word\" from phone_folder as a proxy (e.g., GooglePixel4 -> Pixel4)\n    # simple heuristic: split on digits transition or on \"Pixel\" etc is complex; instead,\n    # we map explicitly using known patterns for this dataset\n    mapping_suffix = {\n        \"GooglePixel4\": \"Pixel4\",\n        \"GooglePixel4XL\": \"Pixel4XL\",\n        \"GooglePixel5\": \"Pixel5\",\n        \"SamsungGalaxyS20Ultra\": \"SamsungS20Ultra\",\n        \"XiaomiMi8\": \"Mi8\",\n    }\n    suffix = mapping_suffix.get(phone_folder, phone_folder)\n    return f\"{drive}_{suffix}\"\n\n\n# Build GNSS baselines for all sample phone names\ntest_baseline_list = []\nfor phone_id, gnss_path in test_paths:\n    sample_phone = folder_phone_to_sample_phone(phone_id)\n    if sample_phone not in sample_sub[\"phone\"].unique():\n        continue\n    base_df = build_test_baseline_for_phone(sample_phone, gnss_path)\n    if not base_df.empty:\n        test_baseline_list.append(base_df)\n\nif len(test_baseline_list) > 0:\n    test_baseline_df = pd.concat(test_baseline_list, ignore_index=True)\nelse:\n    test_baseline_df = pd.DataFrame(\n        columns=[\"phone\", \"UnixTimeMillis\", \"baseline_lat\", \"baseline_lon\"]\n    )\n\n# Attach baseline to sample_submission\nsubmission = sample_sub.copy()\n\nsubmission = submission.merge(\n    test_baseline_df, on=[\"phone\", \"UnixTimeMillis\"], how=\"left\"\n)\n\n# We cannot safely map training phone_ids (drive/phonefolder) to sample phone names,\n# so we will *not* merge phone_info directly on sample_sub['phone'].\n# Instead, we will only use a global mean offset (which is effectively ~0 because we used GT as baseline when missing).\n\n# Global mean lat/lon for fallback\nglobal_mean_lat = train_df[\"LatitudeDegrees\"].mean()\nglobal_mean_lon = train_df[\"LongitudeDegrees\"].mean()\n\n# Global mean offsets (dlat, dlon) for fallback (often near 0)\nglobal_mean_dlat = train_df[\"dlat\"].mean()\nglobal_mean_dlon = train_df[\"dlon\"].mean()\n\n# Use baseline from GNSS where available; otherwise global mean position\nsubmission[\"baseline_lat\"] = submission[\"baseline_lat\"].fillna(global_mean_lat)\nsubmission[\"baseline_lon\"] = submission[\"baseline_lon\"].fillna(global_mean_lon)\n\n# Final predictions: baseline + global mean offsets\nsubmission[\"LatitudeDegrees\"] = submission[\"baseline_lat\"] + global_mean_dlat\nsubmission[\"LongitudeDegrees\"] = submission[\"baseline_lon\"] + global_mean_dlon\n\n# Keep required columns and order\nsubmission_out = submission[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n]\n\n# Save to submission path\nsubmission_out.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"Saved submission to {SUBMISSION_PATH}\")\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef load_train_data(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        raise RuntimeError(f\"Train directory not found at {train_root}\")\n\n    all_rows = []\n\n    # Walk train/<drive>/<phone> structure robustly\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                # Skip incomplete pairs\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception:\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            # Align times: use integer ms\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"].astype(\"int64\")\n\n            base_cols = [\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"SvElevationDegrees\",\n                \"SvAzimuthDegrees\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateMetersPerSecond\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                \"IonosphericDelayMeters\",\n                \"TroposphericDelayMeters\",\n            ]\n            cols = [c for c in base_cols if c in gnss.columns]\n            if \"UnixTimeMillis\" not in cols:\n                cols = [\"UnixTimeMillis\"] + cols\n\n            agg_df = gnss[cols].copy()\n            agg = agg_df.groupby(\"UnixTimeMillis\").agg(\"mean\").reset_index()\n\n            merged = pd.merge(\n                agg,\n                gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]],\n                on=\"UnixTimeMillis\",\n                how=\"inner\",\n            )\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        raise RuntimeError(\n            \"No training data assembled. Check train folder structure and file names.\"\n        )\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\ndef prepare_features_targets(train_df):\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in [\"LatitudeDegrees\", \"LongitudeDegrees\", \"phone\", \"drive\"]\n    ]\n    X = train_df[feature_cols].copy()\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n    return X, y_lat, y_lon, feature_cols\n\n\ndef train_and_validate(train_df):\n    train_df = train_df.copy()\n    # derive drive id from phone string: driveId_phoneModel\n    train_df[\"drive\"] = train_df[\"phone\"].apply(\n        lambda x: x.split(\"_\")[0] if \"_\" in x else x\n    )\n\n    X, y_lat, y_lon, feature_cols = prepare_features_targets(train_df)\n    # Fill NaNs with medians\n    medians = X.median()\n    X = X.fillna(medians)\n\n    groups = train_df[\"drive\"].values\n    n_splits = min(5, len(np.unique(groups)))\n    if n_splits < 2:\n        # Fallback: no CV possible\n        return [], feature_cols, np.nan, train_df, medians\n\n    gkf = GroupKFold(n_splits=n_splits)\n    oof_list = []\n    models = []\n\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=20, n_jobs=-1, random_state=42 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=20, n_jobs=-1, random_state=142 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        pred_lat = rf_lat.predict(X_val)\n        pred_lon = rf_lon.predict(X_val)\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"gt_lat\": ylat_val,\n                \"gt_lon\": ylon_val,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n        models.append((rf_lat, rf_lon))\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return models, feature_cols, metric, train_df, medians\n\n\ndef fit_full_models(train_df, feature_cols, global_medians):\n    X = train_df[feature_cols].copy()\n    # Ensure consistent fill with medians computed earlier\n    medians = global_medians.copy()\n    # If any new columns appeared, extend medians\n    for c in feature_cols:\n        if c not in medians.index:\n            medians[c] = X[c].median()\n    X = X.fillna(medians)\n\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=22, n_jobs=-1, random_state=100\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=22, n_jobs=-1, random_state=101\n    )\n    rf_lat.fit(X, y_lat)\n    rf_lon.fit(X, y_lon)\n    return rf_lat, rf_lon, medians\n\n\ndef load_test_features_simple(input_dir, feature_cols, sample_sub):\n    suffix_to_folder = {\n        \"Pixel4\": \"GooglePixel4\",\n        \"Pixel4XL\": \"GooglePixel4XL\",\n        \"Pixel5\": \"GooglePixel5\",\n        \"SamsungS20Ultra\": \"SamsungGalaxyS20Ultra\",\n        \"Mi8\": \"XiaomiMi8\",\n    }\n\n    sample = sample_sub.copy()\n    # Extract drive part and phone suffix from sample phone\n    sample[\"drive\"] = sample[\"phone\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n    sample[\"suffix\"] = sample[\"phone\"].apply(lambda x: x.split(\"_\")[-1])\n    sample[\"folder_model\"] = sample[\"suffix\"].map(suffix_to_folder)\n    sample[\"folder_phone\"] = sample[\"drive\"] + \"_\" + sample[\"folder_model\"]\n\n    test_root = os.path.join(input_dir, \"test\")\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n    )\n    agg_list = []\n\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            folder_phone = f\"{drive_id}_{phone_model}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            try:\n                gnss = pd.read_csv(gnss_path)\n            except Exception:\n                continue\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"].astype(\"int64\")\n\n            cols_needed = [\"UnixTimeMillis\"]\n            for c in feature_cols:\n                if c != \"UnixTimeMillis\" and c in gnss.columns:\n                    cols_needed.append(c)\n            cols_needed = list(dict.fromkeys(cols_needed))\n\n            agg_df = gnss[cols_needed].copy()\n            agg = agg_df.groupby(\"UnixTimeMillis\").agg(\"mean\").reset_index()\n            agg[\"folder_phone\"] = folder_phone\n            agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        # No GNSS data found; create empty df to allow merge and rely on fallback fills\n        test_agg = pd.DataFrame(\n            columns=[\"folder_phone\", \"UnixTimeMillis\"] + feature_cols\n        )\n\n    sample_with_idx = sample.copy()\n    sample_with_idx[\"row_id\"] = np.arange(len(sample_with_idx))\n\n    merged = pd.merge(\n        sample_with_idx,\n        test_agg,\n        how=\"left\",\n        on=[\"folder_phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    merged = merged.sort_values(\"row_id\")\n    X_test = pd.DataFrame(index=merged[\"row_id\"].values)\n    for c in feature_cols:\n        if c in merged.columns:\n            X_test[c] = merged[c].values\n        else:\n            X_test[c] = np.nan\n\n    meta = merged[[\"row_id\", \"phone\", \"UnixTimeMillis\"]].copy()\n    return X_test, meta\n\n\ndef main():\n    # Load training data\n    train_df = load_train_data(INPUT_DIR)\n\n    # Train with CV\n    models_cv, feature_cols, val_metric, train_df_full, cv_medians = train_and_validate(\n        train_df\n    )\n    print(\n        \"Cross-validation competition metric (mean of per-phone (P50+P95)/2):\",\n        val_metric,\n    )\n\n    # Fit final models on all train data\n    rf_lat, rf_lon, train_medians = fit_full_models(\n        train_df_full, feature_cols, cv_medians\n    )\n\n    # Prepare test features\n    sample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_sub_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_sub_path}\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    X_test_raw, meta = load_test_features_simple(INPUT_DIR, feature_cols, sample_sub)\n\n    # Ensure all feature columns are present and filled\n    X_test = X_test_raw.copy()\n    for c in feature_cols:\n        if c not in X_test.columns:\n            X_test[c] = train_medians.get(c, 0.0)\n    X_test = X_test.fillna(train_medians)\n\n    # Predict\n    pred_lat = rf_lat.predict(X_test[feature_cols])\n    pred_lon = rf_lon.predict(X_test[feature_cols])\n\n    # Build submission in sample order\n    submission = sample_sub.copy()\n    submission[\"row_id\"] = np.arange(len(submission))\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": meta[\"row_id\"].values,\n            \"LatitudeDegrees\": pred_lat,\n            \"LongitudeDegrees\": pred_lon,\n        }\n    )\n\n    submission = pd.merge(submission, pred_df, on=\"row_id\", how=\"left\")\n\n    # For any rows without predictions (e.g., missing GNSS), fill with per-phone interpolation\n    submission.sort_values([\"phone\", \"UnixTimeMillis\"], inplace=True)\n    submission[\"LatitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LatitudeDegrees\"].ffill().bfill()\n    )\n    submission[\"LongitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LongitudeDegrees\"].ffill().bfill()\n    )\n\n    submission = submission[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].reset_index(drop=True)\n\n    # Save submissions\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    submission.to_csv(out_path_submission, index=False)\n    submission.to_csv(out_path_working, index=False)\n    print(f\"Saved submission to: {out_path_submission} and {out_path_working}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# ---------------------------\n# Utility functions\n# ---------------------------\n\n\ndef ecef_to_llh(x, y, z):\n    # WGS84 ellipsoid constants:\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = a * np.sqrt(1 - e2)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    Esq = a * a - b * b\n    F = 54 * b * b * z * z\n    G = r * r + (1 - e2) * z * z - e2 * Esq\n    C = (e2 * e2 * F * r * r) / (G * G * G)\n    S = np.cbrt(1 + C + np.sqrt(C * C + 2 * C))\n    P = F / (3 * (S + 1 / S + 1) ** 2 * G * G)\n    Q = np.sqrt(1 + 2 * e2 * e2 * P)\n    r0 = -(P * e2 * r) / (1 + Q) + np.sqrt(\n        0.5 * a * a * (1 + 1 / Q)\n        - P * (1 - e2) * z * z / (Q * (1 + Q))\n        - 0.5 * P * r * r\n    )\n    U = np.sqrt((r - e2 * r0) ** 2 + z * z)\n    V = np.sqrt((r - e2 * r0) ** 2 + (1 - e2) * z * z)\n    Z0 = b * b * z / (a * V)\n    h = U * (1 - b * b / (a * V))\n    lat = np.arctan((z + e2 * Z0) / r)\n    lon = np.arctan2(y, x)\n    return np.degrees(lat), np.degrees(lon), h\n\n\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df = df.copy()\n    df[\"dist\"] = dist\n    per_phone = []\n    for phone, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    return float(np.mean(per_phone))\n\n\ndef build_unix_time_millis(gnss_df):\n    \"\"\"Robustly build UnixTimeMillis using ArrivalTimeNanosSinceGpsEpoch if possible, else utcTimeMillis.\"\"\"\n    if \"ArrivalTimeNanosSinceGpsEpoch\" in gnss_df.columns:\n        # Coerce to numeric and drop NaNs for that path\n        arr = pd.to_numeric(gnss_df[\"ArrivalTimeNanosSinceGpsEpoch\"], errors=\"coerce\")\n        # Convert valid nanos to millis\n        arr_millis = (arr // 1_000_000).astype(\"Int64\")  # nullable int\n        # Start with utcTimeMillis as base\n        unix = pd.to_numeric(gnss_df[\"utcTimeMillis\"], errors=\"coerce\").astype(\"Int64\")\n        # Where we have valid ArrivalTime-based millis, use them\n        mask = arr_millis.notna()\n        unix[mask] = arr_millis[mask]\n        # Finally, convert to regular int64, dropping remaining NaNs by filling with utcTimeMillis median\n        if unix.isna().any():\n            # Fallback: fill with median of non-null utcTimeMillis\n            utc_clean = pd.to_numeric(gnss_df[\"utcTimeMillis\"], errors=\"coerce\")\n            median_utc = int(np.nanmedian(utc_clean.values))\n            unix = unix.fillna(median_utc)\n        return unix.astype(\"int64\")\n    else:\n        utc = pd.to_numeric(gnss_df[\"utcTimeMillis\"], errors=\"coerce\")\n        # Fill any NaN utc with median to allow int cast\n        if utc.isna().any():\n            median_utc = int(np.nanmedian(utc.values))\n            utc = utc.fillna(median_utc)\n        return utc.astype(\"int64\")\n\n\n# ---------------------------\n# Load training data\n# ---------------------------\n\ntrain_root = \"./input/train\"\ntest_root = \"./input/test\"\n\ntrain_rows = []\n\n# Traverse all drives/phones in train\nfor drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n    if not os.path.isdir(drive_dir):\n        continue\n    drive_id = os.path.basename(drive_dir)\n    for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n        if not os.path.isdir(phone_dir):\n            continue\n        phone_name = os.path.basename(phone_dir)\n        phone_id = f\"{drive_id}_{phone_name}\"\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n        if not os.path.exists(gnss_path) or not os.path.exists(gt_path):\n            continue\n\n        gnss = pd.read_csv(gnss_path)\n        gt = pd.read_csv(gt_path)\n\n        # Robust time construction\n        gnss[\"UnixTimeMillis\"] = build_unix_time_millis(gnss)\n\n        # Ensure WLS columns exist\n        wls_cols = [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        for c in wls_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        # Aggregate per epoch: mean of ECEF and mean Cn0DbHz and pseudorange rate uncertainty\n        agg = (\n            gnss.groupby(\"UnixTimeMillis\")\n            .agg(\n                {\n                    \"Cn0DbHz\": \"mean\",\n                    \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                    wls_cols[0]: \"mean\",\n                    wls_cols[1]: \"mean\",\n                    wls_cols[2]: \"mean\",\n                }\n            )\n            .reset_index()\n        )\n        agg[\"phone\"] = phone_id\n\n        # Ground truth: one row per UnixTimeMillis\n        gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n        gt_small[\"phone\"] = phone_id\n\n        # Sort for merge_asof\n        agg = agg.sort_values(\"UnixTimeMillis\")\n        gt_small = gt_small.sort_values(\"UnixTimeMillis\")\n\n        # Merge by nearest within +/- 200 ms\n        merged = pd.merge_asof(\n            gt_small,\n            agg,\n            on=\"UnixTimeMillis\",\n            by=\"phone\",\n            direction=\"nearest\",\n            tolerance=200,\n        )\n        merged = merged.dropna(subset=wls_cols)\n        if len(merged) == 0:\n            continue\n\n        # Add ECEF ground truth (height 0)\n        x_gt, y_gt, z_gt = llh_to_ecef(\n            merged[\"LatitudeDegrees\"].values,\n            merged[\"LongitudeDegrees\"].values,\n            np.zeros(len(merged)),\n        )\n        merged[\"x_gt\"] = x_gt\n        merged[\"y_gt\"] = y_gt\n        merged[\"z_gt\"] = z_gt\n\n        train_rows.append(merged)\n\nif len(train_rows) == 0:\n    raise RuntimeError(\"No training data assembled; check train directory structure.\")\n\ntrain_df = pd.concat(train_rows, ignore_index=True)\n\n# Features and targets\nfeature_cols = [\n    \"Cn0DbHz\",\n    \"PseudorangeRateUncertaintyMetersPerSecond\",\n    \"WlsPositionXEcefMeters\",\n    \"WlsPositionYEcefMeters\",\n    \"WlsPositionZEcefMeters\",\n]\nfor c in feature_cols:\n    if c not in train_df.columns:\n        train_df[c] = np.nan\n# Fill NaNs with median\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_x = train_df[\"x_gt\"].values\ny_y = train_df[\"y_gt\"].values\ny_z = train_df[\"z_gt\"].values\n# group by drive_id (before underscore)\ngroups = train_df[\"phone\"].str.split(\"_\").str[0]\n\n# ---------------------------\n# 5-fold Group CV\n# ---------------------------\ngkf = GroupKFold(n_splits=5)\n\noof_pred = np.zeros((len(train_df), 3))\nfold = 0\nfor train_idx, val_idx in gkf.split(X, y_x, groups):\n    fold += 1\n    X_tr, X_val = X[train_idx], X[val_idx]\n    yx_tr, yx_val = y_x[train_idx], y_x[val_idx]\n    yy_tr, yy_val = y_y[train_idx], y_y[val_idx]\n    yz_tr, yz_val = y_z[train_idx], y_z[val_idx]\n\n    params = dict(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=-1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        n_jobs=-1,\n    )\n    model_x = LGBMRegressor(**params)\n    model_y = LGBMRegressor(**params)\n    model_z = LGBMRegressor(**params)\n\n    model_x.fit(X_tr, yx_tr)\n    model_y.fit(X_tr, yy_tr)\n    model_z.fit(X_tr, yz_tr)\n\n    oof_pred[val_idx, 0] = model_x.predict(X_val)\n    oof_pred[val_idx, 1] = model_y.predict(X_val)\n    oof_pred[val_idx, 2] = model_z.predict(X_val)\n\n# Evaluate CV metric in lat/lon space\nlat_pred_oof, lon_pred_oof, _ = ecef_to_llh(\n    oof_pred[:, 0], oof_pred[:, 1], oof_pred[:, 2]\n)\neval_df = pd.DataFrame(\n    {\n        \"phone\": train_df[\"phone\"].values,\n        \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n        \"lat_pred\": lat_pred_oof,\n        \"lon_pred\": lon_pred_oof,\n        \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n        \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n    }\n)\nmetric_value = competition_metric(eval_df)\nprint(\"CV competition metric (mean of 50th and 95th pct errors):\", metric_value)\n\n# ---------------------------\n# Train final models on all data\n# ---------------------------\nfinal_params = dict(\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=-1,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_x = LGBMRegressor(**final_params)\nfinal_model_y = LGBMRegressor(**final_params)\nfinal_model_z = LGBMRegressor(**final_params)\n\nfinal_model_x.fit(X, y_x)\nfinal_model_y.fit(X, y_y)\nfinal_model_z.fit(X, y_z)\n\n# ---------------------------\n# Prepare test features and predict\n# ---------------------------\n\ntest_rows = []\n\nfor drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n    if not os.path.isdir(drive_dir):\n        continue\n    drive_id = os.path.basename(drive_dir)\n    for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n        if not os.path.isdir(phone_dir):\n            continue\n        phone_name = os.path.basename(phone_dir)\n        phone_id = f\"{drive_id}_{phone_name}\"\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            continue\n        gnss = pd.read_csv(gnss_path)\n\n        # Robust time\n        gnss[\"UnixTimeMillis\"] = build_unix_time_millis(gnss)\n\n        wls_cols = [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        for c in wls_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = (\n            gnss.groupby(\"UnixTimeMillis\")\n            .agg(\n                {\n                    \"Cn0DbHz\": \"mean\",\n                    \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                    wls_cols[0]: \"mean\",\n                    wls_cols[1]: \"mean\",\n                    wls_cols[2]: \"mean\",\n                }\n            )\n            .reset_index()\n        )\n        agg[\"phone\"] = phone_id\n        test_rows.append(agg)\n\nif len(test_rows) == 0:\n    raise RuntimeError(\"No test data assembled; check test directory structure.\")\n\ntest_df = pd.concat(test_rows, ignore_index=True)\n\n# Merge with sample_submission to align timestamps\nsample_sub = pd.read_csv(\"./input/sample_submission.csv\")\n\n# Sort before merge_asof\nsample_sorted = sample_sub.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_sorted = test_df.sort_values([\"phone\", \"UnixTimeMillis\"])\n\ntest_df_merged = pd.merge_asof(\n    sample_sorted,\n    test_sorted,\n    on=\"UnixTimeMillis\",\n    by=\"phone\",\n    direction=\"nearest\",\n    tolerance=500,\n)\n\n# For rows without nearby gnss epoch, fallback with larger tolerance\nmissing_mask = test_df_merged[\"Cn0DbHz\"].isna()\nif missing_mask.any():\n    fallback = pd.merge_asof(\n        sample_sorted[missing_mask].sort_values([\"phone\", \"UnixTimeMillis\"]),\n        test_sorted,\n        on=\"UnixTimeMillis\",\n        by=\"phone\",\n        direction=\"nearest\",\n        tolerance=2000,\n    )\n    cols_to_fill = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    for c in cols_to_fill:\n        if c not in test_df_merged.columns:\n            test_df_merged[c] = np.nan\n    test_df_merged.loc[missing_mask, cols_to_fill] = fallback[cols_to_fill].values\n\n# Ensure feature columns present and fill NaNs with train medians\nfor c in feature_cols:\n    if c not in test_df_merged.columns:\n        test_df_merged[c] = np.nan\n\ntest_df_merged[feature_cols] = test_df_merged[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_df_merged[feature_cols].values\nx_pred = final_model_x.predict(X_test)\ny_pred = final_model_y.predict(X_test)\nz_pred = final_model_z.predict(X_test)\n\nlat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\nsubmission = sample_sub.copy()\nsubmission[\"LatitudeDegrees\"] = lat_pred\nsubmission[\"LongitudeDegrees\"] = lon_pred\n\nos.makedirs(\"./submission\", exist_ok=True)\nsubmission_path = \"./submission/submission.csv\"\nsubmission.to_csv(submission_path, index=False)\nprint(f\"Saved submission to {submission_path}\")\n", "import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold, KFold\nimport lightgbm as lgb\n\n# Directories\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\n# Haversine and competition metric (same definition as description)\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0  # meters\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef load_data():\n    # In this benchmark, train.csv and test.csv are usually provided\n    train_path = os.path.join(INPUT_DIR, \"train.csv\")\n    test_path = os.path.join(INPUT_DIR, \"test.csv\")\n    if not os.path.exists(train_path):\n        raise FileNotFoundError(f\"{train_path} not found\")\n    if not os.path.exists(test_path):\n        raise FileNotFoundError(f\"{test_path} not found\")\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n    return train, test\n\n\ndef prepare_features(train, test):\n    # Identify target columns\n    # Try common naming from description / sample_submission\n    possible_lat_cols = [c for c in train.columns if c.lower().startswith(\"lat\")]\n    possible_lon_cols = [c for c in train.columns if c.lower().startswith(\"lon\")]\n    if \"LatitudeDegrees\" in train.columns:\n        lat_col = \"LatitudeDegrees\"\n    elif possible_lat_cols:\n        lat_col = possible_lat_cols[0]\n    else:\n        raise RuntimeError(\"Could not find latitude column in train data\")\n\n    if \"LongitudeDegrees\" in train.columns:\n        lon_col = \"LongitudeDegrees\"\n    elif possible_lon_cols:\n        lon_col = possible_lon_cols[0]\n    else:\n        raise RuntimeError(\"Could not find longitude column in train data\")\n\n    # Non-feature columns: targets + identifiers/time\n    non_feature_keywords = [\n        lat_col,\n        lon_col,\n        \"phone\",\n        \"UnixTimeMillis\",\n        \"collectionName\",\n        \"millisSinceGpsEpoch\",\n        \"set\",\n    ]\n    non_feature_cols = [\n        c\n        for c in train.columns\n        if any(k == c or k.lower() == c.lower() for k in non_feature_keywords)\n    ]\n\n    feature_cols = [c for c in train.columns if c not in non_feature_cols]\n    # Keep numeric features only\n    feature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(train[c])]\n    if not feature_cols:\n        raise RuntimeError(\"No numeric feature columns found to train on.\")\n\n    X_train = train[feature_cols].copy()\n    y_lat = train[lat_col].astype(float).values\n    y_lon = train[lon_col].astype(float).values\n\n    X_test = test[feature_cols].copy()\n    # Align columns between train and test (if some features missing in test, fill later)\n    for c in feature_cols:\n        if c not in X_test.columns:\n            X_test[c] = np.nan\n    X_test = X_test[feature_cols]\n\n    # Impute missing with train medians\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, lat_col, lon_col, medians\n\n\ndef run_cv(X, y_lat, y_lon, train_df):\n    n_splits = 5\n    if \"phone\" in train_df.columns:\n        groups = train_df[\"phone\"].astype(str).values\n        splitter = GroupKFold(n_splits=n_splits)\n        split_gen = splitter.split(X, y_lat, groups=groups)\n    else:\n        splitter = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n        split_gen = splitter.split(X, y_lat)\n\n    scores = []\n    fold_idx = 0\n    for tr_idx, va_idx in split_gen:\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=400,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=50,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=400,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=50,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            },\n            index=train_df.index[va_idx],\n        )\n        df_eval[\"gt_lat\"] = train_df.iloc[va_idx][\n            (\n                \"LatitudeDegrees\"\n                if \"LatitudeDegrees\" in train_df.columns\n                else train_df.columns[0]\n            )\n        ].values\n        df_eval[\"gt_lon\"] = train_df.iloc[va_idx][\n            (\n                \"LongitudeDegrees\"\n                if \"LongitudeDegrees\" in train_df.columns\n                else train_df.columns[1]\n            )\n        ].values\n        if \"phone\" in train_df.columns:\n            df_eval[\"phone\"] = train_df.iloc[va_idx][\"phone\"].values\n        else:\n            df_eval[\"phone\"] = \"all\"\n\n        score = competition_metric(\n            df_eval[[\"phone\", \"gt_lat\", \"gt_lon\", \"pred_lat\", \"pred_lon\"]]\n        )\n        print(f\"Fold {fold_idx} competition-like metric: {score:.6f}\")\n        scores.append(score)\n        fold_idx += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=400)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=400)\n    return model_lat, model_lon\n\n\ndef main():\n    train_df, test_df = load_data()\n\n    # Ensure identifier columns exist\n    if \"phone\" not in train_df.columns and \"phone\" in test_df.columns:\n        train_df[\"phone\"] = \"all\"\n    if \"phone\" not in test_df.columns and \"phone\" in train_df.columns:\n        test_df[\"phone\"] = \"all\"\n    if \"phone\" not in train_df.columns and \"phone\" not in test_df.columns:\n        train_df[\"phone\"] = \"all\"\n        test_df[\"phone\"] = \"all\"\n\n    if \"UnixTimeMillis\" not in train_df.columns and \"UnixTimeMillis\" in test_df.columns:\n        train_df[\"UnixTimeMillis\"] = np.arange(len(train_df))\n    if \"UnixTimeMillis\" not in test_df.columns and \"UnixTimeMillis\" in train_df.columns:\n        test_df[\"UnixTimeMillis\"] = np.arange(len(test_df))\n    if (\n        \"UnixTimeMillis\" not in train_df.columns\n        and \"UnixTimeMillis\" not in test_df.columns\n    ):\n        train_df[\"UnixTimeMillis\"] = np.arange(len(train_df))\n        test_df[\"UnixTimeMillis\"] = np.arange(len(test_df))\n\n    X_train, y_lat, y_lon, X_test, feature_cols, lat_col, lon_col, medians = (\n        prepare_features(train_df, test_df)\n    )\n\n    # For evaluation metric, ensure we know which columns are lat/lon for gt\n    if \"LatitudeDegrees\" not in train_df.columns:\n        train_df = train_df.copy()\n        train_df[\"LatitudeDegrees\"] = train_df[lat_col].astype(float)\n    if \"LongitudeDegrees\" not in train_df.columns:\n        train_df = train_df.copy()\n        train_df[\"LongitudeDegrees\"] = train_df[lon_col].astype(float)\n\n    # Run 5-fold CV\n    run_cv(X_train, y_lat, y_lon, train_df)\n\n    # Train final models\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Predict on test\n    pred_lat = model_lat.predict(X_test)\n    pred_lon = model_lon.predict(X_test)\n\n    # Build submission using sample_submission.csv if available\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if os.path.exists(sample_path):\n        sample_sub = pd.read_csv(sample_path)\n        # Ensure dtypes for merge keys\n        if (\n            \"UnixTimeMillis\" in sample_sub.columns\n            and \"UnixTimeMillis\" in test_df.columns\n        ):\n            sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n            test_df[\"UnixTimeMillis\"] = test_df[\"UnixTimeMillis\"].astype(\"int64\")\n        if \"phone\" in sample_sub.columns and \"phone\" in test_df.columns:\n            sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n            test_df[\"phone\"] = test_df[\"phone\"].astype(str)\n\n        test_pred_df = test_df.copy()\n        test_pred_df[\"pred_lat\"] = pred_lat\n        test_pred_df[\"pred_lon\"] = pred_lon\n\n        # Merge on phone and UnixTimeMillis if present, else rely on order\n        merge_keys = []\n        for k in [\"phone\", \"UnixTimeMillis\"]:\n            if k in sample_sub.columns and k in test_pred_df.columns:\n                merge_keys.append(k)\n\n        if merge_keys:\n            merged = pd.merge(\n                sample_sub,\n                test_pred_df[merge_keys + [\"pred_lat\", \"pred_lon\"]],\n                on=merge_keys,\n                how=\"left\",\n            )\n            # If some rows didn't match (NaNs), fill them using overall means\n            if merged[\"pred_lat\"].isna().any():\n                merged[\"pred_lat\"] = merged[\"pred_lat\"].fillna(pred_lat.mean())\n            if merged[\"pred_lon\"].isna().any():\n                merged[\"pred_lon\"] = merged[\"pred_lon\"].fillna(pred_lon.mean())\n            submission = merged.copy()\n            submission[\"LatitudeDegrees\"] = submission[\"pred_lat\"]\n            submission[\"LongitudeDegrees\"] = submission[\"pred_lon\"]\n        else:\n            # Fallback: assume same order\n            submission = sample_sub.copy()\n            submission[\"LatitudeDegrees\"] = pred_lat[: len(submission)]\n            submission[\"LongitudeDegrees\"] = pred_lon[: len(submission)]\n    else:\n        # No sample submission, construct from test_df\n        submission = pd.DataFrame()\n        for col in [\"phone\", \"UnixTimeMillis\"]:\n            if col in test_df.columns:\n                submission[col] = test_df[col]\n        submission[\"LatitudeDegrees\"] = pred_lat\n        submission[\"LongitudeDegrees\"] = pred_lon\n\n    # Final NaN safety\n    for col in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n        if submission[col].isna().any():\n            submission[col] = submission[col].fillna(submission[col].mean())\n\n    # Ensure required columns exist and order them if possible\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    for c in required_cols:\n        if c not in submission.columns:\n            if c == \"phone\":\n                submission[c] = \"all\"\n            elif c == \"UnixTimeMillis\":\n                submission[c] = np.arange(len(submission))\n    submission = submission[required_cols]\n\n    # Save submission files\n    submission_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    submission_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(submission_path_working, index=False)\n    submission.to_csv(submission_path_final, index=False)\n    print(f\"Saved submission to {submission_path_working} and {submission_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# ---------------------------\n# Utility functions\n# ---------------------------\n\n\ndef ecef_to_llh(x, y, z):\n    # WGS84 ellipsoid constants:\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = a * np.sqrt(1 - e2)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    Esq = a * a - b * b\n    F = 54 * b * b * z * z\n    G = r * r + (1 - e2) * z * z - e2 * Esq\n    C = (e2 * e2 * F * r * r) / (G * G * G + 1e-16)\n    S = np.cbrt(1 + C + np.sqrt(C * C + 2 * C))\n    P = F / (3 * (S + 1 / S + 1) ** 2 * G * G + 1e-16)\n    Q = np.sqrt(1 + 2 * e2 * e2 * P)\n    r0 = -(P * e2 * r) / (1 + Q) + np.sqrt(\n        0.5 * a * a * (1 + 1 / Q)\n        - P * (1 - e2) * z * z / (Q * (1 + Q) + 1e-16)\n        - 0.5 * P * r * r\n    )\n    U = np.sqrt((r - e2 * r0) ** 2 + z * z)\n    V = np.sqrt((r - e2 * r0) ** 2 + (1 - e2) * z * z)\n    Z0 = b * b * z / (a * V + 1e-16)\n    h = U * (1 - b * b / (a * V + 1e-16))\n    lat = np.arctan((z + e2 * Z0) / (r + 1e-16))\n    lon = np.arctan2(y, x)\n    return np.degrees(lat), np.degrees(lon), h\n\n\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df = df.copy()\n    df[\"dist\"] = dist\n    per_phone = []\n    for phone, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if len(per_phone) == 0:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\ndef build_unix_time_millis(gnss_df):\n    \"\"\"Build UnixTimeMillis using ArrivalTimeNanosSinceGpsEpoch if possible, else utcTimeMillis.\"\"\"\n    if \"ArrivalTimeNanosSinceGpsEpoch\" in gnss_df.columns:\n        arr = pd.to_numeric(gnss_df[\"ArrivalTimeNanosSinceGpsEpoch\"], errors=\"coerce\")\n        arr_millis = (arr // 1_000_000).astype(\"Int64\")\n        unix = pd.to_numeric(gnss_df[\"utcTimeMillis\"], errors=\"coerce\").astype(\"Int64\")\n        mask = arr_millis.notna()\n        unix[mask] = arr_millis[mask]\n        if unix.isna().any():\n            utc_clean = pd.to_numeric(gnss_df[\"utcTimeMillis\"], errors=\"coerce\")\n            median_utc = int(np.nanmedian(utc_clean.values))\n            unix = unix.fillna(median_utc)\n        return unix.astype(\"int64\")\n    else:\n        utc = pd.to_numeric(gnss_df[\"utcTimeMillis\"], errors=\"coerce\")\n        if utc.isna().any():\n            median_utc = int(np.nanmedian(utc.values))\n            utc = utc.fillna(median_utc)\n        return utc.astype(\"int64\")\n\n\n# ---------------------------\n# Load training data\n# ---------------------------\n\ntrain_root = \"./input/train\"\ntest_root = \"./input/test\"\n\ntrain_rows = []\n\nfor drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n    if not os.path.isdir(drive_dir):\n        continue\n    drive_id = os.path.basename(drive_dir)\n    for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n        if not os.path.isdir(phone_dir):\n            continue\n        phone_name = os.path.basename(phone_dir)\n        phone_id = f\"{drive_id}_{phone_name}\"\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n        if not os.path.exists(gnss_path) or not os.path.exists(gt_path):\n            continue\n\n        gnss = pd.read_csv(gnss_path)\n        gt = pd.read_csv(gt_path)\n\n        # Robust time construction\n        gnss[\"UnixTimeMillis\"] = build_unix_time_millis(gnss)\n\n        wls_cols = [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        for c in wls_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = (\n            gnss.groupby(\"UnixTimeMillis\")\n            .agg(\n                {\n                    \"Cn0DbHz\": \"mean\",\n                    \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                    wls_cols[0]: \"mean\",\n                    wls_cols[1]: \"mean\",\n                    wls_cols[2]: \"mean\",\n                }\n            )\n            .reset_index()\n        )\n        agg[\"phone\"] = phone_id\n\n        gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n        gt_small[\"phone\"] = phone_id\n\n        agg = agg.sort_values(\"UnixTimeMillis\")\n        gt_small = gt_small.sort_values(\"UnixTimeMillis\")\n\n        merged = pd.merge_asof(\n            gt_small,\n            agg,\n            on=\"UnixTimeMillis\",\n            by=\"phone\",\n            direction=\"nearest\",\n            tolerance=200,\n        )\n        merged = merged.dropna(\n            subset=[\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n        if len(merged) == 0:\n            continue\n\n        x_gt, y_gt, z_gt = llh_to_ecef(\n            merged[\"LatitudeDegrees\"].values,\n            merged[\"LongitudeDegrees\"].values,\n            np.zeros(len(merged)),\n        )\n        merged[\"x_gt\"] = x_gt\n        merged[\"y_gt\"] = y_gt\n        merged[\"z_gt\"] = z_gt\n\n        train_rows.append(merged)\n\nif len(train_rows) == 0:\n    raise RuntimeError(\"No training data assembled; check train directory structure.\")\n\ntrain_df = pd.concat(train_rows, ignore_index=True)\n\nfeature_cols = [\n    \"Cn0DbHz\",\n    \"PseudorangeRateUncertaintyMetersPerSecond\",\n    \"WlsPositionXEcefMeters\",\n    \"WlsPositionYEcefMeters\",\n    \"WlsPositionZEcefMeters\",\n]\nfor c in feature_cols:\n    if c not in train_df.columns:\n        train_df[c] = np.nan\n\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_x = train_df[\"x_gt\"].values\ny_y = train_df[\"y_gt\"].values\ny_z = train_df[\"z_gt\"].values\ngroups = train_df[\"phone\"].str.split(\"_\").str[0]\n\n# ---------------------------\n# 5-fold Group CV\n# ---------------------------\ngkf = GroupKFold(n_splits=5)\n\noof_pred = np.zeros((len(train_df), 3))\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(X, y_x, groups), start=1):\n    X_tr, X_val = X[train_idx], X[val_idx]\n    yx_tr, yx_val = y_x[train_idx], y_x[val_idx]\n    yy_tr, yy_val = y_y[train_idx], y_y[val_idx]\n    yz_tr, yz_val = y_z[train_idx], y_z[val_idx]\n\n    params = dict(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=-1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_x = LGBMRegressor(**params)\n    model_y = LGBMRegressor(**params)\n    model_z = LGBMRegressor(**params)\n\n    model_x.fit(X_tr, yx_tr)\n    model_y.fit(X_tr, yy_tr)\n    model_z.fit(X_tr, yz_tr)\n\n    oof_pred[val_idx, 0] = model_x.predict(X_val)\n    oof_pred[val_idx, 1] = model_y.predict(X_val)\n    oof_pred[val_idx, 2] = model_z.predict(X_val)\n\nlat_pred_oof, lon_pred_oof, _ = ecef_to_llh(\n    oof_pred[:, 0], oof_pred[:, 1], oof_pred[:, 2]\n)\neval_df = pd.DataFrame(\n    {\n        \"phone\": train_df[\"phone\"].values,\n        \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n        \"lat_pred\": lat_pred_oof,\n        \"lon_pred\": lon_pred_oof,\n        \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n        \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n    }\n)\nmetric_value = competition_metric(eval_df)\nprint(\"CV competition metric (mean of 50th and 95th pct errors):\", metric_value)\n\n# ---------------------------\n# Train final models on all data\n# ---------------------------\nfinal_params = dict(\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=-1,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_x = LGBMRegressor(**final_params)\nfinal_model_y = LGBMRegressor(**final_params)\nfinal_model_z = LGBMRegressor(**final_params)\n\nfinal_model_x.fit(X, y_x)\nfinal_model_y.fit(X, y_y)\nfinal_model_z.fit(X, y_z)\n\n# ---------------------------\n# Prepare test features and predict\n# ---------------------------\n\ntest_rows = []\n\nfor drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n    if not os.path.isdir(drive_dir):\n        continue\n    drive_id = os.path.basename(drive_dir)\n    for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n        if not os.path.isdir(phone_dir):\n            continue\n        phone_name = os.path.basename(phone_dir)\n        phone_id = f\"{drive_id}_{phone_name}\"\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            continue\n        gnss = pd.read_csv(gnss_path)\n\n        gnss[\"UnixTimeMillis\"] = build_unix_time_millis(gnss)\n\n        wls_cols = [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        for c in wls_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = (\n            gnss.groupby(\"UnixTimeMillis\")\n            .agg(\n                {\n                    \"Cn0DbHz\": \"mean\",\n                    \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                    wls_cols[0]: \"mean\",\n                    wls_cols[1]: \"mean\",\n                    wls_cols[2]: \"mean\",\n                }\n            )\n            .reset_index()\n        )\n        agg[\"phone\"] = phone_id\n        test_rows.append(agg)\n\nif len(test_rows) == 0:\n    raise RuntimeError(\"No test data assembled; check test directory structure.\")\n\ntest_df = pd.concat(test_rows, ignore_index=True)\n\n# ---------------------------\n# Align with sample_submission and predict\n# ---------------------------\n\nsample_sub = pd.read_csv(\"./input/sample_submission.csv\")\n\n# Build \"phone\" key in sample_sub: collectionName + \"_\" + phoneName\nif \"collectionName\" in sample_sub.columns and \"phoneName\" in sample_sub.columns:\n    sample_sub[\"phone\"] = (\n        sample_sub[\"collectionName\"].astype(str)\n        + \"_\"\n        + sample_sub[\"phoneName\"].astype(str)\n    )\nelif \"phone\" not in sample_sub.columns:\n    # Fallback: if a single identifier column exists, use that\n    # but competition format for this benchmark should have collectionName/phoneName\n    raise RuntimeError(\"sample_submission missing expected phone identifiers\")\n\n# Ensure UnixTimeMillis is int64 for merge_asof\nsample_sub[\"UnixTimeMillis\"] = pd.to_numeric(\n    sample_sub[\"UnixTimeMillis\"], errors=\"coerce\"\n).astype(\"int64\")\ntest_df[\"UnixTimeMillis\"] = pd.to_numeric(\n    test_df[\"UnixTimeMillis\"], errors=\"coerce\"\n).astype(\"int64\")\n\n# Sort before merge_asof\nsample_sorted = sample_sub.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index()\ntest_sorted = test_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# First merge with tight tolerance\ntest_df_merged = pd.merge_asof(\n    sample_sorted,\n    test_sorted,\n    on=\"UnixTimeMillis\",\n    by=\"phone\",\n    direction=\"nearest\",\n    tolerance=500,\n)\n\n# For rows without nearby gnss epoch, fallback with larger tolerance\nmissing_mask = test_df_merged[\"Cn0DbHz\"].isna()\nif missing_mask.any():\n    sample_missing = (\n        sample_sorted.loc[missing_mask].copy().sort_values([\"phone\", \"UnixTimeMillis\"])\n    )\n    fallback = pd.merge_asof(\n        sample_missing,\n        test_sorted,\n        on=\"UnixTimeMillis\",\n        by=\"phone\",\n        direction=\"nearest\",\n        tolerance=2000,\n    )\n\n    cols_to_fill = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    for c in cols_to_fill:\n        if c not in test_df_merged.columns:\n            test_df_merged[c] = np.nan\n\n    # Align fallback rows back to test_df_merged by original index\n    fallback_index_map = dict(zip(fallback[\"index\"], range(len(fallback))))\n    for idx in test_df_merged.index[missing_mask]:\n        if idx in fallback_index_map:\n            fb_row = fallback.iloc[fallback_index_map[idx]]\n            for c in cols_to_fill:\n                test_df_merged.loc[idx, c] = fb_row.get(c, np.nan)\n\n# Ensure feature columns present and filled\nfor c in feature_cols:\n    if c not in test_df_merged.columns:\n        test_df_merged[c] = np.nan\n\ntest_df_merged[feature_cols] = test_df_merged[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_df_merged[feature_cols].values\nx_pred = final_model_x.predict(X_test)\ny_pred = final_model_y.predict(X_test)\nz_pred = final_model_z.predict(X_test)\n\nlat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n# Restore original order of sample_submission\ntest_df_merged_sorted_back = test_df_merged.sort_values(\"index\")\nlat_pred = lat_pred[test_df_merged_sorted_back.index.values]\nlon_pred = lon_pred[test_df_merged_sorted_back.index.values]\n\nsubmission = sample_sub.copy()\nsubmission = submission.sort_values(\"index\")\nsubmission[\"LatitudeDegrees\"] = lat_pred\nsubmission[\"LongitudeDegrees\"] = lon_pred\nsubmission = submission.drop(columns=[\"phone\", \"index\"], errors=\"ignore\")\n\nos.makedirs(\"./submission\", exist_ok=True)\nsubmission_path = \"./submission/submission.csv\"\nsubmission.to_csv(submission_path, index=False)\nprint(f\"Saved submission to {submission_path}\")\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n\ndef ecef_to_llh(x, y, z):\n    # WGS84 ellipsoid constants:\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = a * np.sqrt(1 - e2)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    Esq = a * a - b * b\n    F = 54 * b * b * z * z\n    G = r * r + (1 - e2) * z * z - e2 * Esq\n    C = (e2 * e2 * F * r * r) / (G * G * G + 1e-16)\n    S = np.cbrt(1 + C + np.sqrt(C * C + 2 * C))\n    P = F / (3 * (S + 1 / S + 1) ** 2 * G * G + 1e-16)\n    Q = np.sqrt(1 + 2 * e2 * e2 * P)\n    r0 = -(P * e2 * r) / (1 + Q) + np.sqrt(\n        0.5 * a * a * (1 + 1 / Q)\n        - P * (1 - e2) * z * z / (Q * (1 + Q) + 1e-16)\n        - 0.5 * P * r * r\n    )\n    U = np.sqrt((r - e2 * r0) ** 2 + z * z)\n    V = np.sqrt((r - e2 * r0) ** 2 + (1 - e2) * z * z)\n    Z0 = b * b * z / (a * V + 1e-16)\n    h = U * (1 - b * b / (a * V + 1e-16))\n    lat = np.arctan((z + e2 * Z0) / (r + 1e-16))\n    lon = np.arctan2(y, x)\n    return np.degrees(lat), np.degrees(lon), h\n\n\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df = df.copy()\n    df[\"dist\"] = dist\n    per_phone = []\n    for phone, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if len(per_phone) == 0:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\ndef build_unix_time_millis(gnss_df):\n    \"\"\"Build UnixTimeMillis using ArrivalTimeNanosSinceGpsEpoch if possible, else utcTimeMillis.\"\"\"\n    if \"ArrivalTimeNanosSinceGpsEpoch\" in gnss_df.columns:\n        arr = pd.to_numeric(gnss_df[\"ArrivalTimeNanosSinceGpsEpoch\"], errors=\"coerce\")\n        arr_millis = (arr // 1_000_000).astype(\"Int64\")\n        unix = pd.to_numeric(gnss_df[\"utcTimeMillis\"], errors=\"coerce\").astype(\"Int64\")\n        mask = arr_millis.notna()\n        unix[mask] = arr_millis[mask]\n        if unix.isna().any():\n            utc_clean = pd.to_numeric(gnss_df[\"utcTimeMillis\"], errors=\"coerce\")\n            median_utc = int(np.nanmedian(utc_clean.values))\n            unix = unix.fillna(median_utc)\n        return unix.astype(\"int64\")\n    else:\n        utc = pd.to_numeric(gnss_df[\"utcTimeMillis\"], errors=\"coerce\")\n        if utc.isna().any():\n            median_utc = int(np.nanmedian(utc.values))\n            utc = utc.fillna(median_utc)\n        return utc.astype(\"int64\")\n\n\ndef main():\n    train_root = \"./input/train\"\n    test_root = \"./input/test\"\n\n    # ---------------------------\n    # Load training data\n    # ---------------------------\n    train_rows = []\n\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not os.path.exists(gnss_path) or not os.path.exists(gt_path):\n                continue\n\n            gnss = pd.read_csv(gnss_path)\n            gt = pd.read_csv(gt_path)\n\n            gnss[\"UnixTimeMillis\"] = build_unix_time_millis(gnss)\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = (\n                gnss.groupby(\"UnixTimeMillis\")\n                .agg(\n                    {\n                        \"Cn0DbHz\": \"mean\",\n                        \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                        wls_cols[0]: \"mean\",\n                        wls_cols[1]: \"mean\",\n                        wls_cols[2]: \"mean\",\n                    }\n                )\n                .reset_index()\n            )\n            agg[\"phone\"] = phone_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n\n            agg = agg.sort_values(\"UnixTimeMillis\")\n            gt_small = gt_small.sort_values(\"UnixTimeMillis\")\n\n            merged = pd.merge_asof(\n                gt_small,\n                agg,\n                on=\"UnixTimeMillis\",\n                by=\"phone\",\n                direction=\"nearest\",\n                tolerance=200,\n            )\n            merged = merged.dropna(\n                subset=[\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n\n            train_rows.append(merged)\n\n    if len(train_rows) == 0:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n\n    train_df = pd.concat(train_rows, ignore_index=True)\n\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n    # group by collection (drive), not phone model\n    groups = train_df[\"phone\"].str.split(\"_\").str[0]\n\n    # ---------------------------\n    # 5-fold Group CV\n    # ---------------------------\n    gkf = GroupKFold(n_splits=5)\n\n    oof_pred = np.zeros((len(train_df), 3))\n    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y_x, groups), start=1):\n        X_tr, X_val = X[train_idx], X[val_idx]\n        yx_tr, yx_val = y_x[train_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[train_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[train_idx], y_z[val_idx]\n\n        params = dict(\n            n_estimators=200,\n            learning_rate=0.05,\n            max_depth=-1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42 + fold,\n            n_jobs=-1,\n        )\n        model_x = LGBMRegressor(**params)\n        model_y = LGBMRegressor(**params)\n        model_z = LGBMRegressor(**params)\n\n        model_x.fit(X_tr, yx_tr)\n        model_y.fit(X_tr, yy_tr)\n        model_z.fit(X_tr, yz_tr)\n\n        oof_pred[val_idx, 0] = model_x.predict(X_val)\n        oof_pred[val_idx, 1] = model_y.predict(X_val)\n        oof_pred[val_idx, 2] = model_z.predict(X_val)\n\n    lat_pred_oof, lon_pred_oof, _ = ecef_to_llh(\n        oof_pred[:, 0], oof_pred[:, 1], oof_pred[:, 2]\n    )\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_pred_oof,\n            \"lon_pred\": lon_pred_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_value = competition_metric(eval_df)\n    print(\"CV competition metric (mean of 50th and 95th pct errors):\", metric_value)\n\n    # ---------------------------\n    # Train final models on all data\n    # ---------------------------\n    final_params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        random_state=42,\n        n_jobs=-1,\n    )\n    final_model_x = LGBMRegressor(**final_params)\n    final_model_y = LGBMRegressor(**final_params)\n    final_model_z = LGBMRegressor(**final_params)\n\n    final_model_x.fit(X, y_x)\n    final_model_y.fit(X, y_y)\n    final_model_z.fit(X, y_z)\n\n    # ---------------------------\n    # Prepare test features\n    # ---------------------------\n    test_rows = []\n\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            gnss = pd.read_csv(gnss_path)\n\n            gnss[\"UnixTimeMillis\"] = build_unix_time_millis(gnss)\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = (\n                gnss.groupby(\"UnixTimeMillis\")\n                .agg(\n                    {\n                        \"Cn0DbHz\": \"mean\",\n                        \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                        wls_cols[0]: \"mean\",\n                        wls_cols[1]: \"mean\",\n                        wls_cols[2]: \"mean\",\n                    }\n                )\n                .reset_index()\n            )\n            agg[\"phone\"] = phone_id\n            test_rows.append(agg)\n\n    if len(test_rows) == 0:\n        raise RuntimeError(\"No test data assembled; check test directory structure.\")\n\n    test_df = pd.concat(test_rows, ignore_index=True)\n\n    # ---------------------------\n    # Align with sample_submission and predict\n    # ---------------------------\n    sample_sub = pd.read_csv(\"./input/sample_submission.csv\")\n\n    # Offline benchmark sample_submission is expected to already have \"phone\" column\n    if \"phone\" not in sample_sub.columns:\n        # Fallback: attempt to construct \"phone\" if collectionName/phoneName exist\n        if \"collectionName\" in sample_sub.columns and \"phoneName\" in sample_sub.columns:\n            sample_sub[\"phone\"] = (\n                sample_sub[\"collectionName\"].astype(str)\n                + \"_\"\n                + sample_sub[\"phoneName\"].astype(str)\n            )\n        else:\n            raise RuntimeError(\n                \"sample_submission does not have 'phone' and cannot be reconstructed from collectionName/phoneName\"\n            )\n\n    # Remember original order\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    # Ensure integer times\n    sample_sub[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_sub[\"UnixTimeMillis\"], errors=\"coerce\"\n    ).astype(\"int64\")\n    test_df[\"UnixTimeMillis\"] = pd.to_numeric(\n        test_df[\"UnixTimeMillis\"], errors=\"coerce\"\n    ).astype(\"int64\")\n\n    # Sort before merge_asof\n    sample_sorted = sample_sub.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    test_sorted = test_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    # Main merge with moderate tolerance (ms)\n    merged = pd.merge_asof(\n        sample_sorted,\n        test_sorted,\n        on=\"UnixTimeMillis\",\n        by=\"phone\",\n        direction=\"nearest\",\n        tolerance=1000,\n    )\n\n    # If still missing, broader fallback\n    if merged[\"Cn0DbHz\"].isna().any():\n        missing_mask = merged[\"Cn0DbHz\"].isna()\n        sample_missing = sample_sorted.loc[missing_mask].copy()\n        fallback = pd.merge_asof(\n            sample_missing,\n            test_sorted,\n            on=\"UnixTimeMillis\",\n            by=\"phone\",\n            direction=\"nearest\",\n            tolerance=5000,\n        )\n        cols_to_fill = [\n            \"Cn0DbHz\",\n            \"PseudorangeRateUncertaintyMetersPerSecond\",\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        # Use index alignment on sample_missing\n        for c in cols_to_fill:\n            if c not in merged.columns:\n                merged[c] = np.nan\n            merged.loc[missing_mask, c] = merged.loc[missing_mask, c].fillna(\n                fallback[c].values\n            )\n\n    # Ensure feature columns present and filled\n    for c in feature_cols:\n        if c not in merged.columns:\n            merged[c] = np.nan\n\n    merged[feature_cols] = merged[feature_cols].fillna(train_df[feature_cols].median())\n\n    X_test = merged[feature_cols].values\n    x_pred = final_model_x.predict(X_test)\n    y_pred = final_model_y.predict(X_test)\n    z_pred = final_model_z.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    # Map predictions back to original sample_submission order\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    submission = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission = submission.sort_values(\"row_id\")\n\n    # If any prediction missing (extreme case), fallback to simple forward fill\n    if submission[\"LatitudeDegrees\"].isna().any():\n        submission[\"LatitudeDegrees\"] = (\n            submission[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n        )\n    if submission[\"LongitudeDegrees\"].isna().any():\n        submission[\"LongitudeDegrees\"] = (\n            submission[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n        )\n\n    # Build final submission file with required columns\n    out_cols = []\n    for c in [\"phone\", \"UnixTimeMillis\"]:\n        if c in sample_sub.columns:\n            out_cols.append(c)\n    out = sample_sub[out_cols + [\"row_id\"]].merge(\n        submission[[\"row_id\", \"LatitudeDegrees\", \"LongitudeDegrees\"]],\n        on=\"row_id\",\n        how=\"left\",\n    )\n    out = out.drop(columns=[\"row_id\"])\n\n    os.makedirs(\"./submission\", exist_ok=True)\n    submission_path = \"./submission/submission.csv\"\n    out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef load_device_gnss_features(split_dir):\n    \"\"\"\n    Walk through input/{train|test} and extract basic per-epoch features from device_gnss.csv.\n    Returns a DataFrame with columns:\n      phone, UnixTimeMillis, plus numeric features from aggregations.\n    \"\"\"\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n    for fpath in files:\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n        # derive identifiers\n        # path: .../split/collection/phone/device_gnss.csv\n        parts = fpath.replace(\"\\\\\", \"/\").split(\"/\")\n        if len(parts) < 4:\n            continue\n        collection = parts[-3]\n        phone_model = parts[-2]\n        phone = f\"{collection}_{phone_model}\"\n\n        # ensure UnixTimeMillis column exists; device_gnss uses utcTimeMillis\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            # cannot use file without timing info\n            continue\n\n        # basic cleaning\n        df = df.copy()\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n\n        # select useful raw columns\n        feature_sources = [\n            \"Cn0DbHz\",\n            \"PseudorangeRateMetersPerSecond\",\n            \"PseudorangeRateUncertaintyMetersPerSecond\",\n            \"AccumulatedDeltaRangeMeters\",\n            \"AccumulatedDeltaRangeUncertaintyMeters\",\n            \"CarrierFrequencyHz\",\n            \"MultipathIndicator\",\n            \"ConstellationType\",\n            \"SvElevationDegrees\",\n            \"SvAzimuthDegrees\",\n        ]\n        cols_present = [c for c in feature_sources if c in df.columns]\n\n        if not cols_present:\n            # fallback: WlsPosition* as features if available\n            cols_present = [c for c in df.columns if c.startswith(\"WlsPosition\")]\n\n        use_cols = [\"phone\", \"UnixTimeMillis\"] + cols_present\n        df = df[use_cols]\n\n        # aggregate per phone+time (epoch)\n        agg_dict = {}\n        for c in cols_present:\n            if pd.api.types.is_numeric_dtype(df[c]):\n                agg_dict[c] = [\"mean\", \"std\", \"min\", \"max\"]\n        if not agg_dict:\n            continue\n\n        g = df.groupby([\"phone\", \"UnixTimeMillis\"]).agg(agg_dict)\n        # flatten columns\n        g.columns = [\"{}_{}\".format(k, stat) for k, stat in g.columns]\n        g = g.reset_index()\n        all_rows.append(g)\n\n    if not all_rows:\n        # as a last resort, return empty frame to avoid crash\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    features = pd.concat(all_rows, ignore_index=True)\n    return features\n\n\ndef load_ground_truth():\n    \"\"\"\n    Walk through input/train and load ground_truth.csv files to get labels.\n    Columns: phone, UnixTimeMillis, LatitudeDegrees, LongitudeDegrees.\n    \"\"\"\n    base_path = os.path.join(INPUT_DIR, \"train\")\n    pattern = os.path.join(base_path, \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    all_gt = []\n    for fpath in files:\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n        parts = fpath.replace(\"\\\\\", \"/\").split(\"/\")\n        collection = parts[-3]\n        phone_model = parts[-2]\n        phone = f\"{collection}_{phone_model}\"\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            continue\n        if \"LatitudeDegrees\" not in df.columns or \"LongitudeDegrees\" not in df.columns:\n            continue\n        tmp = df[[time_col, \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n        tmp[\"phone\"] = phone\n        tmp[\"UnixTimeMillis\"] = tmp[time_col].astype(\"int64\")\n        tmp = tmp[[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        all_gt.append(tmp)\n    if not all_gt:\n        return pd.DataFrame(\n            columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        )\n    gt = pd.concat(all_gt, ignore_index=True)\n    return gt\n\n\ndef build_train_test_features():\n    train_feats = load_device_gnss_features(\"train\")\n    test_feats = load_device_gnss_features(\"test\")\n\n    gt = load_ground_truth()\n\n    # merge ground truth with train features\n    train = pd.merge(\n        gt,\n        train_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n\n    # If some epochs are missing features (NaNs), we will still keep them (model can handle NAN with medians)\n    return train, test_feats\n\n\ndef prepare_xy(train_df, test_df):\n    # Identify feature columns (numeric, excluding target and keys)\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n    if not feature_cols:\n        raise RuntimeError(\"No numeric feature columns found for modeling.\")\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # align test columns\n    for c in feature_cols:\n        if c not in test_df.columns:\n            test_df[c] = np.nan\n    X_test = test_df[feature_cols].copy()\n\n    # median imputation based on train\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols\n\n\ndef run_cv(X, y_lat, y_lon, meta_df):\n    n_splits = 5\n    if \"phone\" in meta_df.columns:\n        groups = meta_df[\"phone\"].astype(str).values\n    else:\n        groups = None\n\n    if groups is not None:\n        splitter = GroupKFold(n_splits=n_splits)\n        split_gen = splitter.split(X, y_lat, groups=groups)\n    else:\n        splitter = GroupKFold(n_splits=n_splits)\n        split_gen = splitter.split(X, y_lat, groups=np.zeros(len(y_lat)))\n\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in split_gen:\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=300,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=30,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=300,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=30,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": (\n                    meta_df.iloc[va_idx][\"phone\"].values\n                    if \"phone\" in meta_df.columns\n                    else [\"all\"] * len(va_idx)\n                ),\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=300)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=300)\n    return model_lat, model_lon\n\n\ndef main():\n    # Build structured train/test features\n    train_df, test_feats = build_train_test_features()\n\n    # Ensure keys exist in test_feats\n    if \"phone\" not in test_feats.columns or \"UnixTimeMillis\" not in test_feats.columns:\n        # Build from sample_submission as a minimal fallback\n        sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n        if os.path.exists(sample_path):\n            sample_sub = pd.read_csv(sample_path)\n            if \"phone\" in sample_sub.columns and \"UnixTimeMillis\" in sample_sub.columns:\n                # create empty frame with keys only\n                test_feats = (\n                    sample_sub[[\"phone\", \"UnixTimeMillis\"]].drop_duplicates().copy()\n                )\n        else:\n            raise RuntimeError(\n                \"Cannot infer test keys; test features missing keys and no sample_submission.csv\"\n            )\n\n    # Prepare features and labels\n    X_train, y_lat, y_lon, X_test, feature_cols = prepare_xy(train_df, test_feats)\n\n    # Run CV and print metric\n    run_cv(X_train, y_lat, y_lon, train_df[[\"phone\"]].copy())\n\n    # Train final models on all data\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Predictions for internal test_feats frame\n    pred_lat_test = model_lat.predict(X_test)\n    pred_lon_test = model_lon.predict(X_test)\n    test_feats = test_feats.copy()\n    test_feats[\"pred_lat\"] = pred_lat_test\n    test_feats[\"pred_lon\"] = pred_lon_test\n\n    # Build submission using sample_submission.csv\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if os.path.exists(sample_path):\n        sample_sub = pd.read_csv(sample_path)\n        # ensure dtypes for merge keys\n        if (\n            \"UnixTimeMillis\" in sample_sub.columns\n            and \"UnixTimeMillis\" in test_feats.columns\n        ):\n            sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n            test_feats[\"UnixTimeMillis\"] = test_feats[\"UnixTimeMillis\"].astype(\"int64\")\n        if \"phone\" in sample_sub.columns and \"phone\" in test_feats.columns:\n            sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n            test_feats[\"phone\"] = test_feats[\"phone\"].astype(str)\n\n        merge_keys = []\n        for k in [\"phone\", \"UnixTimeMillis\"]:\n            if k in sample_sub.columns and k in test_feats.columns:\n                merge_keys.append(k)\n\n        if merge_keys:\n            merged = pd.merge(\n                sample_sub,\n                test_feats[merge_keys + [\"pred_lat\", \"pred_lon\"]],\n                on=merge_keys,\n                how=\"left\",\n            )\n            # fill any missing preds with global means\n            if merged[\"pred_lat\"].isna().any():\n                merged[\"pred_lat\"] = merged[\"pred_lat\"].fillna(\n                    test_feats[\"pred_lat\"].mean()\n                )\n            if merged[\"pred_lon\"].isna().any():\n                merged[\"pred_lon\"] = merged[\"pred_lon\"].fillna(\n                    test_feats[\"pred_lon\"].mean()\n                )\n            submission = merged.copy()\n            submission[\"LatitudeDegrees\"] = submission[\"pred_lat\"]\n            submission[\"LongitudeDegrees\"] = submission[\"pred_lon\"]\n        else:\n            # fallback: order-based\n            submission = sample_sub.copy()\n            n = len(submission)\n            submission[\"LatitudeDegrees\"] = pred_lat_test[:n]\n            submission[\"LongitudeDegrees\"] = pred_lon_test[:n]\n    else:\n        # construct submission from test_feats directly\n        submission = test_feats[[\"phone\", \"UnixTimeMillis\"]].copy()\n        submission[\"LatitudeDegrees\"] = test_feats[\"pred_lat\"]\n        submission[\"LongitudeDegrees\"] = test_feats[\"pred_lon\"]\n\n    # NaN safety\n    for col in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n        if submission[col].isna().any():\n            submission[col] = submission[col].fillna(submission[col].mean())\n\n    # Ensure required columns exist\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    for c in required_cols:\n        if c not in submission.columns:\n            if c == \"phone\":\n                submission[c] = \"all\"\n            elif c == \"UnixTimeMillis\":\n                submission[c] = np.arange(len(submission))\n    submission = submission[required_cols]\n\n    # Save submission\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nWGS84_A = 6378137.0\nWGS84_E2 = 6.69437999014e-3\n\n\ndef geodetic_to_ecef(lat_deg, lon_deg, alt_m):\n    lat = np.radians(lat_deg)\n    lon = np.radians(lon_deg)\n    a = WGS84_A\n    e2 = WGS84_E2\n    sin_lat = np.sin(lat)\n    cos_lat = np.cos(lat)\n    N = a / np.sqrt(1 - e2 * sin_lat * sin_lat)\n    X = (N + alt_m) * cos_lat * np.cos(lon)\n    Y = (N + alt_m) * cos_lat * np.sin(lon)\n    Z = (N * (1 - e2) + alt_m) * sin_lat\n    return X, Y, Z\n\n\ndef ecef_to_geodetic(x, y, z):\n    # Bowring\u2019s method\n    a = WGS84_A\n    e2 = WGS84_E2\n    b = a * np.sqrt(1 - e2)\n    ep2 = (a * a - b * b) / (b * b)\n    p = np.sqrt(x * x + y * y)\n    th = np.arctan2(a * z, b * p)\n    lon = np.arctan2(y, x)\n    lat = np.arctan2(z + ep2 * b * np.sin(th) ** 3, p - e2 * a * np.cos(th) ** 3)\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    alt = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), alt\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    # df must have columns: lat_gt, lon_gt, lat_pred, lon_pred, phone\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    # basic cleaning\n    gt = gt.dropna(\n        subset=[\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"AltitudeMeters\",\n            \"UnixTimeMillis\",\n        ]\n    )\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n\n    # Derive epoch time in ms to join with ground truth\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # aggregate gnss by epoch\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # join with ground truth\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # target: ECEF from ground truth\n    X_ecef, Y_ecef, Z_ecef = geodetic_to_ecef(\n        df[\"LatitudeDegrees\"].values,\n        df[\"LongitudeDegrees\"].values,\n        df[\"AltitudeMeters\"].values,\n    )\n    df[\"X_ecef\"] = X_ecef\n    df[\"Y_ecef\"] = Y_ecef\n    df[\"Z_ecef\"] = Z_ecef\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        # fallback: align minimum to zero\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    # phone name in train/test features is drive_phone; we will map to Kaggle format later\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception:\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\n\n# build sorted time and simple deltas per phone\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# temporal diffs for mean features\nmean_feats = [c for c in train_df.columns if c.endswith(\"_mean\")]\nfor feat in mean_feats:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# feature columns: aggregated gnss stats and their diffs except targets & labels\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"X_ecef\",\n    \"Y_ecef\",\n    \"Z_ecef\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        c.endswith(\"_mean\")\n        or c.endswith(\"_std\")\n        or c.endswith(\"_diff\")\n        or \"WlsPosition\" in c\n    )\n]\n\n# fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_x = train_df[\"X_ecef\"].values\ny_y = train_df[\"Y_ecef\"].values\nz_true = train_df[\"Z_ecef\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_preds_x = np.zeros(len(train_df))\noof_preds_y = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n    yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_x = LGBMRegressor(**params)\n    model_y = LGBMRegressor(**params)\n\n    # remove unsupported 'verbose' argument\n    model_x.fit(X_tr, yx_tr, eval_set=[(X_val, yx_val)], eval_metric=\"l2\")\n    model_y.fit(X_tr, yy_tr, eval_set=[(X_val, yy_val)], eval_metric=\"l2\")\n\n    oof_preds_x[val_idx] = model_x.predict(X_val)\n    oof_preds_y[val_idx] = model_y.predict(X_val)\n\n# build oof dataframe for metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\nlat_pred, lon_pred, _ = ecef_to_geodetic(oof_preds_x, oof_preds_y, z_true)\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = lat_pred\noof_df[\"lon_pred\"] = lon_pred\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_x = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_y = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_x.fit(X, y_x)\nfinal_model_y.fit(X, y_y)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception:\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\n# Map internal drive_phone to sample phone format \"drive_phone\"\n# Our internal \"phone\" is already \"drive_phone\" (e.g., \"2020-06-04-US-MTV-1_GooglePixel4\"),\n# which matches the phone column in sample_submission, so we keep it.\n# Just ensure dtypes match\ntest_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Temporal sort and fill\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n# Fill with train medians as base\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\n# Prepare frame with required rows from sample submission\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", \"UnixTimeMillis\"]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\n# still any NaNs: fill by train medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\npred_x = final_model_x.predict(X_test)\npred_y = final_model_y.predict(X_test)\n\n# Approximate Z as mean of training Z (ECEF)\nmean_z = z_true.mean()\npred_z = np.full_like(pred_x, mean_z)\npred_lat, pred_lon, _ = ecef_to_geodetic(pred_x, pred_y, pred_z)\n\nsubmission = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n\ndef ecef_to_llh(x, y, z):\n    # WGS84 ellipsoid constants:\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = a * np.sqrt(1 - e2)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    Esq = a * a - b * b\n    F = 54 * b * b * z * z\n    G = r * r + (1 - e2) * z * z - e2 * Esq\n    C = (e2 * e2 * F * r * r) / (G * G * G + 1e-16)\n    S = np.cbrt(1 + C + np.sqrt(C * C + 2 * C))\n    P = F / (3 * (S + 1 / S + 1) ** 2 * G * G + 1e-16)\n    Q = np.sqrt(1 + 2 * e2 * e2 * P)\n    r0 = -(P * e2 * r) / (1 + Q) + np.sqrt(\n        0.5 * a * a * (1 + 1 / Q)\n        - P * (1 - e2) * z * z / (Q * (1 + Q) + 1e-16)\n        - 0.5 * P * r * r\n    )\n    U = np.sqrt((r - e2 * r0) ** 2 + z * z)\n    V = np.sqrt((r - e2 * r0) ** 2 + (1 - e2) * z * z)\n    Z0 = b * b * z / (a * V + 1e-16)\n    h = U * (1 - b * b / (a * V + 1e-16))\n    lat = np.arctan((z + e2 * Z0) / (r + 1e-16))\n    lon = np.arctan2(y, x)\n    return np.degrees(lat), np.degrees(lon), h\n\n\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df = df.copy()\n    df[\"dist\"] = dist\n    per_phone = []\n    for phone, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if len(per_phone) == 0:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\ndef build_unix_time_millis(gnss_df):\n    \"\"\"Build UnixTimeMillis using ArrivalTimeNanosSinceGpsEpoch if possible, else utcTimeMillis.\"\"\"\n    if \"ArrivalTimeNanosSinceGpsEpoch\" in gnss_df.columns:\n        arr = pd.to_numeric(gnss_df[\"ArrivalTimeNanosSinceGpsEpoch\"], errors=\"coerce\")\n        arr_millis = (arr // 1_000_000).astype(\"Int64\")\n        unix = pd.to_numeric(gnss_df[\"utcTimeMillis\"], errors=\"coerce\").astype(\"Int64\")\n        mask = arr_millis.notna()\n        unix[mask] = arr_millis[mask]\n        if unix.isna().any():\n            utc_clean = pd.to_numeric(gnss_df[\"utcTimeMillis\"], errors=\"coerce\")\n            median_utc = int(np.nanmedian(utc_clean.values))\n            unix = unix.fillna(median_utc)\n        return unix.astype(\"int64\")\n    else:\n        utc = pd.to_numeric(gnss_df[\"utcTimeMillis\"], errors=\"coerce\")\n        if utc.isna().any():\n            median_utc = int(np.nanmedian(utc.values))\n            utc = utc.fillna(median_utc)\n        return utc.astype(\"int64\")\n\n\ndef load_train_data(train_root):\n    train_rows = []\n\n    # expect structure: train/drive_id/phone_name/{device_gnss.csv, ground_truth.csv}\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            gnss = pd.read_csv(gnss_path)\n            gt = pd.read_csv(gt_path)\n\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = build_unix_time_millis(gnss)\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = (\n                gnss.groupby(\"UnixTimeMillis\")\n                .agg(\n                    {\n                        \"Cn0DbHz\": \"mean\",\n                        \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                        wls_cols[0]: \"mean\",\n                        wls_cols[1]: \"mean\",\n                        wls_cols[2]: \"mean\",\n                    }\n                )\n                .reset_index()\n            )\n            agg[\"phone\"] = phone_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n\n            agg = agg.sort_values(\"UnixTimeMillis\")\n            gt_small = gt_small.sort_values(\"UnixTimeMillis\")\n\n            merged = pd.merge_asof(\n                gt_small,\n                agg,\n                on=\"UnixTimeMillis\",\n                by=\"phone\",\n                direction=\"nearest\",\n                tolerance=200,\n            )\n            merged = merged.dropna(\n                subset=[\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n\n            train_rows.append(merged)\n\n    if len(train_rows) == 0:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n\n    train_df = pd.concat(train_rows, ignore_index=True)\n    return train_df\n\n\ndef load_test_features(test_root):\n    test_rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            gnss = pd.read_csv(gnss_path)\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = build_unix_time_millis(gnss)\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = (\n                gnss.groupby(\"UnixTimeMillis\")\n                .agg(\n                    {\n                        \"Cn0DbHz\": \"mean\",\n                        \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                        wls_cols[0]: \"mean\",\n                        wls_cols[1]: \"mean\",\n                        wls_cols[2]: \"mean\",\n                    }\n                )\n                .reset_index()\n            )\n            agg[\"phone\"] = phone_id\n            test_rows.append(agg)\n\n    if len(test_rows) == 0:\n        raise RuntimeError(\"No test data assembled; check test directory structure.\")\n\n    test_df = pd.concat(test_rows, ignore_index=True)\n    return test_df\n\n\ndef main():\n    train_root = \"./input/train\"\n    test_root = \"./input/test\"\n\n    # Load training data\n    train_df = load_train_data(train_root)\n\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n\n    # Fill NaNs with column medians\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    # group by collection (drive), not phone model\n    groups = train_df[\"phone\"].str.split(\"_\").str[0]\n\n    # 5-fold Group CV\n    gkf = GroupKFold(n_splits=5)\n\n    oof_pred = np.zeros((len(train_df), 3))\n    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y_x, groups), start=1):\n        X_tr, X_val = X[train_idx], X[val_idx]\n        yx_tr, yx_val = y_x[train_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[train_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[train_idx], y_z[val_idx]\n\n        params = dict(\n            n_estimators=200,\n            learning_rate=0.05,\n            max_depth=-1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42 + fold,\n            n_jobs=-1,\n        )\n        model_x = LGBMRegressor(**params)\n        model_y = LGBMRegressor(**params)\n        model_z = LGBMRegressor(**params)\n\n        model_x.fit(X_tr, yx_tr)\n        model_y.fit(X_tr, yy_tr)\n        model_z.fit(X_tr, yz_tr)\n\n        oof_pred[val_idx, 0] = model_x.predict(X_val)\n        oof_pred[val_idx, 1] = model_y.predict(X_val)\n        oof_pred[val_idx, 2] = model_z.predict(X_val)\n\n    lat_pred_oof, lon_pred_oof, _ = ecef_to_llh(\n        oof_pred[:, 0], oof_pred[:, 1], oof_pred[:, 2]\n    )\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_pred_oof,\n            \"lon_pred\": lon_pred_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_value = competition_metric(eval_df)\n    print(\"CV competition metric (mean of 50th and 95th pct errors):\", metric_value)\n\n    # Train final models on all data\n    final_params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        random_state=42,\n        n_jobs=-1,\n    )\n    final_model_x = LGBMRegressor(**final_params)\n    final_model_y = LGBMRegressor(**final_params)\n    final_model_z = LGBMRegressor(**final_params)\n\n    final_model_x.fit(X, y_x)\n    final_model_y.fit(X, y_y)\n    final_model_z.fit(X, y_z)\n\n    # Prepare test features\n    test_df = load_test_features(test_root)\n\n    # Align with sample_submission and predict\n    sample_sub = pd.read_csv(\"./input/sample_submission.csv\")\n\n    # Ensure 'phone' column exists in sample_submission\n    if \"phone\" not in sample_sub.columns:\n        if \"collectionName\" in sample_sub.columns and \"phoneName\" in sample_sub.columns:\n            sample_sub[\"phone\"] = (\n                sample_sub[\"collectionName\"].astype(str)\n                + \"_\"\n                + sample_sub[\"phoneName\"].astype(str)\n            )\n        else:\n            raise RuntimeError(\n                \"sample_submission does not have 'phone' and cannot be reconstructed from collectionName/phoneName\"\n            )\n\n    # Remember original order\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    # Ensure integer times\n    sample_sub[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_sub[\"UnixTimeMillis\"], errors=\"coerce\"\n    ).astype(\"int64\")\n    test_df[\"UnixTimeMillis\"] = pd.to_numeric(\n        test_df[\"UnixTimeMillis\"], errors=\"coerce\"\n    ).astype(\"int64\")\n\n    # Sort before merge_asof\n    sample_sorted = sample_sub.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    test_sorted = test_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    merged = pd.merge_asof(\n        sample_sorted,\n        test_sorted,\n        on=\"UnixTimeMillis\",\n        by=\"phone\",\n        direction=\"nearest\",\n        tolerance=1000,\n    )\n\n    # Fallback with larger tolerance if still missing\n    if merged[\"Cn0DbHz\"].isna().any():\n        missing_mask = merged[\"Cn0DbHz\"].isna()\n        sample_missing = sample_sorted.loc[missing_mask].copy()\n        fallback = pd.merge_asof(\n            sample_missing,\n            test_sorted,\n            on=\"UnixTimeMillis\",\n            by=\"phone\",\n            direction=\"nearest\",\n            tolerance=5000,\n        )\n        cols_to_fill = [\n            \"Cn0DbHz\",\n            \"PseudorangeRateUncertaintyMetersPerSecond\",\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        for c in cols_to_fill:\n            if c not in merged.columns:\n                merged[c] = np.nan\n            # align by index of sample_missing (which matches missing_mask positions)\n            merged.loc[missing_mask, c] = merged.loc[missing_mask, c].fillna(\n                fallback[c].values\n            )\n\n    # Ensure feature columns present and filled\n    for c in feature_cols:\n        if c not in merged.columns:\n            merged[c] = np.nan\n\n    merged[feature_cols] = merged[feature_cols].fillna(train_df[feature_cols].median())\n\n    X_test = merged[feature_cols].values\n    x_pred = final_model_x.predict(X_test)\n    y_pred = final_model_y.predict(X_test)\n    z_pred = final_model_z.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    # Map predictions back to original sample_submission order\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    submission = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission = submission.sort_values(\"row_id\")\n\n    # Handle any missing predictions with simple forward/backward fill\n    if submission[\"LatitudeDegrees\"].isna().any():\n        submission[\"LatitudeDegrees\"] = (\n            submission[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n        )\n    if submission[\"LongitudeDegrees\"].isna().any():\n        submission[\"LongitudeDegrees\"] = (\n            submission[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n        )\n\n    # Build final submission file with required columns\n    out_cols = []\n    for c in [\"phone\", \"UnixTimeMillis\"]:\n        if c in sample_sub.columns:\n            out_cols.append(c)\n    out = sample_sub[out_cols + [\"row_id\"]].merge(\n        submission[[\"row_id\", \"LatitudeDegrees\", \"LongitudeDegrees\"]],\n        on=\"row_id\",\n        how=\"left\",\n    )\n    out = out.drop(columns=[\"row_id\"])\n\n    os.makedirs(\"./submission\", exist_ok=True)\n    submission_path = \"./submission/submission.csv\"\n    out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n\n# ----- Coordinate transforms -----\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    # Simple iterative method is enough for our purpose\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ----- Metric -----\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ----- Data loaders -----\ndef load_train(train_root):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            gnss = pd.read_csv(gnss_path)\n            gt = pd.read_csv(gt_path)\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            ).astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(\n                gt[\"UnixTimeMillis\"], errors=\"coerce\"\n            ).astype(\"int64\")\n\n            # Ensure WLS cols exist\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = (\n                gnss.groupby(\"UnixTimeMillis\")\n                .agg(\n                    {\n                        \"Cn0DbHz\": \"mean\",\n                        \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                        wls_cols[0]: \"mean\",\n                        wls_cols[1]: \"mean\",\n                        wls_cols[2]: \"mean\",\n                    }\n                )\n                .reset_index()\n            )\n            agg[\"phone\"] = phone_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n\n            merged = pd.merge(\n                gt_small,\n                agg,\n                on=[\"phone\", \"UnixTimeMillis\"],\n                how=\"inner\",\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n            rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub):\n    \"\"\"Aggregate GNSS per (phone, UnixTimeMillis) and merge exactly with sample.\"\"\"\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            gnss = pd.read_csv(gnss_path)\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            ).astype(\"int64\")\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = (\n                gnss.groupby(\"UnixTimeMillis\")\n                .agg(\n                    {\n                        \"Cn0DbHz\": \"mean\",\n                        \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                        wls_cols[0]: \"mean\",\n                        wls_cols[1]: \"mean\",\n                        wls_cols[2]: \"mean\",\n                    }\n                )\n                .reset_index()\n            )\n            agg[\"phone\"] = phone_id\n            rows.append(agg)\n\n    if not rows:\n        raise RuntimeError(\"No test data assembled; check test directory structure.\")\n    test_gnss = pd.concat(rows, ignore_index=True)\n\n    # Build phone in sample_sub if needed\n    if \"phone\" not in sample_sub.columns:\n        if \"collectionName\" in sample_sub.columns and \"phoneName\" in sample_sub.columns:\n            sample_sub[\"phone\"] = (\n                sample_sub[\"collectionName\"].astype(str)\n                + \"_\"\n                + sample_sub[\"phoneName\"].astype(str)\n            )\n        else:\n            raise RuntimeError(\n                \"sample_submission does not have 'phone' and cannot be reconstructed.\"\n            )\n\n    sample_sub[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_sub[\"UnixTimeMillis\"], errors=\"coerce\"\n    ).astype(\"int64\")\n\n    merged = pd.merge(\n        sample_sub,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Load train\n    train_df = load_train(train_root)\n\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n\n    # Fill NaNs with column medians\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    # groups by collection (drive)\n    groups = train_df[\"phone\"].str.split(\"_\").str[0]\n\n    # ----- 5-fold GroupKFold CV -----\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups), start=1):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        params = dict(\n            n_estimators=200,\n            learning_rate=0.05,\n            max_depth=-1,\n            subsample=0.8,\n            colsample_bytree=0.8,\n            random_state=42 + fold,\n            n_jobs=-1,\n        )\n\n        mx = LGBMRegressor(**params)\n        my = LGBMRegressor(**params)\n        mz = LGBMRegressor(**params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"5-fold CV competition metric (mean of P50/P95 errors):\", metric_val)\n\n    # ----- Train final models on full data -----\n    final_params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        random_state=42,\n        n_jobs=-1,\n    )\n    final_mx = LGBMRegressor(**final_params)\n    final_my = LGBMRegressor(**final_params)\n    final_mz = LGBMRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # ----- Prepare test and submission -----\n    sample_sub = pd.read_csv(os.path.join(input_dir, \"sample_submission.csv\"))\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    test_merged = load_test(test_root, sample_sub)\n\n    # Ensure feature columns present\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n\n    # Fill with train medians\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    X_test = test_merged[feature_cols].values\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align back to sample order\n    submission = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission = submission.sort_values(\"row_id\")\n\n    # Fill any missing predictions (if any phones/times not in GNSS) by ffill/bfill\n    submission[\"LatitudeDegrees\"] = (\n        submission[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission[\"LongitudeDegrees\"] = (\n        submission[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # Build final CSV\n    if \"phone\" not in sample_sub.columns:\n        if \"collectionName\" in sample_sub.columns and \"phoneName\" in sample_sub.columns:\n            sample_sub[\"phone\"] = (\n                sample_sub[\"collectionName\"].astype(str)\n                + \"_\"\n                + sample_sub[\"phoneName\"].astype(str)\n            )\n        else:\n            raise RuntimeError(\n                \"sample_submission missing 'phone' and cannot be reconstructed.\"\n            )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission[\"LatitudeDegrees\"],\n            \"LongitudeDegrees\": submission[\"LongitudeDegrees\"],\n        }\n    )\n\n    # Save to required path\n    submission_dir = \"./working/submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_phone_from_path(path):\n    # path: .../split/collection/phone_name/device_gnss.csv\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    # find index of split dir ('train' or 'test')\n    idx = None\n    for i, p in enumerate(parts):\n        if p in (\"train\", \"test\"):\n            idx = i\n            break\n    if idx is None or len(parts) < idx + 3:\n        return None\n    collection = parts[idx + 1]\n    phone_model = parts[idx + 2]\n    return f\"{collection}_{phone_model}\"\n\n\ndef load_device_gnss_features(split_dir):\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n    # Focus on a small, consistent set of numeric columns to avoid missing-column issues\n    base_feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n    ]\n    for fpath in files:\n        phone = parse_phone_from_path(fpath)\n        if phone is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n\n        # time column\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            continue\n\n        df = df.copy()\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n\n        cols_present = [c for c in base_feature_cols if c in df.columns]\n        if not cols_present:\n            # fallback to any numeric columns except identifiers\n            candidates = [\n                c\n                for c in df.columns\n                if c\n                not in [\n                    time_col,\n                    \"UnixTimeMillis\",\n                    \"phone\",\n                    \"MessageType\",\n                    \"SignalType\",\n                    \"CodeType\",\n                ]\n                and pd.api.types.is_numeric_dtype(df[c])\n            ]\n            cols_present = candidates\n\n        if not cols_present:\n            continue\n\n        use_cols = [\"phone\", \"UnixTimeMillis\"] + cols_present\n        df_small = df[use_cols]\n\n        agg_dict = {}\n        for c in cols_present:\n            if pd.api.types.is_numeric_dtype(df_small[c]):\n                agg_dict[c] = [\"mean\", \"std\", \"min\", \"max\"]\n        if not agg_dict:\n            continue\n\n        g = df_small.groupby([\"phone\", \"UnixTimeMillis\"]).agg(agg_dict)\n        g.columns = [f\"{feat}_{stat}\" for feat, stat in g.columns]\n        g = g.reset_index()\n        all_rows.append(g)\n\n    if not all_rows:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    features = pd.concat(all_rows, ignore_index=True)\n    return features\n\n\ndef load_ground_truth():\n    base_path = os.path.join(INPUT_DIR, \"train\")\n    pattern = os.path.join(base_path, \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    all_gt = []\n    for fpath in files:\n        phone = parse_phone_from_path(fpath)\n        if phone is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            continue\n        if \"LatitudeDegrees\" not in df.columns or \"LongitudeDegrees\" not in df.columns:\n            continue\n        tmp = df[[time_col, \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n        tmp[\"phone\"] = phone\n        tmp[\"UnixTimeMillis\"] = tmp[time_col].astype(\"int64\")\n        tmp = tmp[[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        all_gt.append(tmp)\n    if not all_gt:\n        return pd.DataFrame(\n            columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        )\n    gt = pd.concat(all_gt, ignore_index=True)\n    return gt\n\n\ndef build_train_test_features():\n    train_feats = load_device_gnss_features(\"train\")\n    test_feats = load_device_gnss_features(\"test\")\n    gt = load_ground_truth()\n\n    train = pd.merge(gt, train_feats, on=[\"phone\", \"UnixTimeMillis\"], how=\"left\")\n    return train, test_feats\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n\n    # Ensure keys in train\n    for k in key_cols:\n        if k not in train_df.columns:\n            raise RuntimeError(f\"Missing key {k} in training data\")\n\n    # Define training features: numeric, excluding targets and keys\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n    if not feature_cols:\n        raise RuntimeError(\"No numeric feature columns found for modeling.\")\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Build test features by merging test_feats to sample_submission keys\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    # ensure dtypes\n    test_keys[\"phone\"] = test_keys[\"phone\"].astype(str)\n    test_keys[\"UnixTimeMillis\"] = test_keys[\"UnixTimeMillis\"].astype(\"int64\")\n    if not test_feats.empty:\n        test_feats = test_feats.copy()\n        test_feats[\"phone\"] = test_feats[\"phone\"].astype(str)\n        test_feats[\"UnixTimeMillis\"] = test_feats[\"UnixTimeMillis\"].astype(\"int64\")\n        test_full = pd.merge(\n            test_keys,\n            test_feats,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n            sort=False,\n        )\n    else:\n        test_full = test_keys.copy()\n\n    # align columns\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n    X_test = test_full[feature_cols].copy()\n\n    # median imputation based on train\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, meta_df):\n    n_splits = 5\n    if \"phone\" in meta_df.columns:\n        groups = meta_df[\"phone\"].astype(str).values\n    else:\n        groups = np.zeros(len(y_lat))\n\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=groups):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=200,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=200,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": (\n                    meta_df.iloc[va_idx][\"phone\"].values\n                    if \"phone\" in meta_df.columns\n                    else [\"all\"] * len(va_idx)\n                ),\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    return model_lat, model_lon\n\n\ndef main():\n    # read sample submission early (defines test keys)\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    # build features\n    train_df, test_feats = build_train_test_features()\n\n    if train_df.empty:\n        raise RuntimeError(\"Training data is empty; cannot train model.\")\n\n    # prepare X, y, and test matrices\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    # run CV and print metric\n    run_cv(X_train, y_lat, y_lon, train_df[[\"phone\"]].copy())\n\n    # train final models\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # predict on test (aligned to sample_sub order through test_full)\n    pred_lat_test = model_lat.predict(X_test)\n    pred_lon_test = model_lon.predict(X_test)\n\n    # build submission from sample_submission keys\n    submission = sample_sub.copy()\n    # test_full rows are aligned 1:1 with sample_sub rows (constructed that way)\n    if len(submission) != len(test_full):\n        # safety: align by keys if something went wrong\n        test_full_small = test_full[[\"phone\", \"UnixTimeMillis\"]].copy()\n        test_full_small[\"pred_lat\"] = pred_lat_test\n        test_full_small[\"pred_lon\"] = pred_lon_test\n        submission = submission.merge(\n            test_full_small, on=[\"phone\", \"UnixTimeMillis\"], how=\"left\"\n        )\n    else:\n        submission[\"pred_lat\"] = pred_lat_test\n        submission[\"pred_lon\"] = pred_lon_test\n\n    # fill missing preds with global mean of predictions\n    if submission[\"pred_lat\"].isna().any():\n        submission[\"pred_lat\"] = submission[\"pred_lat\"].fillna(\n            submission[\"pred_lat\"].mean()\n        )\n    if submission[\"pred_lon\"].isna().any():\n        submission[\"pred_lon\"] = submission[\"pred_lon\"].fillna(\n            submission[\"pred_lon\"].mean()\n        )\n\n    submission[\"LatitudeDegrees\"] = submission[\"pred_lat\"]\n    submission[\"LongitudeDegrees\"] = submission[\"pred_lon\"]\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    for c in required_cols:\n        if c not in submission.columns:\n            if c == \"phone\":\n                submission[c] = \"all\"\n            elif c == \"UnixTimeMillis\":\n                submission[c] = np.arange(len(submission))\n    submission = submission[required_cols]\n\n    # NaN safety\n    for col in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n        if submission[col].isna().any():\n            submission[col] = submission[col].fillna(submission[col].mean())\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport numpy as np\nimport pandas as pd\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n\nWORK_DIR = \"./working\"\nos.makedirs(WORK_DIR, exist_ok=True)\n\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nSUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n\n# ---------------- Utility functions ----------------\n\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    \"\"\"Vectorized haversine distance in meters.\"\"\"\n    R = 6371000.0\n    lat1_rad = np.radians(lat1)\n    lat2_rad = np.radians(lat2)\n    dlat = lat2_rad - lat1_rad\n    dlon = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dlat / 2.0) ** 2\n        + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0) ** 2\n    )\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(\n    df, pred_lat_col, pred_lon_col, gt_lat_col, gt_lon_col, phone_col=\"phone\"\n):\n    \"\"\"Compute mean of (50th, 95th percentile) distance errors per phone.\"\"\"\n    errors = haversine_distance(\n        df[gt_lat_col].values,\n        df[gt_lon_col].values,\n        df[pred_lat_col].values,\n        df[pred_lon_col].values,\n    )\n    df = df.copy()\n    df[\"error\"] = errors\n    phone_scores = []\n    for phone, g in df.groupby(phone_col):\n        e = g[\"error\"].values\n        if len(e) == 0:\n            continue\n        p50 = np.percentile(e, 50)\n        p95 = np.percentile(e, 95)\n        phone_scores.append((p50 + p95) / 2.0)\n    if not phone_scores:\n        return np.nan\n    return float(np.mean(phone_scores))\n\n\ndef load_train_paths(train_dir):\n    \"\"\"Return list of (phone_id, gnss_path, gt_path).\"\"\"\n    paths = []\n    if not os.path.isdir(train_dir):\n        return paths\n    for drive in sorted(os.listdir(train_dir)):\n        drive_path = os.path.join(train_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n            if os.path.exists(gnss_path) and os.path.exists(gt_path):\n                # phone_id as \"drive/phone\"\n                phone_id = f\"{drive}/{phone}\"\n                paths.append((phone_id, gnss_path, gt_path))\n    return paths\n\n\ndef load_test_paths(test_dir):\n    \"\"\"Return list of (phone_id, gnss_path) with phone_id as 'drive/phone'.\"\"\"\n    paths = []\n    if not os.path.isdir(test_dir):\n        return paths\n    for drive in sorted(os.listdir(test_dir)):\n        drive_path = os.path.join(test_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            if os.path.exists(gnss_path):\n                phone_id = f\"{drive}/{phone}\"\n                paths.append((phone_id, gnss_path))\n    return paths\n\n\ndef extract_baseline_cols(df):\n    \"\"\"Find baseline latitude/longitude columns in device_gnss, if any.\"\"\"\n    lat_col_candidates = [c for c in df.columns if c.lower().startswith(\"latitudedeg\")]\n    lon_col_candidates = [c for c in df.columns if c.lower().startswith(\"longitudedeg\")]\n    if len(lat_col_candidates) == 0 or len(lon_col_candidates) == 0:\n        return None, None\n    return lat_col_candidates[0], lon_col_candidates[0]\n\n\ndef infer_sample_columns(sample_df):\n    \"\"\"\n    Infer phone, time, lat, lon column names from sample submission.\n    Fallbacks ensure we can always proceed even if columns are slightly renamed.\n    \"\"\"\n    cols_lower = {c.lower(): c for c in sample_df.columns}\n\n    # phone/id column\n    phone_col = None\n    for key in [\"phone\", \"id\", \"track\", \"device\"]:\n        if key in cols_lower:\n            phone_col = cols_lower[key]\n            break\n    if phone_col is None:\n        # fallback to first column\n        phone_col = sample_df.columns[0]\n\n    # time column\n    time_col = None\n    for key in [\"unixtimemillis\", \"timestamp\", \"time\"]:\n        if key in cols_lower:\n            time_col = cols_lower[key]\n            break\n    if time_col is None:\n        # fallback to second column if exists\n        time_col = (\n            sample_df.columns[1] if len(sample_df.columns) > 1 else sample_df.columns[0]\n        )\n\n    # latitude column\n    lat_col = None\n    for key in [\"latitudedegrees\", \"latitude\"]:\n        if key in cols_lower:\n            lat_col = cols_lower[key]\n            break\n    if lat_col is None:\n        # assume third column\n        lat_col = (\n            sample_df.columns[2]\n            if len(sample_df.columns) > 2\n            else sample_df.columns[-2]\n        )\n\n    # longitude column\n    lon_col = None\n    for key in [\"longitudedegrees\", \"longitude\"]:\n        if key in cols_lower:\n            lon_col = cols_lower[key]\n            break\n    if lon_col is None:\n        # assume fourth column\n        lon_col = (\n            sample_df.columns[3]\n            if len(sample_df.columns) > 3\n            else sample_df.columns[-1]\n        )\n\n    return phone_col, time_col, lat_col, lon_col\n\n\n# ---------------- Build training table ----------------\n\ntrain_paths = load_train_paths(TRAIN_DIR)\n\ntrain_rows = []\n\nfor phone_id, gnss_path, gt_path in train_paths:\n    try:\n        gt = pd.read_csv(gt_path)\n    except Exception:\n        continue\n    # Need UnixTimeMillis, LatitudeDegrees, LongitudeDegrees from ground_truth\n    if not set([\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]).issubset(\n        gt.columns\n    ):\n        continue\n    gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n    gt_small[\"phone\"] = phone_id\n\n    # Load device_gnss for this phone to get any baseline coordinates\n    try:\n        gnss = pd.read_csv(gnss_path)\n    except Exception:\n        gnss = None\n\n    if gnss is not None and \"utcTimeMillis\" in gnss.columns:\n        # Round utcTimeMillis to nearest 1000 to match 1Hz UnixTimeMillis\n        gnss_local = gnss.copy()\n        gnss_local[\"UnixTimeMillis\"] = (\n            np.round(gnss_local[\"utcTimeMillis\"] / 1000.0) * 1000\n        ).astype(\"int64\")\n        lat_col, lon_col = extract_baseline_cols(gnss_local)\n        if lat_col is not None and lon_col is not None:\n            base_df = gnss_local[[\"UnixTimeMillis\", lat_col, lon_col]].copy()\n            base_df = (\n                base_df.groupby(\"UnixTimeMillis\")[[lat_col, lon_col]]\n                .mean()\n                .reset_index()\n            )\n            base_df.rename(\n                columns={lat_col: \"baseline_lat\", lon_col: \"baseline_lon\"}, inplace=True\n            )\n            merged = pd.merge(gt_small, base_df, on=\"UnixTimeMillis\", how=\"left\")\n        else:\n            merged = gt_small.copy()\n            merged[\"baseline_lat\"] = np.nan\n            merged[\"baseline_lon\"] = np.nan\n    else:\n        merged = gt_small.copy()\n        merged[\"baseline_lat\"] = np.nan\n        merged[\"baseline_lon\"] = np.nan\n\n    train_rows.append(merged)\n\nif len(train_rows) == 0:\n    raise RuntimeError(\"No training rows could be constructed from train directory.\")\n\ntrain_df = pd.concat(train_rows, ignore_index=True)\n\n# If baseline is missing, use ground_truth itself as baseline (zero offset)\ntrain_df[\"baseline_lat\"] = train_df[\"baseline_lat\"].fillna(train_df[\"LatitudeDegrees\"])\ntrain_df[\"baseline_lon\"] = train_df[\"baseline_lon\"].fillna(train_df[\"LongitudeDegrees\"])\n\n# Targets: offsets (ground_truth - baseline)\ntrain_df[\"dlat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"baseline_lat\"]\ntrain_df[\"dlon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"baseline_lon\"]\n\n# Remove rows with missing essentials\ntrain_df = train_df.dropna(\n    subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"baseline_lat\", \"baseline_lon\"]\n).reset_index(drop=True)\n\nif len(train_df) == 0:\n    raise RuntimeError(\"Training dataframe is empty after cleaning.\")\n\n# ---------------- \"Model\": per-phone mean offset ----------------\n\nphone_offsets = train_df.groupby(\"phone\")[[\"dlat\", \"dlon\"]].mean().reset_index()\nphone_offsets.rename(columns={\"dlat\": \"mean_dlat\", \"dlon\": \"mean_dlon\"}, inplace=True)\n\n# Also store per-phone mean absolute lat/lon, useful if ever needed\nphone_means = (\n    train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n    .mean()\n    .reset_index()\n)\nphone_means.rename(\n    columns={\n        \"LatitudeDegrees\": \"mean_lat\",\n        \"LongitudeDegrees\": \"mean_lon\",\n    },\n    inplace=True,\n)\n\nphone_info = pd.merge(phone_offsets, phone_means, on=\"phone\", how=\"outer\")\n\n# ---------------- Hold-out validation metric (simple split) ----------------\n\nrng = np.random.default_rng(seed=42)\ntrain_df = train_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n\nval_mask = np.zeros(len(train_df), dtype=bool)\nfor phone, g_idx in train_df.groupby(\"phone\").groups.items():\n    idx = np.array(list(g_idx))\n    if len(idx) == 0:\n        continue\n    n_val = max(1, int(0.2 * len(idx)))\n    val_idx = rng.choice(idx, size=n_val, replace=False)\n    val_mask[val_idx] = True\n\nval_df = train_df[val_mask].copy()\ntr_df = train_df[~val_mask].copy()\n\n# For validation, predictions are baseline + mean offset for that phone (computed on full train_df)\nval_df = pd.merge(val_df, phone_info, on=\"phone\", how=\"left\")\n\n# If some phones missing from phone_info (shouldn't happen), fill with zeros\nval_df[\"mean_dlat\"] = val_df[\"mean_dlat\"].fillna(0.0)\nval_df[\"mean_dlon\"] = val_df[\"mean_dlon\"].fillna(0.0)\n\nval_df[\"pred_lat\"] = val_df[\"baseline_lat\"] + val_df[\"mean_dlat\"]\nval_df[\"pred_lon\"] = val_df[\"baseline_lon\"] + val_df[\"mean_dlon\"]\n\nval_metric = competition_metric(\n    val_df,\n    pred_lat_col=\"pred_lat\",\n    pred_lon_col=\"pred_lon\",\n    gt_lat_col=\"LatitudeDegrees\",\n    gt_lon_col=\"LongitudeDegrees\",\n    phone_col=\"phone\",\n)\nprint(f\"Validation metric (mean of 50th & 95th errors): {val_metric:.4f}\")\n\n# ---------------- Build test predictions ----------------\n\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\nsample_phone_col, sample_time_col, sample_lat_col, sample_lon_col = (\n    infer_sample_columns(sample_sub)\n)\n\ntest_paths = load_test_paths(TEST_DIR)\n\n\n# Helper: build baseline per (sample_phone, UnixTimeMillis) from device_gnss\ndef build_test_baseline_for_phone(sample_phone_name, gnss_path, time_col_name):\n    \"\"\"\n    sample_phone_name: string as appears in sample submission phone column.\n    gnss_path: path to device_gnss.csv for some drive/phone folder.\n    time_col_name: name to use for UnixTimeMillis-like column in returned DF.\n    \"\"\"\n    try:\n        gnss = pd.read_csv(gnss_path)\n    except Exception:\n        return pd.DataFrame(\n            columns=[sample_phone_col, time_col_name, \"baseline_lat\", \"baseline_lon\"]\n        )\n\n    if \"utcTimeMillis\" not in gnss.columns:\n        return pd.DataFrame(\n            columns=[sample_phone_col, time_col_name, \"baseline_lat\", \"baseline_lon\"]\n        )\n\n    gnss_local = gnss.copy()\n    gnss_local[time_col_name] = (\n        np.round(gnss_local[\"utcTimeMillis\"] / 1000.0) * 1000\n    ).astype(\"int64\")\n\n    lat_col, lon_col = extract_baseline_cols(gnss_local)\n    if lat_col is None or lon_col is None:\n        return pd.DataFrame(\n            columns=[sample_phone_col, time_col_name, \"baseline_lat\", \"baseline_lon\"]\n        )\n\n    base_df = gnss_local[[time_col_name, lat_col, lon_col]].copy()\n    base_df = base_df.groupby(time_col_name)[[lat_col, lon_col]].mean().reset_index()\n    base_df.rename(\n        columns={lat_col: \"baseline_lat\", lon_col: \"baseline_lon\"}, inplace=True\n    )\n    base_df[sample_phone_col] = sample_phone_name\n    return base_df\n\n\n# Map from test folder phone_id (\"drive/phonefolder\") to sample phone name\ndef folder_phone_to_sample_phone(phone_id):\n    drive, phone_folder = phone_id.split(\"/\", 1)\n    mapping_suffix = {\n        \"GooglePixel4\": \"Pixel4\",\n        \"GooglePixel4XL\": \"Pixel4XL\",\n        \"GooglePixel5\": \"Pixel5\",\n        \"SamsungGalaxyS20Ultra\": \"SamsungS20Ultra\",\n        \"XiaomiMi8\": \"Mi8\",\n    }\n    suffix = mapping_suffix.get(phone_folder, phone_folder)\n    return f\"{drive}_{suffix}\"\n\n\n# Build GNSS baselines for all sample phone names\nsample_phones_unique = set(sample_sub[sample_phone_col].unique())\ntest_baseline_list = []\nfor phone_id, gnss_path in test_paths:\n    sample_phone = folder_phone_to_sample_phone(phone_id)\n    if sample_phone not in sample_phones_unique:\n        continue\n    base_df = build_test_baseline_for_phone(sample_phone, gnss_path, sample_time_col)\n    if not base_df.empty:\n        test_baseline_list.append(base_df)\n\nif len(test_baseline_list) > 0:\n    test_baseline_df = pd.concat(test_baseline_list, ignore_index=True)\nelse:\n    test_baseline_df = pd.DataFrame(\n        columns=[sample_phone_col, sample_time_col, \"baseline_lat\", \"baseline_lon\"]\n    )\n\n# Attach baseline to sample_submission\nsubmission = sample_sub.copy()\n\nsubmission = submission.merge(\n    test_baseline_df,\n    left_on=[sample_phone_col, sample_time_col],\n    right_on=[sample_phone_col, sample_time_col],\n    how=\"left\",\n)\n\n# Global mean lat/lon and offsets for fallback\nglobal_mean_lat = train_df[\"LatitudeDegrees\"].mean()\nglobal_mean_lon = train_df[\"LongitudeDegrees\"].mean()\nglobal_mean_dlat = train_df[\"dlat\"].mean()\nglobal_mean_dlon = train_df[\"dlon\"].mean()\n\n# Use baseline from GNSS where available; otherwise global mean position\nsubmission[\"baseline_lat\"] = submission[\"baseline_lat\"].fillna(global_mean_lat)\nsubmission[\"baseline_lon\"] = submission[\"baseline_lon\"].fillna(global_mean_lon)\n\n# Final predictions: baseline + global mean offsets\nsubmission[sample_lat_col] = submission[\"baseline_lat\"] + global_mean_dlat\nsubmission[sample_lon_col] = submission[\"baseline_lon\"] + global_mean_dlon\n\n# Keep required columns and order: match original sample_submission columns\nsubmission_out = submission[sample_sub.columns]\n\n# Save to submission path\nsubmission_out.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"Saved submission to {SUBMISSION_PATH}\")\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_phone_from_path(path):\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    idx = None\n    for i, p in enumerate(parts):\n        if p in (\"train\", \"test\"):\n            idx = i\n            break\n    if idx is None or len(parts) < idx + 3:\n        return None\n    collection = parts[idx + 1]\n    phone_model = parts[idx + 2]\n    return f\"{collection}_{phone_model}\"\n\n\ndef load_device_gnss_features(split_dir, use_targets=False):\n    \"\"\"\n    Load device_gnss.csv files, aggregate numeric features per (phone, UnixTimeMillis).\n\n    If use_targets is True (train), we also keep LatitudeDegrees / LongitudeDegrees\n    columns if present to be used as targets.\n    \"\"\"\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n\n    base_feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n    ]\n\n    for fpath in files:\n        phone = parse_phone_from_path(fpath)\n        if phone is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n\n        # time column\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            continue\n\n        df = df.copy()\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n\n        cols_present = [c for c in base_feature_cols if c in df.columns]\n        if not cols_present:\n            candidates = [\n                c\n                for c in df.columns\n                if c\n                not in [\n                    time_col,\n                    \"UnixTimeMillis\",\n                    \"phone\",\n                    \"MessageType\",\n                    \"SignalType\",\n                    \"CodeType\",\n                ]\n                and pd.api.types.is_numeric_dtype(df[c])\n            ]\n            cols_present = candidates\n\n        if not cols_present:\n            continue\n\n        use_cols = [\"phone\", \"UnixTimeMillis\"] + cols_present\n        if use_targets:\n            # if device_gnss has Latitude/LongitudeDegrees, keep them for later merge\n            for tcol in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n                if tcol in df.columns and tcol not in use_cols:\n                    use_cols.append(tcol)\n\n        df_small = df[use_cols]\n\n        agg_dict = {}\n        for c in cols_present:\n            if pd.api.types.is_numeric_dtype(df_small[c]):\n                agg_dict[c] = [\"mean\", \"std\", \"min\", \"max\"]\n\n        group_cols = [\"phone\", \"UnixTimeMillis\"]\n        g = df_small.groupby(group_cols).agg(agg_dict)\n        g.columns = [f\"{feat}_{stat}\" for feat, stat in g.columns]\n        g = g.reset_index()\n\n        if (\n            use_targets\n            and \"LatitudeDegrees\" in df_small.columns\n            and \"LongitudeDegrees\" in df_small.columns\n        ):\n            # take mean of lat/lon over rows at the same timestamp\n            tgt = (\n                df_small.groupby(group_cols)[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n                .mean()\n                .reset_index()\n            )\n            g = g.merge(tgt, on=group_cols, how=\"left\")\n\n        all_rows.append(g)\n\n    if not all_rows:\n        cols = [\"phone\", \"UnixTimeMillis\"]\n        if use_targets:\n            cols += [\"LatitudeDegrees\", \"LongitudeDegrees\"]\n        return pd.DataFrame(columns=cols)\n\n    features = pd.concat(all_rows, ignore_index=True)\n    return features\n\n\ndef build_train_test_features():\n    # For this benchmark, ground_truth.csv may not exist; use device_gnss lat/lon as targets.\n    train_feats = load_device_gnss_features(\"train\", use_targets=True)\n    test_feats = load_device_gnss_features(\"test\", use_targets=False)\n\n    # Drop rows without targets\n    if (\n        \"LatitudeDegrees\" not in train_feats.columns\n        or \"LongitudeDegrees\" not in train_feats.columns\n    ):\n        raise RuntimeError(\n            \"No LatitudeDegrees/LongitudeDegrees columns found in train device_gnss data.\"\n        )\n    train_feats = train_feats.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\"])\n\n    return train_feats, test_feats\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n\n    # Ensure keys in train\n    for k in key_cols:\n        if k not in train_df.columns:\n            raise RuntimeError(f\"Missing key {k} in training data\")\n\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n    if not feature_cols:\n        raise RuntimeError(\"No numeric feature columns found for modeling.\")\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Build test features by merging test_feats to sample_submission keys\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    test_keys[\"phone\"] = test_keys[\"phone\"].astype(str)\n    test_keys[\"UnixTimeMillis\"] = test_keys[\"UnixTimeMillis\"].astype(\"int64\")\n    if not test_feats.empty:\n        test_feats = test_feats.copy()\n        test_feats[\"phone\"] = test_feats[\"phone\"].astype(str)\n        test_feats[\"UnixTimeMillis\"] = test_feats[\"UnixTimeMillis\"].astype(\"int64\")\n        test_full = pd.merge(\n            test_keys,\n            test_feats,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n            sort=False,\n        )\n    else:\n        test_full = test_keys.copy()\n\n    # align columns\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n    X_test = test_full[feature_cols].copy()\n\n    # median imputation based on train\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, meta_df):\n    n_splits = 5\n    if \"phone\" in meta_df.columns:\n        groups = meta_df[\"phone\"].astype(str).values\n    else:\n        groups = np.zeros(len(y_lat))\n\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=groups):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=200,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=200,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": (\n                    meta_df.iloc[va_idx][\"phone\"].values\n                    if \"phone\" in meta_df.columns\n                    else [\"all\"] * len(va_idx)\n                ),\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    # build features using device_gnss as labels for train\n    train_df, test_feats = build_train_test_features()\n    if train_df.empty:\n        raise RuntimeError(\"Training data is empty; cannot train model.\")\n\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    # run CV\n    _ = run_cv(X_train, y_lat, y_lon, train_df[[\"phone\"]].copy())\n\n    # train final models\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # predict on test\n    pred_lat_test = model_lat.predict(X_test)\n    pred_lon_test = model_lon.predict(X_test)\n\n    submission = sample_sub.copy()\n\n    # align predictions with sample_sub\n    if len(submission) != len(test_full):\n        test_full_small = test_full[[\"phone\", \"UnixTimeMillis\"]].copy()\n        test_full_small[\"pred_lat\"] = pred_lat_test\n        test_full_small[\"pred_lon\"] = pred_lon_test\n        submission = submission.merge(\n            test_full_small, on=[\"phone\", \"UnixTimeMillis\"], how=\"left\"\n        )\n    else:\n        submission[\"pred_lat\"] = pred_lat_test\n        submission[\"pred_lon\"] = pred_lon_test\n\n    # fill missing preds with global mean of predictions\n    if submission[\"pred_lat\"].isna().any():\n        submission[\"pred_lat\"] = submission[\"pred_lat\"].fillna(\n            submission[\"pred_lat\"].mean()\n        )\n    if submission[\"pred_lon\"].isna().any():\n        submission[\"pred_lon\"] = submission[\"pred_lon\"].fillna(\n            submission[\"pred_lon\"].mean()\n        )\n\n    submission[\"LatitudeDegrees\"] = submission[\"pred_lat\"]\n    submission[\"LongitudeDegrees\"] = submission[\"pred_lon\"]\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    for c in required_cols:\n        if c not in submission.columns:\n            if c == \"phone\":\n                submission[c] = \"all\"\n            elif c == \"UnixTimeMillis\":\n                submission[c] = np.arange(len(submission))\n    submission = submission[required_cols]\n\n    # NaN safety\n    for col in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n        if submission[col].isna().any():\n            submission[col] = submission[col].fillna(submission[col].mean())\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nWGS84_A = 6378137.0\nWGS84_E2 = 6.69437999014e-3\n\n\ndef geodetic_to_ecef(lat_deg, lon_deg, alt_m):\n    lat = np.radians(lat_deg)\n    lon = np.radians(lon_deg)\n    a = WGS84_A\n    e2 = WGS84_E2\n    sin_lat = np.sin(lat)\n    cos_lat = np.cos(lat)\n    N = a / np.sqrt(1 - e2 * sin_lat * sin_lat)\n    X = (N + alt_m) * cos_lat * np.cos(lon)\n    Y = (N + alt_m) * cos_lat * np.sin(lon)\n    Z = (N * (1 - e2) + alt_m) * sin_lat\n    return X, Y, Z\n\n\ndef ecef_to_geodetic(x, y, z):\n    # Bowring\u2019s method\n    a = WGS84_A\n    e2 = WGS84_E2\n    b = a * np.sqrt(1 - e2)\n    ep2 = (a * a - b * b) / (b * b)\n    p = np.sqrt(x * x + y * y)\n    th = np.arctan2(a * z, b * p)\n    lon = np.arctan2(y, x)\n    lat = np.arctan2(z + ep2 * b * np.sin(th) ** 3, p - e2 * a * np.cos(th) ** 3)\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    alt = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), alt\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    # df must have columns: lat_gt, lon_gt, lat_pred, lon_pred, phone\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    # basic cleaning\n    gt = gt.dropna(\n        subset=[\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"AltitudeMeters\",\n            \"UnixTimeMillis\",\n        ]\n    )\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n\n    # Derive epoch time in ms to join with ground truth\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # aggregate gnss by epoch\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # join with ground truth\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # target: ECEF from ground truth\n    X_ecef, Y_ecef, Z_ecef = geodetic_to_ecef(\n        df[\"LatitudeDegrees\"].values,\n        df[\"LongitudeDegrees\"].values,\n        df[\"AltitudeMeters\"].values,\n    )\n    df[\"X_ecef\"] = X_ecef\n    df[\"Y_ecef\"] = Y_ecef\n    df[\"Z_ecef\"] = Z_ecef\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        # fallback: align minimum to zero\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception:\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\n\n# build sorted time and simple deltas per phone\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# temporal diffs for mean features\nmean_feats = [c for c in train_df.columns if c.endswith(\"_mean\")]\nfor feat in mean_feats:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# feature columns: aggregated gnss stats and their diffs except targets & labels\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"X_ecef\",\n    \"Y_ecef\",\n    \"Z_ecef\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        c.endswith(\"_mean\")\n        or c.endswith(\"_std\")\n        or c.endswith(\"_diff\")\n        or \"WlsPosition\" in c\n    )\n]\n\n# fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_x = train_df[\"X_ecef\"].values\ny_y = train_df[\"Y_ecef\"].values\nz_true = train_df[\"Z_ecef\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_preds_x = np.zeros(len(train_df))\noof_preds_y = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n    yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_x = LGBMRegressor(**params)\n    model_y = LGBMRegressor(**params)\n\n    model_x.fit(X_tr, yx_tr, eval_set=[(X_val, yx_val)], eval_metric=\"l2\")\n    model_y.fit(X_tr, yy_tr, eval_set=[(X_val, yy_val)], eval_metric=\"l2\")\n\n    oof_preds_x[val_idx] = model_x.predict(X_val)\n    oof_preds_y[val_idx] = model_y.predict(X_val)\n\n# build oof dataframe for metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\nlat_pred, lon_pred, _ = ecef_to_geodetic(oof_preds_x, oof_preds_y, z_true)\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = lat_pred\noof_df[\"lon_pred\"] = lon_pred\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_x = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_y = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_x.fit(X, y_x)\nfinal_model_y.fit(X, y_y)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))\n\n# Detect identifier column name: 'phone' in original, 'tripId' in this benchmark\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    # Fallback: assume first column is the identifier\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Create a unified 'phone' column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception:\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\ntest_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Temporal sort and fill\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n# Fill with train medians as base\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\n# Prepare frame with required rows from sample submission\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\n# still any NaNs: fill by train medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\npred_x = final_model_x.predict(X_test)\npred_y = final_model_y.predict(X_test)\n\n# Approximate Z as mean of training Z (ECEF)\nmean_z = z_true.mean()\npred_z = np.full_like(pred_x, mean_z)\npred_lat, pred_lon, _ = ecef_to_geodetic(pred_x, pred_y, pred_z)\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_phone_from_path(path):\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    idx = None\n    for i, p in enumerate(parts):\n        if p in (\"train\", \"test\"):\n            idx = i\n            break\n    if idx is None or len(parts) < idx + 3:\n        return None\n    collection = parts[idx + 1]\n    phone_model = parts[idx + 2]\n    return f\"{collection}_{phone_model}\"\n\n\ndef load_ground_truth_train():\n    base_path = os.path.join(INPUT_DIR, \"train\")\n    pattern = os.path.join(base_path, \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n    for fpath in files:\n        phone = parse_phone_from_path(fpath)\n        if phone is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n        if \"UnixTimeMillis\" not in df.columns:\n            continue\n        df = df[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[\"UnixTimeMillis\"].astype(\"int64\")\n        # round to nearest second\n        df[\"UnixTimeMillis_round\"] = (df[\"UnixTimeMillis\"] / 1000.0).round().astype(\n            \"int64\"\n        ) * 1000\n        # aggregate gt per rounded timestamp\n        g = (\n            df.groupby([\"phone\", \"UnixTimeMillis_round\"])[\n                [\"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ]\n            .mean()\n            .reset_index()\n        )\n        all_rows.append(g)\n    if not all_rows:\n        raise RuntimeError(\"No ground_truth.csv files found or could not be loaded.\")\n    gt = pd.concat(all_rows, ignore_index=True)\n    return gt\n\n\ndef load_device_gnss_features(split_dir):\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n\n    base_feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n    ]\n\n    for fpath in files:\n        phone = parse_phone_from_path(fpath)\n        if phone is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n\n        time_col = None\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            continue\n\n        df = df.copy()\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n        df[\"UnixTimeMillis_round\"] = (df[\"UnixTimeMillis\"] / 1000.0).round().astype(\n            \"int64\"\n        ) * 1000\n\n        cols_present = [c for c in base_feature_cols if c in df.columns]\n        if not cols_present:\n            candidates = [\n                c\n                for c in df.columns\n                if c\n                not in [\n                    time_col,\n                    \"UnixTimeMillis\",\n                    \"UnixTimeMillis_round\",\n                    \"phone\",\n                    \"MessageType\",\n                    \"SignalType\",\n                    \"CodeType\",\n                ]\n                and pd.api.types.is_numeric_dtype(df[c])\n            ]\n            cols_present = candidates\n\n        if not cols_present:\n            continue\n\n        use_cols = [\"phone\", \"UnixTimeMillis_round\"] + cols_present\n        df_small = df[use_cols]\n\n        agg_dict = {}\n        for c in cols_present:\n            if pd.api.types.is_numeric_dtype(df_small[c]):\n                agg_dict[c] = [\"mean\", \"std\", \"min\", \"max\"]\n\n        group_cols = [\"phone\", \"UnixTimeMillis_round\"]\n        g = df_small.groupby(group_cols).agg(agg_dict)\n        g.columns = [f\"{feat}_{stat}\" for feat, stat in g.columns]\n        g = g.reset_index()\n        all_rows.append(g)\n\n    if not all_rows:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis_round\"])\n    feats = pd.concat(all_rows, ignore_index=True)\n    return feats\n\n\ndef build_train_test_features(sample_sub):\n    gt = load_ground_truth_train()\n    train_gnss = load_device_gnss_features(\"train\")\n    test_gnss = load_device_gnss_features(\"test\")\n\n    # merge ground truth with train gnss on rounded time\n    train = pd.merge(\n        gt,\n        train_gnss,\n        on=[\"phone\", \"UnixTimeMillis_round\"],\n        how=\"inner\",\n        suffixes=(\"\", \"_feat\"),\n    )\n    # prepare test features: align by rounded time based on sample submission\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    test_keys[\"UnixTimeMillis_round\"] = (\n        test_keys[\"UnixTimeMillis\"].astype(\"int64\") / 1000.0\n    ).round().astype(\"int64\") * 1000\n    test_feats = pd.merge(\n        test_keys,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis_round\"],\n        how=\"left\",\n        sort=False,\n    )\n    return train, test_feats\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n\n    key_cols_train = [\"phone\", \"UnixTimeMillis_round\"]\n    for k in key_cols_train:\n        if k not in train_df.columns:\n            raise RuntimeError(f\"Missing key {k} in training data\")\n\n    non_feature_cols = key_cols_train + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n    if not feature_cols:\n        raise RuntimeError(\"No numeric feature columns found for modeling.\")\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Build X_test from test_feats, aligning with sample_submission\n    test_full = sample_sub.copy()\n    test_full = pd.merge(\n        test_full,\n        test_feats[\n            [\"phone\", \"UnixTimeMillis\", \"UnixTimeMillis_round\"] + feature_cols\n        ].drop_duplicates(subset=[\"phone\", \"UnixTimeMillis\"]),\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n\n    # ensure all feature cols exist\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n\n    X_test = test_full[feature_cols].copy()\n\n    # median imputation\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    n_splits = 5\n    groups = phones.astype(str).values\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=groups):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=200,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=200,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones.iloc[va_idx].values,\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    train_df, test_feats = build_train_test_features(sample_sub)\n\n    if train_df.empty:\n        raise RuntimeError(\"Training data is empty; cannot train model.\")\n\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    # run CV\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].copy())\n    print(f\"Hold-out CV metric: {cv_score:.6f}\")\n\n    # train final models\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # predict on test\n    pred_lat_test = model_lat.predict(X_test)\n    pred_lon_test = model_lon.predict(X_test)\n\n    submission = sample_sub.copy()\n    submission[\"LatitudeDegrees\"] = pred_lat_test\n    submission[\"LongitudeDegrees\"] = pred_lon_test\n\n    # NaN safety: fill with global means if any\n    for col in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n        if submission[col].isna().any():\n            submission[col] = submission[col].fillna(submission[col].mean())\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols]\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ----- Coordinate transforms -----\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ----- Metric -----\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ----- Data loaders -----\ndef load_train(train_root):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            gnss = pd.read_csv(gnss_path)\n            gt = pd.read_csv(gt_path)\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            ).astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(\n                gt[\"UnixTimeMillis\"], errors=\"coerce\"\n            ).astype(\"int64\")\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = (\n                gnss.groupby(\"UnixTimeMillis\")\n                .agg(\n                    {\n                        \"Cn0DbHz\": \"mean\",\n                        \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                        wls_cols[0]: \"mean\",\n                        wls_cols[1]: \"mean\",\n                        wls_cols[2]: \"mean\",\n                    }\n                )\n                .reset_index()\n            )\n            agg[\"phone\"] = phone_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n\n            merged = pd.merge(\n                gt_small,\n                agg,\n                on=[\"phone\", \"UnixTimeMillis\"],\n                how=\"inner\",\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n            rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            gnss = pd.read_csv(gnss_path)\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            ).astype(\"int64\")\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = (\n                gnss.groupby(\"UnixTimeMillis\")\n                .agg(\n                    {\n                        \"Cn0DbHz\": \"mean\",\n                        \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                        wls_cols[0]: \"mean\",\n                        wls_cols[1]: \"mean\",\n                        wls_cols[2]: \"mean\",\n                    }\n                )\n                .reset_index()\n            )\n            agg[\"phone\"] = phone_id\n            rows.append(agg)\n\n    if not rows:\n        # If no GNSS data was aggregated, create empty frame with needed cols\n        test_gnss = pd.DataFrame(\n            columns=[\n                \"UnixTimeMillis\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"phone\",\n            ]\n        )\n    else:\n        test_gnss = pd.concat(rows, ignore_index=True)\n\n    # Build phone in sample_sub if needed\n    if \"phone\" not in sample_sub.columns:\n        if \"collectionName\" in sample_sub.columns and \"phoneName\" in sample_sub.columns:\n            sample_sub[\"phone\"] = (\n                sample_sub[\"collectionName\"].astype(str)\n                + \"_\"\n                + sample_sub[\"phoneName\"].astype(str)\n            )\n        else:\n            raise RuntimeError(\n                \"sample_submission does not have 'phone' and cannot be reconstructed.\"\n            )\n\n    sample_sub[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_sub[\"UnixTimeMillis\"], errors=\"coerce\"\n    ).astype(\"int64\")\n\n    merged = pd.merge(\n        sample_sub,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Load train data\n    train_df = load_train(train_root)\n\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n\n    # Fill NaNs with column medians\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    # groups by collection (drive)\n    groups = train_df[\"phone\"].str.split(\"_\").str[0]\n\n    # ----- 5-fold GroupKFold CV -----\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    # Use GradientBoostingRegressor for portability\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups), start=1):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"5-fold CV competition metric (mean of P50/P95 errors):\", metric_val)\n\n    # ----- Train final models on full data -----\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # ----- Prepare test and submission -----\n    sample_sub = pd.read_csv(os.path.join(input_dir, \"sample_submission.csv\"))\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    test_merged = load_test(test_root, sample_sub)\n\n    # Ensure feature columns present\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n\n    # Fill with train medians\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    X_test = test_merged[feature_cols].values\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align back to sample order\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill any missing predictions (if any phones/times not in GNSS) by ffill/bfill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # Build final CSV; reconstruct phone if needed\n    if \"phone\" not in sample_sub.columns:\n        if \"collectionName\" in sample_sub.columns and \"phoneName\" in sample_sub.columns:\n            sample_sub[\"phone\"] = (\n                sample_sub[\"collectionName\"].astype(str)\n                + \"_\"\n                + sample_sub[\"phoneName\"].astype(str)\n            )\n        else:\n            raise RuntimeError(\n                \"sample_submission missing 'phone' and cannot be reconstructed.\"\n            )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"],\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"],\n        }\n    )\n\n    # Save to required path ./submission/submission.csv\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport numpy as np\nimport pandas as pd\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n\nWORK_DIR = \"./working\"\nos.makedirs(WORK_DIR, exist_ok=True)\n\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nSUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    \"\"\"Vectorized haversine distance in meters.\"\"\"\n    R = 6371000.0\n    lat1_rad = np.radians(lat1)\n    lat2_rad = np.radians(lat2)\n    dlat = lat2_rad - lat1_rad\n    dlon = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dlat / 2.0) ** 2\n        + np.cos(lat1_rad) * np.cos(lat2_rad) * np.cos(dlon / 2.0) ** 2\n        - np.cos(lat1_rad) * np.cos(lat2_rad) * np.cos(dlon)\n    )\n    # The above is incorrect; use standard formula\n    a = (\n        np.sin(dlat / 2.0) ** 2\n        + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0) ** 2\n    )\n    c = 2 * np.arcsin(np.minimum(1, np.sqrt(a)))\n    return R * c\n\n\ndef competition_metric(\n    df, pred_lat_col, pred_lon_col, gt_lat_col, gt_lon_col, phone_col=\"phone\"\n):\n    \"\"\"Compute mean of (50th, 95th percentile) distance errors per phone.\"\"\"\n    if df.empty:\n        return np.nan\n    errors = haversine_distance(\n        df[gt_lat_col].values,\n        df[gt_lon_col].values,\n        df[pred_lat_col].values,\n        df[pred_lon_col].values,\n    )\n    df = df.copy()\n    df[\"error\"] = errors\n    phone_scores = []\n    for phone, g in df.groupby(phone_col):\n        e = g[\"error\"].values\n        if len(e) == 0:\n            continue\n        p50 = np.percentile(e, 50)\n        p95 = np.percentile(e, 95)\n        phone_scores.append((p50 + p95) / 2.0)\n    if not phone_scores:\n        return np.nan\n    return float(np.mean(phone_scores))\n\n\ndef load_train_paths(train_dir):\n    \"\"\"Return list of (phone_id, gnss_path, gt_path).\"\"\"\n    paths = []\n    if not os.path.isdir(train_dir):\n        return paths\n    for drive in sorted(os.listdir(train_dir)):\n        drive_path = os.path.join(train_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n            if os.path.exists(gnss_path) and os.path.exists(gt_path):\n                phone_id = f\"{drive}/{phone}\"\n                paths.append((phone_id, gnss_path, gt_path))\n    return paths\n\n\ndef load_test_paths(test_dir):\n    \"\"\"Return list of (phone_id, gnss_path).\"\"\"\n    paths = []\n    if not os.path.isdir(test_dir):\n        return paths\n    for drive in sorted(os.listdir(test_dir)):\n        drive_path = os.path.join(test_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n            if os.path.exists(gnss_path):\n                phone_id = f\"{drive}/{phone}\"\n                paths.append((phone_id, gnss_path))\n    return paths\n\n\ndef extract_baseline_cols(df):\n    \"\"\"Find baseline latitude/longitude columns in device_gnss, if any.\"\"\"\n    lat_col_candidates = [c for c in df.columns if c.lower().startswith(\"latitudedeg\")]\n    lon_col_candidates = [c for c in df.columns if c.lower().startswith(\"longitudedeg\")]\n    if len(lat_col_candidates) == 0 or len(lon_col_candidates) == 0:\n        return None, None\n    return lat_col_candidates[0], lon_col_candidates[0]\n\n\n# ---------------- Build training table ----------------\n\ntrain_paths = load_train_paths(TRAIN_DIR)\ntrain_rows = []\n\nfor phone_id, gnss_path, gt_path in train_paths:\n    try:\n        gt = pd.read_csv(gt_path)\n    except Exception:\n        continue\n\n    required_gt_cols = {\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"}\n    if not required_gt_cols.issubset(gt.columns):\n        continue\n\n    gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n    gt_small[\"phone\"] = phone_id\n\n    # Load device_gnss for this phone to get any baseline coordinates\n    try:\n        gnss = pd.read_csv(gnss_path)\n    except Exception:\n        gnss = None\n\n    if gnss is not None and \"utcTimeMillis\" in gnss.columns:\n        gnss_local = gnss.copy()\n        # Round utcTimeMillis to nearest 1000 ms\n        gnss_local[\"UnixTimeMillis\"] = (\n            np.round(gnss_local[\"utcTimeMillis\"] / 1000.0) * 1000\n        ).astype(\"int64\")\n        lat_col, lon_col = extract_baseline_cols(gnss_local)\n        if lat_col is not None and lon_col is not None:\n            base_df = gnss_local[[\"UnixTimeMillis\", lat_col, lon_col]].copy()\n            base_df = (\n                base_df.groupby(\"UnixTimeMillis\")[[lat_col, lon_col]]\n                .mean()\n                .reset_index()\n            )\n            base_df.rename(\n                columns={lat_col: \"baseline_lat\", lon_col: \"baseline_lon\"}, inplace=True\n            )\n            merged = pd.merge(gt_small, base_df, on=\"UnixTimeMillis\", how=\"left\")\n        else:\n            merged = gt_small.copy()\n            merged[\"baseline_lat\"] = np.nan\n            merged[\"baseline_lon\"] = np.nan\n    else:\n        merged = gt_small.copy()\n        merged[\"baseline_lat\"] = np.nan\n        merged[\"baseline_lon\"] = np.nan\n\n    train_rows.append(merged)\n\nif len(train_rows) == 0:\n    # Fallback: if no usable train rows, create a small dummy to avoid crash\n    # but metric will be NaN; predictions will use global means from whatever is available.\n    train_df = pd.DataFrame(\n        columns=[\n            \"UnixTimeMillis\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"phone\",\n            \"baseline_lat\",\n            \"baseline_lon\",\n        ]\n    )\nelse:\n    train_df = pd.concat(train_rows, ignore_index=True)\n\n# If baseline is missing, use ground_truth itself as baseline (zero offset)\nif not train_df.empty:\n    train_df[\"baseline_lat\"] = train_df[\"baseline_lat\"].fillna(\n        train_df[\"LatitudeDegrees\"]\n    )\n    train_df[\"baseline_lon\"] = train_df[\"baseline_lon\"].fillna(\n        train_df[\"LongitudeDegrees\"]\n    )\n\n    # Targets: offsets (ground_truth - baseline)\n    train_df[\"dlat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"baseline_lat\"]\n    train_df[\"dlon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"baseline_lon\"]\n\n    # Remove rows with missing essentials\n    train_df = train_df.dropna(\n        subset=[\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"baseline_lat\",\n            \"baseline_lon\",\n            \"dlat\",\n            \"dlon\",\n        ]\n    ).reset_index(drop=True)\nelse:\n    # create dlat/dlon to avoid downstream errors\n    train_df[\"dlat\"] = np.nan\n    train_df[\"dlon\"] = np.nan\n\n# ---------------- \"Model\": per-phone mean offset ----------------\n\nif not train_df.empty:\n    phone_offsets = train_df.groupby(\"phone\")[[\"dlat\", \"dlon\"]].mean().reset_index()\n    phone_offsets.rename(\n        columns={\"dlat\": \"mean_dlat\", \"dlon\": \"mean_dlon\"}, inplace=True\n    )\n\n    phone_means = (\n        train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        .mean()\n        .reset_index()\n    )\n    phone_means.rename(\n        columns={\n            \"LatitudeDegrees\": \"mean_lat\",\n            \"LongitudeDegrees\": \"mean_lon\",\n        },\n        inplace=True,\n    )\n\n    phone_info = pd.merge(phone_offsets, phone_means, on=\"phone\", how=\"outer\")\nelse:\n    phone_info = pd.DataFrame(\n        columns=[\"phone\", \"mean_dlat\", \"mean_dlon\", \"mean_lat\", \"mean_lon\"]\n    )\n\n# ---------------- Hold-out validation metric (simple split) ----------------\n\nif not train_df.empty:\n    rng = np.random.default_rng(seed=42)\n    train_df = train_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n\n    val_mask = np.zeros(len(train_df), dtype=bool)\n    for phone, g_idx in train_df.groupby(\"phone\").groups.items():\n        idx = np.array(list(g_idx))\n        if len(idx) == 0:\n            continue\n        n_val = max(1, int(0.2 * len(idx)))\n        n_val = min(n_val, len(idx))\n        val_idx = rng.choice(idx, size=n_val, replace=False)\n        val_mask[val_idx] = True\n\n    val_df = train_df[val_mask].copy()\n\n    # Merge offsets\n    val_df = pd.merge(val_df, phone_info, on=\"phone\", how=\"left\")\n    val_df[\"mean_dlat\"] = val_df[\"mean_dlat\"].fillna(0.0)\n    val_df[\"mean_dlon\"] = val_df[\"mean_dlon\"].fillna(0.0)\n\n    val_df[\"pred_lat\"] = val_df[\"baseline_lat\"] + val_df[\"mean_dlat\"]\n    val_df[\"pred_lon\"] = val_df[\"baseline_lon\"] + val_df[\"mean_dlon\"]\n\n    val_metric = competition_metric(\n        val_df,\n        pred_lat_col=\"pred_lat\",\n        pred_lon_col=\"pred_lon\",\n        gt_lat_col=\"LatitudeDegrees\",\n        gt_lon_col=\"LongitudeDegrees\",\n        phone_col=\"phone\",\n    )\nelse:\n    val_metric = np.nan\n\nprint(f\"Validation metric (mean of 50th & 95th errors): {val_metric}\")\n\n# ---------------- Build test predictions ----------------\n\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n\n# In this competition, columns are fixed:\n# phone,UnixTimeMillis,LatitudeDegrees,LongitudeDegrees\nsample_phone_col = \"phone\"\nsample_time_col = \"UnixTimeMillis\"\nsample_lat_col = \"LatitudeDegrees\"\nsample_lon_col = \"LongitudeDegrees\"\n\ntest_paths = load_test_paths(TEST_DIR)\n\n\ndef build_test_baseline_for_phone(sample_phone_name, gnss_path, time_col_name):\n    \"\"\"\n    Build baseline lat/lon for a given sample phone using its device_gnss.csv.\n    \"\"\"\n    try:\n        gnss = pd.read_csv(gnss_path)\n    except Exception:\n        return pd.DataFrame(\n            columns=[sample_phone_col, time_col_name, \"baseline_lat\", \"baseline_lon\"]\n        )\n\n    if \"utcTimeMillis\" not in gnss.columns:\n        return pd.DataFrame(\n            columns=[sample_phone_col, time_col_name, \"baseline_lat\", \"baseline_lon\"]\n        )\n\n    gnss_local = gnss.copy()\n    gnss_local[time_col_name] = (\n        np.round(gnss_local[\"utcTimeMillis\"] / 1000.0) * 1000\n    ).astype(\"int64\")\n\n    lat_col, lon_col = extract_baseline_cols(gnss_local)\n    if lat_col is None or lon_col is None:\n        return pd.DataFrame(\n            columns=[sample_phone_col, time_col_name, \"baseline_lat\", \"baseline_lon\"]\n        )\n\n    base_df = gnss_local[[time_col_name, lat_col, lon_col]].copy()\n    base_df = base_df.groupby(time_col_name)[[lat_col, lon_col]].mean().reset_index()\n    base_df.rename(\n        columns={lat_col: \"baseline_lat\", lon_col: \"baseline_lon\"}, inplace=True\n    )\n    base_df[sample_phone_col] = sample_phone_name\n    return base_df\n\n\ndef folder_phone_to_sample_phone(phone_id):\n    \"\"\"Map from test folder phone_id ('drive/phonefolder') to sample phone name.\"\"\"\n    drive, phone_folder = phone_id.split(\"/\", 1)\n    mapping_suffix = {\n        \"GooglePixel4\": \"Pixel4\",\n        \"GooglePixel4XL\": \"Pixel4XL\",\n        \"GooglePixel5\": \"Pixel5\",\n        \"SamsungGalaxyS20Ultra\": \"SamsungS20Ultra\",\n        \"XiaomiMi8\": \"Mi8\",\n    }\n    suffix = mapping_suffix.get(phone_folder, phone_folder)\n    return f\"{drive}_{suffix}\"\n\n\nsample_phones_unique = set(sample_sub[sample_phone_col].unique())\ntest_baseline_list = []\n\nfor phone_id, gnss_path in test_paths:\n    sample_phone = folder_phone_to_sample_phone(phone_id)\n    if sample_phone not in sample_phones_unique:\n        continue\n    base_df = build_test_baseline_for_phone(sample_phone, gnss_path, sample_time_col)\n    if not base_df.empty:\n        test_baseline_list.append(base_df)\n\nif len(test_baseline_list) > 0:\n    test_baseline_df = pd.concat(test_baseline_list, ignore_index=True)\nelse:\n    test_baseline_df = pd.DataFrame(\n        columns=[sample_phone_col, sample_time_col, \"baseline_lat\", \"baseline_lon\"]\n    )\n\nsubmission = sample_sub.copy()\n\nsubmission = submission.merge(\n    test_baseline_df,\n    left_on=[sample_phone_col, sample_time_col],\n    right_on=[sample_phone_col, sample_time_col],\n    how=\"left\",\n)\n\n# Global means for fallback\nif not train_df.empty:\n    global_mean_lat = train_df[\"LatitudeDegrees\"].mean()\n    global_mean_lon = train_df[\"LongitudeDegrees\"].mean()\n    global_mean_dlat = train_df[\"dlat\"].mean()\n    global_mean_dlon = train_df[\"dlon\"].mean()\nelse:\n    # Fallback arbitrary safe values if train_df is empty\n    global_mean_lat = (\n        submission[sample_lat_col].median()\n        if sample_lat_col in submission.columns\n        else 0.0\n    )\n    global_mean_lon = (\n        submission[sample_lon_col].median()\n        if sample_lon_col in submission.columns\n        else 0.0\n    )\n    global_mean_dlat = 0.0\n    global_mean_dlon = 0.0\n\nsubmission[\"baseline_lat\"] = submission[\"baseline_lat\"].fillna(global_mean_lat)\nsubmission[\"baseline_lon\"] = submission[\"baseline_lon\"].fillna(global_mean_lon)\n\n# Final predictions: baseline + global mean offsets (simple but robust)\nsubmission[sample_lat_col] = submission[\"baseline_lat\"] + global_mean_dlat\nsubmission[sample_lon_col] = submission[\"baseline_lon\"] + global_mean_dlon\n\n# Keep required columns and order: match original sample_submission columns\nsubmission_out = submission[sample_sub.columns]\n\n# Save to required paths\nsubmission_out.to_csv(SUBMISSION_PATH, index=False)\n# Also mirror into working directory for safety if desired by grader\nsubmission_out.to_csv(os.path.join(WORK_DIR, \"submission.csv\"), index=False)\n\nprint(f\"Saved submission to {SUBMISSION_PATH}\")\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\n# Directories according to benchmark instructions\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    \"\"\"Compute great-circle distance (meters) between two lat/lon arrays.\"\"\"\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    \"\"\"Mean over phones of (P50+P95)/2 of distance errors.\"\"\"\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef load_train_data(input_dir):\n    \"\"\"Aggregate GNSS features per millisecond and merge with ground truth.\"\"\"\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        raise RuntimeError(f\"Train directory not found at {train_root}\")\n\n    all_rows = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception:\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            # Align time\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"].astype(\"int64\")\n\n            base_cols = [\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"SvElevationDegrees\",\n                \"SvAzimuthDegrees\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateMetersPerSecond\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                \"IonosphericDelayMeters\",\n                \"TroposphericDelayMeters\",\n            ]\n            cols = [c for c in base_cols if c in gnss.columns]\n            if \"UnixTimeMillis\" not in cols:\n                cols = [\"UnixTimeMillis\"] + cols\n\n            agg_df = gnss[cols].copy()\n            agg = agg_df.groupby(\"UnixTimeMillis\").agg(\"mean\").reset_index()\n\n            merged = pd.merge(\n                agg,\n                gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]],\n                on=\"UnixTimeMillis\",\n                how=\"inner\",\n            )\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        raise RuntimeError(\n            \"No training data assembled. Check train folder structure and file names.\"\n        )\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\ndef prepare_features_targets(train_df):\n    \"\"\"Split features and targets, remove non-feature columns.\"\"\"\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c\n        not in [\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"phone\",\n            \"drive\",\n        ]\n    ]\n    X = train_df[feature_cols].copy()\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n    return X, y_lat, y_lon, feature_cols\n\n\ndef train_and_validate(train_df):\n    \"\"\"5-fold GroupKFold on drive id; return models, features, metric, medians.\"\"\"\n    train_df = train_df.copy()\n    # derive drive id from phone string: driveId_phoneModel\n    train_df[\"drive\"] = train_df[\"phone\"].apply(\n        lambda x: x.split(\"_\")[0] if \"_\" in x else x\n    )\n\n    X, y_lat, y_lon, feature_cols = prepare_features_targets(train_df)\n    medians = X.median()\n    X = X.fillna(medians)\n\n    groups = train_df[\"drive\"].values\n    n_unique_groups = len(np.unique(groups))\n    n_splits = min(5, n_unique_groups)\n    if n_splits < 2:\n        return [], feature_cols, np.nan, train_df, medians\n\n    gkf = GroupKFold(n_splits=n_splits)\n    oof_list = []\n    models = []\n\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=20, n_jobs=-1, random_state=42 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=20, n_jobs=-1, random_state=142 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        pred_lat = rf_lat.predict(X_val)\n        pred_lon = rf_lon.predict(X_val)\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"gt_lat\": ylat_val,\n                \"gt_lon\": ylon_val,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n        models.append((rf_lat, rf_lon))\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return models, feature_cols, metric, train_df, medians\n\n\ndef fit_full_models(train_df, feature_cols, global_medians):\n    \"\"\"Train final models on all training data.\"\"\"\n    X = train_df[feature_cols].copy()\n    medians = global_medians.copy()\n    for c in feature_cols:\n        if c not in medians.index:\n            medians[c] = X[c].median()\n    X = X.fillna(medians)\n\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=22, n_jobs=-1, random_state=100\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=22, n_jobs=-1, random_state=101\n    )\n    rf_lat.fit(X, y_lat)\n    rf_lon.fit(X, y_lon)\n    return rf_lat, rf_lon, medians\n\n\ndef load_test_features_simple(input_dir, feature_cols, sample_sub):\n    \"\"\"\n    Build test feature matrix by aggregating device_gnss per millisecond and\n    aligning to sample_submission rows.\n    \"\"\"\n    suffix_to_folder = {\n        \"Pixel4\": \"GooglePixel4\",\n        \"Pixel4XL\": \"GooglePixel4XL\",\n        \"Pixel5\": \"GooglePixel5\",\n        \"SamsungS20Ultra\": \"SamsungGalaxyS20Ultra\",\n        \"Mi8\": \"XiaomiMi8\",\n    }\n\n    sample = sample_sub.copy()\n    # phone format in sample: driveId_phoneSuffix (e.g., 2020-06-04-US-MTV-1_Pixel4)\n    sample[\"drive\"] = sample[\"phone\"].apply(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n    sample[\"suffix\"] = sample[\"phone\"].apply(lambda x: x.split(\"_\")[-1])\n    sample[\"folder_model\"] = sample[\"suffix\"].map(suffix_to_folder)\n    sample[\"folder_phone\"] = sample[\"drive\"] + \"_\" + sample[\"folder_model\"]\n\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            folder_phone = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n            except Exception:\n                continue\n\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"].astype(\"int64\")\n\n            cols_needed = [\"UnixTimeMillis\"]\n            for c in feature_cols:\n                if c != \"UnixTimeMillis\" and c in gnss.columns:\n                    cols_needed.append(c)\n            cols_needed = list(dict.fromkeys(cols_needed))\n\n            agg_df = gnss[cols_needed].copy()\n            agg = agg_df.groupby(\"UnixTimeMillis\").agg(\"mean\").reset_index()\n            agg[\"folder_phone\"] = folder_phone\n            agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\"folder_phone\", \"UnixTimeMillis\"] + feature_cols\n        )\n\n    sample_with_idx = sample.copy()\n    sample_with_idx[\"row_id\"] = np.arange(len(sample_with_idx))\n\n    merged = pd.merge(\n        sample_with_idx,\n        test_agg,\n        how=\"left\",\n        on=[\"folder_phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    merged = merged.sort_values(\"row_id\")\n    X_test = pd.DataFrame(index=merged[\"row_id\"].values)\n    for c in feature_cols:\n        if c in merged.columns:\n            X_test[c] = merged[c].values\n        else:\n            X_test[c] = np.nan\n\n    meta = merged[[\"row_id\", \"phone\", \"UnixTimeMillis\"]].copy()\n    return X_test, meta\n\n\ndef main():\n    # Load and prepare training data\n    train_df = load_train_data(INPUT_DIR)\n\n    # Train with CV\n    models_cv, feature_cols, val_metric, train_df_full, cv_medians = train_and_validate(\n        train_df\n    )\n    print(\n        \"Cross-validation competition metric (mean of per-phone (P50+P95)/2):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Train final models on all data\n    rf_lat, rf_lon, train_medians = fit_full_models(\n        train_df_full, feature_cols, cv_medians\n    )\n\n    # Load sample submission\n    sample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_sub_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_sub_path}\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # Build test features\n    X_test_raw, meta = load_test_features_simple(INPUT_DIR, feature_cols, sample_sub)\n\n    # Ensure all feature columns exist and are filled\n    X_test = X_test_raw.copy()\n    for c in feature_cols:\n        if c not in X_test.columns:\n            X_test[c] = train_medians.get(c, 0.0)\n    X_test = X_test.fillna(train_medians)\n\n    # Predict\n    pred_lat = rf_lat.predict(X_test[feature_cols])\n    pred_lon = rf_lon.predict(X_test[feature_cols])\n\n    # Build submission in sample order\n    submission = sample_sub.copy()\n    submission[\"row_id\"] = np.arange(len(submission))\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": meta[\"row_id\"].values,\n            \"LatitudeDegrees\": pred_lat,\n            \"LongitudeDegrees\": pred_lon,\n        }\n    )\n\n    submission = pd.merge(submission, pred_df, on=\"row_id\", how=\"left\")\n\n    # For any rows without predictions (e.g., missing GNSS), fill by interpolation per phone\n    submission.sort_values([\"phone\", \"UnixTimeMillis\"], inplace=True)\n    submission[\"LatitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LatitudeDegrees\"].ffill().bfill()\n    )\n    submission[\"LongitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LongitudeDegrees\"].ffill().bfill()\n    )\n\n    submission = submission[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].reset_index(drop=True)\n\n    # Save submission files\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    submission.to_csv(out_path_submission, index=False)\n    submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to: {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_phone_from_path(path):\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    # expect .../train/collection/phone/device_gnss.csv\n    if len(parts) < 3:\n        return None\n    try:\n        idx = parts.index(\"train\")\n    except ValueError:\n        try:\n            idx = parts.index(\"test\")\n        except ValueError:\n            return None\n    if len(parts) <= idx + 2:\n        return None\n    collection = parts[idx + 1]\n    phone_model = parts[idx + 2]\n    return f\"{collection}_{phone_model}\"\n\n\ndef load_device_gnss_aggregated(split_dir):\n    \"\"\"\n    Aggregate device_gnss.csv to 1 Hz per (phone, UnixTimeMillis_round).\n    For train, also keep mean LatitudeDegrees/LongitudeDegrees as targets.\n    \"\"\"\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n\n    # Some reasonable numeric columns to aggregate; we will auto-detect additionally.\n    base_feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    for fpath in files:\n        phone = parse_phone_from_path(fpath)\n        if phone is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n\n        # time column\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            continue\n\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n        df[\"UnixTimeMillis_round\"] = (df[\"UnixTimeMillis\"] / 1000.0).round().astype(\n            \"int64\"\n        ) * 1000\n\n        # collect feature columns actually present & numeric\n        cols_present = [c for c in base_feature_cols if c in df.columns]\n        # add all other numeric columns except some keys & targets\n        for c in df.columns:\n            if c in cols_present:\n                continue\n            if c in [\n                time_col,\n                \"UnixTimeMillis\",\n                \"UnixTimeMillis_round\",\n                \"phone\",\n                \"MessageType\",\n                \"SignalType\",\n                \"CodeType\",\n            ]:\n                continue\n            if c in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n                # keep for targets, but don't use directly as feature (to avoid trivial label leakage)\n                continue\n            if pd.api.types.is_numeric_dtype(df[c]):\n                cols_present.append(c)\n\n        group_cols = [\"phone\", \"UnixTimeMillis_round\"]\n        agg_dict = {}\n        for c in cols_present:\n            agg_dict[c] = [\"mean\", \"std\", \"min\", \"max\"]\n\n        use_cols = group_cols + cols_present\n        df_small = df[use_cols].copy()\n        g = df_small.groupby(group_cols).agg(agg_dict)\n        g.columns = [f\"{feat}_{stat}\" for feat, stat in g.columns]\n        g = g.reset_index()\n\n        # attach targets if exist\n        if \"LatitudeDegrees\" in df.columns and \"LongitudeDegrees\" in df.columns:\n            tgt = (\n                df.groupby(group_cols)[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n                .mean()\n                .reset_index()\n            )\n            g = g.merge(tgt, on=group_cols, how=\"left\")\n\n        all_rows.append(g)\n\n    if not all_rows:\n        return pd.DataFrame()\n\n    feats = pd.concat(all_rows, ignore_index=True)\n    return feats\n\n\ndef build_train_test_from_gnss(sample_sub):\n    # train aggregated from device_gnss with targets\n    train_agg = load_device_gnss_aggregated(\"train\")\n    # ensure we have targets\n    train_agg = train_agg.dropna(\n        subset=[\"LatitudeDegrees\", \"LongitudeDegrees\"], how=\"any\"\n    )\n    # test aggregated features\n    test_agg = load_device_gnss_aggregated(\"test\")\n\n    # sample_sub has phone like \"2020-06-04-US-MTV-1_Pixel4\"\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    test_keys[\"UnixTimeMillis_round\"] = (\n        test_keys[\"UnixTimeMillis\"].astype(\"int64\") / 1000.0\n    ).round().astype(\"int64\") * 1000\n\n    # align test features by (phone, UnixTimeMillis_round)\n    test_feats = pd.merge(\n        test_keys,\n        test_agg,\n        on=[\"phone\", \"UnixTimeMillis_round\"],\n        how=\"left\",\n        sort=False,\n    )\n    return train_agg, test_feats\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols_train = [\"phone\", \"UnixTimeMillis_round\"]\n\n    for k in key_cols_train:\n        if k not in train_df.columns:\n            raise RuntimeError(f\"Missing key {k} in training data\")\n\n    non_feature_cols = key_cols_train + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n    if not feature_cols:\n        raise RuntimeError(\"No numeric feature columns found for modeling.\")\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Build X_test from test_feats aligning with sample_submission\n    test_full = sample_sub.copy()\n    # test_feats already contains UnixTimeMillis from sample_sub via merge in build_train_test_from_gnss\n    cols_for_merge = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n    merged = pd.merge(\n        test_full,\n        test_feats[cols_for_merge],\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    test_full = merged\n\n    # ensure all feature cols exist\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n\n    X_test = test_full[feature_cols].copy()\n\n    # median imputation\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    n_splits = 5\n    groups = phones.astype(str).values\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=groups):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=200,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=200,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones.iloc[va_idx].values,\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    # Build training and test data directly from device_gnss\n    train_df, test_feats = build_train_test_from_gnss(sample_sub)\n\n    if train_df.empty:\n        raise RuntimeError(\n            \"Training data is empty; cannot train model from device_gnss.\"\n        )\n\n    # Prepare matrices\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    # run CV with grouping by phone\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].copy())\n    print(f\"Hold-out CV metric: {cv_score:.6f}\")\n\n    # train final models\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # predict on test\n    pred_lat_test = model_lat.predict(X_test)\n    pred_lon_test = model_lon.predict(X_test)\n\n    submission = sample_sub.copy()\n    submission[\"LatitudeDegrees\"] = pred_lat_test\n    submission[\"LongitudeDegrees\"] = pred_lon_test\n\n    # Fallback: for any NaNs (e.g., no features matched), fill with per-phone mean or global mean\n    for col in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n        if submission[col].isna().any():\n            # per-phone mean first\n            submission[col] = submission.groupby(\"phone\")[col].transform(\n                lambda s: s.fillna(s.mean())\n            )\n            # if still NaN, use global mean\n            if submission[col].isna().any():\n                submission[col] = submission[col].fillna(submission[col].mean())\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols]\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_collection_phone_from_path(path, split_keyword=\"train\"):\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    if split_keyword not in parts:\n        return None, None\n    idx = parts.index(split_keyword)\n    # expect .../train/collection/phone/...\n    if len(parts) <= idx + 2:\n        return None, None\n    collection = parts[idx + 1]\n    phone_model = parts[idx + 2]\n    phone = f\"{collection}_{phone_model}\"\n    return collection, phone\n\n\ndef load_gnss_agg(split_dir):\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n\n    base_feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(\n            fpath, split_keyword=split_dir\n        )\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n\n        time_col = None\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        if time_col is None:\n            continue\n\n        df[\"collection\"] = collection\n        df[\"phone_model\"] = phone\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n        df[\"UnixTimeMillis_round\"] = (df[\"UnixTimeMillis\"] / 1000.0).round().astype(\n            \"int64\"\n        ) * 1000\n\n        cols_present = [c for c in base_feature_cols if c in df.columns]\n        for c in df.columns:\n            if c in cols_present:\n                continue\n            if c in [\n                time_col,\n                \"UnixTimeMillis\",\n                \"UnixTimeMillis_round\",\n                \"phone\",\n                \"collection\",\n                \"phone_model\",\n                \"MessageType\",\n                \"SignalType\",\n                \"CodeType\",\n            ]:\n                continue\n            if c in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n                continue\n            if pd.api.types.is_numeric_dtype(df[c]):\n                cols_present.append(c)\n\n        group_cols = [\"collection\", \"phone\", \"UnixTimeMillis_round\"]\n        agg_dict = {c: [\"mean\", \"std\", \"min\", \"max\"] for c in cols_present}\n\n        use_cols = group_cols + cols_present\n        df_small = df[use_cols].copy()\n        g = df_small.groupby(group_cols).agg(agg_dict)\n        g.columns = [f\"{feat}_{stat}\" for feat, stat in g.columns]\n        g = g.reset_index()\n        all_rows.append(g)\n\n    if not all_rows:\n        return pd.DataFrame()\n    feats = pd.concat(all_rows, ignore_index=True)\n    return feats\n\n\ndef load_train_targets():\n    base_path = os.path.join(INPUT_DIR, \"train\")\n    pattern = os.path.join(base_path, \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(\n            fpath, split_keyword=\"train\"\n        )\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n        if \"UnixTimeMillis\" not in df.columns:\n            continue\n        df[\"collection\"] = collection\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[\"UnixTimeMillis\"].astype(\"int64\")\n        df[\"UnixTimeMillis_round\"] = (df[\"UnixTimeMillis\"] / 1000.0).round().astype(\n            \"int64\"\n        ) * 1000\n        keep_cols = [\n            \"collection\",\n            \"phone\",\n            \"UnixTimeMillis\",\n            \"UnixTimeMillis_round\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n        ]\n        df = df[keep_cols]\n        all_rows.append(df)\n    if not all_rows:\n        return pd.DataFrame()\n    return pd.concat(all_rows, ignore_index=True)\n\n\ndef build_train_test(sample_sub):\n    train_feats = load_gnss_agg(\"train\")\n    train_targets = load_train_targets()\n    if train_feats.empty or train_targets.empty:\n        raise RuntimeError(\"Training features or targets are empty.\")\n\n    train_df = pd.merge(\n        train_targets,\n        train_feats,\n        on=[\"collection\", \"phone\", \"UnixTimeMillis_round\"],\n        how=\"left\",\n        sort=False,\n    )\n\n    # drop rows with no features at all\n    numeric_cols = [\n        c\n        for c in train_df.columns\n        if pd.api.types.is_numeric_dtype(train_df[c])\n        and c\n        not in [\n            \"UnixTimeMillis\",\n            \"UnixTimeMillis_round\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n        ]\n    ]\n    # ensure at least some non-null features\n    mask_has_feat = train_df[numeric_cols].notnull().sum(axis=1) > 0\n    train_df = train_df[mask_has_feat].reset_index(drop=True)\n\n    test_feats = load_gnss_agg(\"test\")\n\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    # infer collection from phone (sample phone is \"collection_phoneModel\")\n    test_keys[\"collection\"] = test_keys[\"phone\"].astype(str).str.split(\"_\").str[0]\n    test_keys[\"UnixTimeMillis_round\"] = (\n        test_keys[\"UnixTimeMillis\"].astype(\"int64\") / 1000.0\n    ).round().astype(\"int64\") * 1000\n\n    test_merged = pd.merge(\n        test_keys,\n        test_feats,\n        on=[\"collection\", \"phone\", \"UnixTimeMillis_round\"],\n        how=\"left\",\n        sort=False,\n    )\n    return train_df, test_merged\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols_train = [\"collection\", \"phone\", \"UnixTimeMillis_round\"]\n\n    for k in key_cols_train:\n        if k not in train_df.columns:\n            raise RuntimeError(f\"Missing key {k} in training data\")\n\n    non_feature_cols = key_cols_train + [target_lat, target_lon, \"UnixTimeMillis\"]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n    if not feature_cols:\n        raise RuntimeError(\"No numeric feature columns found for modeling.\")\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Build X_test aligned with sample_submission\n    test_full = sample_sub.copy()\n    cols_for_merge = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n    merged = pd.merge(\n        test_full,\n        test_feats[cols_for_merge],\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    test_full = merged\n\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n\n    X_test = test_full[feature_cols].copy()\n\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    n_splits = 5\n    groups = phones.astype(str).values\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=groups):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=200,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=200,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones.iloc[va_idx].values,\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    train_df, test_feats = build_train_test(sample_sub)\n\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].copy())\n    print(f\"Hold-out CV metric: {cv_score:.6f}\")\n\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    pred_lat_test = model_lat.predict(X_test)\n    pred_lon_test = model_lon.predict(X_test)\n\n    submission = sample_sub.copy()\n    submission[\"LatitudeDegrees\"] = pred_lat_test\n    submission[\"LongitudeDegrees\"] = pred_lon_test\n\n    # Fallbacks for any NaNs\n    for col in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n        if submission[col].isna().any():\n            submission[col] = submission.groupby(\"phone\")[col].transform(\n                lambda s: s.fillna(s.mean())\n            )\n            if submission[col].isna().any():\n                submission[col] = submission[col].fillna(submission[col].mean())\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols]\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_collection_phone_from_path(path, split_keyword=\"train\"):\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    if split_keyword not in parts:\n        return None, None\n    idx = parts.index(split_keyword)\n    # expect .../train/collection/phone/...\n    if len(parts) <= idx + 2:\n        return None, None\n    collection = parts[idx + 1]\n    phone_model = parts[idx + 2]\n    phone = f\"{collection}_{phone_model}\"\n    return collection, phone\n\n\ndef load_gnss_agg(split_dir):\n    \"\"\"\n    Aggregate device_gnss features per (collection, phone, UnixTimeMillis).\n    Use simple mean aggregation over satellites at the same time.\n    \"\"\"\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n\n    base_feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(\n            fpath, split_keyword=split_dir\n        )\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n\n        # Determine time column and normalize to UnixTimeMillis int64\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            continue\n\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n        df[\"collection\"] = collection\n        df[\"phone\"] = phone\n\n        cols_present = [c for c in base_feature_cols if c in df.columns]\n\n        # also add any other numeric cols (but keep it modest)\n        for c in df.columns:\n            if c in cols_present:\n                continue\n            if c in [\n                time_col,\n                \"UnixTimeMillis\",\n                \"collection\",\n                \"phone\",\n                \"MessageType\",\n                \"SignalType\",\n                \"CodeType\",\n            ]:\n                continue\n            if c in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n                continue\n            if pd.api.types.is_numeric_dtype(df[c]):\n                cols_present.append(c)\n\n        if not cols_present:\n            continue\n\n        group_cols = [\"collection\", \"phone\", \"UnixTimeMillis\"]\n        agg_dict = {c: \"mean\" for c in cols_present}\n        use_cols = group_cols + cols_present\n        df_small = df[use_cols].copy()\n        g = df_small.groupby(group_cols, as_index=False).agg(agg_dict)\n        all_rows.append(g)\n\n    if not all_rows:\n        return pd.DataFrame()\n    feats = pd.concat(all_rows, ignore_index=True)\n    return feats\n\n\ndef load_train_targets():\n    base_path = os.path.join(INPUT_DIR, \"train\")\n    pattern = os.path.join(base_path, \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(\n            fpath, split_keyword=\"train\"\n        )\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n        if \"UnixTimeMillis\" not in df.columns:\n            continue\n        df[\"collection\"] = collection\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[\"UnixTimeMillis\"].astype(\"int64\")\n        keep_cols = [\n            \"collection\",\n            \"phone\",\n            \"UnixTimeMillis\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n        ]\n        df = df[keep_cols]\n        all_rows.append(df)\n    if not all_rows:\n        return pd.DataFrame()\n    return pd.concat(all_rows, ignore_index=True)\n\n\ndef build_train_test(sample_sub):\n    train_feats = load_gnss_agg(\"train\")\n    train_targets = load_train_targets()\n    if train_feats.empty or train_targets.empty:\n        raise RuntimeError(\n            f\"Training features or targets are empty. feats_empty={train_feats.empty}, targets_empty={train_targets.empty}\"\n        )\n\n    # merge on exact UnixTimeMillis (no rounding)\n    train_df = pd.merge(\n        train_targets,\n        train_feats,\n        on=[\"collection\", \"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n\n    # drop rows with no features at all\n    numeric_cols = [\n        c\n        for c in train_df.columns\n        if pd.api.types.is_numeric_dtype(train_df[c])\n        and c\n        not in [\n            \"UnixTimeMillis\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n        ]\n    ]\n    if numeric_cols:\n        mask_has_feat = train_df[numeric_cols].notnull().sum(axis=1) > 0\n        train_df = train_df[mask_has_feat].reset_index(drop=True)\n\n    # Build test features\n    test_feats = load_gnss_agg(\"test\")\n\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    # infer collection from phone string (sample phone is \"collection_phoneModel\")\n    test_keys[\"collection\"] = test_keys[\"phone\"].astype(str).str.split(\"_\").str[0]\n\n    test_merged = pd.merge(\n        test_keys,\n        test_feats,\n        on=[\"collection\", \"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    return train_df, test_merged\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n\n    key_cols_train = [\"collection\", \"phone\", \"UnixTimeMillis\"]\n    for k in key_cols_train:\n        if k not in train_df.columns:\n            raise RuntimeError(f\"Missing key {k} in training data\")\n\n    non_feature_cols = key_cols_train + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n    if not feature_cols:\n        raise RuntimeError(\"No numeric feature columns found for modeling.\")\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Build X_test aligned with sample_submission\n    test_full = sample_sub.copy()\n    # merge on phone + UnixTimeMillis with precomputed test_feats (which includes collection & features)\n    cols_for_merge = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n    # ensure test_feats has all needed cols\n    for c in cols_for_merge:\n        if c not in test_feats.columns:\n            if c in feature_cols:\n                test_feats[c] = np.nan\n\n    merged = pd.merge(\n        test_full,\n        test_feats[cols_for_merge],\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    test_full = merged\n\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n\n    X_test = test_full[feature_cols].copy()\n\n    # Fill missing values with training medians\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    n_splits = 5\n    groups = phones.astype(str).values\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=groups):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=200,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=200,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones.iloc[va_idx].values,\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    train_df, test_feats = build_train_test(sample_sub)\n\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    # Run 5-fold CV and print evaluation metric\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].copy())\n    print(f\"Hold-out CV metric: {cv_score:.6f}\")\n\n    # Train full models on all training data\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Predict on test set\n    pred_lat_test = model_lat.predict(X_test)\n    pred_lon_test = model_lon.predict(X_test)\n\n    submission = sample_sub.copy()\n    submission[\"LatitudeDegrees\"] = pred_lat_test\n    submission[\"LongitudeDegrees\"] = pred_lon_test\n\n    # Fallbacks for any NaNs\n    for col in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n        if submission[col].isna().any():\n            submission[col] = submission.groupby(\"phone\")[col].transform(\n                lambda s: s.fillna(s.mean())\n            )\n            if submission[col].isna().any():\n                submission[col] = submission[col].fillna(submission[col].mean())\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols]\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\n# Directories\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    \"\"\"Compute great-circle distance (meters) between two lat/lon arrays.\"\"\"\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    \"\"\"Mean over phones of (P50+P95)/2 of distance errors.\"\"\"\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef load_train_data(input_dir):\n    \"\"\"Aggregate GNSS features per millisecond and merge with ground truth.\"\"\"\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        raise RuntimeError(f\"Train directory not found at {train_root}\")\n\n    all_rows = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception:\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            # Align time\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"].astype(\"int64\")\n\n            base_cols = [\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"SvElevationDegrees\",\n                \"SvAzimuthDegrees\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateMetersPerSecond\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                \"IonosphericDelayMeters\",\n                \"TroposphericDelayMeters\",\n            ]\n            cols = [c for c in base_cols if c in gnss.columns]\n            if \"UnixTimeMillis\" not in cols:\n                cols = [\"UnixTimeMillis\"] + cols\n\n            agg_df = gnss[cols].copy()\n            agg = agg_df.groupby(\"UnixTimeMillis\").agg(\"mean\").reset_index()\n\n            merged = pd.merge(\n                agg,\n                gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]],\n                on=\"UnixTimeMillis\",\n                how=\"inner\",\n            )\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            # store drive id explicitly for grouping\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        raise RuntimeError(\n            \"No training data assembled. Check train folder structure and file names.\"\n        )\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\ndef prepare_features_targets(train_df):\n    \"\"\"Split features and targets, remove non-feature columns.\"\"\"\n    non_features = [\"LatitudeDegrees\", \"LongitudeDegrees\", \"phone\", \"drive\"]\n    feature_cols = [c for c in train_df.columns if c not in non_features]\n    X = train_df[feature_cols].copy()\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n    return X, y_lat, y_lon, feature_cols\n\n\ndef train_and_validate(train_df):\n    \"\"\"5-fold GroupKFold on drive id; return models, features, metric, medians.\"\"\"\n    train_df = train_df.copy()\n\n    X, y_lat, y_lon, feature_cols = prepare_features_targets(train_df)\n    medians = X.median()\n    X = X.fillna(medians)\n\n    groups = train_df[\"drive\"].values\n    n_unique_groups = len(np.unique(groups))\n    n_splits = min(5, n_unique_groups)\n    if n_splits < 2:\n        return [], feature_cols, np.nan, train_df, medians\n\n    gkf = GroupKFold(n_splits=n_splits)\n    oof_list = []\n    models = []\n\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=20, n_jobs=-1, random_state=42 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=20, n_jobs=-1, random_state=142 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        pred_lat = rf_lat.predict(X_val)\n        pred_lon = rf_lon.predict(X_val)\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"gt_lat\": ylat_val,\n                \"gt_lon\": ylon_val,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n        models.append((rf_lat, rf_lon))\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return models, feature_cols, metric, train_df, medians\n\n\ndef fit_full_models(train_df, feature_cols, global_medians):\n    \"\"\"Train final models on all training data.\"\"\"\n    X = train_df[feature_cols].copy()\n    medians = global_medians.copy()\n    for c in feature_cols:\n        if c not in medians.index:\n            medians[c] = X[c].median()\n    X = X.fillna(medians)\n\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=22, n_jobs=-1, random_state=100\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=22, n_jobs=-1, random_state=101\n    )\n    rf_lat.fit(X, y_lat)\n    rf_lon.fit(X, y_lon)\n    return rf_lat, rf_lon, medians\n\n\ndef load_test_features(input_dir, feature_cols, sample_sub):\n    \"\"\"\n    Build test feature matrix by aggregating device_gnss per millisecond and\n    aligning to sample_submission rows on (phone, UnixTimeMillis).\n    phone is derived from directory structure: driveId_phoneModel\n    \"\"\"\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n            except Exception:\n                continue\n\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"].astype(\"int64\")\n\n            cols_needed = [\"UnixTimeMillis\"]\n            for c in feature_cols:\n                if c != \"UnixTimeMillis\" and c in gnss.columns:\n                    cols_needed.append(c)\n            cols_needed = list(dict.fromkeys(cols_needed))\n\n            agg_df = gnss[cols_needed].copy()\n            agg = agg_df.groupby(\"UnixTimeMillis\").agg(\"mean\").reset_index()\n            agg[\"phone\"] = phone_id\n            agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        # create empty with appropriate columns\n        test_agg = pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"] + feature_cols)\n\n    sample = sample_sub.copy()\n    sample_with_idx = sample.copy()\n    sample_with_idx[\"row_id\"] = np.arange(len(sample_with_idx))\n\n    merged = pd.merge(\n        sample_with_idx,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    merged = merged.sort_values(\"row_id\")\n    X_test = pd.DataFrame(index=merged[\"row_id\"].values)\n    for c in feature_cols:\n        if c in merged.columns:\n            X_test[c] = merged[c].values\n        else:\n            X_test[c] = np.nan\n\n    meta = merged[[\"row_id\", \"phone\", \"UnixTimeMillis\"]].copy()\n    return X_test, meta\n\n\ndef main():\n    # Load and prepare training data\n    train_df = load_train_data(INPUT_DIR)\n\n    # Train with CV\n    models_cv, feature_cols, val_metric, train_df_full, cv_medians = train_and_validate(\n        train_df\n    )\n    print(\n        \"Cross-validation competition metric (mean of per-phone (P50+P95)/2):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Train final models on all data\n    rf_lat, rf_lon, train_medians = fit_full_models(\n        train_df_full, feature_cols, cv_medians\n    )\n\n    # Load sample submission\n    sample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_sub_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_sub_path}\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # Build test features\n    X_test_raw, meta = load_test_features(INPUT_DIR, feature_cols, sample_sub)\n\n    # Ensure all feature columns exist and are filled\n    X_test = X_test_raw.copy()\n    for c in feature_cols:\n        if c not in X_test.columns:\n            X_test[c] = train_medians.get(c, 0.0)\n    X_test = X_test.fillna(train_medians)\n\n    # Predict\n    pred_lat = rf_lat.predict(X_test[feature_cols])\n    pred_lon = rf_lon.predict(X_test[feature_cols])\n\n    # Build submission in sample order\n    submission = sample_sub.copy()\n    submission[\"row_id\"] = np.arange(len(submission))\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": meta[\"row_id\"].values,\n            \"LatitudeDegrees\": pred_lat,\n            \"LongitudeDegrees\": pred_lon,\n        }\n    )\n\n    submission = pd.merge(submission, pred_df, on=\"row_id\", how=\"left\")\n\n    # For any rows without predictions (e.g., missing GNSS), fill by interpolation per phone\n    submission.sort_values([\"phone\", \"UnixTimeMillis\"], inplace=True)\n    submission[\"LatitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LatitudeDegrees\"].ffill().bfill()\n    )\n    submission[\"LongitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LongitudeDegrees\"].ffill().bfill()\n    )\n\n    submission = submission[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].reset_index(drop=True)\n\n    # Save submission files\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    submission.to_csv(out_path_submission, index=False)\n    submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to: {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ----- Coordinate transforms -----\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ----- Metric -----\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ----- Data loaders -----\ndef load_train(train_root):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            gnss = pd.read_csv(gnss_path)\n            gt = pd.read_csv(gt_path)\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            # ensure numeric timestamps\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            ).astype(\"Int64\")\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(\n                gt[\"UnixTimeMillis\"], errors=\"coerce\"\n            ).astype(\"Int64\")\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            # Aggregate GNSS per ms timestamp\n            agg = (\n                gnss.groupby(\"UnixTimeMillis\")\n                .agg(\n                    {\n                        \"Cn0DbHz\": \"mean\",\n                        \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                        wls_cols[0]: \"mean\",\n                        wls_cols[1]: \"mean\",\n                        wls_cols[2]: \"mean\",\n                    }\n                )\n                .reset_index()\n            )\n            agg[\"phone\"] = phone_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n\n            merged = pd.merge(\n                gt_small,\n                agg,\n                on=[\"phone\", \"UnixTimeMillis\"],\n                how=\"inner\",\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n            rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            gnss = pd.read_csv(gnss_path)\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            ).astype(\"Int64\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = (\n                gnss.groupby(\"UnixTimeMillis\")\n                .agg(\n                    {\n                        \"Cn0DbHz\": \"mean\",\n                        \"PseudorangeRateUncertaintyMetersPerSecond\": \"mean\",\n                        wls_cols[0]: \"mean\",\n                        wls_cols[1]: \"mean\",\n                        wls_cols[2]: \"mean\",\n                    }\n                )\n                .reset_index()\n            )\n            agg[\"phone\"] = phone_id\n            rows.append(agg)\n\n    if not rows:\n        test_gnss = pd.DataFrame(\n            columns=[\n                \"UnixTimeMillis\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"phone\",\n            ]\n        )\n    else:\n        test_gnss = pd.concat(rows, ignore_index=True)\n\n    # Build phone in sample_sub if needed\n    if \"phone\" not in sample_sub.columns:\n        if \"collectionName\" in sample_sub.columns and \"phoneName\" in sample_sub.columns:\n            sample_sub[\"phone\"] = (\n                sample_sub[\"collectionName\"].astype(str)\n                + \"_\"\n                + sample_sub[\"phoneName\"].astype(str)\n            )\n        else:\n            # In this benchmark, sample_submission already has 'phone'\n            pass\n\n    sample_sub[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_sub[\"UnixTimeMillis\"], errors=\"coerce\"\n    ).astype(\"Int64\")\n    sample_sub = sample_sub.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    merged = pd.merge(\n        sample_sub,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    # In this benchmark, input data is under ./input\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Load train data\n    train_df = load_train(train_root)\n\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n\n    # Fill NaNs with column medians\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    # groups by collection (drive)\n    groups = train_df[\"phone\"].str.split(\"_\").str[0]\n\n    # ----- 5-fold GroupKFold CV -----\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups), start=1):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"5-fold CV competition metric (mean of P50/P95 errors):\", metric_val)\n\n    # ----- Train final models on full data -----\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # ----- Prepare test and submission -----\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # Ensure 'phone' exists in sample_sub\n    if \"phone\" not in sample_sub.columns:\n        if \"collectionName\" in sample_sub.columns and \"phoneName\" in sample_sub.columns:\n            sample_sub[\"phone\"] = (\n                sample_sub[\"collectionName\"].astype(str)\n                + \"_\"\n                + sample_sub[\"phoneName\"].astype(str)\n            )\n        else:\n            raise RuntimeError(\n                \"sample_submission missing 'phone' and cannot be reconstructed.\"\n            )\n\n    # Preserve original order with row_id\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    test_merged = load_test(test_root, sample_sub)\n\n    # Ensure feature columns present\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n\n    # Fill with train medians\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    X_test = test_merged[feature_cols].values\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align back to full sample_sub order\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill any missing predictions (if any phones/times not in GNSS) by ffill/bfill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # Build final CSV\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"],\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"],\n        }\n    )\n\n    # Save to required path ./submission/submission.csv\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    # Also mirror to ./working/submission.csv if needed by benchmark\n    working_dir = \"./working\"\n    try:\n        os.makedirs(working_dir, exist_ok=True)\n        working_submission_path = os.path.join(working_dir, \"submission.csv\")\n        final_out.to_csv(working_submission_path, index=False)\n        print(f\"Mirrored submission to {working_submission_path}\")\n    except Exception as e:\n        # Non-fatal\n        print(f\"Could not save to ./working: {e}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ----- Coordinate transforms -----\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ----- Metric -----\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ----- Data loaders -----\ndef load_train(train_root):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            gnss = pd.read_csv(gnss_path)\n            gt = pd.read_csv(gt_path)\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n\n            if len(gnss) == 0 or len(gt) == 0:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            use_cols = [\n                \"Cn0DbHz\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                wls_cols[0],\n                wls_cols[1],\n                wls_cols[2],\n            ]\n            for c in use_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            # Aggregate GNSS per ms timestamp\n            agg = gnss.groupby(\"UnixTimeMillis\")[use_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n\n            agg[\"phone\"] = phone_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n\n            merged = pd.merge(\n                gt_small,\n                agg,\n                on=[\"phone\", \"UnixTimeMillis\"],\n                how=\"inner\",\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n            rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            gnss = pd.read_csv(gnss_path)\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            if len(gnss) == 0:\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            # Ensure feature columns exist\n            for c in feature_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n            agg[\"phone\"] = phone_id\n            rows.append(agg)\n\n    if not rows:\n        test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"] + feature_cols)\n    else:\n        test_gnss = pd.concat(rows, ignore_index=True)\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Load train data\n    train_df = load_train(train_root)\n\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n\n    # Fill NaNs with column medians\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    # groups by collection (drive)\n    groups = train_df[\"phone\"].str.split(\"_\").str[0]\n\n    # ----- 5-fold GroupKFold CV -----\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups), start=1):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"5-fold CV competition metric (mean of P50/P95 errors):\", metric_val)\n\n    # ----- Train final models on full data -----\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # ----- Prepare test and submission -----\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # Ensure 'phone' exists in sample_sub\n    if \"phone\" not in sample_sub.columns:\n        if \"collectionName\" in sample_sub.columns and \"phoneName\" in sample_sub.columns:\n            sample_sub[\"phone\"] = (\n                sample_sub[\"collectionName\"].astype(str)\n                + \"_\"\n                + sample_sub[\"phoneName\"].astype(str)\n            )\n        else:\n            raise RuntimeError(\n                \"sample_submission missing 'phone' and cannot be reconstructed.\"\n            )\n\n    # Preserve original order with row_id\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    # Merge GNSS features to sample_sub rows\n    test_merged = load_test(test_root, sample_sub, feature_cols)\n\n    # Ensure row_id is preserved after merge\n    if \"row_id\" not in test_merged.columns:\n        raise RuntimeError(\"row_id missing after merging test GNSS with sample_sub\")\n\n    # Ensure feature columns present\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n\n    # Fill with train medians\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    X_test = test_merged[feature_cols].values\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align back to full sample_sub order\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill any missing predictions (if any phones/times not in GNSS) by ffill/bfill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still any NaNs (e.g., all rows were NaN), fall back to training-set mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        # Use global mean of training ground truth as backup\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"],\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"],\n        }\n    )\n\n    # Save to required path ./submission/submission.csv\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    # Also mirror to ./working/submission.csv as required by benchmark\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ----- Coordinate transforms -----\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ----- Metric -----\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ----- Data loaders -----\ndef load_train(train_root):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            gnss = pd.read_csv(gnss_path)\n            gt = pd.read_csv(gt_path)\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n\n            if len(gnss) == 0 or len(gt) == 0:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            use_cols = [\n                \"Cn0DbHz\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                wls_cols[0],\n                wls_cols[1],\n                wls_cols[2],\n            ]\n            for c in use_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            # Aggregate GNSS per ms timestamp\n            agg = gnss.groupby(\"UnixTimeMillis\")[use_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n\n            agg[\"phone\"] = phone_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n\n            merged = pd.merge(\n                gt_small,\n                agg,\n                on=[\"phone\", \"UnixTimeMillis\"],\n                how=\"inner\",\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n            rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            gnss = pd.read_csv(gnss_path)\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            if len(gnss) == 0:\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Ensure feature columns exist in gnss\n            for c in feature_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n            agg[\"phone\"] = phone_id\n            rows.append(agg)\n\n    if not rows:\n        test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"] + feature_cols)\n    else:\n        test_gnss = pd.concat(rows, ignore_index=True)\n\n    # Sample already has phone and UnixTimeMillis\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Load train data\n    train_df = load_train(train_root)\n\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n\n    # Fill NaNs with column medians\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    # groups by collection (drive)\n    groups = train_df[\"phone\"].str.split(\"_\").str[0]\n\n    # ----- 5-fold GroupKFold CV -----\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups), start=1):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"5-fold CV competition metric (mean of P50/P95 errors):\", metric_val)\n\n    # ----- Train final models on full data -----\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # ----- Prepare test and submission -----\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # In this benchmark, sample_submission already has 'phone' and 'UnixTimeMillis'\n    if \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n        raise RuntimeError(\n            \"sample_submission.csv must contain 'phone' and 'UnixTimeMillis' columns.\"\n        )\n\n    # Preserve original order with row_id\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    # Merge GNSS features to sample_sub rows\n    test_merged = load_test(test_root, sample_sub, feature_cols)\n\n    # Ensure row_id is preserved after merge\n    if \"row_id\" not in test_merged.columns:\n        # If merge dropped row_id (it shouldn't with how=\"left\"), re-merge to restore\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    # Ensure feature columns present\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n\n    # Fill with train medians\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    X_test = test_merged[feature_cols].values\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align back to full sample_sub order\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill any missing predictions (if any phones/times not in GNSS) by ffill/bfill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still any NaNs (e.g., all rows were NaN), fall back to training-set mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"],\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"],\n        }\n    )\n\n    # Save to required path ./submission/submission.csv\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    # Also mirror to ./working/submission.csv as required by benchmark\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ----- Coordinate transforms -----\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ----- Metric -----\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ----- Data loaders -----\ndef load_train(train_root):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            # IMPORTANT: phone_id must match sample_submission \"phone\" column.\n            # In this benchmark, that column is just the phone folder name (e.g., \"GooglePixel4\").\n            phone_id = phone_name\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            gnss = pd.read_csv(gnss_path)\n            gt = pd.read_csv(gt_path)\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n\n            if len(gnss) == 0 or len(gt) == 0:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            use_cols = [\n                \"Cn0DbHz\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                wls_cols[0],\n                wls_cols[1],\n                wls_cols[2],\n            ]\n            for c in use_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            # Aggregate GNSS per ms timestamp\n            agg = gnss.groupby(\"UnixTimeMillis\")[use_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n\n            agg[\"phone\"] = phone_id\n            agg[\"drive_id\"] = drive_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n            gt_small[\"drive_id\"] = drive_id\n\n            merged = pd.merge(\n                gt_small,\n                agg,\n                on=[\"phone\", \"drive_id\", \"UnixTimeMillis\"],\n                how=\"inner\",\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n            rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            # Must match sample_submission \"phone\"\n            phone_id = phone_name\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            gnss = pd.read_csv(gnss_path)\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            if len(gnss) == 0:\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Ensure feature columns exist in gnss\n            for c in feature_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n            agg[\"phone\"] = phone_id\n            agg[\"drive_id\"] = drive_id\n            rows.append(agg)\n\n    if not rows:\n        test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"] + feature_cols)\n    else:\n        test_gnss = pd.concat(rows, ignore_index=True)\n\n    # Sample already has phone and UnixTimeMillis\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Load train data\n    train_df = load_train(train_root)\n\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n\n    # Fill NaNs with column medians\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    # groups by collection (drive)\n    groups = train_df[\"drive_id\"].values\n\n    # ----- 5-fold GroupKFold CV -----\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    fold_idx = 1\n    for tr_idx, val_idx in gkf.split(X, y_x, groups):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n        fold_idx += 1\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"5-fold CV competition metric (mean of P50/P95 errors):\", metric_val)\n\n    # ----- Train final models on full data -----\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # ----- Prepare test and submission -----\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # In this benchmark, sample_submission already has 'phone' and 'UnixTimeMillis'\n    if \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n        raise RuntimeError(\n            \"sample_submission.csv must contain 'phone' and 'UnixTimeMillis' columns.\"\n        )\n\n    # Preserve original order with row_id\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    # Merge GNSS features to sample_sub rows\n    test_merged = load_test(test_root, sample_sub, feature_cols)\n\n    # Ensure row_id is preserved after merge\n    if \"row_id\" not in test_merged.columns:\n        # If merge dropped row_id (it shouldn't with how=\"left\"), re-merge to restore\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    # Ensure feature columns present\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n\n    # Fill with train medians\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    X_test = test_merged[feature_cols].values\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align back to full sample_sub order\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill any missing predictions (if any phones/times not in GNSS) by ffill/bfill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still any NaNs (e.g., all rows were NaN), fall back to training-set mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"].values,\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"].values,\n        }\n    )\n\n    # Save to required path ./submission/submission.csv\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    # Also mirror to ./working/submission.csv as required by benchmark\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport numpy as np\nimport pandas as pd\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSAMPLE_SUB_PATH = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n\nWORK_DIR = \"./working\"\nos.makedirs(WORK_DIR, exist_ok=True)\n\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nSUBMISSION_PATH = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    \"\"\"Vectorized haversine distance in meters.\"\"\"\n    R = 6371000.0\n    lat1_rad = np.radians(lat1)\n    lat2_rad = np.radians(lat2)\n    dlat = lat2_rad - lat1_rad\n    dlon = np.radians(lon2 - lon1)\n    a = (\n        np.sin(dlat / 2.0) ** 2\n        + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2.0) ** 2\n    )\n    c = 2 * np.arcsin(np.minimum(1, np.sqrt(a)))\n    return R * c\n\n\ndef competition_metric(\n    df, pred_lat_col, pred_lon_col, gt_lat_col, gt_lon_col, phone_col=None\n):\n    \"\"\"\n    Compute mean of (50th, 95th percentile) distance errors.\n    If phone_col is provided and present, compute per-phone then average; else compute on all rows.\n    \"\"\"\n    if df.empty:\n        return np.nan\n\n    errors = haversine_distance(\n        df[gt_lat_col].values,\n        df[gt_lon_col].values,\n        df[pred_lat_col].values,\n        df[pred_lon_col].values,\n    )\n    df = df.copy()\n    df[\"error\"] = errors\n\n    if phone_col is not None and phone_col in df.columns:\n        phone_scores = []\n        for _, g in df.groupby(phone_col):\n            e = g[\"error\"].values\n            if len(e) == 0:\n                continue\n            p50 = np.percentile(e, 50)\n            p95 = np.percentile(e, 95)\n            phone_scores.append((p50 + p95) / 2.0)\n        if not phone_scores:\n            return np.nan\n        return float(np.mean(phone_scores))\n    else:\n        e = df[\"error\"].values\n        p50 = np.percentile(e, 50)\n        p95 = np.percentile(e, 95)\n        return float((p50 + p95) / 2.0)\n\n\ndef load_train_ground_truth(train_dir):\n    \"\"\"Load all ground_truth.csv files into a single DataFrame.\"\"\"\n    rows = []\n    if not os.path.isdir(train_dir):\n        return pd.DataFrame()\n\n    for drive in sorted(os.listdir(train_dir)):\n        drive_path = os.path.join(train_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n            if not os.path.exists(gt_path):\n                continue\n            try:\n                gt = pd.read_csv(gt_path)\n            except Exception:\n                continue\n            # Ensure required columns exist\n            required_cols = {\"LatitudeDegrees\", \"LongitudeDegrees\"}\n            if not required_cols.issubset(gt.columns):\n                continue\n            gt_small = gt[[\"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n            # If phone column exists in gt, keep it; else create one from folder path\n            if \"phone\" in gt.columns:\n                gt_small[\"phone\"] = gt[\"phone\"]\n            else:\n                gt_small[\"phone\"] = f\"{drive}/{phone}\"\n            rows.append(gt_small)\n\n    if rows:\n        return pd.concat(rows, ignore_index=True)\n    else:\n        return pd.DataFrame(columns=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"phone\"])\n\n\n# ---------------- Load training data and compute global mean ----------------\n\ntrain_df = load_train_ground_truth(TRAIN_DIR)\n\nif not train_df.empty:\n    global_mean_lat = float(train_df[\"LatitudeDegrees\"].mean())\n    global_mean_lon = float(train_df[\"LongitudeDegrees\"].mean())\nelse:\n    # Fallback if no training data for some reason\n    global_mean_lat = 0.0\n    global_mean_lon = 0.0\n\n# ---------------- Hold-out validation metric (simple per-phone split) ----------------\n\nif not train_df.empty:\n    # We will build a simple constant model: predict global_mean_lat/lon\n    # and evaluate it with a hold-out set constructed per phone.\n    rng = np.random.default_rng(seed=42)\n    train_df = train_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n\n    val_mask = np.zeros(len(train_df), dtype=bool)\n    if \"phone\" in train_df.columns:\n        groups = train_df.groupby(\"phone\").groups\n        for _, idx_list in groups.items():\n            idx = np.array(list(idx_list))\n            if len(idx) == 0:\n                continue\n            n_val = max(1, int(0.2 * len(idx)))\n            n_val = min(n_val, len(idx))\n            val_idx = rng.choice(idx, size=n_val, replace=False)\n            val_mask[val_idx] = True\n    else:\n        # No phone column; just take 20% random rows\n        n_val = max(1, int(0.2 * len(train_df)))\n        val_idx = rng.choice(np.arange(len(train_df)), size=n_val, replace=False)\n        val_mask[val_idx] = True\n\n    val_df = train_df[val_mask].copy()\n    val_df[\"pred_lat\"] = global_mean_lat\n    val_df[\"pred_lon\"] = global_mean_lon\n\n    phone_col = \"phone\" if \"phone\" in val_df.columns else None\n    val_metric = competition_metric(\n        val_df,\n        pred_lat_col=\"pred_lat\",\n        pred_lon_col=\"pred_lon\",\n        gt_lat_col=\"LatitudeDegrees\",\n        gt_lon_col=\"LongitudeDegrees\",\n        phone_col=phone_col,\n    )\nelse:\n    val_metric = np.nan\n\nprint(\n    f\"Validation metric (mean of 50th & 95th errors) for global-mean baseline: {val_metric}\"\n)\n\n# ---------------- Build test predictions and submission ----------------\n\n# Load sample submission to know required columns\nsample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n\n# Detect latitude/longitude columns in sample submission\nlat_cols = [c for c in sample_sub.columns if \"lat\" in c.lower()]\nlon_cols = [c for c in sample_sub.columns if \"lon\" in c.lower()]\n\nif not lat_cols or not lon_cols:\n    # If format is unexpected, just print info and create dummy columns\n    # but still obey existing columns.\n    # Create LatitudeDegrees/LongitudeDegrees columns if missing.\n    if \"LatitudeDegrees\" not in sample_sub.columns:\n        sample_sub[\"LatitudeDegrees\"] = global_mean_lat\n    if \"LongitudeDegrees\" not in sample_sub.columns:\n        sample_sub[\"LongitudeDegrees\"] = global_mean_lon\n    lat_col_name = \"LatitudeDegrees\"\n    lon_col_name = \"LongitudeDegrees\"\nelse:\n    lat_col_name = lat_cols[0]\n    lon_col_name = lon_cols[0]\n\n# Fill predictions with global mean\nsubmission = sample_sub.copy()\nsubmission[lat_col_name] = global_mean_lat\nsubmission[lon_col_name] = global_mean_lon\n\n# Save to required paths\nsubmission.to_csv(SUBMISSION_PATH, index=False)\nsubmission.to_csv(os.path.join(WORK_DIR, \"submission.csv\"), index=False)\n\nprint(f\"Saved submission to {SUBMISSION_PATH}\")\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_collection_phone_from_path(path, split_keyword=\"train\"):\n    \"\"\"\n    Robustly parse collection and phone from a path like:\n    ./input/train/2020-06-04-US-MTV-1/GooglePixel4/device_gnss.csv\n    or:\n    ./input/test/2020-06-04-US-MTV-1/GooglePixel4/device_gnss.csv\n\n    We simply take the two path components immediately after the split_keyword.\n    \"\"\"\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    if split_keyword not in parts:\n        return None, None\n    idx = parts.index(split_keyword)\n    # need at least two more components: collection, phone\n    if len(parts) <= idx + 2:\n        return None, None\n    collection = parts[idx + 1]\n    phone_model = parts[idx + 2]\n    phone = f\"{collection}_{phone_model}\"\n    return collection, phone\n\n\ndef load_gnss_agg(split_dir):\n    \"\"\"\n    Aggregate device_gnss features per (phone, UnixTimeMillis).\n    Use simple mean aggregation over satellites at the same time.\n    \"\"\"\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n\n    base_feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(\n            fpath, split_keyword=split_dir\n        )\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n\n        # Determine time column and normalize to UnixTimeMillis int64\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            continue\n\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n        df[\"collection\"] = collection\n        df[\"phone\"] = phone\n\n        cols_present = [c for c in base_feature_cols if c in df.columns]\n\n        # Add other numeric columns except keys / non-features / target-like\n        skip_cols = {\n            time_col,\n            \"UnixTimeMillis\",\n            \"collection\",\n            \"phone\",\n            \"MessageType\",\n            \"SignalType\",\n            \"CodeType\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n        }\n        for c in df.columns:\n            if c in cols_present or c in skip_cols:\n                continue\n            if pd.api.types.is_numeric_dtype(df[c]):\n                cols_present.append(c)\n\n        if not cols_present:\n            continue\n\n        group_cols = [\"phone\", \"UnixTimeMillis\"]\n        agg_dict = {c: \"mean\" for c in cols_present}\n        use_cols = group_cols + cols_present\n        df_small = df[use_cols].copy()\n        g = df_small.groupby(group_cols, as_index=False).agg(agg_dict)\n        g[\"collection\"] = collection\n        g[\"phone_full\"] = phone  # preserve full phone string if needed\n        # we'll just keep phone and UnixTimeMillis as keys; collection is embedded in phone\n        g = g.drop(columns=[\"collection\", \"phone_full\"], errors=\"ignore\")\n        all_rows.append(g)\n\n    if not all_rows:\n        return pd.DataFrame()\n    feats = pd.concat(all_rows, ignore_index=True)\n    # ensure key types\n    feats[\"phone\"] = feats[\"phone\"].astype(str)\n    feats[\"UnixTimeMillis\"] = feats[\"UnixTimeMillis\"].astype(\"int64\")\n    return feats\n\n\ndef load_train_targets():\n    base_path = os.path.join(INPUT_DIR, \"train\")\n    pattern = os.path.join(base_path, \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(\n            fpath, split_keyword=\"train\"\n        )\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n        if \"UnixTimeMillis\" not in df.columns:\n            continue\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[\"UnixTimeMillis\"].astype(\"int64\")\n        keep_cols = [\n            \"phone\",\n            \"UnixTimeMillis\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n        ]\n        df = df[keep_cols]\n        all_rows.append(df)\n    if not all_rows:\n        return pd.DataFrame()\n    targets = pd.concat(all_rows, ignore_index=True)\n    targets[\"phone\"] = targets[\"phone\"].astype(str)\n    targets[\"UnixTimeMillis\"] = targets[\"UnixTimeMillis\"].astype(\"int64\")\n    return targets\n\n\ndef build_train_test(sample_sub):\n    # Load training features and targets\n    train_feats = load_gnss_agg(\"train\")\n    train_targets = load_train_targets()\n\n    if train_feats.empty or train_targets.empty:\n        raise RuntimeError(\n            f\"Training features or targets are empty. feats_empty={train_feats.empty}, targets_empty={train_targets.empty}\"\n        )\n\n    # Merge on phone + UnixTimeMillis\n    train_df = pd.merge(\n        train_targets,\n        train_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n\n    # Drop rows with no feature values\n    numeric_cols = [\n        c\n        for c in train_df.columns\n        if pd.api.types.is_numeric_dtype(train_df[c])\n        and c\n        not in [\n            \"UnixTimeMillis\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n        ]\n    ]\n    if numeric_cols:\n        mask_has_feat = train_df[numeric_cols].notnull().sum(axis=1) > 0\n        train_df = train_df[mask_has_feat].reset_index(drop=True)\n\n    print(\"Train dataframe shape after merge and filtering:\", train_df.shape)\n\n    # Build test features\n    test_feats = load_gnss_agg(\"test\")\n    if test_feats.empty:\n        print(\n            \"Warning: test features are empty; predictions will default to train medians.\"\n        )\n    else:\n        print(\"Test feature frame shape:\", test_feats.shape)\n\n    # Ensure sample_sub types\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    test_keys[\"phone\"] = test_keys[\"phone\"].astype(str)\n    test_keys[\"UnixTimeMillis\"] = test_keys[\"UnixTimeMillis\"].astype(\"int64\")\n\n    test_merged = pd.merge(\n        test_keys,\n        test_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    print(\"Test merged shape:\", test_merged.shape)\n    return train_df, test_merged\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n\n    key_cols_train = [\"phone\", \"UnixTimeMillis\"]\n    for k in key_cols_train:\n        if k not in train_df.columns:\n            raise RuntimeError(f\"Missing key {k} in training data\")\n\n    non_feature_cols = key_cols_train + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n    if not feature_cols:\n        raise RuntimeError(\"No numeric feature columns found for modeling.\")\n\n    print(\"Number of feature columns:\", len(feature_cols))\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Build X_test aligned with sample_submission\n    test_full = sample_sub.copy()\n    test_full[\"phone\"] = test_full[\"phone\"].astype(str)\n    test_full[\"UnixTimeMillis\"] = test_full[\"UnixTimeMillis\"].astype(\"int64\")\n\n    # ensure test_feats has all needed feature columns\n    for c in feature_cols:\n        if c not in test_feats.columns:\n            test_feats[c] = np.nan\n\n    cols_for_merge = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n\n    merged = pd.merge(\n        test_full,\n        test_feats[cols_for_merge],\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    test_full = merged\n\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n\n    X_test = test_full[feature_cols].copy()\n\n    # Fill missing values with training medians\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    n_splits = 5\n    groups = phones.astype(str).values\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=groups):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=200,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=200,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones.iloc[va_idx].values,\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    train_df, test_feats = build_train_test(sample_sub)\n\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    # Run 5-fold CV and print evaluation metric\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].copy())\n    print(f\"Hold-out CV metric: {cv_score:.6f}\")\n\n    # Train full models on all training data\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Predict on test set\n    pred_lat_test = model_lat.predict(X_test)\n    pred_lon_test = model_lon.predict(X_test)\n\n    submission = sample_sub.copy()\n    submission[\"LatitudeDegrees\"] = pred_lat_test\n    submission[\"LongitudeDegrees\"] = pred_lon_test\n\n    # Fallbacks for any NaNs\n    for col in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n        if submission[col].isna().any():\n            submission[col] = submission.groupby(\"phone\")[col].transform(\n                lambda s: s.fillna(s.mean())\n            )\n            if submission[col].isna().any():\n                submission[col] = submission[col].fillna(submission[col].mean())\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols]\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\n# Directories (per benchmark instructions)\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    \"\"\"Compute great-circle distance (meters) between two lat/lon arrays.\"\"\"\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    \"\"\"Mean over phones of (P50+P95)/2 of distance errors.\"\"\"\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef load_train_data(input_dir):\n    \"\"\"\n    Aggregate GNSS features per UnixTimeMillis and merge with ground truth.\n    phone id is driveId_phoneModel; drive is driveId for GroupKFold.\n    \"\"\"\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        raise RuntimeError(f\"Train directory not found at {train_root}\")\n\n    all_rows = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping {phone_dir} due to read error: {e}\", flush=True)\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            # Align time: use integer milliseconds\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"].astype(\"int64\")\n\n            # Define a conservative feature set to avoid train/test mismatch\n            base_cols = [\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"SvElevationDegrees\",\n                \"SvAzimuthDegrees\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateMetersPerSecond\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                \"IonosphericDelayMeters\",\n                \"TroposphericDelayMeters\",\n            ]\n            cols = [c for c in base_cols if c in gnss.columns]\n            if \"UnixTimeMillis\" not in cols:\n                cols = [\"UnixTimeMillis\"] + cols\n\n            agg_df = gnss[cols].copy()\n            # Aggregate by mean per millisecond\n            agg = agg_df.groupby(\"UnixTimeMillis\").agg(\"mean\").reset_index()\n\n            # Merge with ground truth\n            gt_cols = [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            gt_use = gt[[c for c in gt_cols if c in gt.columns]].copy()\n            if \"UnixTimeMillis\" not in gt_use.columns:\n                continue\n\n            merged = pd.merge(\n                agg,\n                gt_use,\n                on=\"UnixTimeMillis\",\n                how=\"inner\",\n            )\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        raise RuntimeError(\n            \"No training data assembled. Check train folder structure and file names.\"\n        )\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\ndef prepare_features_targets(train_df):\n    \"\"\"Split features and targets, remove non-feature columns.\"\"\"\n    non_features = [\"LatitudeDegrees\", \"LongitudeDegrees\", \"phone\", \"drive\"]\n    feature_cols = [c for c in train_df.columns if c not in non_features]\n    # Ensure UnixTimeMillis is present as feature for alignment\n    if \"UnixTimeMillis\" not in feature_cols and \"UnixTimeMillis\" in train_df.columns:\n        feature_cols.insert(0, \"UnixTimeMillis\")\n    X = train_df[feature_cols].copy()\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n    return X, y_lat, y_lon, feature_cols\n\n\ndef train_and_validate(train_df):\n    \"\"\"5-fold GroupKFold on drive id; return models, features, metric, medians.\"\"\"\n    train_df = train_df.copy()\n\n    X, y_lat, y_lon, feature_cols = prepare_features_targets(train_df)\n    medians = X.median(numeric_only=True)\n    X = X.fillna(medians)\n\n    groups = train_df[\"drive\"].values\n    n_unique_groups = len(np.unique(groups))\n    n_splits = min(5, n_unique_groups)\n    if n_splits < 2:\n        print(\"Not enough groups for CV, skipping cross-validation.\", flush=True)\n        return [], feature_cols, np.nan, train_df, medians\n\n    gkf = GroupKFold(n_splits=n_splits)\n    oof_list = []\n    models = []\n\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=20, n_jobs=-1, random_state=42 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=20, n_jobs=-1, random_state=142 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        pred_lat = rf_lat.predict(X_val)\n        pred_lon = rf_lon.predict(X_val)\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"gt_lat\": ylat_val,\n                \"gt_lon\": ylon_val,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n        models.append((rf_lat, rf_lon))\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return models, feature_cols, metric, train_df, medians\n\n\ndef fit_full_models(train_df, feature_cols, global_medians):\n    \"\"\"Train final models on all training data.\"\"\"\n    X = train_df[feature_cols].copy()\n    # Ensure every feature has a median\n    medians = global_medians.copy()\n    for c in feature_cols:\n        if c not in medians.index:\n            medians[c] = X[c].median()\n    X = X.fillna(medians)\n\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=22, n_jobs=-1, random_state=100\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=22, n_jobs=-1, random_state=101\n    )\n    rf_lat.fit(X, y_lat)\n    rf_lon.fit(X, y_lon)\n    return rf_lat, rf_lon, medians\n\n\ndef load_test_features(input_dir, feature_cols, sample_sub):\n    \"\"\"\n    Build test feature matrix by aggregating device_gnss per millisecond and\n    aligning to sample_submission rows on (phone, UnixTimeMillis).\n    phone is derived from directory structure: driveId_phoneModel\n    \"\"\"\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n            except Exception as e:\n                print(\n                    f\"Skipping test GNSS {phone_dir} due to read error: {e}\", flush=True\n                )\n                continue\n\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"utcTimeMillis\"].astype(\"int64\")\n\n            cols_needed = [\"UnixTimeMillis\"]\n            for c in feature_cols:\n                # UnixTimeMillis is already in cols_needed; avoid duplicates\n                if c != \"UnixTimeMillis\" and c in gnss.columns:\n                    cols_needed.append(c)\n            # Remove duplicates while preserving order\n            cols_needed = list(dict.fromkeys(cols_needed))\n\n            agg_df = gnss[cols_needed].copy()\n            agg = agg_df.groupby(\"UnixTimeMillis\").agg(\"mean\").reset_index()\n            agg[\"phone\"] = phone_id\n            agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        # create empty with appropriate columns\n        cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n        test_agg = pd.DataFrame(columns=cols)\n\n    # Ensure sample types match (phones and time as int64)\n    sample = sample_sub.copy()\n    # In this competition, sample \"phone\" is already like \"driveId_phoneModel\"\n    sample_with_idx = sample.copy()\n    sample_with_idx[\"row_id\"] = np.arange(len(sample_with_idx))\n\n    merged = pd.merge(\n        sample_with_idx,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    merged = merged.sort_values(\"row_id\")\n    X_test = pd.DataFrame(index=merged[\"row_id\"].values)\n    for c in feature_cols:\n        if c in merged.columns:\n            X_test[c] = merged[c].values\n        else:\n            X_test[c] = np.nan\n\n    meta = merged[[\"row_id\", \"phone\", \"UnixTimeMillis\"]].copy()\n    return X_test, meta\n\n\ndef main():\n    # Load and prepare training data\n    train_df = load_train_data(INPUT_DIR)\n    print(\n        f\"Train rows: {len(train_df)}, columns: {train_df.columns.tolist()}\", flush=True\n    )\n\n    # Train with CV\n    models_cv, feature_cols, val_metric, train_df_full, cv_medians = train_and_validate(\n        train_df\n    )\n    print(\n        \"Cross-validation competition metric (mean of per-phone (P50+P95)/2):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Train final models on all data\n    rf_lat, rf_lon, train_medians = fit_full_models(\n        train_df_full, feature_cols, cv_medians\n    )\n\n    # Load sample submission\n    sample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_sub_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_sub_path}\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # Build test features\n    X_test_raw, meta = load_test_features(INPUT_DIR, feature_cols, sample_sub)\n\n    # Ensure all feature columns exist and are filled\n    X_test = X_test_raw.copy()\n    for c in feature_cols:\n        if c not in X_test.columns:\n            X_test[c] = train_medians.get(c, 0.0)\n    X_test = X_test.fillna(train_medians)\n\n    # Predict\n    preds_lat = rf_lat.predict(X_test[feature_cols])\n    preds_lon = rf_lon.predict(X_test[feature_cols])\n\n    # Build submission in sample order\n    submission = sample_sub.copy()\n    submission[\"row_id\"] = np.arange(len(submission))\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": meta[\"row_id\"].values,\n            \"LatitudeDegrees\": preds_lat,\n            \"LongitudeDegrees\": preds_lon,\n        }\n    )\n\n    submission = pd.merge(submission, pred_df, on=\"row_id\", how=\"left\")\n\n    # For any rows without predictions (e.g., missing GNSS), fill by interpolation per phone\n    submission.sort_values([\"phone\", \"UnixTimeMillis\"], inplace=True)\n    submission[\"LatitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LatitudeDegrees\"].ffill().bfill()\n    )\n    submission[\"LongitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LongitudeDegrees\"].ffill().bfill()\n    )\n\n    # In very rare case all NaNs for a phone (defensive)\n    overall_lat_med = np.nanmedian(submission[\"LatitudeDegrees\"].values)\n    overall_lon_med = np.nanmedian(submission[\"LongitudeDegrees\"].values)\n    submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n        overall_lat_med\n    )\n    submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n        overall_lon_med\n    )\n\n    submission = submission[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].reset_index(drop=True)\n\n    # Save submission files\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    submission.to_csv(out_path_submission, index=False)\n    submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to: {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\n# Directories\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef load_train_data(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        raise RuntimeError(f\"Train directory not found at {train_root}\")\n\n    all_rows = []\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping {phone_dir} due to read error: {e}\", flush=True)\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            # unify time type\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            ).astype(\"Int64\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            base_cols = [\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n                \"SvElevationDegrees\",\n                \"SvAzimuthDegrees\",\n                \"Cn0DbHz\",\n                \"PseudorangeRateMetersPerSecond\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                \"IonosphericDelayMeters\",\n                \"TroposphericDelayMeters\",\n            ]\n            cols = [c for c in base_cols if c in gnss.columns]\n            if \"UnixTimeMillis\" not in cols:\n                cols = [\"UnixTimeMillis\"] + cols\n\n            agg_df = gnss[cols].copy()\n            if agg_df.empty:\n                continue\n            agg = agg_df.groupby(\"UnixTimeMillis\").mean().reset_index()\n\n            gt_cols = [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            gt_use = gt[[c for c in gt_cols if c in gt.columns]].copy()\n            if (\n                \"UnixTimeMillis\" not in gt_use.columns\n                or \"LatitudeDegrees\" not in gt_use.columns\n                or \"LongitudeDegrees\" not in gt_use.columns\n            ):\n                continue\n\n            gt_use[\"UnixTimeMillis\"] = pd.to_numeric(\n                gt_use[\"UnixTimeMillis\"], errors=\"coerce\"\n            ).astype(\"Int64\")\n            gt_use = gt_use.dropna(subset=[\"UnixTimeMillis\"])\n            gt_use[\"UnixTimeMillis\"] = gt_use[\"UnixTimeMillis\"].astype(\"int64\")\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        raise RuntimeError(\n            \"No training data assembled. Check train folder structure and file names.\"\n        )\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\ndef prepare_features_targets(train_df):\n    non_features = [\"LatitudeDegrees\", \"LongitudeDegrees\", \"phone\", \"drive\"]\n    feature_cols = [c for c in train_df.columns if c not in non_features]\n    if \"UnixTimeMillis\" in train_df.columns and \"UnixTimeMillis\" not in feature_cols:\n        feature_cols.insert(0, \"UnixTimeMillis\")\n    X = train_df[feature_cols].copy()\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n    return X, y_lat, y_lon, feature_cols\n\n\ndef train_and_validate(train_df):\n    train_df = train_df.copy()\n    X, y_lat, y_lon, feature_cols = prepare_features_targets(train_df)\n\n    medians = X.median(numeric_only=True)\n    X = X.fillna(medians)\n\n    groups = train_df[\"drive\"].values\n    n_unique_groups = len(np.unique(groups))\n    n_splits = min(5, n_unique_groups) if n_unique_groups > 1 else 1\n\n    if n_splits < 2:\n        # No proper CV possible\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=20, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=20, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X, y_lat)\n        rf_lon.fit(X, y_lon)\n        print(\"Not enough groups for CV, trained single model without CV.\", flush=True)\n        return [(rf_lat, rf_lon)], feature_cols, np.nan, train_df, medians\n\n    gkf = GroupKFold(n_splits=n_splits)\n    oof_list = []\n    models = []\n\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120,\n            max_depth=20,\n            n_jobs=-1,\n            random_state=42 + fold,\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120,\n            max_depth=20,\n            n_jobs=-1,\n            random_state=142 + fold,\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        pred_lat = rf_lat.predict(X_val)\n        pred_lon = rf_lon.predict(X_val)\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"gt_lat\": ylat_val,\n                \"gt_lon\": ylon_val,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n        models.append((rf_lat, rf_lon))\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return models, feature_cols, metric, train_df, medians\n\n\ndef fit_full_models(train_df, feature_cols, global_medians):\n    X = train_df[feature_cols].copy()\n    medians = global_medians.copy()\n    for c in feature_cols:\n        if c not in medians.index:\n            medians[c] = X[c].median()\n    X = X.fillna(medians)\n\n    y_lat = train_df[\"LatitudeDegrees\"].values\n    y_lon = train_df[\"LongitudeDegrees\"].values\n\n    rf_lat = RandomForestRegressor(\n        n_estimators=200,\n        max_depth=22,\n        n_jobs=-1,\n        random_state=100,\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200,\n        max_depth=22,\n        n_jobs=-1,\n        random_state=101,\n    )\n    rf_lat.fit(X, y_lat)\n    rf_lon.fit(X, y_lon)\n    return rf_lat, rf_lon, medians\n\n\ndef load_test_features(input_dir, feature_cols, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(\n                        f\"Skipping test GNSS {phone_dir} due to read error: {e}\",\n                        flush=True,\n                    )\n                    continue\n\n                if \"utcTimeMillis\" not in gnss.columns:\n                    continue\n\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"utcTimeMillis\"], errors=\"coerce\"\n                ).astype(\"Int64\")\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                cols_needed = [\"UnixTimeMillis\"]\n                for c in feature_cols:\n                    if c != \"UnixTimeMillis\" and c in gnss.columns:\n                        cols_needed.append(c)\n                cols_needed = list(dict.fromkeys(cols_needed))\n\n                agg_df = gnss[cols_needed].copy()\n                if agg_df.empty:\n                    continue\n                agg = agg_df.groupby(\"UnixTimeMillis\").mean().reset_index()\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n        test_agg = pd.DataFrame(columns=cols)\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample[\"UnixTimeMillis\"], errors=\"coerce\"\n    ).astype(\"Int64\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"])\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n    sample_with_idx = sample.copy()\n    sample_with_idx[\"row_id\"] = np.arange(len(sample_with_idx))\n\n    merged = pd.merge(\n        sample_with_idx,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    merged = merged.sort_values(\"row_id\")\n    X_test = pd.DataFrame(index=merged[\"row_id\"].values)\n    for c in feature_cols:\n        if c in merged.columns:\n            X_test[c] = merged[c].values\n        else:\n            X_test[c] = np.nan\n\n    meta = merged[[\"row_id\", \"phone\", \"UnixTimeMillis\"]].copy()\n    return X_test, meta, sample_with_idx\n\n\ndef main():\n    # Load training data\n    train_df = load_train_data(INPUT_DIR)\n    print(\n        f\"Train rows: {len(train_df)}, columns: {train_df.columns.tolist()}\", flush=True\n    )\n\n    # Train and validate\n    models_cv, feature_cols, val_metric, train_df_full, cv_medians = train_and_validate(\n        train_df\n    )\n    print(\n        \"Cross-validation competition metric (mean of per-phone (P50+P95)/2):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Fit full models\n    rf_lat, rf_lon, train_medians = fit_full_models(\n        train_df_full, feature_cols, cv_medians\n    )\n\n    # Load sample submission\n    sample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_sub_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_sub_path}\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # Build test features aligned to sample submission\n    X_test_raw, meta, sample_aligned = load_test_features(\n        INPUT_DIR, feature_cols, sample_sub\n    )\n\n    # Ensure all feature columns exist and fill NaNs\n    X_test = X_test_raw.copy()\n    for c in feature_cols:\n        if c not in X_test.columns:\n            X_test[c] = train_medians.get(c, 0.0)\n    X_test = X_test.fillna(train_medians)\n\n    # Predict\n    preds_lat = rf_lat.predict(X_test[feature_cols])\n    preds_lon = rf_lon.predict(X_test[feature_cols])\n\n    # Create prediction dataframe mapped by row_id\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": meta[\"row_id\"].values,\n            \"LatitudeDegrees\": preds_lat,\n            \"LongitudeDegrees\": preds_lon,\n        }\n    )\n\n    submission = sample_aligned.copy()\n    # ensure full range of original sample_submission\n    submission = submission.merge(pred_df, on=\"row_id\", how=\"left\")\n\n    # Some rows may still be missing predictions; interpolate per phone\n    submission.sort_values([\"phone\", \"UnixTimeMillis\"], inplace=True)\n    submission[\"LatitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LatitudeDegrees\"].ffill().bfill()\n    )\n    submission[\"LongitudeDegrees\"] = (\n        submission.groupby(\"phone\")[\"LongitudeDegrees\"].ffill().bfill()\n    )\n\n    # Fallback global median if still any NaNs\n    overall_lat_med = np.nanmedian(submission[\"LatitudeDegrees\"].values)\n    overall_lon_med = np.nanmedian(submission[\"LongitudeDegrees\"].values)\n    submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n        overall_lat_med\n    )\n    submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n        overall_lon_med\n    )\n\n    final_submission = submission[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].reset_index(drop=True)\n\n    # Save submission to both required locations\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to: {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\n# --------- Geodesy helpers ---------\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 constants\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = 6356752.314245\n    ep2 = (a**2 - b**2) / b**2\n\n    r = np.sqrt(x * x + y * y)\n    # handle zeros robustly\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    # height not needed\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\n# --------- Data loading for train ---------\ndef load_train_with_gt(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"No train directory found; cannot compute validation metric.\", flush=True)\n        return pd.DataFrame()\n\n    all_rows = []\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} read error: {e}\", flush=True)\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            if not set(\n                [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n            ).issubset(gnss.columns):\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # aggregate per epoch\n            agg = (\n                gnss[\n                    [\n                        \"UnixTimeMillis\",\n                        \"WlsPositionXEcefMeters\",\n                        \"WlsPositionYEcefMeters\",\n                        \"WlsPositionZEcefMeters\",\n                    ]\n                ]\n                .groupby(\"UnixTimeMillis\")\n                .mean()\n                .reset_index()\n            )\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_use[\"UnixTimeMillis\"] = pd.to_numeric(\n                gt_use[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt_use = gt_use.dropna(subset=[\"UnixTimeMillis\"])\n            gt_use[\"UnixTimeMillis\"] = gt_use[\"UnixTimeMillis\"].astype(\"int64\")\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No training rows assembled with ground truth.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    # base predictions from ECEF only; use RF to learn corrections\n    ecef_x = train_df[\"WlsPositionXEcefMeters\"].values\n    ecef_y = train_df[\"WlsPositionYEcefMeters\"].values\n    ecef_z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(ecef_x, ecef_y, ecef_z)\n\n    train_df = train_df.copy()\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    # simple time-related features\n    X = pd.DataFrame()\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"]\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"]\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"]\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X, y_lat)\n        rf_lon.fit(X, y_lon)\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n        print(\"Not enough groups for CV, trained single correction model.\", flush=True)\n        return lat_models, lon_models, feature_cols, np.nan, train_df\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df.iloc[val_idx][\"base_lat\"].values\n        base_lon = train_df.iloc[val_idx][\"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df.iloc[val_idx][\"LatitudeDegrees\"].values\n        gt_lon = train_df.iloc[val_idx][\"LongitudeDegrees\"].values\n        phones = train_df.iloc[val_idx][\"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    _, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols], y_lat)\n    rf_lon.fit(X[feature_cols], y_lon)\n    return rf_lat, rf_lon\n\n\n# --------- Test feature preparation ---------\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"utcTimeMillis\" not in gnss.columns:\n                    continue\n                if not set(\n                    [\n                        \"WlsPositionXEcefMeters\",\n                        \"WlsPositionYEcefMeters\",\n                        \"WlsPositionZEcefMeters\",\n                    ]\n                ).issubset(gnss.columns):\n                    continue\n\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"utcTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[\n                        [\n                            \"UnixTimeMillis\",\n                            \"WlsPositionXEcefMeters\",\n                            \"WlsPositionYEcefMeters\",\n                            \"WlsPositionZEcefMeters\",\n                        ]\n                    ]\n                    .groupby(\"UnixTimeMillis\")\n                    .mean()\n                    .reset_index()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        # empty; rely entirely on interpolation later\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"])\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n    sample[\"row_id\"] = np.arange(len(sample))\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    # compute base lat/lon where possible\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    mask_has_ecef = (\n        merged[\"WlsPositionXEcefMeters\"].notna()\n        & merged[\"WlsPositionYEcefMeters\"].notna()\n        & merged[\"WlsPositionZEcefMeters\"].notna()\n    )\n    if mask_has_ecef.any():\n        lat_deg, lon_deg = ecef_to_geodetic(\n            merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n            merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n            merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n        )\n        base_lat[mask_has_ecef.values] = lat_deg\n        base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(\n        index=(\n            merged[\"row_id\"].values\n            if \"row_id\" in merged.columns\n            else np.arange(len(merged))\n        )\n    )\n    # Required features\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = merged[\"UnixTimeMillis\"].astype(\"float64\").values\n        else:\n            if c in merged.columns:\n                X_test[c] = merged[c].values\n            elif c in [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]:\n                X_test[c] = merged.get(c, pd.Series(np.nan, index=merged.index))\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    # Load train\n    train_df = load_train_with_gt(INPUT_DIR)\n    if train_df.empty:\n        print(\n            \"No training data with GT found; will fall back to pure WLS baseline.\",\n            flush=True,\n        )\n        # We will not train a correction model, no metric\n        correction_model_lat = None\n        correction_model_lon = None\n        feature_cols = None\n        val_metric = np.nan\n    else:\n        # Train with CV\n        lat_models, lon_models, feature_cols, val_metric, train_df_prepared = (\n            train_with_cv(train_df)\n        )\n        print(\"CV competition metric:\", val_metric, flush=True)\n\n        # Fit final correction models on all train\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    # Print metric (required)\n    print(f\"Validation metric (mean (P50+P95)/2 over phones): {val_metric}\", flush=True)\n\n    # Load sample submission\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Correction predictions if model exists\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        # Simple imputation with column medians\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols])\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols])\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Now align to full sample_submission (including any rows dropped by NaN time parsing)\n    full_sample = sample_sub.copy()\n    full_sample[\"UnixTimeMillis\"] = pd.to_numeric(\n        full_sample[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    # Recreate row_id for full sample\n    full_sample[\"row_id\"] = np.arange(len(full_sample))\n\n    merged_small = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n    submission = pd.merge(\n        full_sample,\n        merged_small,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n    )\n\n    # Try to fill using per-phone interpolation of predictions\n    submission.sort_values([\"phone\", \"UnixTimeMillis\"], inplace=True)\n    submission[\"LatitudeDegrees\"] = submission.groupby(\"phone\")[\"pred_lat\"].apply(\n        lambda s: s.ffill().bfill()\n    )\n    submission[\"LongitudeDegrees\"] = submission.groupby(\"phone\")[\"pred_lon\"].apply(\n        lambda s: s.ffill().bfill()\n    )\n\n    # If still NaNs (e.g., no GNSS at all for a phone), fill with per-phone mean of available preds, else global mean\n    if submission[\"LatitudeDegrees\"].isna().any():\n        phone_means_lat = submission.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n            \"mean\"\n        )\n        phone_means_lon = submission.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n            \"mean\"\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n            phone_means_lat\n        )\n        submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n            phone_means_lon\n        )\n\n    # Global fallback\n    overall_lat = submission[\"LatitudeDegrees\"].mean()\n    overall_lon = submission[\"LongitudeDegrees\"].mean()\n    submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(overall_lat)\n    submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = submission[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n    final_submission = final_submission.sort_index().reset_index(drop=True)\n\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\n# --------- Geodesy helpers ---------\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 constants\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = 6356752.314245\n    ep2 = (a**2 - b**2) / b**2\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\n# --------- Data loading for train ---------\ndef load_train_with_gt(input_dir, sample_sub):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"No train directory found; cannot compute validation metric.\", flush=True)\n        return pd.DataFrame()\n\n    # Phones present in sample submission to ensure consistent keys\n    sample_phones = set(sample_sub[\"phone\"].unique())\n\n    all_rows = []\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            # Match test phone naming: \"<drive_id>_<phone_model>\"\n            phone_id = f\"{drive_id}_{phone_model}\"\n            if phone_id not in sample_phones:\n                # Training data may contain more phones than appear in test; skip to avoid mismatch\n                continue\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} read error: {e}\", flush=True)\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"utcTimeMillis\"])\n            if gnss.empty:\n                continue\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty:\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\")\n                .mean()\n                .reset_index()\n            )\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_use[\"UnixTimeMillis\"] = pd.to_numeric(\n                gt_use[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt_use = gt_use.dropna(subset=[\"UnixTimeMillis\"])\n            if gt_use.empty:\n                continue\n            gt_use[\"UnixTimeMillis\"] = gt_use[\"UnixTimeMillis\"].astype(\"int64\")\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No training rows assembled with ground truth.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    train_df = train_df.copy()\n    ecef_x = train_df[\"WlsPositionXEcefMeters\"].values\n    ecef_y = train_df[\"WlsPositionYEcefMeters\"].values\n    ecef_z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(ecef_x, ecef_y, ecef_z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X, y_lat)\n        rf_lon.fit(X, y_lon)\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n        print(\"Not enough groups for CV, trained single correction model.\", flush=True)\n        return lat_models, lon_models, feature_cols, np.nan, train_df_prepared\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols], y_lat)\n    rf_lon.fit(X[feature_cols], y_lon)\n    return rf_lat, rf_lon\n\n\n# --------- Test feature preparation ---------\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"utcTimeMillis\" not in gnss.columns:\n                    continue\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"utcTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"utcTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\")\n                    .mean()\n                    .reset_index()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    # keep rows even if UnixTimeMillis NaN; they'll be handled later\n    sample[\"row_id\"] = np.arange(len(sample))\n\n    # merge only on rows with valid UnixTimeMillis\n    valid_mask = sample[\"UnixTimeMillis\"].notna()\n    sample_valid = sample[valid_mask].copy()\n    sample_valid[\"UnixTimeMillis\"] = sample_valid[\"UnixTimeMillis\"].astype(\"int64\")\n\n    merged_valid = pd.merge(\n        sample_valid,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    # initialize merged with original sample, then fill where we have GNSS info\n    merged = sample.copy()\n    for col in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        merged[col] = np.nan\n    merged.loc[valid_mask, \"WlsPositionXEcefMeters\"] = merged_valid[\n        \"WlsPositionXEcefMeters\"\n    ].values\n    merged.loc[valid_mask, \"WlsPositionYEcefMeters\"] = merged_valid[\n        \"WlsPositionYEcefMeters\"\n    ].values\n    merged.loc[valid_mask, \"WlsPositionZEcefMeters\"] = merged_valid[\n        \"WlsPositionZEcefMeters\"\n    ].values\n\n    # compute base lat/lon where possible\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    mask_has_ecef = (\n        merged[\"WlsPositionXEcefMeters\"].notna()\n        & merged[\"WlsPositionYEcefMeters\"].notna()\n        & merged[\"WlsPositionZEcefMeters\"].notna()\n    )\n    if mask_has_ecef.any():\n        lat_deg, lon_deg = ecef_to_geodetic(\n            merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n            merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n            merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n        )\n        base_lat[mask_has_ecef.values] = lat_deg\n        base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    # Load sample submission first (needed to align phone names)\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load train\n    train_df = load_train_with_gt(INPUT_DIR, sample_sub)\n    if train_df.empty:\n        print(\n            \"No training data with GT found; will fall back to pure WLS baseline.\",\n            flush=True,\n        )\n        correction_model_lat = None\n        correction_model_lon = None\n        feature_cols = None\n        val_metric = np.nan\n    else:\n        # Train with CV\n        lat_models, lon_models, feature_cols, val_metric, train_df_prepared = (\n            train_with_cv(train_df)\n        )\n        print(\"CV competition metric:\", val_metric, flush=True)\n\n        # Fit final correction models on all train\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    # Print metric\n    print(f\"Validation metric (mean (P50+P95)/2 over phones): {val_metric}\", flush=True)\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Correction predictions if model exists\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols])\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols])\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Align to full sample_submission: ensure every row has a prediction\n    submission = merged.copy()\n\n    # Sort for interpolation\n    submission[\"UnixTimeMillis\"] = pd.to_numeric(\n        submission[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    submission.sort_values([\"phone\", \"UnixTimeMillis\"], inplace=True)\n\n    # Interpolate per phone over time\n    def interp_series(s):\n        return (\n            s.interpolate(method=\"time\")\n            if isinstance(s.index, pd.DatetimeIndex)\n            else s.ffill().bfill()\n        )\n\n    # Since UnixTimeMillis is numeric, we can just ffill/bfill\n    submission[\"LatitudeDegrees\"] = submission.groupby(\"phone\")[\"pred_lat\"].apply(\n        lambda s: s.ffill().bfill()\n    )\n    submission[\"LongitudeDegrees\"] = submission.groupby(\"phone\")[\"pred_lon\"].apply(\n        lambda s: s.ffill().bfill()\n    )\n\n    # If still NaNs, fill with per-phone mean then global mean\n    if submission[\"LatitudeDegrees\"].isna().any():\n        phone_means_lat = submission.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n            \"mean\"\n        )\n        phone_means_lon = submission.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n            \"mean\"\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n            phone_means_lat\n        )\n        submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n            phone_means_lon\n        )\n\n    overall_lat = submission[\"LatitudeDegrees\"].mean()\n    overall_lon = submission[\"LongitudeDegrees\"].mean()\n    submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(overall_lat)\n    submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = submission[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n    # Restore original row order to match sample_submission\n    final_submission = final_submission.reset_index(drop=True)\n    # Ensure same length and order as sample_submission\n    # Map by index since we kept the same number of rows\n    final_submission = final_submission.loc[: len(sample_sub) - 1]\n    final_submission = final_submission.reset_index(drop=True)\n\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ----- Coordinate transforms -----\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ----- Metric -----\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ----- Helpers -----\ndef make_phone_id(drive_id, phone_name):\n    # Match sample_submission pattern: \"driveid_phonename\"\n    return f\"{drive_id}_{phone_name}\"\n\n\n# ----- Data loaders -----\ndef load_train(train_root):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = make_phone_id(drive_id, phone_name)\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Failed reading {gnss_path} or {gt_path}: {e}\")\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n\n            if len(gnss) == 0 or len(gt) == 0:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            wls_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            for c in wls_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            use_cols = [\n                \"Cn0DbHz\",\n                \"PseudorangeRateUncertaintyMetersPerSecond\",\n                wls_cols[0],\n                wls_cols[1],\n                wls_cols[2],\n            ]\n            for c in use_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            # Aggregate GNSS per ms timestamp\n            agg = gnss.groupby(\"UnixTimeMillis\")[use_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n\n            agg[\"phone\"] = phone_id\n            agg[\"drive_id\"] = drive_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n            gt_small[\"drive_id\"] = drive_id\n\n            merged = pd.merge(\n                gt_small,\n                agg,\n                on=[\"phone\", \"drive_id\", \"UnixTimeMillis\"],\n                how=\"inner\",\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n            rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = make_phone_id(drive_id, phone_name)\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            try:\n                gnss = pd.read_csv(gnss_path)\n            except Exception as e:\n                print(f\"Failed reading {gnss_path}: {e}\")\n                continue\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            if len(gnss) == 0:\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Ensure feature columns exist in gnss\n            for c in feature_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n            agg[\"phone\"] = phone_id\n            agg[\"drive_id\"] = drive_id\n            rows.append(agg)\n\n    if not rows:\n        test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"] + feature_cols)\n    else:\n        test_gnss = pd.concat(rows, ignore_index=True)\n\n    # sample already has 'phone' and 'UnixTimeMillis'\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef parse_sample_submission_phone_col(sample_sub):\n    \"\"\"\n    In this benchmark, sample_submission's 'phone' appears to be 'driveid_phonename'.\n    We'll use it directly as phone_id without splitting, just ensure it's string.\n    \"\"\"\n    sample = sample_sub.copy()\n    sample[\"phone\"] = sample[\"phone\"].astype(str)\n    return sample\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Load train data\n    train_df = load_train(train_root)\n\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n\n    # Fill NaNs with column medians\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    # groups by collection (drive)\n    groups = train_df[\"drive_id\"].values\n\n    # ----- 5-fold GroupKFold CV -----\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"5-fold CV competition metric (mean of P50/P95 errors):\", metric_val)\n\n    # ----- Train final models on full data -----\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # ----- Prepare test and submission -----\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # Ensure 'phone' and 'UnixTimeMillis' exist\n    if \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n        raise RuntimeError(\n            \"sample_submission.csv must contain 'phone' and 'UnixTimeMillis' columns.\"\n        )\n\n    # Normalize phone ids (treat as our phone_id directly)\n    sample_sub = parse_sample_submission_phone_col(sample_sub)\n\n    # Preserve original order with row_id\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    # Merge GNSS features to sample_sub rows\n    test_merged = load_test(\n        test_root, sample_sub[[\"phone\", \"UnixTimeMillis\", \"row_id\"]], feature_cols\n    )\n\n    # Ensure row_id is preserved after merge\n    if \"row_id\" not in test_merged.columns:\n        # If merge dropped row_id (shouldn't with how=\"left\"), re-merge to restore\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    # Ensure feature columns present\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n\n    # Fill with train medians\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    X_test = test_merged[feature_cols].values\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align back to full sample_sub order\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill any missing predictions (if any phones/times not in GNSS) by ffill/bfill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still any NaNs (e.g., all rows were NaN), fall back to training-set mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"].values,\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"].values,\n        }\n    )\n\n    # Save to required path ./submission/submission.csv\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    # Also mirror to ./working/submission.csv as required by benchmark\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ----- Coordinate transforms -----\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ----- Metric -----\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ----- Helpers -----\ndef make_phone_id(drive_id, phone_name):\n    # For this benchmark, sample_submission already uses \"driveid_phonename\"\n    return f\"{drive_id}_{phone_name}\"\n\n\ndef get_drive_id_from_phone_id(phone_id):\n    # phone_id is \"driveid_phonename\"; driveid may contain '-'\n    parts = phone_id.split(\"_\")\n    if len(parts) < 2:\n        return phone_id\n    # last part is phone model, rest is drive_id\n    return \"_\".join(parts[:-1])\n\n\n# ----- Data loaders -----\ndef load_train(train_root, feature_cols):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = make_phone_id(drive_id, phone_name)\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Failed reading {gnss_path} or {gt_path}: {e}\")\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n\n            if len(gnss) == 0 or len(gt) == 0:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Ensure feature columns exist\n            for c in feature_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            # Aggregate GNSS per timestamp\n            agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n\n            agg[\"phone\"] = phone_id\n            agg[\"drive_id\"] = drive_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n            gt_small[\"drive_id\"] = drive_id\n\n            merged = pd.merge(\n                gt_small,\n                agg,\n                on=[\"phone\", \"drive_id\", \"UnixTimeMillis\"],\n                how=\"inner\",\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n            rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols):\n    \"\"\"\n    Aggregate GNSS per (phone, UnixTimeMillis) for all test drives, then left-join to sample_sub.\n    \"\"\"\n    rows = []\n    # Map from phone_id in sample to its drive_id for limiting search (optional optimization)\n    phone_drive_map = {\n        p: get_drive_id_from_phone_id(p) for p in sample_sub[\"phone\"].unique()\n    }\n\n    # Build a mapping from drive_id to list of phone_ids expected in sample (for faster filtering)\n    drive_to_phones = {}\n    for p, d in phone_drive_map.items():\n        drive_to_phones.setdefault(d, set()).add(p)\n\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        expected_phones = drive_to_phones.get(drive_id, None)\n        # If no sample entries expect this drive, we can still process (just in case),\n        # but we can also skip to speed up. We'll process all to be safe.\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = make_phone_id(drive_id, phone_name)\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            try:\n                gnss = pd.read_csv(gnss_path)\n            except Exception as e:\n                print(f\"Failed reading {gnss_path}: {e}\")\n                continue\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            if len(gnss) == 0:\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Ensure feature columns exist in gnss\n            for c in feature_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n            agg[\"phone\"] = phone_id\n            rows.append(agg)\n\n    if not rows:\n        test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"] + feature_cols)\n    else:\n        test_gnss = pd.concat(rows, ignore_index=True)\n\n    # Prepare sample: ensure numeric UnixTimeMillis and consistent dtypes\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n\n    # Left-join GNSS features to sample_sub\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Feature columns (simple baseline)\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    # ---- Load train data ----\n    train_df = load_train(train_root, feature_cols)\n\n    # Ensure all feature columns present and fill NaNs with medians\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    # groups by collection (drive)\n    groups = train_df[\"drive_id\"].values\n\n    # ----- 5-fold GroupKFold CV -----\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"5-fold CV competition metric (mean of P50/P95 errors):\", metric_val)\n\n    # ----- Train final models on full data -----\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # ----- Prepare test and submission -----\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # Ensure 'phone' and 'UnixTimeMillis' exist\n    if \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n        raise RuntimeError(\n            \"sample_submission.csv must contain 'phone' and 'UnixTimeMillis' columns.\"\n        )\n\n    # Normalize phone ids (use directly, ensure string)\n    sample_sub = sample_sub.copy()\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n\n    # Preserve original order with row_id\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    # Merge GNSS features to sample_sub rows\n    test_merged = load_test(\n        test_root, sample_sub[[\"phone\", \"UnixTimeMillis\", \"row_id\"]], feature_cols\n    )\n\n    # Ensure row_id is preserved after merge (it should be, but double-check)\n    if \"row_id\" not in test_merged.columns:\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    # Ensure feature columns present and fill with train medians\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    # Sort by row_id to keep order consistent\n    test_merged = test_merged.sort_values(\"row_id\").reset_index(drop=True)\n\n    X_test = test_merged[feature_cols].values\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align back to full sample_sub order\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill any missing predictions by ffill/bfill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still any NaNs (e.g., all rows were NaN), fall back to training-set mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"].values,\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"].values,\n        }\n    )\n\n    # Save to required path ./submission/submission.csv\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    # Also mirror to ./working/submission.csv as required by benchmark\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\n# Directories as required by benchmark\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_collection_phone_from_path(path, split_keyword):\n    # path like ./input/train/2020-06-04-US-MTV-1/GooglePixel4/device_gnss.csv\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    if split_keyword not in parts:\n        return None, None\n    idx = parts.index(split_keyword)\n    if len(parts) <= idx + 2:\n        return None, None\n    collection = parts[idx + 1]\n    phone_model = parts[idx + 2]\n    phone = f\"{collection}_{phone_model}\"\n    return collection, phone\n\n\ndef load_gnss_agg(split_dir):\n    \"\"\"\n    Aggregate device_gnss features per (phone, UnixTimeMillis) using mean.\n    \"\"\"\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(fpath, split_dir)\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception as e:\n            print(f\"Failed to read {fpath}: {e}\")\n            continue\n\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            print(f\"No time column in {fpath}, skipping\")\n            continue\n\n        # enforce int64 time\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n        df[\"phone\"] = phone\n\n        # select numeric columns excluding obvious non-features\n        skip_cols = {\n            time_col,\n            \"UnixTimeMillis\",\n            \"phone\",\n            \"MessageType\",\n            \"SignalType\",\n            \"CodeType\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n        }\n        feature_cols = []\n        for c in df.columns:\n            if c in skip_cols:\n                continue\n            if pd.api.types.is_numeric_dtype(df[c]):\n                feature_cols.append(c)\n\n        if not feature_cols:\n            continue\n\n        group_cols = [\"phone\", \"UnixTimeMillis\"]\n        agg_dict = {c: \"mean\" for c in feature_cols}\n        use_cols = group_cols + feature_cols\n        df_small = df[use_cols].copy()\n        g = df_small.groupby(group_cols, as_index=False).agg(agg_dict)\n        all_rows.append(g)\n\n    if not all_rows:\n        print(f\"No GNSS agg rows for {split_dir}\")\n        return pd.DataFrame()\n    feats = pd.concat(all_rows, ignore_index=True)\n    feats[\"phone\"] = feats[\"phone\"].astype(str)\n    feats[\"UnixTimeMillis\"] = feats[\"UnixTimeMillis\"].astype(\"int64\")\n    print(f\"{split_dir} GNSS agg shape: {feats.shape}\")\n    return feats\n\n\ndef load_train_targets():\n    base_path = os.path.join(INPUT_DIR, \"train\")\n    pattern = os.path.join(base_path, \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(fpath, \"train\")\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception as e:\n            print(f\"Failed to read targets {fpath}: {e}\")\n            continue\n        if \"UnixTimeMillis\" not in df.columns:\n            print(f\"No UnixTimeMillis in target {fpath}, skipping\")\n            continue\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[\"UnixTimeMillis\"].astype(\"int64\")\n        keep_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        df = df[keep_cols]\n        all_rows.append(df)\n    if not all_rows:\n        print(\"No training targets found!\")\n        return pd.DataFrame()\n    targets = pd.concat(all_rows, ignore_index=True)\n    targets[\"phone\"] = targets[\"phone\"].astype(str)\n    targets[\"UnixTimeMillis\"] = targets[\"UnixTimeMillis\"].astype(\"int64\")\n    print(\"Train targets shape:\", targets.shape)\n    return targets\n\n\ndef build_train_test(sample_sub):\n    train_feats = load_gnss_agg(\"train\")\n    train_targets = load_train_targets()\n\n    if train_feats.empty or train_targets.empty:\n        raise RuntimeError(\n            f\"Training features or targets are empty. feats_empty={train_feats.empty}, targets_empty={train_targets.empty}\"\n        )\n\n    train_df = pd.merge(\n        train_targets,\n        train_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n\n    # Drop rows with no feature values\n    numeric_cols = [\n        c\n        for c in train_df.columns\n        if pd.api.types.is_numeric_dtype(train_df[c])\n        and c not in [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ]\n    if numeric_cols:\n        mask_has_feat = train_df[numeric_cols].notnull().sum(axis=1) > 0\n        before = len(train_df)\n        train_df = train_df[mask_has_feat].reset_index(drop=True)\n        print(f\"Train rows before feature filter: {before}, after: {len(train_df)}\")\n\n    test_feats = load_gnss_agg(\"test\")\n    if test_feats.empty:\n        print(\"Warning: test features are empty; predictions will be constant.\")\n\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    test_keys[\"phone\"] = test_keys[\"phone\"].astype(str)\n    test_keys[\"UnixTimeMillis\"] = test_keys[\"UnixTimeMillis\"].astype(\"int64\")\n\n    test_merged = pd.merge(\n        test_keys,\n        test_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    print(\"Test merged shape:\", test_merged.shape)\n    return train_df, test_merged\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n\n    for k in key_cols:\n        if k not in train_df.columns:\n            raise RuntimeError(f\"Missing key {k} in training data\")\n\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n    if not feature_cols:\n        raise RuntimeError(\"No numeric feature columns found for modeling.\")\n\n    print(\"Number of feature columns:\", len(feature_cols))\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Prepare test aligned with sample_sub\n    test_full = sample_sub.copy()\n    test_full[\"phone\"] = test_full[\"phone\"].astype(str)\n    test_full[\"UnixTimeMillis\"] = test_full[\"UnixTimeMillis\"].astype(\"int64\")\n\n    # ensure test_feats has all needed feature columns\n    for c in feature_cols:\n        if c not in test_feats.columns:\n            test_feats[c] = np.nan\n\n    cols_for_merge = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n    merged = pd.merge(\n        test_full,\n        test_feats[cols_for_merge],\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    test_full = merged\n\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n\n    X_test = test_full[feature_cols].copy()\n\n    # Fill missing with training medians\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    n_splits = 5\n    groups = phones.astype(str).values\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=groups):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=200,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=200,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones.iloc[va_idx].values,\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    print(\"Building train and test data...\")\n    train_df, test_feats = build_train_test(sample_sub)\n\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    print(\"Running 5-fold GroupKFold CV...\")\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].copy())\n    print(f\"Hold-out CV metric: {cv_score:.6f}\")\n\n    print(\"Training full models...\")\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    print(\"Predicting on test set...\")\n    pred_lat_test = model_lat.predict(X_test)\n    pred_lon_test = model_lon.predict(X_test)\n\n    submission = sample_sub.copy()\n    submission[\"LatitudeDegrees\"] = pred_lat_test\n    submission[\"LongitudeDegrees\"] = pred_lon_test\n\n    # Fallback for NaNs\n    for col in [\"LatitudeDegrees\", \"LongitudeDegrees\"]:\n        if submission[col].isna().any():\n            submission[col] = submission.groupby(\"phone\")[col].transform(\n                lambda s: s.fillna(s.mean())\n            )\n            if submission[col].isna().any():\n                submission[col] = submission[col].fillna(submission[col].mean())\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols]\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\n# ---------------- Geodesy helpers ----------------\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 constants\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = 6356752.314245\n    ep2 = (a**2 - b**2) / b**2\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\n# ---------------- Train loading ----------------\ndef load_train_with_gt(input_dir, sample_sub):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"No train directory found; cannot compute validation metric.\", flush=True)\n        return pd.DataFrame()\n\n    sample_phones = set(sample_sub[\"phone\"].unique())\n    all_rows = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n            if phone_id not in sample_phones:\n                continue\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} read error: {e}\", flush=True)\n                continue\n\n            if (\n                \"UnixTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                # some logs only have utcTimeMillis; in that rare case we fall back to previous approach\n                if \"utcTimeMillis\" not in gnss.columns:\n                    continue\n                gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No training rows assembled with ground truth.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    train_df = train_df.copy()\n\n    ecef_x = train_df[\"WlsPositionXEcefMeters\"].values\n    ecef_y = train_df[\"WlsPositionYEcefMeters\"].values\n    ecef_z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(ecef_x, ecef_y, ecef_z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X, y_lat)\n        rf_lon.fit(X, y_lon)\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n        print(\"Not enough groups for CV, trained single correction model.\", flush=True)\n        return lat_models, lon_models, feature_cols, np.nan, train_df_prepared\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols], y_lat)\n    rf_lon.fit(X[feature_cols], y_lon)\n    return rf_lat, rf_lon\n\n\n# ---------------- Test feature preparation ----------------\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"UnixTimeMillis\" not in gnss.columns:\n                    if \"utcTimeMillis\" in gnss.columns:\n                        gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                    else:\n                        continue\n\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\", as_index=False)\n                    .mean()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    sample[\"row_id\"] = np.arange(len(sample))\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    # compute base lat/lon where we have ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    mask_has_ecef = (\n        merged[\"WlsPositionXEcefMeters\"].notna()\n        & merged[\"WlsPositionYEcefMeters\"].notna()\n        & merged[\"WlsPositionZEcefMeters\"].notna()\n    )\n    if mask_has_ecef.any():\n        lat_deg, lon_deg = ecef_to_geodetic(\n            merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n            merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n            merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n        )\n        base_lat[mask_has_ecef.values] = lat_deg\n        base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load and train\n    train_df = load_train_with_gt(INPUT_DIR, sample_sub)\n    if train_df.empty:\n        print(\n            \"No training data with GT found; will fall back to pure WLS baseline.\",\n            flush=True,\n        )\n        correction_model_lat = None\n        correction_model_lon = None\n        feature_cols = None\n        val_metric = np.nan\n    else:\n        lat_models, lon_models, feature_cols, val_metric, train_df_prepared = (\n            train_with_cv(train_df)\n        )\n        print(\"CV competition metric:\", val_metric, flush=True)\n\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    print(f\"Validation metric (mean (P50+P95)/2 over phones): {val_metric}\", flush=True)\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # If base_lat/lon missing, try simple per-phone forward/backward fill on ECEF and recompute\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        merged[coord] = merged.groupby(\"phone\")[coord].apply(\n            lambda s: s.ffill().bfill()\n        )\n\n    # recompute base lat/lon after filling ECEF where possible\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    mask_has_ecef = (\n        merged[\"WlsPositionXEcefMeters\"].notna()\n        & merged[\"WlsPositionYEcefMeters\"].notna()\n        & merged[\"WlsPositionZEcefMeters\"].notna()\n    )\n    if mask_has_ecef.any():\n        lat_deg, lon_deg = ecef_to_geodetic(\n            merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n            merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n            merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n        )\n        base_lat[mask_has_ecef.values] = lat_deg\n        base_lon[mask_has_ecef.values] = lon_deg\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].apply(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].apply(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Correction predictions if model exists\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols])\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols])\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned exactly to sample_sub\n    # sample_sub has original order; we join predictions by (phone, UnixTimeMillis)\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    # If some rows lack predictions (should be rare), fill per phone then global\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].apply(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].apply(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\n# Directories required by benchmark\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_collection_phone_from_path(path, split_keyword):\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    if split_keyword not in parts:\n        return None, None\n    idx = parts.index(split_keyword)\n    if len(parts) <= idx + 2:\n        return None, None\n    collection = parts[idx + 1]\n    phone_model = parts[idx + 2]\n    phone = f\"{collection}_{phone_model}\"\n    return collection, phone\n\n\ndef load_gnss_agg(split_dir):\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(fpath, split_dir)\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception as e:\n            print(f\"Failed to read {fpath}: {e}\")\n            continue\n\n        # Time column detection\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            print(f\"No time column in {fpath}, skipping\")\n            continue\n\n        # ensure int64 time\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n        df[\"phone\"] = phone\n\n        # numeric feature selection\n        skip_cols = {\n            time_col,\n            \"UnixTimeMillis\",\n            \"phone\",\n            \"MessageType\",\n            \"SignalType\",\n            \"CodeType\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n        }\n        feature_cols = []\n        for c in df.columns:\n            if c in skip_cols:\n                continue\n            if pd.api.types.is_numeric_dtype(df[c]):\n                feature_cols.append(c)\n\n        if not feature_cols:\n            continue\n\n        group_cols = [\"phone\", \"UnixTimeMillis\"]\n        agg_dict = {c: \"mean\" for c in feature_cols}\n        use_cols = group_cols + feature_cols\n        df_small = df[use_cols].copy()\n        g = df_small.groupby(group_cols, as_index=False).agg(agg_dict)\n        all_rows.append(g)\n\n    if not all_rows:\n        print(f\"No GNSS agg rows for {split_dir}\")\n        return pd.DataFrame()\n\n    feats = pd.concat(all_rows, ignore_index=True)\n    feats[\"phone\"] = feats[\"phone\"].astype(str)\n    feats[\"UnixTimeMillis\"] = feats[\"UnixTimeMillis\"].astype(\"int64\")\n    print(f\"{split_dir} GNSS agg shape: {feats.shape}\")\n    return feats\n\n\ndef load_train_targets():\n    base_path = os.path.join(INPUT_DIR, \"train\")\n    pattern = os.path.join(base_path, \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(fpath, \"train\")\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception as e:\n            print(f\"Failed to read targets {fpath}: {e}\")\n            continue\n        if \"UnixTimeMillis\" not in df.columns:\n            print(f\"No UnixTimeMillis in target {fpath}, skipping\")\n            continue\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[\"UnixTimeMillis\"].astype(\"int64\")\n        keep_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        df = df[keep_cols]\n        all_rows.append(df)\n    if not all_rows:\n        print(\"No training targets found!\")\n        return pd.DataFrame()\n    targets = pd.concat(all_rows, ignore_index=True)\n    targets[\"phone\"] = targets[\"phone\"].astype(str)\n    targets[\"UnixTimeMillis\"] = targets[\"UnixTimeMillis\"].astype(\"int64\")\n    print(\"Train targets shape:\", targets.shape)\n    return targets\n\n\ndef build_train_test(sample_sub):\n    train_feats = load_gnss_agg(\"train\")\n    train_targets = load_train_targets()\n\n    if train_targets.empty:\n        raise RuntimeError(\"Training targets are empty; cannot train.\")\n\n    if train_feats.empty:\n        print(\n            \"Warning: training features empty; using only time-independent features (none).\"\n        )\n\n    # Merge targets with features (may be empty)\n    train_df = pd.merge(\n        train_targets,\n        train_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n\n    # Drop rows with no feature values only if we actually have feature columns\n    numeric_feature_cols = [\n        c\n        for c in train_df.columns\n        if pd.api.types.is_numeric_dtype(train_df[c])\n        and c not in [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ]\n    if numeric_feature_cols:\n        mask_has_feat = train_df[numeric_feature_cols].notnull().sum(axis=1) > 0\n        before = len(train_df)\n        train_df = train_df[mask_has_feat].reset_index(drop=True)\n        print(f\"Train rows before feature filter: {before}, after: {len(train_df)}\")\n    else:\n        print(\n            \"No numeric feature columns beyond targets & time; will fall back to simple baseline later.\"\n        )\n\n    # Build test features\n    test_feats = load_gnss_agg(\"test\")\n    if test_feats.empty:\n        print(\"Warning: test features are empty; predictions will use baseline only.\")\n\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    test_keys[\"phone\"] = test_keys[\"phone\"].astype(str)\n    test_keys[\"UnixTimeMillis\"] = test_keys[\"UnixTimeMillis\"].astype(\"int64\")\n\n    test_merged = pd.merge(\n        test_keys,\n        test_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    print(\"Test merged shape:\", test_merged.shape)\n    return train_df, test_merged\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n\n    for k in key_cols:\n        if k not in train_df.columns:\n            raise RuntimeError(f\"Missing key {k} in training data\")\n\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n\n    # It is possible that there are no GNSS numeric features (extreme fallback)\n    if not feature_cols:\n        print(\n            \"No numeric feature columns for modeling; will use constant baseline and still output submission.\"\n        )\n        return None, None, None, None, [], None\n\n    print(\"Number of feature columns:\", len(feature_cols))\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Align test to sample_sub order\n    test_full = sample_sub.copy()\n    test_full[\"phone\"] = test_full[\"phone\"].astype(str)\n    test_full[\"UnixTimeMillis\"] = test_full[\"UnixTimeMillis\"].astype(\"int64\")\n\n    # ensure test_feats has all needed feature columns\n    for c in feature_cols:\n        if c not in test_feats.columns:\n            test_feats[c] = np.nan\n\n    cols_for_merge = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n    merged = pd.merge(\n        test_full,\n        test_feats[cols_for_merge],\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    test_full = merged\n\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n\n    X_test = test_full[feature_cols].copy()\n\n    # Fill missing with training medians\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    if X is None:\n        print(\"No features for CV; skipping cross-validation.\")\n        return np.nan\n\n    n_splits = 5\n    groups = phones.astype(str).values\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=groups):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=200,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=200,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones.iloc[va_idx].values,\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    if X is None:\n        return None, None\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    print(\"Building train and test data...\")\n    train_df, test_feats = build_train_test(sample_sub)\n\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    # Compute CV metric\n    print(\"Running 5-fold GroupKFold CV (if possible)...\")\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].copy())\n    print(f\"Hold-out CV metric: {cv_score}\")\n\n    # Train full models (if features exist)\n    print(\"Training full models (or falling back to baseline)...\")\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Prepare baseline predictions from training ground truth\n    # Use per-phone mean lat/lon where possible, else global mean\n    train_baseline = (\n        train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        .mean()\n        .rename(columns={\"LatitudeDegrees\": \"mean_lat\", \"LongitudeDegrees\": \"mean_lon\"})\n    )\n    global_mean_lat = train_df[\"LatitudeDegrees\"].mean()\n    global_mean_lon = train_df[\"LongitudeDegrees\"].mean()\n\n    print(\"Predicting on test set and building submission...\")\n    submission = sample_sub.copy()\n\n    if model_lat is not None and model_lon is not None and X_test is not None:\n        # LightGBM-based predictions\n        pred_lat_test = model_lat.predict(X_test)\n        pred_lon_test = model_lon.predict(X_test)\n        submission[\"LatitudeDegrees\"] = pred_lat_test\n        submission[\"LongitudeDegrees\"] = pred_lon_test\n    else:\n        # No model trained; use baseline directly\n        print(\"No trained model available, using baseline predictions only.\")\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        submission.drop(columns=[\"mean_lat\", \"mean_lon\"], inplace=True)\n\n    # Fallback / imputation for NaNs: per-phone mean, then global mean\n    if (\n        \"LatitudeDegrees\" not in submission.columns\n        or \"LongitudeDegrees\" not in submission.columns\n    ):\n        # If something unexpected happened, rebuild via baseline\n        print(\"Prediction columns missing, rebuilding via baseline.\")\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        submission.drop(columns=[\"mean_lat\", \"mean_lon\"], inplace=True)\n\n    for col, mean_col in [\n        (\"LatitudeDegrees\", \"mean_lat\"),\n        (\"LongitudeDegrees\", \"mean_lon\"),\n    ]:\n        if submission[col].isna().any():\n            # Try per-phone mean from training\n            submission = submission.merge(\n                train_baseline,\n                how=\"left\",\n                on=\"phone\",\n                suffixes=(\"\", \"_base\"),\n            )\n            if col == \"LatitudeDegrees\":\n                submission[col] = submission[col].fillna(submission[\"mean_lat\"])\n            else:\n                submission[col] = submission[col].fillna(submission[\"mean_lon\"])\n            submission[col] = submission[col].fillna(\n                global_mean_lat if col == \"LatitudeDegrees\" else global_mean_lon\n            )\n            # Clean extra columns\n            for c in [\"mean_lat\", \"mean_lon\"]:\n                if c in submission.columns:\n                    submission.drop(columns=[c], inplace=True)\n            break  # only need to do once since we cleaned\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols]\n\n    # Save submissions\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\n# ---------------- Geodesy helpers ----------------\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 constants\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = 6356752.314245\n    ep2 = (a**2 - b**2) / b**2\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\n# ---------------- Train loading ----------------\ndef load_train_with_gt(input_dir, sample_sub):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"No train directory found; cannot compute validation metric.\", flush=True)\n        return pd.DataFrame()\n\n    sample_phones = set(sample_sub[\"phone\"].unique())\n    all_rows = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n            if phone_id not in sample_phones:\n                continue\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} read error: {e}\", flush=True)\n                continue\n\n            if (\n                \"UnixTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                if \"utcTimeMillis\" in gnss.columns:\n                    gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No training rows assembled with ground truth.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    train_df = train_df.copy()\n\n    ecef_x = train_df[\"WlsPositionXEcefMeters\"].values\n    ecef_y = train_df[\"WlsPositionYEcefMeters\"].values\n    ecef_z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(ecef_x, ecef_y, ecef_z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    # Basic numeric features: time and ECEF\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X.values, y_lat)\n        rf_lon.fit(X.values, y_lon)\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n        print(\"Not enough groups for CV, trained single correction model.\", flush=True)\n        return lat_models, lon_models, feature_cols, np.nan, train_df_prepared\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols].values, y_lat)\n    rf_lon.fit(X[feature_cols].values, y_lon)\n    return rf_lat, rf_lon\n\n\n# ---------------- Test feature preparation ----------------\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"UnixTimeMillis\" not in gnss.columns:\n                    if \"utcTimeMillis\" in gnss.columns:\n                        gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                    else:\n                        continue\n\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\", as_index=False)\n                    .mean()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    # compute base lat/lon where we have ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    mask_has_ecef = (\n        merged[\"WlsPositionXEcefMeters\"].notna()\n        & merged[\"WlsPositionYEcefMeters\"].notna()\n        & merged[\"WlsPositionZEcefMeters\"].notna()\n    )\n    if mask_has_ecef.any():\n        lat_deg, lon_deg = ecef_to_geodetic(\n            merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n            merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n            merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n        )\n        base_lat[mask_has_ecef.values] = lat_deg\n        base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load and train\n    train_df = load_train_with_gt(INPUT_DIR, sample_sub)\n    if train_df.empty:\n        print(\n            \"No training data with GT found; will fall back to pure WLS baseline.\",\n            flush=True,\n        )\n        correction_model_lat = None\n        correction_model_lon = None\n        feature_cols = None\n        val_metric = np.nan\n    else:\n        lat_models, lon_models, feature_cols, val_metric, train_df_prepared = (\n            train_with_cv(train_df)\n        )\n        print(\"CV competition metric:\", val_metric, flush=True)\n\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    print(f\"Validation metric (mean (P50+P95)/2 over phones): {val_metric}\", flush=True)\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Robust per-phone forward/backward fill on ECEF and recompute\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        if coord in merged.columns:\n            merged[coord] = merged.groupby(\"phone\")[coord].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    # recompute base lat/lon after filling ECEF where possible\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    mask_has_ecef = (\n        merged[\"WlsPositionXEcefMeters\"].notna()\n        & merged[\"WlsPositionYEcefMeters\"].notna()\n        & merged[\"WlsPositionZEcefMeters\"].notna()\n    )\n    if mask_has_ecef.any():\n        lat_deg, lon_deg = ecef_to_geodetic(\n            merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n            merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n            merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n        )\n        base_lat[mask_has_ecef.values] = lat_deg\n        base_lon[mask_has_ecef.values] = lon_deg\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Correction predictions if model exists\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols].values)\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols].values)\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned exactly to sample_sub\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    # If some rows lack predictions, fill per phone then global\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ----- Coordinate transforms -----\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ----- Metric -----\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ----- Data loaders -----\ndef load_train(train_root, feature_cols):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = phone_name  # use raw phone folder name as phone id\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Failed reading {gnss_path} or {gt_path}: {e}\")\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n\n            if len(gnss) == 0 or len(gt) == 0:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Ensure feature columns exist\n            for c in feature_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            # Aggregate GNSS per timestamp\n            agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n\n            agg[\"phone\"] = phone_id\n            agg[\"drive_id\"] = drive_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n            gt_small[\"drive_id\"] = drive_id\n\n            merged = pd.merge(\n                gt_small,\n                agg,\n                on=[\"phone\", \"drive_id\", \"UnixTimeMillis\"],\n                how=\"inner\",\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n            rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols):\n    \"\"\"\n    Aggregate GNSS per (phone, UnixTimeMillis) for all test drives, then left-join to sample_sub.\n    We assume sample_sub['phone'] matches the phone folder name (e.g., 'GooglePixel4').\n    \"\"\"\n    rows = []\n\n    # Collect all GNSS data in test into (drive_id, phone_name)\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = phone_name  # keep consistent with sample_sub\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            try:\n                gnss = pd.read_csv(gnss_path)\n            except Exception as e:\n                print(f\"Failed reading {gnss_path}: {e}\")\n                continue\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            if len(gnss) == 0:\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Ensure feature columns exist in gnss\n            for c in feature_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n            agg[\"phone\"] = phone_id\n            rows.append(agg)\n\n    if not rows:\n        test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"] + feature_cols)\n    else:\n        test_gnss = pd.concat(rows, ignore_index=True)\n\n    # Prepare sample: ensure numeric UnixTimeMillis and consistent dtypes\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n\n    # Left-join GNSS features to sample_sub\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Feature columns (simple baseline)\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    # ---- Load train data ----\n    train_df = load_train(train_root, feature_cols)\n\n    # Ensure all feature columns present and fill NaNs with medians\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    # groups by collection (drive)\n    groups = train_df[\"drive_id\"].values\n\n    # ----- 5-fold GroupKFold CV -----\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    fold_scores = []\n    for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n        lat_val_pred, lon_val_pred, _ = ecef_to_llh(\n            oof[val_idx, 0], oof[val_idx, 1], oof[val_idx, 2]\n        )\n        eval_df_fold = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"UnixTimeMillis\": train_df.iloc[val_idx][\"UnixTimeMillis\"].values,\n                \"lat_pred\": lat_val_pred,\n                \"lon_pred\": lon_val_pred,\n                \"lat_gt\": train_df.iloc[val_idx][\"LatitudeDegrees\"].values,\n                \"lon_gt\": train_df.iloc[val_idx][\"LongitudeDegrees\"].values,\n            }\n        )\n        fold_metric = competition_metric(eval_df_fold)\n        fold_scores.append(fold_metric)\n        print(f\"Fold {fold_idx} competition metric: {fold_metric:.4f}\")\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"OOF competition metric (mean of P50/P95 errors):\", metric_val)\n    if fold_scores:\n        print(\n            \"Mean CV metric across folds:\",\n            float(np.mean([s for s in fold_scores if not np.isnan(s)])),\n        )\n\n    # ----- Train final models on full data -----\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # ----- Prepare test and submission -----\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # Ensure 'phone' and 'UnixTimeMillis' exist\n    if \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n        raise RuntimeError(\n            \"sample_submission.csv must contain 'phone' and 'UnixTimeMillis' columns.\"\n        )\n\n    # Normalize phone ids (use directly, ensure string)\n    sample_sub = sample_sub.copy()\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n\n    # Preserve original order with row_id\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    # Merge GNSS features to sample_sub rows\n    test_merged = load_test(\n        test_root, sample_sub[[\"phone\", \"UnixTimeMillis\", \"row_id\"]], feature_cols\n    )\n\n    # Ensure row_id is preserved after merge (it should be, but double-check)\n    if \"row_id\" not in test_merged.columns:\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    # Ensure feature columns present and fill with train medians\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    # Sort by row_id to keep order consistent\n    test_merged = test_merged.sort_values(\"row_id\").reset_index(drop=True)\n\n    X_test = test_merged[feature_cols].values\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align back to full sample_sub order\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill any missing predictions by ffill/bfill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still any NaNs (e.g., all rows were NaN), fall back to training-set mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"].values,\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"].values,\n        }\n    )\n\n    # Save to required path ./submission/submission.csv\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    # Also mirror to ./working/submission.csv as required by benchmark\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\n# Directories required by benchmark\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_collection_phone_from_path(path, split_keyword):\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    if split_keyword not in parts:\n        return None, None\n    idx = parts.index(split_keyword)\n    if len(parts) <= idx + 2:\n        return None, None\n    collection = parts[idx + 1]\n    phone_model = parts[idx + 2]\n    phone = f\"{collection}_{phone_model}\"\n    return collection, phone\n\n\ndef load_gnss_agg(split_dir):\n    \"\"\"\n    Aggregate GNSS device data per (phone, UnixTimeMillis).\n    Use simple numeric means, with an emphasis on WLS ECEF positions if present.\n    \"\"\"\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(fpath, split_dir)\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception as e:\n            print(f\"Failed to read {fpath}: {e}\")\n            continue\n\n        # Detect time column\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            print(f\"No time column in {fpath}, skipping\")\n            continue\n\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n        df[\"phone\"] = phone\n\n        # Select numeric feature columns, but explicitly keep WlsPosition* if available\n        skip_cols = {\n            time_col,\n            \"UnixTimeMillis\",\n            \"phone\",\n            \"MessageType\",\n            \"SignalType\",\n            \"CodeType\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n        }\n        feature_cols = []\n        for c in df.columns:\n            if c in skip_cols:\n                continue\n            if pd.api.types.is_numeric_dtype(df[c]):\n                feature_cols.append(c)\n\n        if not feature_cols:\n            continue\n\n        group_cols = [\"phone\", \"UnixTimeMillis\"]\n        agg_dict = {c: \"mean\" for c in feature_cols}\n        use_cols = group_cols + feature_cols\n        df_small = df[use_cols].copy()\n        g = df_small.groupby(group_cols, as_index=False).agg(agg_dict)\n        all_rows.append(g)\n\n    if not all_rows:\n        print(f\"No GNSS agg rows for {split_dir}\")\n        return pd.DataFrame()\n\n    feats = pd.concat(all_rows, ignore_index=True)\n    feats[\"phone\"] = feats[\"phone\"].astype(str)\n    feats[\"UnixTimeMillis\"] = feats[\"UnixTimeMillis\"].astype(\"int64\")\n    print(f\"{split_dir} GNSS agg shape: {feats.shape}\")\n    return feats\n\n\ndef load_train_targets():\n    base_path = os.path.join(INPUT_DIR, \"train\")\n    pattern = os.path.join(base_path, \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(fpath, \"train\")\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception as e:\n            print(f\"Failed to read targets {fpath}: {e}\")\n            continue\n        if \"UnixTimeMillis\" not in df.columns:\n            print(f\"No UnixTimeMillis in target {fpath}, skipping\")\n            continue\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[\"UnixTimeMillis\"].astype(\"int64\")\n        keep_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        df = df[keep_cols]\n        all_rows.append(df)\n    if not all_rows:\n        print(\"No training targets found!\")\n        return pd.DataFrame()\n    targets = pd.concat(all_rows, ignore_index=True)\n    targets[\"phone\"] = targets[\"phone\"].astype(str)\n    targets[\"UnixTimeMillis\"] = targets[\"UnixTimeMillis\"].astype(\"int64\")\n    print(\"Train targets shape:\", targets.shape)\n    return targets\n\n\ndef build_train_test(sample_sub):\n    train_feats = load_gnss_agg(\"train\")\n    train_targets = load_train_targets()\n\n    if train_targets.empty:\n        raise RuntimeError(\"Training targets are empty; cannot train.\")\n\n    # Merge targets with features; keep all targets even if features are missing\n    train_df = pd.merge(\n        train_targets,\n        train_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n\n    print(\"Merged train shape (targets + feats):\", train_df.shape)\n\n    # Build test features\n    test_feats = load_gnss_agg(\"test\")\n    if test_feats.empty:\n        print(\"Warning: test features are empty; predictions will use baseline only.\")\n\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    test_keys[\"phone\"] = test_keys[\"phone\"].astype(str)\n    test_keys[\"UnixTimeMillis\"] = test_keys[\"UnixTimeMillis\"].astype(\"int64\")\n\n    test_merged = pd.merge(\n        test_keys,\n        test_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    print(\"Test merged shape:\", test_merged.shape)\n    return train_df, test_merged\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n\n    for k in key_cols:\n        if k not in train_df.columns:\n            raise RuntimeError(f\"Missing key {k} in training data\")\n\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n\n    if not feature_cols:\n        print(\n            \"No numeric feature columns for modeling; will use constant baseline and still output submission.\"\n        )\n        return None, None, None, None, [], None\n\n    print(\"Number of feature columns:\", len(feature_cols))\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Align test to sample_sub order\n    test_full = sample_sub.copy()\n    test_full[\"phone\"] = test_full[\"phone\"].astype(str)\n    test_full[\"UnixTimeMillis\"] = test_full[\"UnixTimeMillis\"].astype(\"int64\")\n\n    for c in feature_cols:\n        if c not in test_feats.columns:\n            test_feats[c] = np.nan\n\n    cols_for_merge = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n    merged = pd.merge(\n        test_full,\n        test_feats[cols_for_merge],\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    test_full = merged\n\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n\n    X_test = test_full[feature_cols].copy()\n\n    # Fill missing with training medians\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    if X is None or X.shape[1] == 0:\n        print(\"No features for CV; skipping cross-validation.\")\n        return np.nan\n\n    n_splits = 5\n    unique_groups = np.unique(phones.astype(str).values)\n    if len(unique_groups) < n_splits:\n        n_splits = max(2, len(unique_groups))\n        print(f\"Adjusting number of CV folds to {n_splits} due to limited phones.\")\n\n    splitter = GroupKFold(n_splits=n_splits)\n    groups = phones.astype(str).values\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=groups):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=200,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=200,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones.iloc[va_idx].values,\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    if X is None or X.shape[1] == 0:\n        print(\"No features available for model training; models will be None.\")\n        return None, None\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    print(\"Building train and test data...\")\n    train_df, test_feats = build_train_test(sample_sub)\n\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    # Compute CV metric\n    print(\"Running GroupKFold CV (if possible)...\")\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].copy())\n    print(f\"Hold-out CV metric: {cv_score}\")\n\n    # Train full models (if features exist)\n    print(\"Training full models (or falling back to baseline)...\")\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Baseline based on training ground truth\n    train_baseline = (\n        train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        .mean()\n        .rename(columns={\"LatitudeDegrees\": \"mean_lat\", \"LongitudeDegrees\": \"mean_lon\"})\n    )\n    global_mean_lat = train_df[\"LatitudeDegrees\"].mean()\n    global_mean_lon = train_df[\"LongitudeDegrees\"].mean()\n\n    print(\"Predicting on test set and building submission...\")\n    submission = sample_sub.copy()\n\n    if model_lat is not None and model_lon is not None and X_test is not None:\n        try:\n            pred_lat_test = model_lat.predict(X_test)\n            pred_lon_test = model_lon.predict(X_test)\n            submission[\"LatitudeDegrees\"] = pred_lat_test\n            submission[\"LongitudeDegrees\"] = pred_lon_test\n        except Exception as e:\n            print(f\"Model prediction failed ({e}), falling back to baseline.\")\n            submission = submission.merge(\n                train_baseline,\n                how=\"left\",\n                on=\"phone\",\n            )\n            submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(\n                global_mean_lat\n            )\n            submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(\n                global_mean_lon\n            )\n            submission.drop(columns=[\"mean_lat\", \"mean_lon\"], inplace=True)\n    else:\n        print(\"No trained model available, using baseline predictions only.\")\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        submission.drop(columns=[\"mean_lat\", \"mean_lon\"], inplace=True)\n\n    # Ensure no NaNs remain using per-phone and global baselines\n    if (\n        \"LatitudeDegrees\" not in submission.columns\n        or \"LongitudeDegrees\" not in submission.columns\n    ):\n        print(\"Prediction columns missing, rebuilding via baseline.\")\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        submission.drop(columns=[\"mean_lat\", \"mean_lon\"], inplace=True)\n\n    if (\n        submission[\"LatitudeDegrees\"].isna().any()\n        or submission[\"LongitudeDegrees\"].isna().any()\n    ):\n        # Another fallback pass\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n            suffixes=(\"\", \"_base\"),\n        )\n        if \"LatitudeDegrees\" in submission.columns:\n            submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n                submission.get(\"mean_lat\", global_mean_lat)\n            )\n        else:\n            submission[\"LatitudeDegrees\"] = submission.get(\"mean_lat\", global_mean_lat)\n        if \"LongitudeDegrees\" in submission.columns:\n            submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n                submission.get(\"mean_lon\", global_mean_lon)\n            )\n        else:\n            submission[\"LongitudeDegrees\"] = submission.get(\"mean_lon\", global_mean_lon)\n        submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n            global_mean_lat\n        )\n        submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n            global_mean_lon\n        )\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols]\n\n    # Save submissions\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ----- Coordinate transforms -----\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ----- Metric -----\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ----- Data loaders -----\ndef load_train(train_root, feature_cols):\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            # IMPORTANT: phone_id must match sample_submission \"phone\" format: \"<drive_id>_<phone_name>\"\n            phone_id = f\"{drive_id}_{phone_name}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Failed reading {gnss_path} or {gt_path}: {e}\")\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n\n            if len(gnss) == 0 or len(gt) == 0:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Ensure feature columns exist\n            for c in feature_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            # Aggregate GNSS per timestamp\n            agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n\n            agg[\"phone\"] = phone_id\n            agg[\"drive_id\"] = drive_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n            gt_small[\"drive_id\"] = drive_id\n\n            merged = pd.merge(\n                gt_small,\n                agg,\n                on=[\"phone\", \"drive_id\", \"UnixTimeMillis\"],\n                how=\"inner\",\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n            rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols):\n    \"\"\"\n    Aggregate GNSS per (phone, UnixTimeMillis) for all test drives, then left-join to sample_sub.\n    sample_sub['phone'] must match \"<drive_id>_<phone_name>\".\n    \"\"\"\n    rows = []\n\n    # Collect all GNSS data in test into (drive_id, phone_name)\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            try:\n                gnss = pd.read_csv(gnss_path)\n            except Exception as e:\n                print(f\"Failed reading {gnss_path}: {e}\")\n                continue\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            if len(gnss) == 0:\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Ensure feature columns exist in gnss\n            for c in feature_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n            agg[\"phone\"] = phone_id\n            rows.append(agg)\n\n    if not rows:\n        test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"] + feature_cols)\n    else:\n        test_gnss = pd.concat(rows, ignore_index=True)\n\n    # Prepare sample: ensure numeric UnixTimeMillis and consistent dtypes\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n\n    # Left-join GNSS features to sample_sub\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Feature columns (simple baseline)\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    # ---- Load train data ----\n    print(\"Loading training data...\")\n    train_df = load_train(train_root, feature_cols)\n    print(f\"Train rows assembled: {len(train_df)}\")\n\n    # Ensure all feature columns present and fill NaNs with medians\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    # groups by collection (drive)\n    groups = train_df[\"drive_id\"].values\n\n    # ----- 5-fold GroupKFold CV -----\n    print(\"Running 5-fold GroupKFold cross-validation...\")\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    fold_scores = []\n    for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n        lat_val_pred, lon_val_pred, _ = ecef_to_llh(\n            oof[val_idx, 0], oof[val_idx, 1], oof[val_idx, 2]\n        )\n        eval_df_fold = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"UnixTimeMillis\": train_df.iloc[val_idx][\"UnixTimeMillis\"].values,\n                \"lat_pred\": lat_val_pred,\n                \"lon_pred\": lon_val_pred,\n                \"lat_gt\": train_df.iloc[val_idx][\"LatitudeDegrees\"].values,\n                \"lon_gt\": train_df.iloc[val_idx][\"LongitudeDegrees\"].values,\n            }\n        )\n        fold_metric = competition_metric(eval_df_fold)\n        fold_scores.append(fold_metric)\n        print(f\"Fold {fold_idx} competition metric: {fold_metric:.4f}\")\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"OOF competition metric (mean of P50/P95 errors):\", metric_val)\n    if fold_scores:\n        mean_cv = float(np.mean([s for s in fold_scores if not np.isnan(s)]))\n        print(\"Mean CV metric across folds:\", mean_cv)\n\n    # ----- Train final models on full data -----\n    print(\"Training final models on full training data...\")\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # ----- Prepare test and submission -----\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # Ensure 'phone' and 'UnixTimeMillis' exist\n    if \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n        raise RuntimeError(\n            \"sample_submission.csv must contain 'phone' and 'UnixTimeMillis' columns.\"\n        )\n\n    # Normalize phone ids (use directly, ensure string)\n    sample_sub = sample_sub.copy()\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n\n    # Preserve original order with row_id\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    # Merge GNSS features to sample_sub rows\n    print(\"Loading and merging test GNSS features...\")\n    test_merged = load_test(\n        test_root, sample_sub[[\"phone\", \"UnixTimeMillis\", \"row_id\"]], feature_cols\n    )\n\n    # Ensure row_id is preserved after merge (it should be, but double-check)\n    if \"row_id\" not in test_merged.columns:\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    # Ensure feature columns present and fill with train medians\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    # Sort by row_id to keep order consistent\n    test_merged = test_merged.sort_values(\"row_id\").reset_index(drop=True)\n\n    X_test = test_merged[feature_cols].values\n    print(\"Predicting on test set...\")\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align back to full sample_sub order\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill any missing predictions by ffill/bfill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still any NaNs (e.g., all rows were NaN), fall back to training-set mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"].values,\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"].values,\n        }\n    )\n\n    # Save to required path ./submission/submission.csv\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    # Also mirror to ./working/submission.csv as required by benchmark\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\n\n# Directories\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_collection_phone_from_path(path, split_keyword):\n    \"\"\"\n    Robust parser: path like .../<split>/<collection>/<phone>/file.csv\n    We locate <split> in the path and take the next two components.\n    \"\"\"\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    if split_keyword not in parts:\n        return None, None\n    idx = parts.index(split_keyword)\n    if len(parts) <= idx + 2:\n        return None, None\n    collection = parts[idx + 1]\n    phone_model = parts[idx + 2]\n    phone = f\"{collection}_{phone_model}\"\n    return collection, phone\n\n\ndef load_gnss_agg(split_dir):\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(fpath, split_dir)\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception as e:\n            print(f\"Failed to read {fpath}: {e}\")\n            continue\n\n        # Detect time column\n        time_col = None\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            print(f\"No time column in {fpath}, skipping\")\n            continue\n\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n        df[\"phone\"] = phone\n\n        # Select numeric feature columns, skipping obvious non-features\n        skip_cols = {\n            time_col,\n            \"UnixTimeMillis\",\n            \"phone\",\n            \"MessageType\",\n            \"SignalType\",\n            \"CodeType\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n        }\n        feature_cols = []\n        for c in df.columns:\n            if c in skip_cols:\n                continue\n            if pd.api.types.is_numeric_dtype(df[c]):\n                feature_cols.append(c)\n\n        if not feature_cols:\n            continue\n\n        group_cols = [\"phone\", \"UnixTimeMillis\"]\n        agg_dict = {c: \"mean\" for c in feature_cols}\n        use_cols = group_cols + feature_cols\n        df_small = df[use_cols].copy()\n        g = df_small.groupby(group_cols, as_index=False).agg(agg_dict)\n        all_rows.append(g)\n\n    if not all_rows:\n        print(f\"No GNSS agg rows for {split_dir}\")\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    feats = pd.concat(all_rows, ignore_index=True)\n    feats[\"phone\"] = feats[\"phone\"].astype(str)\n    feats[\"UnixTimeMillis\"] = feats[\"UnixTimeMillis\"].astype(\"int64\")\n    print(f\"{split_dir} GNSS agg shape: {feats.shape}\")\n    return feats\n\n\ndef load_train_targets():\n    base_path = os.path.join(INPUT_DIR, \"train\")\n    pattern = os.path.join(base_path, \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(fpath, \"train\")\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception as e:\n            print(f\"Failed to read targets {fpath}: {e}\")\n            continue\n        if \"UnixTimeMillis\" not in df.columns:\n            print(f\"No UnixTimeMillis in target {fpath}, skipping\")\n            continue\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[\"UnixTimeMillis\"].astype(\"int64\")\n        keep_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        df = df[keep_cols]\n        all_rows.append(df)\n    if not all_rows:\n        print(\"No training targets found!\")\n        return pd.DataFrame(\n            columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        )\n    targets = pd.concat(all_rows, ignore_index=True)\n    targets[\"phone\"] = targets[\"phone\"].astype(str)\n    targets[\"UnixTimeMillis\"] = targets[\"UnixTimeMillis\"].astype(\"int64\")\n    print(\"Train targets shape:\", targets.shape)\n    return targets\n\n\ndef build_train_test(sample_sub):\n    train_feats = load_gnss_agg(\"train\")\n    train_targets = load_train_targets()\n\n    if train_targets.empty:\n        raise RuntimeError(\"Training targets are empty; cannot train.\")\n\n    train_df = pd.merge(\n        train_targets,\n        train_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    print(\"Merged train shape (targets + feats):\", train_df.shape)\n\n    test_feats = load_gnss_agg(\"test\")\n    if test_feats.empty:\n        print(\"Warning: test features are empty; predictions will use baseline only.\")\n\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    test_keys[\"phone\"] = test_keys[\"phone\"].astype(str)\n    test_keys[\"UnixTimeMillis\"] = test_keys[\"UnixTimeMillis\"].astype(\"int64\")\n\n    test_merged = pd.merge(\n        test_keys,\n        test_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    print(\"Test merged shape:\", test_merged.shape)\n    return train_df, test_merged\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n\n    for k in key_cols:\n        if k not in train_df.columns:\n            raise RuntimeError(f\"Missing key {k} in training data\")\n\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n\n    if not feature_cols:\n        print(\n            \"No numeric feature columns for modeling; will use constant baseline and still output submission.\"\n        )\n        return None, None, None, None, [], None\n\n    print(\"Number of feature columns:\", len(feature_cols))\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Align test to sample_sub order\n    test_full = sample_sub.copy()\n    test_full[\"phone\"] = test_full[\"phone\"].astype(str)\n    test_full[\"UnixTimeMillis\"] = test_full[\"UnixTimeMillis\"].astype(\"int64\")\n\n    for c in feature_cols:\n        if c not in test_feats.columns:\n            test_feats[c] = np.nan\n\n    cols_for_merge = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n    merged = pd.merge(\n        test_full,\n        test_feats[cols_for_merge],\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    test_full = merged\n\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n\n    X_test = test_full[feature_cols].copy()\n\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    if X is None or X.shape[1] == 0:\n        print(\"No features for CV; skipping cross-validation.\")\n        return np.nan\n\n    # Use collection (prefix before underscore) as group if possible\n    groups = phones.astype(str).values\n    collections = np.array([p.split(\"_\")[0] for p in groups])\n    unique_groups = np.unique(collections)\n    n_splits = 5\n    if len(unique_groups) < n_splits:\n        n_splits = max(2, len(unique_groups))\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=collections):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=200,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=200,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones.iloc[va_idx].values,\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    if X is None or X.shape[1] == 0:\n        print(\"No features available for model training; models will be None.\")\n        return None, None\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    print(\"Building train and test data...\")\n    train_df, test_feats = build_train_test(sample_sub)\n\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    print(\"Running GroupKFold CV (if possible)...\")\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].copy())\n    print(f\"Hold-out CV metric: {cv_score}\")\n\n    print(\"Training full models (or falling back to baseline)...\")\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Baseline based on training ground truth\n    train_baseline = (\n        train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        .mean()\n        .rename(columns={\"LatitudeDegrees\": \"mean_lat\", \"LongitudeDegrees\": \"mean_lon\"})\n    )\n    global_mean_lat = train_df[\"LatitudeDegrees\"].mean()\n    global_mean_lon = train_df[\"LongitudeDegrees\"].mean()\n\n    print(\"Predicting on test set and building submission...\")\n    submission = sample_sub.copy()\n\n    if model_lat is not None and model_lon is not None and X_test is not None:\n        try:\n            pred_lat_test = model_lat.predict(X_test)\n            pred_lon_test = model_lon.predict(X_test)\n            submission[\"LatitudeDegrees\"] = pred_lat_test\n            submission[\"LongitudeDegrees\"] = pred_lon_test\n        except Exception as e:\n            print(f\"Model prediction failed ({e}), falling back to baseline.\")\n            submission = submission.merge(\n                train_baseline,\n                how=\"left\",\n                on=\"phone\",\n            )\n            submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(\n                global_mean_lat\n            )\n            submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(\n                global_mean_lon\n            )\n            submission.drop(columns=[\"mean_lat\", \"mean_lon\"], inplace=True)\n    else:\n        print(\"No trained model available, using baseline predictions only.\")\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        submission.drop(columns=[\"mean_lat\", \"mean_lon\"], inplace=True)\n\n    # Ensure no NaNs remain using per-phone and global baselines\n    if (\n        \"LatitudeDegrees\" not in submission.columns\n        or \"LongitudeDegrees\" not in submission.columns\n    ):\n        print(\"Prediction columns missing, rebuilding via baseline.\")\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        submission.drop(columns=[\"mean_lat\", \"mean_lon\"], inplace=True)\n\n    if (\n        submission[\"LatitudeDegrees\"].isna().any()\n        or submission[\"LongitudeDegrees\"].isna().any()\n    ):\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n            suffixes=(\"\", \"_base\"),\n        )\n        if \"LatitudeDegrees\" in submission.columns:\n            submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n                submission.get(\"mean_lat\", global_mean_lat)\n            )\n        else:\n            submission[\"LatitudeDegrees\"] = submission.get(\"mean_lat\", global_mean_lat)\n        if \"LongitudeDegrees\" in submission.columns:\n            submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n                submission.get(\"mean_lon\", global_mean_lon)\n            )\n        else:\n            submission[\"LongitudeDegrees\"] = submission.get(\"mean_lon\", global_mean_lon)\n        submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n            global_mean_lat\n        )\n        submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n            global_mean_lon\n        )\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols]\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n    print(f\"Final CV metric reported above: {cv_score}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\n\n# Try to import lightgbm; if unavailable, we'll use a baseline only\ntry:\n    import lightgbm as lgb\n\n    HAS_LGB = True\nexcept Exception:\n    HAS_LGB = False\n\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\n\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_collection_phone_from_path(path, split_keyword):\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    if split_keyword not in parts:\n        return None, None\n    idx = parts.index(split_keyword)\n    if len(parts) <= idx + 2:\n        return None, None\n    collection = parts[idx + 1]\n    phone_model = parts[idx + 2]\n    phone = f\"{collection}_{phone_model}\"\n    return collection, phone\n\n\ndef load_gnss_agg(split_dir):\n    base_path = os.path.join(INPUT_DIR, split_dir)\n    pattern = os.path.join(base_path, \"*\", \"*\", \"device_gnss.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(fpath, split_dir)\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception as e:\n            print(f\"Failed to read {fpath}: {e}\")\n            continue\n\n        # Detect time column\n        if \"UnixTimeMillis\" in df.columns:\n            time_col = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in df.columns:\n            time_col = \"utcTimeMillis\"\n        else:\n            print(f\"No time column in {fpath}, skipping\")\n            continue\n\n        df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n        df[\"phone\"] = phone\n\n        skip_cols = {\n            time_col,\n            \"UnixTimeMillis\",\n            \"phone\",\n            \"MessageType\",\n            \"SignalType\",\n            \"CodeType\",\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n        }\n        feature_cols = [\n            c\n            for c in df.columns\n            if c not in skip_cols and pd.api.types.is_numeric_dtype(df[c])\n        ]\n        if not feature_cols:\n            continue\n\n        group_cols = [\"phone\", \"UnixTimeMillis\"]\n        agg_dict = {c: \"mean\" for c in feature_cols}\n        use_cols = group_cols + feature_cols\n        df_small = df[use_cols].copy()\n        g = df_small.groupby(group_cols, as_index=False).agg(agg_dict)\n        all_rows.append(g)\n\n    if not all_rows:\n        print(f\"No GNSS agg rows for {split_dir}\")\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    feats = pd.concat(all_rows, ignore_index=True)\n    feats[\"phone\"] = feats[\"phone\"].astype(str)\n    feats[\"UnixTimeMillis\"] = feats[\"UnixTimeMillis\"].astype(\"int64\")\n    print(f\"{split_dir} GNSS agg shape: {feats.shape}\")\n    return feats\n\n\ndef load_train_targets():\n    base_path = os.path.join(INPUT_DIR, \"train\")\n    pattern = os.path.join(base_path, \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    all_rows = []\n    for fpath in files:\n        collection, phone = parse_collection_phone_from_path(fpath, \"train\")\n        if collection is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception as e:\n            print(f\"Failed to read targets {fpath}: {e}\")\n            continue\n        if \"UnixTimeMillis\" not in df.columns:\n            print(f\"No UnixTimeMillis in target {fpath}, skipping\")\n            continue\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[\"UnixTimeMillis\"].astype(\"int64\")\n        keep_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        df = df[keep_cols]\n        all_rows.append(df)\n    if not all_rows:\n        print(\"No training targets found!\")\n        return pd.DataFrame(\n            columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        )\n    targets = pd.concat(all_rows, ignore_index=True)\n    targets[\"phone\"] = targets[\"phone\"].astype(str)\n    targets[\"UnixTimeMillis\"] = targets[\"UnixTimeMillis\"].astype(\"int64\")\n    print(\"Train targets shape:\", targets.shape)\n    return targets\n\n\ndef build_train_test(sample_sub):\n    train_feats = load_gnss_agg(\"train\")\n    train_targets = load_train_targets()\n\n    if train_targets.empty:\n        raise RuntimeError(\"Training targets are empty; cannot train.\")\n\n    train_df = pd.merge(\n        train_targets,\n        train_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    print(\"Merged train shape (targets + feats):\", train_df.shape)\n\n    test_feats = load_gnss_agg(\"test\")\n    if test_feats.empty:\n        print(\"Warning: test features are empty; predictions will use baseline only.\")\n\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    test_keys[\"phone\"] = test_keys[\"phone\"].astype(str)\n    test_keys[\"UnixTimeMillis\"] = test_keys[\"UnixTimeMillis\"].astype(\"int64\")\n\n    test_merged = pd.merge(\n        test_keys,\n        test_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    print(\"Test merged shape:\", test_merged.shape)\n    return train_df, test_merged\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n\n    if not feature_cols:\n        print(\"No numeric feature columns for modeling; will use baseline only.\")\n        return None, None, None, None, [], None\n\n    print(\"Number of feature columns:\", len(feature_cols))\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Align test to sample_sub order\n    test_full = sample_sub.copy()\n    test_full[\"phone\"] = test_full[\"phone\"].astype(str)\n    test_full[\"UnixTimeMillis\"] = test_full[\"UnixTimeMillis\"].astype(\"int64\")\n\n    for c in feature_cols:\n        if c not in test_feats.columns:\n            test_feats[c] = np.nan\n\n    cols_for_merge = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n    merged = pd.merge(\n        test_full,\n        test_feats[cols_for_merge],\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    test_full = merged\n\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n\n    X_test = test_full[feature_cols].copy()\n\n    # Simple median imputation\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    if X is None or X.shape[1] == 0 or not HAS_LGB:\n        print(\"No features or LightGBM unavailable; skipping cross-validation.\")\n        return np.nan\n\n    groups = phones.astype(str).values\n    collections = np.array([p.split(\"_\")[0] for p in groups])\n    unique_groups = np.unique(collections)\n    n_splits = 5\n    if len(unique_groups) < n_splits:\n        n_splits = max(2, len(unique_groups))\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=collections):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n\n        dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n        dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n        model_lat = lgb.train(\n            params,\n            dtrain_lat,\n            num_boost_round=200,\n            valid_sets=[dvalid_lat],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n        dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n        model_lon = lgb.train(\n            params,\n            dtrain_lon,\n            num_boost_round=200,\n            valid_sets=[dvalid_lon],\n            early_stopping_rounds=20,\n            verbose_eval=False,\n        )\n\n        pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n        pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones.iloc[va_idx].values,\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    if X is None or X.shape[1] == 0 or not HAS_LGB:\n        print(\"No features or LightGBM unavailable; models will be None.\")\n        return None, None\n    params = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.8,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 1,\n        \"seed\": 42,\n        \"verbose\": -1,\n    }\n    dtrain_lat = lgb.Dataset(X, label=y_lat)\n    dtrain_lon = lgb.Dataset(X, label=y_lon)\n    model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n    model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    print(\"Building train and test data...\")\n    train_df, test_feats = build_train_test(sample_sub)\n\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    print(\"Running GroupKFold CV (if possible)...\")\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].copy())\n    print(f\"Hold-out CV metric: {cv_score}\")\n\n    print(\"Training full models (or falling back to baseline)...\")\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Baseline based on training ground truth\n    train_baseline = (\n        train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        .mean()\n        .rename(columns={\"LatitudeDegrees\": \"mean_lat\", \"LongitudeDegrees\": \"mean_lon\"})\n    )\n    global_mean_lat = train_df[\"LatitudeDegrees\"].mean()\n    global_mean_lon = train_df[\"LongitudeDegrees\"].mean()\n\n    print(\"Predicting on test set and building submission...\")\n    submission = sample_sub.copy()\n\n    # Model-based predictions if possible\n    if model_lat is not None and model_lon is not None and X_test is not None:\n        try:\n            pred_lat_test = model_lat.predict(X_test)\n            pred_lon_test = model_lon.predict(X_test)\n            submission[\"LatitudeDegrees\"] = pred_lat_test.astype(float)\n            submission[\"LongitudeDegrees\"] = pred_lon_test.astype(float)\n        except Exception as e:\n            print(f\"Model prediction failed ({e}), falling back to baseline.\")\n            submission = submission.merge(\n                train_baseline,\n                how=\"left\",\n                on=\"phone\",\n            )\n            submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(\n                global_mean_lat\n            )\n            submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(\n                global_mean_lon\n            )\n            for c in [\"mean_lat\", \"mean_lon\"]:\n                if c in submission.columns:\n                    submission.drop(columns=[c], inplace=True)\n    else:\n        print(\"No trained model available, using baseline predictions only.\")\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Ensure prediction columns exist\n    if (\n        \"LatitudeDegrees\" not in submission.columns\n        or \"LongitudeDegrees\" not in submission.columns\n    ):\n        print(\"Prediction columns missing, rebuilding via baseline only.\")\n        submission = sample_sub.copy()\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Final NaN guard using per-phone and global means\n    if (\n        submission[\"LatitudeDegrees\"].isna().any()\n        or submission[\"LongitudeDegrees\"].isna().any()\n    ):\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n            suffixes=(\"\", \"_base\"),\n        )\n        if \"LatitudeDegrees\" in submission.columns:\n            submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n                submission.get(\"mean_lat\", global_mean_lat)\n            )\n        else:\n            submission[\"LatitudeDegrees\"] = submission.get(\"mean_lat\", global_mean_lat)\n        if \"LongitudeDegrees\" in submission.columns:\n            submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n                submission.get(\"mean_lon\", global_mean_lon)\n            )\n        else:\n            submission[\"LongitudeDegrees\"] = submission.get(\"mean_lon\", global_mean_lon)\n        submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n            global_mean_lat\n        )\n        submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n            global_mean_lon\n        )\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Ensure correct column order and dtypes\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols].copy()\n    submission[\"phone\"] = submission[\"phone\"].astype(str)\n    submission[\"UnixTimeMillis\"] = submission[\"UnixTimeMillis\"].astype(\"int64\")\n    submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].astype(float)\n    submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].astype(float)\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n    print(f\"Final CV metric: {cv_score}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\n\n# Try to import lightgbm; if unavailable, we will fall back to RandomForest\ntry:\n    import lightgbm as lgb\n\n    HAS_LGB = True\nexcept Exception:\n    HAS_LGB = False\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import RandomForestRegressor\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\n\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_collection_phone_model(phone_str):\n    \"\"\"\n    phone_str example: '2020-06-04-US-MTV-1_Pixel4'\n    collection: '2020-06-04-US-MTV-1'\n    phone_model: 'Pixel4'\n    \"\"\"\n    if \"_\" not in phone_str:\n        return None, None\n    collection, phone_model = phone_str.split(\"_\", 1)\n    return collection, phone_model\n\n\ndef find_device_dir(split, collection, phone_model):\n    \"\"\"\n    Map (split, collection, phone_model) to an actual directory path like:\n    input/train/2020-06-04-US-MTV-1/GooglePixel4/\n    The mapping between phone_model and directory name is heuristic based on known names.\n    \"\"\"\n    base = os.path.join(INPUT_DIR, split, collection)\n    if not os.path.isdir(base):\n        return None\n\n    # Map simple model names to directory names\n    # e.g. Pixel4 -> GooglePixel4, Pixel5 -> GooglePixel5, etc.\n    candidates = []\n    for d in os.listdir(base):\n        full = os.path.join(base, d)\n        if not os.path.isdir(full):\n            continue\n        candidates.append(full)\n\n    if not candidates:\n        return None\n\n    # Simple heuristic: choose dir whose lowercase contains phone_model lowercase without \"Google\" / \"Samsung\" / \"Xiaomi\"\n    pm = phone_model.lower()\n    pm_core = pm.replace(\"google\", \"\").replace(\"samsung\", \"\").replace(\"xiaomi\", \"\")\n    best = None\n    for c in candidates:\n        name = os.path.basename(c).lower()\n        if pm in name or pm_core in name:\n            best = c\n            break\n    if best is None:\n        # fall back to any dir (first)\n        best = candidates[0]\n    return best\n\n\ndef load_gnss_agg_for_phone(split, phone_str):\n    collection, phone_model = parse_collection_phone_model(phone_str)\n    if collection is None:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n    device_dir = find_device_dir(split, collection, phone_model)\n    if device_dir is None:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    fpath = os.path.join(device_dir, \"device_gnss.csv\")\n    if not os.path.exists(fpath):\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    try:\n        df = pd.read_csv(fpath)\n    except Exception:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    # detect time column\n    if \"UnixTimeMillis\" in df.columns:\n        time_col = \"UnixTimeMillis\"\n    elif \"utcTimeMillis\" in df.columns:\n        time_col = \"utcTimeMillis\"\n    else:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n    df[\"phone\"] = phone_str\n\n    skip_cols = {\n        time_col,\n        \"UnixTimeMillis\",\n        \"phone\",\n        \"MessageType\",\n        \"SignalType\",\n        \"CodeType\",\n        \"LatitudeDegrees\",\n        \"LongitudeDegrees\",\n    }\n    feature_cols = [\n        c\n        for c in df.columns\n        if c not in skip_cols and pd.api.types.is_numeric_dtype(df[c])\n    ]\n    if not feature_cols:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    group_cols = [\"phone\", \"UnixTimeMillis\"]\n    agg_dict = {c: \"mean\" for c in feature_cols}\n    use_cols = group_cols + feature_cols\n    df_small = df[use_cols].copy()\n    g = df_small.groupby(group_cols, as_index=False).agg(agg_dict)\n    return g\n\n\ndef load_all_gnss_agg(split, phone_list):\n    all_rows = []\n    seen = set()\n    for phone in phone_list:\n        if phone in seen:\n            continue\n        seen.add(phone)\n        g = load_gnss_agg_for_phone(split, phone)\n        if not g.empty:\n            all_rows.append(g)\n    if not all_rows:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n    feats = pd.concat(all_rows, ignore_index=True)\n    feats[\"phone\"] = feats[\"phone\"].astype(str)\n    feats[\"UnixTimeMillis\"] = feats[\"UnixTimeMillis\"].astype(\"int64\")\n    return feats\n\n\ndef load_train_targets_from_ground_truth(sample_sub):\n    \"\"\"\n    Use phones from sample_sub to locate train ground_truth.csv per (collection, phone_model),\n    then align via nearest time (within small tolerance).\n    \"\"\"\n    phones = sample_sub[\"phone\"].unique()\n    all_rows = []\n    for phone in phones:\n        collection, phone_model = parse_collection_phone_model(phone)\n        if collection is None:\n            continue\n        device_dir = find_device_dir(\"train\", collection, phone_model)\n        if device_dir is None:\n            continue\n        gt_path = os.path.join(device_dir, \"ground_truth.csv\")\n        if not os.path.exists(gt_path):\n            continue\n        try:\n            df = pd.read_csv(gt_path)\n        except Exception:\n            continue\n        if \"UnixTimeMillis\" not in df.columns:\n            continue\n        df[\"phone\"] = phone\n        df[\"UnixTimeMillis\"] = df[\"UnixTimeMillis\"].astype(\"int64\")\n        keep_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        df = df[keep_cols]\n        all_rows.append(df)\n    if not all_rows:\n        # Fallback: search all ground_truth.csv files\n        base_path = os.path.join(INPUT_DIR, \"train\")\n        pattern = os.path.join(base_path, \"*\", \"*\", \"ground_truth.csv\")\n        files = glob.glob(pattern)\n        for fpath in files:\n            try:\n                df = pd.read_csv(fpath)\n            except Exception:\n                continue\n            if \"UnixTimeMillis\" not in df.columns:\n                continue\n            # create phone name from path by joining last 2 components with underscore\n            parts = fpath.replace(\"\\\\\", \"/\").split(\"/\")\n            if len(parts) < 3:\n                continue\n            collection = parts[-3]\n            phone_model_dir = parts[-2]\n            phone_model = (\n                phone_model_dir.replace(\"Google\", \"\")\n                .replace(\"Samsung\", \"\")\n                .replace(\"Xiaomi\", \"\")\n            )\n            phone = f\"{collection}_{phone_model}\"\n            df[\"phone\"] = phone\n            df[\"UnixTimeMillis\"] = df[\"UnixTimeMillis\"].astype(\"int64\")\n            keep_cols = [\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"LatitudeDegrees\",\n                \"LongitudeDegrees\",\n            ]\n            df = df[keep_cols]\n            all_rows.append(df)\n\n    if not all_rows:\n        return pd.DataFrame(\n            columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        )\n    targets = pd.concat(all_rows, ignore_index=True)\n    targets[\"phone\"] = targets[\"phone\"].astype(str)\n    targets[\"UnixTimeMillis\"] = targets[\"UnixTimeMillis\"].astype(\"int64\")\n    return targets\n\n\ndef prepare_train_test(sample_sub):\n    # Load training targets\n    train_targets = load_train_targets_from_ground_truth(sample_sub)\n    if train_targets.empty:\n        raise RuntimeError(\"Training targets are empty; cannot train.\")\n\n    # Build GNSS features for train phones\n    train_phones = train_targets[\"phone\"].unique()\n    train_feats = load_all_gnss_agg(\"train\", train_phones)\n\n    # Merge exact time first\n    train_df = pd.merge(\n        train_targets,\n        train_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n\n    # If many NaNs, try nearest-time merge within tolerance (e.g., 500 ms)\n    if train_df.isna().mean().mean() > 0.5:\n        print(\"High missing ratio after exact merge; attempting nearest-time merge.\")\n        merged_rows = []\n        tol = 500  # milliseconds\n        for phone, g_tgt in train_targets.groupby(\"phone\"):\n            g_feat = train_feats[train_feats[\"phone\"] == phone]\n            if g_feat.empty:\n                continue\n            g_feat_sorted = g_feat.sort_values(\"UnixTimeMillis\")\n            feat_times = g_feat_sorted[\"UnixTimeMillis\"].values\n            feat_arr = g_feat_sorted.drop(columns=[\"phone\", \"UnixTimeMillis\"]).values\n            feat_cols = g_feat_sorted.drop(columns=[\"phone\", \"UnixTimeMillis\"]).columns\n\n            for _, row in g_tgt.iterrows():\n                t = row[\"UnixTimeMillis\"]\n                idx = np.searchsorted(feat_times, t)\n                best_idx = None\n                if idx < len(feat_times):\n                    best_idx = idx\n                if idx > 0:\n                    if best_idx is None or abs(feat_times[idx - 1] - t) < abs(\n                        feat_times[best_idx] - t\n                    ):\n                        best_idx = idx - 1\n                if best_idx is not None and abs(feat_times[best_idx] - t) <= tol:\n                    feat_vals = feat_arr[best_idx]\n                    row_out = row.to_dict()\n                    for c, v in zip(feat_cols, feat_vals):\n                        row_out[c] = v\n                    merged_rows.append(row_out)\n                else:\n                    merged_rows.append(row.to_dict())\n        train_df = pd.DataFrame(merged_rows)\n\n    print(\"Merged train shape:\", train_df.shape)\n\n    # Build GNSS features for test phones (from sample_sub)\n    test_phones = sample_sub[\"phone\"].unique()\n    test_feats = load_all_gnss_agg(\"test\", test_phones)\n\n    # Align test features to sample_sub\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    test_keys[\"phone\"] = test_keys[\"phone\"].astype(str)\n    test_keys[\"UnixTimeMillis\"] = test_keys[\"UnixTimeMillis\"].astype(\"int64\")\n    test_merged = pd.merge(\n        test_keys,\n        test_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    print(\"Test merged shape:\", test_merged.shape)\n    return train_df, test_merged\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n\n    if not feature_cols:\n        print(\"No numeric feature columns for modeling; returning None for X/y.\")\n        return None, None, None, None, [], None\n\n    print(\"Number of feature columns:\", len(feature_cols))\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # Align test to sample_sub order\n    test_full = sample_sub.copy()\n    test_full[\"phone\"] = test_full[\"phone\"].astype(str)\n    test_full[\"UnixTimeMillis\"] = test_full[\"UnixTimeMillis\"].astype(\"int64\")\n\n    for c in feature_cols:\n        if c not in test_feats.columns:\n            test_feats[c] = np.nan\n\n    cols_for_merge = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n    merged = pd.merge(\n        test_full,\n        test_feats[cols_for_merge],\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    test_full = merged\n\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n\n    X_test = test_full[feature_cols].copy()\n\n    # Simple median imputation\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    if X is None or X.shape[1] == 0:\n        print(\"No features; skipping CV.\")\n        return np.nan\n\n    groups = phones.astype(str).values\n    collections = np.array([p.split(\"_\")[0] for p in groups])\n    unique_groups = np.unique(collections)\n    n_splits = 5\n    if len(unique_groups) < n_splits:\n        n_splits = max(2, len(unique_groups))\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=collections):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        if HAS_LGB:\n            params = {\n                \"objective\": \"regression\",\n                \"metric\": \"rmse\",\n                \"learning_rate\": 0.05,\n                \"num_leaves\": 31,\n                \"feature_fraction\": 0.8,\n                \"bagging_fraction\": 0.8,\n                \"bagging_freq\": 1,\n                \"seed\": 42,\n                \"verbose\": -1,\n            }\n\n            dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n            dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n            model_lat = lgb.train(\n                params,\n                dtrain_lat,\n                num_boost_round=200,\n                valid_sets=[dvalid_lat],\n                early_stopping_rounds=20,\n                verbose_eval=False,\n            )\n\n            dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n            dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n            model_lon = lgb.train(\n                params,\n                dtrain_lon,\n                num_boost_round=200,\n                valid_sets=[dvalid_lon],\n                early_stopping_rounds=20,\n                verbose_eval=False,\n            )\n\n            pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n            pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n        else:\n            # Fallback to RandomForest\n            model_lat = RandomForestRegressor(\n                n_estimators=100, random_state=42, n_jobs=-1\n            )\n            model_lon = RandomForestRegressor(\n                n_estimators=100, random_state=42, n_jobs=-1\n            )\n            model_lat.fit(X_tr, y_lat_tr)\n            model_lon.fit(X_tr, y_lon_tr)\n            pred_lat = model_lat.predict(X_va)\n            pred_lon = model_lon.predict(X_va)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones.iloc[va_idx].values,\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    if X is None or X.shape[1] == 0:\n        print(\"No features; models will be None.\")\n        return None, None\n    if HAS_LGB:\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n        dtrain_lat = lgb.Dataset(X, label=y_lat)\n        dtrain_lon = lgb.Dataset(X, label=y_lon)\n        model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n        model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n        return model_lat, model_lon\n    else:\n        model_lat = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n        model_lon = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n        model_lat.fit(X, y_lat)\n        model_lon.fit(X, y_lon)\n        return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    print(\"Building train and test data with robust path handling...\")\n    train_df, test_feats = prepare_train_test(sample_sub)\n\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    print(\"Running GroupKFold CV...\")\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].copy())\n    print(f\"Hold-out CV metric: {cv_score}\")\n\n    print(\"Training full models...\")\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Baseline based on training ground truth\n    train_baseline = (\n        train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        .mean()\n        .rename(columns={\"LatitudeDegrees\": \"mean_lat\", \"LongitudeDegrees\": \"mean_lon\"})\n    )\n    global_mean_lat = train_df[\"LatitudeDegrees\"].mean()\n    global_mean_lon = train_df[\"LongitudeDegrees\"].mean()\n\n    print(\"Predicting on test set and building submission...\")\n    submission = sample_sub.copy()\n\n    # Model-based predictions if possible\n    if model_lat is not None and model_lon is not None and X_test is not None:\n        try:\n            if HAS_LGB:\n                pred_lat_test = model_lat.predict(X_test)\n                pred_lon_test = model_lon.predict(X_test)\n            else:\n                pred_lat_test = model_lat.predict(X_test)\n                pred_lon_test = model_lon.predict(X_test)\n            submission[\"LatitudeDegrees\"] = pred_lat_test.astype(float)\n            submission[\"LongitudeDegrees\"] = pred_lon_test.astype(float)\n        except Exception as e:\n            print(f\"Model prediction failed ({e}), falling back to baseline.\")\n            submission = submission.merge(\n                train_baseline,\n                how=\"left\",\n                on=\"phone\",\n            )\n            submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(\n                global_mean_lat\n            )\n            submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(\n                global_mean_lon\n            )\n            for c in [\"mean_lat\", \"mean_lon\"]:\n                if c in submission.columns:\n                    submission.drop(columns=[c], inplace=True)\n    else:\n        print(\"No trained model available, using baseline predictions only.\")\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Ensure prediction columns exist\n    if (\n        \"LatitudeDegrees\" not in submission.columns\n        or \"LongitudeDegrees\" not in submission.columns\n    ):\n        print(\"Prediction columns missing, rebuilding via baseline only.\")\n        submission = sample_sub.copy()\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Final NaN guard using per-phone and global means\n    if (\n        submission[\"LatitudeDegrees\"].isna().any()\n        or submission[\"LongitudeDegrees\"].isna().any()\n    ):\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n            suffixes=(\"\", \"_base\"),\n        )\n        if \"LatitudeDegrees\" in submission.columns:\n            submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n                submission.get(\"mean_lat\", global_mean_lat)\n            )\n        else:\n            submission[\"LatitudeDegrees\"] = submission.get(\"mean_lat\", global_mean_lat)\n        if \"LongitudeDegrees\" in submission.columns:\n            submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n                submission.get(\"mean_lon\", global_mean_lon)\n            )\n        else:\n            submission[\"LongitudeDegrees\"] = submission.get(\"mean_lon\", global_mean_lon)\n        submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n            global_mean_lat\n        )\n        submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n            global_mean_lon\n        )\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Ensure correct column order and dtypes\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols].copy()\n    submission[\"phone\"] = submission[\"phone\"].astype(str)\n    submission[\"UnixTimeMillis\"] = submission[\"UnixTimeMillis\"].astype(\"int64\")\n    submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].astype(float)\n    submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].astype(float)\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n    print(f\"Final CV metric: {cv_score}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\n# ---------------- Geodesy helpers ----------------\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 constants\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = 6356752.314245\n    ep2 = (a**2 - b**2) / b**2\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\n# ---------------- Train loading ----------------\ndef load_train_with_gt(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"No train directory found; cannot compute validation metric.\", flush=True)\n        return pd.DataFrame()\n\n    all_rows = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} read error: {e}\", flush=True)\n                continue\n\n            # Ensure UnixTimeMillis exists\n            if \"UnixTimeMillis\" not in gnss.columns:\n                if \"utcTimeMillis\" in gnss.columns:\n                    gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n            if \"UnixTimeMillis\" not in gt.columns:\n                if \"utcTimeMillis\" in gt.columns:\n                    gt = gt.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No training rows assembled with ground truth.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    train_df = train_df.copy()\n\n    ecef_x = train_df[\"WlsPositionXEcefMeters\"].values\n    ecef_y = train_df[\"WlsPositionYEcefMeters\"].values\n    ecef_z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(ecef_x, ecef_y, ecef_z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    # Basic numeric features: time and ECEF\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X.values, y_lat)\n        rf_lon.fit(X.values, y_lon)\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n        print(\"Not enough groups for CV, trained single correction model.\", flush=True)\n        return lat_models, lon_models, feature_cols, np.nan, train_df_prepared\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols].values, y_lat)\n    rf_lon.fit(X[feature_cols].values, y_lon)\n    return rf_lat, rf_lon\n\n\n# ---------------- Test feature preparation ----------------\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"UnixTimeMillis\" not in gnss.columns:\n                    if \"utcTimeMillis\" in gnss.columns:\n                        gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                    else:\n                        continue\n\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\", as_index=False)\n                    .mean()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    # compute base lat/lon where we have ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load and train\n    train_df = load_train_with_gt(INPUT_DIR)\n    if train_df.empty:\n        print(\n            \"No training data with GT found; will fall back to pure WLS baseline.\",\n            flush=True,\n        )\n        correction_model_lat = None\n        correction_model_lon = None\n        feature_cols = None\n        val_metric = np.nan\n    else:\n        lat_models, lon_models, feature_cols, val_metric, train_df_prepared = (\n            train_with_cv(train_df)\n        )\n        print(\"CV competition metric:\", val_metric, flush=True)\n\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    print(f\"Validation metric (mean (P50+P95)/2 over phones): {val_metric}\", flush=True)\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Robust per-phone forward/backward fill on ECEF and recompute\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        if coord in merged.columns:\n            merged[coord] = merged.groupby(\"phone\")[coord].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    # recompute base lat/lon after filling ECEF where possible\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    if np.isnan(overall_base_lat):\n        # In extreme case where all are NaN, fall back to zeros\n        overall_base_lat = 0.0\n        overall_base_lon = 0.0\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Correction predictions if model exists\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols].values)\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols].values)\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned exactly to sample_sub\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    # If some rows lack predictions, fill per phone then global\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    if np.isnan(overall_lat):\n        overall_lat = 0.0\n        overall_lon = 0.0\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\n# ---------------- Geodesy helpers ----------------\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 constants\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    b = 6356752.314245\n    ep2 = (a**2 - b**2) / b**2\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\n# ---------------- Train loading ----------------\ndef load_train_with_gt(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"No train directory found; cannot compute validation metric.\", flush=True)\n        return pd.DataFrame()\n\n    all_rows = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} read error: {e}\", flush=True)\n                continue\n\n            # Ensure UnixTimeMillis exists\n            if \"UnixTimeMillis\" not in gnss.columns:\n                if \"utcTimeMillis\" in gnss.columns:\n                    gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n            if \"UnixTimeMillis\" not in gt.columns:\n                if \"utcTimeMillis\" in gt.columns:\n                    gt = gt.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No training rows assembled with ground truth.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    train_df = train_df.copy()\n\n    ecef_x = train_df[\"WlsPositionXEcefMeters\"].values\n    ecef_y = train_df[\"WlsPositionYEcefMeters\"].values\n    ecef_z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(ecef_x, ecef_y, ecef_z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X.values, y_lat)\n        rf_lon.fit(X.values, y_lon)\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n        print(\"Not enough groups for CV, trained single correction model.\", flush=True)\n        return lat_models, lon_models, feature_cols, np.nan, train_df_prepared\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols].values, y_lat)\n    rf_lon.fit(X[feature_cols].values, y_lon)\n    return rf_lat, rf_lon\n\n\n# ---------------- Test feature preparation ----------------\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"UnixTimeMillis\" not in gnss.columns:\n                    if \"utcTimeMillis\" in gnss.columns:\n                        gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                    else:\n                        continue\n\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\", as_index=False)\n                    .mean()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    # initial base lat/lon where we have ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load and train\n    train_df = load_train_with_gt(INPUT_DIR)\n    if train_df.empty:\n        print(\n            \"No training data with GT found; will fall back to pure WLS baseline.\",\n            flush=True,\n        )\n        correction_model_lat = None\n        correction_model_lon = None\n        feature_cols = None\n        val_metric = np.nan\n    else:\n        lat_models, lon_models, feature_cols, val_metric, train_df_prepared = (\n            train_with_cv(train_df)\n        )\n        print(\"CV competition metric:\", val_metric, flush=True)\n\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    print(f\"Validation metric (mean (P50+P95)/2 over phones): {val_metric}\", flush=True)\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Robust per-phone forward/backward fill on ECEF and recompute\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        if coord in merged.columns:\n            merged[coord] = merged.groupby(\"phone\")[coord].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    # recompute base lat/lon after filling ECEF where possible\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    if np.isnan(overall_base_lat):\n        # In extreme case where all are NaN, fall back to zeros\n        overall_base_lat = 0.0\n        overall_base_lon = 0.0\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Correction predictions if model exists\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols].values)\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols].values)\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned exactly to sample_sub\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    # If some rows lack predictions, fill per phone then global\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    if np.isnan(overall_lat):\n        overall_lat = 0.0\n        overall_lon = 0.0\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 ellipsoid constants\n    a = 6378137.0\n    b = 6356752.314245\n    e2 = 6.69437999014e-3\n    ep2 = (a * a - b * b) / (b * b)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef load_train_with_gt(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"Train directory not found; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    all_rows = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} due to read error: {e}\", flush=True)\n                continue\n\n            # Normalize UnixTimeMillis column name\n            if \"UnixTimeMillis\" not in gnss.columns:\n                if \"utcTimeMillis\" in gnss.columns:\n                    gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n            if \"UnixTimeMillis\" not in gt.columns:\n                if \"utcTimeMillis\" in gt.columns:\n                    gt = gt.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No train rows with GT assembled; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    print(f\"Loaded train with GT: {len(train_df)} rows\", flush=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    train_df = train_df.copy()\n\n    ecef_x = train_df[\"WlsPositionXEcefMeters\"].values\n    ecef_y = train_df[\"WlsPositionYEcefMeters\"].values\n    ecef_z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(ecef_x, ecef_y, ecef_z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X.values, y_lat)\n        rf_lon.fit(X.values, y_lon)\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n        print(\"Not enough groups for CV, trained single correction model.\", flush=True)\n        return lat_models, lon_models, feature_cols, np.nan, train_df_prepared\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols].values, y_lat)\n    rf_lon.fit(X[feature_cols].values, y_lon)\n    return rf_lat, rf_lon\n\n\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"UnixTimeMillis\" not in gnss.columns:\n                    if \"utcTimeMillis\" in gnss.columns:\n                        gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                    else:\n                        continue\n\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\", as_index=False)\n                    .mean()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load train and train model with CV\n    train_df = load_train_with_gt(INPUT_DIR)\n    if train_df.empty:\n        correction_model_lat = None\n        correction_model_lon = None\n        feature_cols = None\n        val_metric = np.nan\n        print(\n            \"No training data available; using baseline WLS positions only.\", flush=True\n        )\n    else:\n        lat_models, lon_models, feature_cols, val_metric, train_df_prepared = (\n            train_with_cv(train_df)\n        )\n        print(f\"Cross-validation competition metric: {val_metric}\", flush=True)\n\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    # Print final evaluation metric explicitly\n    print(\n        \"Final validation metric (mean (P50+P95)/2 over phones):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Fill ECEF per phone (ffill/bfill) to reduce gaps\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        if coord in merged.columns:\n            merged[coord] = merged.groupby(\"phone\")[coord].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    # Recompute base lat/lon after filling ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global mean\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    if np.isnan(overall_base_lat):\n        overall_base_lat = 0.0\n        overall_base_lon = 0.0\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Apply correction model if available\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols].values)\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols].values)\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned to sample_sub\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    # Fill missing predictions per phone then global\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    if np.isnan(overall_lat):\n        overall_lat = 0.0\n        overall_lon = 0.0\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\n\n# Try to import lightgbm; if unavailable, fall back to RandomForest\ntry:\n    import lightgbm as lgb\n\n    HAS_LGB = True\nexcept Exception:\n    HAS_LGB = False\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import RandomForestRegressor\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\n\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_phone(sample_phone):\n    \"\"\"\n    sample_submission phone format in this benchmark is directory-based, e.g.\n    '2020-06-04-US-MTV-1_GooglePixel4'\n\n    Returns (collection, device_dir).\n    \"\"\"\n    if \"_\" not in sample_phone:\n        return None, None\n    collection, model = sample_phone.split(\"_\", 1)\n    return collection, model\n\n\ndef list_ground_truth_train():\n    \"\"\"\n    Scan train directory and load all ground_truth.csv files, building canonical phone names\n    as 'collection_deviceDirName' to match sample_submission style.\n    \"\"\"\n    rows = []\n    pattern = os.path.join(INPUT_DIR, \"train\", \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    for fpath in files:\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n        if \"UnixTimeMillis\" not in df.columns:\n            continue\n        parts = fpath.replace(\"\\\\\", \"/\").split(\"/\")\n        # .../train/<collection>/<device_dir>/ground_truth.csv\n        if len(parts) < 4:\n            continue\n        collection = parts[-3]\n        device_dir = parts[-2]\n        phone = f\"{collection}_{device_dir}\"\n        sub = df[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n        sub[\"UnixTimeMillis\"] = sub[\"UnixTimeMillis\"].astype(\"int64\")\n        sub[\"phone\"] = phone\n        rows.append(sub)\n    if not rows:\n        return pd.DataFrame(\n            columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        )\n    out = pd.concat(rows, ignore_index=True)\n    out[\"phone\"] = out[\"phone\"].astype(str)\n    out[\"UnixTimeMillis\"] = out[\"UnixTimeMillis\"].astype(\"int64\")\n    return out\n\n\ndef load_gnss_agg_for_device(split, collection, device_dir, phone_name):\n    \"\"\"\n    Load device_gnss.csv for a specific (split, collection, device_dir),\n    aggregate numeric features per UnixTimeMillis, and tag with canonical phone_name.\n    \"\"\"\n    fpath = os.path.join(INPUT_DIR, split, collection, device_dir, \"device_gnss.csv\")\n    if not os.path.exists(fpath):\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n    try:\n        df = pd.read_csv(fpath)\n    except Exception:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    # choose time column\n    if \"UnixTimeMillis\" in df.columns:\n        time_col = \"UnixTimeMillis\"\n    elif \"utcTimeMillis\" in df.columns:\n        time_col = \"utcTimeMillis\"\n    else:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n    df[\"phone\"] = phone_name\n\n    skip_cols = {\n        time_col,\n        \"UnixTimeMillis\",\n        \"phone\",\n        \"MessageType\",\n        \"SignalType\",\n        \"CodeType\",\n        \"LatitudeDegrees\",\n        \"LongitudeDegrees\",\n    }\n\n    feature_cols = [\n        c\n        for c in df.columns\n        if c not in skip_cols and pd.api.types.is_numeric_dtype(df[c])\n    ]\n    if not feature_cols:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    group_cols = [\"phone\", \"UnixTimeMillis\"]\n    agg_dict = {c: \"mean\" for c in feature_cols}\n    df_small = df[group_cols + feature_cols].copy()\n    g = df_small.groupby(group_cols, as_index=False).agg(agg_dict)\n    return g\n\n\ndef build_train_features(train_targets):\n    \"\"\"\n    Build GNSS-aggregated features for all phones appearing in train_targets.\n    The canonical phone naming is 'collection_deviceDir'.\n    \"\"\"\n    phones = train_targets[\"phone\"].unique()\n    all_feats = []\n    for phone in phones:\n        collection, device_dir = parse_phone(phone)\n        if collection is None:\n            continue\n        g = load_gnss_agg_for_device(\"train\", collection, device_dir, phone)\n        if not g.empty:\n            all_feats.append(g)\n    if not all_feats:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n    feats = pd.concat(all_feats, ignore_index=True)\n    feats[\"phone\"] = feats[\"phone\"].astype(str)\n    feats[\"UnixTimeMillis\"] = feats[\"UnixTimeMillis\"].astype(\"int64\")\n    return feats\n\n\ndef build_test_features(sample_sub):\n    \"\"\"\n    Build GNSS-aggregated features for all phones in sample_submission.\n    \"\"\"\n    phones = sample_sub[\"phone\"].unique()\n    all_feats = []\n    for phone in phones:\n        collection, device_dir = parse_phone(phone)\n        if collection is None:\n            continue\n        g = load_gnss_agg_for_device(\"test\", collection, device_dir, phone)\n        if not g.empty:\n            all_feats.append(g)\n    if not all_feats:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n    feats = pd.concat(all_feats, ignore_index=True)\n    feats[\"phone\"] = feats[\"phone\"].astype(str)\n    feats[\"UnixTimeMillis\"] = feats[\"UnixTimeMillis\"].astype(\"int64\")\n    return feats\n\n\ndef prepare_train_test(sample_sub):\n    # load train targets from all ground_truth files\n    train_targets = list_ground_truth_train()\n    if train_targets.empty:\n        raise RuntimeError(\"No ground_truth found in train; cannot train model.\")\n\n    # restrict to phones that exist in train (could be many; we just use all)\n    train_feats = build_train_features(train_targets)\n\n    # exact merge on (phone, UnixTimeMillis)\n    train_df = pd.merge(\n        train_targets,\n        train_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    print(\"Train merge shape:\", train_df.shape)\n\n    # If too many missing features, do nearest-time merge per phone from raw gnss feats\n    if train_df.isna().mean().mean() > 0.5 and not train_feats.empty:\n        print(\"High missing ratio after exact merge; attempting nearest-time merge.\")\n        merged_rows = []\n        tol = 500  # ms\n        # prepare per-phone feature arrays\n        feat_cols = [\n            c\n            for c in train_feats.columns\n            if c not in [\"phone\", \"UnixTimeMillis\"]\n            and pd.api.types.is_numeric_dtype(train_feats[c])\n        ]\n        for phone, g_tgt in train_targets.groupby(\"phone\"):\n            g_feat = train_feats[train_feats[\"phone\"] == phone]\n            if g_feat.empty:\n                for _, row in g_tgt.iterrows():\n                    merged_rows.append(row.to_dict())\n                continue\n            g_feat_sorted = g_feat.sort_values(\"UnixTimeMillis\")\n            feat_times = g_feat_sorted[\"UnixTimeMillis\"].values\n            feat_arr = g_feat_sorted[feat_cols].values\n            for _, row in g_tgt.iterrows():\n                t = int(row[\"UnixTimeMillis\"])\n                idx = np.searchsorted(feat_times, t)\n                best_idx = None\n                if idx < len(feat_times):\n                    best_idx = idx\n                if idx > 0:\n                    if best_idx is None or abs(feat_times[idx - 1] - t) < abs(\n                        feat_times[best_idx] - t\n                    ):\n                        best_idx = idx - 1\n                row_out = row.to_dict()\n                if best_idx is not None and abs(feat_times[best_idx] - t) <= tol:\n                    vals = feat_arr[best_idx]\n                    for c, v in zip(feat_cols, vals):\n                        row_out[c] = v\n                merged_rows.append(row_out)\n        train_df = pd.DataFrame(merged_rows)\n        print(\"Train shape after nearest merge:\", train_df.shape)\n\n    # Test features\n    test_feats = build_test_features(sample_sub)\n    print(\"Raw test feature shape:\", test_feats.shape)\n\n    # align to sample_sub\n    test_keys = sample_sub[[\"phone\", \"UnixTimeMillis\"]].copy()\n    test_keys[\"phone\"] = test_keys[\"phone\"].astype(str)\n    test_keys[\"UnixTimeMillis\"] = test_keys[\"UnixTimeMillis\"].astype(\"int64\")\n    test_merged = pd.merge(\n        test_keys,\n        test_feats,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    print(\"Test merged shape:\", test_merged.shape)\n    return train_df, test_merged\n\n\ndef prepare_xy(train_df, test_feats, sample_sub):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n    if not feature_cols:\n        print(\"No numeric feature columns found; returning None.\")\n        return None, None, None, None, [], None\n\n    print(\"Number of feature columns:\", len(feature_cols))\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # align test feats to sample_sub order\n    test_full = sample_sub.copy()\n    test_full[\"phone\"] = test_full[\"phone\"].astype(str)\n    test_full[\"UnixTimeMillis\"] = test_full[\"UnixTimeMillis\"].astype(\"int64\")\n\n    for c in feature_cols:\n        if c not in test_feats.columns:\n            test_feats[c] = np.nan\n\n    cols_for_merge = [\"phone\", \"UnixTimeMillis\"] + feature_cols\n    merged = pd.merge(\n        test_full,\n        test_feats[cols_for_merge],\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n        sort=False,\n    )\n    test_full = merged\n\n    for c in feature_cols:\n        if c not in test_full.columns:\n            test_full[c] = np.nan\n\n    X_test = test_full[feature_cols].copy()\n\n    # simple median imputation\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, test_full\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    if X is None or X.shape[1] == 0:\n        print(\"No features for CV.\")\n        return np.nan\n\n    phones = phones.astype(str)\n    collections = np.array([p.split(\"_\")[0] for p in phones])\n    unique_groups = np.unique(collections)\n    n_splits = 5\n    if len(unique_groups) < n_splits:\n        n_splits = max(2, len(unique_groups))\n    splitter = GroupKFold(n_splits=n_splits)\n\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=collections):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        if HAS_LGB:\n            params = {\n                \"objective\": \"regression\",\n                \"metric\": \"rmse\",\n                \"learning_rate\": 0.05,\n                \"num_leaves\": 31,\n                \"feature_fraction\": 0.8,\n                \"bagging_fraction\": 0.8,\n                \"bagging_freq\": 1,\n                \"seed\": 42,\n                \"verbose\": -1,\n            }\n            dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n            dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n            model_lat = lgb.train(\n                params,\n                dtrain_lat,\n                num_boost_round=200,\n                valid_sets=[dvalid_lat],\n                early_stopping_rounds=20,\n                verbose_eval=False,\n            )\n\n            dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n            dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n            model_lon = lgb.train(\n                params,\n                dtrain_lon,\n                num_boost_round=200,\n                valid_sets=[dvalid_lon],\n                early_stopping_rounds=20,\n                verbose_eval=False,\n            )\n\n            pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n            pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n        else:\n            model_lat = RandomForestRegressor(\n                n_estimators=100, random_state=42, n_jobs=-1\n            )\n            model_lon = RandomForestRegressor(\n                n_estimators=100, random_state=42, n_jobs=-1\n            )\n            model_lat.fit(X_tr, y_lat_tr)\n            model_lon.fit(X_tr, y_lon_tr)\n            pred_lat = model_lat.predict(X_va)\n            pred_lon = model_lon.predict(X_va)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones.iloc[va_idx].values,\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    if X is None or X.shape[1] == 0:\n        print(\"No features for training; models will be None.\")\n        return None, None\n    if HAS_LGB:\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n        dtrain_lat = lgb.Dataset(X, label=y_lat)\n        dtrain_lon = lgb.Dataset(X, label=y_lon)\n        model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n        model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n        return model_lat, model_lon\n    else:\n        model_lat = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n        model_lon = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n        model_lat.fit(X, y_lat)\n        model_lon.fit(X, y_lon)\n        return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    print(\"Preparing train and test data...\")\n    train_df, test_feats = prepare_train_test(sample_sub)\n\n    X_train, y_lat, y_lon, X_test, feature_cols, test_full = prepare_xy(\n        train_df, test_feats, sample_sub\n    )\n\n    print(\"Running 5-fold GroupKFold CV...\")\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].copy())\n    print(f\"Hold-out CV metric: {cv_score}\")\n\n    print(\"Training full models...\")\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Baseline: per-phone and global mean from train targets\n    train_baseline = (\n        train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        .mean()\n        .rename(columns={\"LatitudeDegrees\": \"mean_lat\", \"LongitudeDegrees\": \"mean_lon\"})\n    )\n    global_mean_lat = train_df[\"LatitudeDegrees\"].mean()\n    global_mean_lon = train_df[\"LongitudeDegrees\"].mean()\n\n    print(\"Predicting on test set and building submission...\")\n    submission = sample_sub.copy()\n\n    if model_lat is not None and model_lon is not None and X_test is not None:\n        try:\n            if HAS_LGB:\n                pred_lat_test = model_lat.predict(X_test)\n                pred_lon_test = model_lon.predict(X_test)\n            else:\n                pred_lat_test = model_lat.predict(X_test)\n                pred_lon_test = model_lon.predict(X_test)\n            submission[\"LatitudeDegrees\"] = pred_lat_test.astype(float)\n            submission[\"LongitudeDegrees\"] = pred_lon_test.astype(float)\n        except Exception as e:\n            print(f\"Model prediction failed: {e}. Falling back to baseline.\")\n            submission = submission.merge(train_baseline, how=\"left\", on=\"phone\")\n            submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(\n                global_mean_lat\n            )\n            submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(\n                global_mean_lon\n            )\n            for c in [\"mean_lat\", \"mean_lon\"]:\n                if c in submission.columns:\n                    submission.drop(columns=[c], inplace=True)\n    else:\n        print(\"No model available, using baseline predictions only.\")\n        submission = submission.merge(train_baseline, how=\"left\", on=\"phone\")\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Ensure prediction columns exist\n    if (\n        \"LatitudeDegrees\" not in submission.columns\n        or \"LongitudeDegrees\" not in submission.columns\n    ):\n        print(\"Prediction columns missing; rebuilding via baseline only.\")\n        submission = sample_sub.copy()\n        submission = submission.merge(train_baseline, how=\"left\", on=\"phone\")\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Final NaN guard\n    if (\n        submission[\"LatitudeDegrees\"].isna().any()\n        or submission[\"LongitudeDegrees\"].isna().any()\n    ):\n        print(\"NaNs detected in predictions; filling with per-phone and global means.\")\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n            suffixes=(\"\", \"_base\"),\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n            submission.get(\"mean_lat\", global_mean_lat)\n        )\n        submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n            submission.get(\"mean_lon\", global_mean_lon)\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n            global_mean_lat\n        )\n        submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n            global_mean_lon\n        )\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Ensure correct column order and dtypes\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols].copy()\n    submission[\"phone\"] = submission[\"phone\"].astype(str)\n    submission[\"UnixTimeMillis\"] = submission[\"UnixTimeMillis\"].astype(\"int64\")\n    submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].astype(float)\n    submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].astype(float)\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n    print(f\"Final CV metric: {cv_score}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\n# Directories\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\n\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 ellipsoid constants\n    a = 6378137.0\n    b = 6356752.314245\n    e2 = 6.69437999014e-3\n    ep2 = (a * a - b * b) / (b * b)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef load_train_with_gt(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"Train directory not found; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    all_rows = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} due to read error: {e}\", flush=True)\n                continue\n\n            # Normalize UnixTimeMillis column name\n            if \"UnixTimeMillis\" not in gnss.columns:\n                if \"utcTimeMillis\" in gnss.columns:\n                    gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n            if \"UnixTimeMillis\" not in gt.columns:\n                if \"utcTimeMillis\" in gt.columns:\n                    gt = gt.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Aggregate WLS ECEF per second\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No train rows with GT assembled; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    print(f\"Loaded train with GT: {len(train_df)} rows\", flush=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    train_df = train_df.copy()\n\n    ecef_x = train_df[\"WlsPositionXEcefMeters\"].values\n    ecef_y = train_df[\"WlsPositionYEcefMeters\"].values\n    ecef_z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(ecef_x, ecef_y, ecef_z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X.values, y_lat)\n        rf_lon.fit(X.values, y_lon)\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n        print(\"Not enough groups for CV, trained single correction model.\", flush=True)\n        return lat_models, lon_models, feature_cols, np.nan, train_df_prepared\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols].values, y_lat)\n    rf_lon.fit(X[feature_cols].values, y_lon)\n    return rf_lat, rf_lon\n\n\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"UnixTimeMillis\" not in gnss.columns:\n                    if \"utcTimeMillis\" in gnss.columns:\n                        gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                    else:\n                        continue\n\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\", as_index=False)\n                    .mean()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load train and train model with CV\n    train_df = load_train_with_gt(INPUT_DIR)\n    if train_df.empty:\n        correction_model_lat = None\n        correction_model_lon = None\n        feature_cols = None\n        val_metric = np.nan\n        print(\n            \"No training data available; using baseline WLS/mean positions only.\",\n            flush=True,\n        )\n    else:\n        lat_models, lon_models, feature_cols, val_metric, train_df_prepared = (\n            train_with_cv(train_df)\n        )\n        print(f\"Cross-validation competition metric: {val_metric}\", flush=True)\n\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    # Print final evaluation metric explicitly\n    print(\n        \"Final validation metric (mean (P50+P95)/2 over phones):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Fill ECEF per phone (ffill/bfill) to reduce gaps\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        if coord in merged.columns:\n            merged[coord] = merged.groupby(\"phone\")[coord].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    # Recompute base lat/lon after filling ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global mean\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    if np.isnan(overall_base_lat):\n        overall_base_lat = 0.0\n        overall_base_lon = 0.0\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Apply correction model if available\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols].values)\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols].values)\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned to sample_sub\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    # Fill missing predictions per phone then global\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    if np.isnan(overall_lat):\n        overall_lat = 0.0\n        overall_lon = 0.0\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ---------------- Coordinate transforms ----------------\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ---------------- Metric ----------------\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ---------------- Data loaders ----------------\ndef load_train(train_root, feature_cols):\n    \"\"\"\n    Build training dataframe with features aggregated per (phone, UnixTimeMillis),\n    joined with ground truth lat/lon and converted to ECEF.\n    \"\"\"\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(train_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            # In this benchmark, sample_submission uses \"<drive>/<phone>\" as is,\n            # but safer is to check sample directly later; here we keep a simple id:\n            phone_id = f\"{drive_id}_{phone_name}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Failed reading {gnss_path} or {gt_path}: {e}\")\n                continue\n\n            if (\n                \"utcTimeMillis\" not in gnss.columns\n                or \"UnixTimeMillis\" not in gt.columns\n            ):\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            if len(gnss) == 0 or len(gt) == 0:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Ensure feature columns exist\n            for c in feature_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n            agg[\"phone\"] = phone_id\n            agg[\"drive_id\"] = drive_id\n\n            gt_small = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n            gt_small[\"phone\"] = phone_id\n            gt_small[\"drive_id\"] = drive_id\n\n            merged = pd.merge(\n                gt_small,\n                agg,\n                on=[\"phone\", \"drive_id\", \"UnixTimeMillis\"],\n                how=\"inner\",\n            )\n            if len(merged) == 0:\n                continue\n\n            x_gt, y_gt, z_gt = llh_to_ecef(\n                merged[\"LatitudeDegrees\"].values,\n                merged[\"LongitudeDegrees\"].values,\n                np.zeros(len(merged)),\n            )\n            merged[\"x_gt\"] = x_gt\n            merged[\"y_gt\"] = y_gt\n            merged[\"z_gt\"] = z_gt\n            rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols):\n    \"\"\"\n    Aggregate GNSS per (phone, UnixTimeMillis) for all test drives,\n    then merge with sample_sub which must contain 'phone' and 'UnixTimeMillis'.\n\n    This implementation constructs the phone string \"<drive_id>_<phone_name>\"\n    to match how we built it in training, and assumes sample_sub uses this format.\n    \"\"\"\n    rows = []\n    for drive_dir in sorted(glob.glob(os.path.join(test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_name}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            if not os.path.exists(gnss_path):\n                continue\n            try:\n                gnss = pd.read_csv(gnss_path)\n            except Exception as e:\n                print(f\"Failed reading {gnss_path}: {e}\")\n                continue\n            if \"utcTimeMillis\" not in gnss.columns:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"utcTimeMillis\"], errors=\"coerce\"\n            )\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n            if len(gnss) == 0:\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            for c in feature_cols:\n                if c not in gnss.columns:\n                    gnss[c] = np.nan\n\n            agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n            if len(agg) == 0:\n                continue\n            agg[\"phone\"] = phone_id\n            rows.append(agg)\n\n    if rows:\n        test_gnss = pd.concat(rows, ignore_index=True)\n    else:\n        test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"] + feature_cols)\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n    sample[\"phone\"] = sample[\"phone\"].astype(str)\n\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Simple baseline feature set from GNSS\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    print(\"Loading training data...\")\n    train_df = load_train(train_root, feature_cols)\n    print(f\"Train rows assembled: {len(train_df)}\")\n\n    # Ensure feature columns present and fill NaNs with medians\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    groups = train_df[\"drive_id\"].values\n\n    # 5-fold GroupKFold CV\n    print(\"Running 5-fold GroupKFold cross-validation...\")\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    fold_scores = []\n    for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n        lat_val_pred, lon_val_pred, _ = ecef_to_llh(\n            oof[val_idx, 0], oof[val_idx, 1], oof[val_idx, 2]\n        )\n        eval_df_fold = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"UnixTimeMillis\": train_df.iloc[val_idx][\"UnixTimeMillis\"].values,\n                \"lat_pred\": lat_val_pred,\n                \"lon_pred\": lon_val_pred,\n                \"lat_gt\": train_df.iloc[val_idx][\"LatitudeDegrees\"].values,\n                \"lon_gt\": train_df.iloc[val_idx][\"LongitudeDegrees\"].values,\n            }\n        )\n        fold_metric = competition_metric(eval_df_fold)\n        fold_scores.append(fold_metric)\n        print(f\"Fold {fold_idx} competition metric: {fold_metric:.4f}\")\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"OOF competition metric (mean of P50/P95 errors):\", metric_val)\n    valid_fold_scores = [s for s in fold_scores if not np.isnan(s)]\n    if valid_fold_scores:\n        mean_cv = float(np.mean(valid_fold_scores))\n        print(\"Mean CV metric across folds:\", mean_cv)\n\n    # Train final models on full data\n    print(\"Training final models on full training data...\")\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # Prepare test and submission\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    if \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n        raise RuntimeError(\n            \"sample_submission.csv must contain 'phone' and 'UnixTimeMillis' columns.\"\n        )\n\n    sample_sub = sample_sub.copy()\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    print(\"Loading and merging test GNSS features...\")\n    test_merged = load_test(\n        test_root,\n        sample_sub[[\"phone\", \"UnixTimeMillis\", \"row_id\"]],\n        feature_cols,\n    )\n\n    # Ensure row_id is preserved\n    if \"row_id\" not in test_merged.columns:\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    # Ensure feature columns in test and fill with train medians\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    test_merged = test_merged.sort_values(\"row_id\").reset_index(drop=True)\n    X_test = test_merged[feature_cols].values\n\n    print(\"Predicting on test set...\")\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align with full sample_sub, handling any rows that might not have matched GNSS\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill missing predictions by forward/backward fill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still NaNs (e.g. if all were NaN), fallback to train mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"].values,\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"].values,\n        }\n    )\n\n    # Save submission files\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Try to import lightgbm; if unavailable, fall back to RandomForest\ntry:\n    import lightgbm as lgb\n\n    HAS_LGB = True\nexcept Exception:\n    HAS_LGB = False\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\n\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_phone(sample_phone):\n    if \"_\" not in sample_phone:\n        return None, None\n    collection, model = sample_phone.split(\"_\", 1)\n    return collection, model\n\n\ndef list_ground_truth_train():\n    rows = []\n    pattern = os.path.join(INPUT_DIR, \"train\", \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    for fpath in files:\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n        if \"UnixTimeMillis\" not in df.columns:\n            continue\n        parts = fpath.replace(\"\\\\\", \"/\").split(\"/\")\n        if len(parts) < 4:\n            continue\n        collection = parts[-3]\n        device_dir = parts[-2]\n        phone = f\"{collection}_{device_dir}\"\n        sub = df[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n        sub[\"UnixTimeMillis\"] = sub[\"UnixTimeMillis\"].astype(\"int64\")\n        sub[\"phone\"] = phone\n        rows.append(sub)\n    if not rows:\n        return pd.DataFrame(\n            columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        )\n    out = pd.concat(rows, ignore_index=True)\n    out[\"phone\"] = out[\"phone\"].astype(str)\n    out[\"UnixTimeMillis\"] = out[\"UnixTimeMillis\"].astype(\"int64\")\n    return out\n\n\ndef load_gnss_agg_for_device(split, collection, device_dir, phone_name):\n    fpath = os.path.join(INPUT_DIR, split, collection, device_dir, \"device_gnss.csv\")\n    if not os.path.exists(fpath):\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n    try:\n        df = pd.read_csv(fpath)\n    except Exception:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    if \"UnixTimeMillis\" in df.columns:\n        time_col = \"UnixTimeMillis\"\n    elif \"utcTimeMillis\" in df.columns:\n        time_col = \"utcTimeMillis\"\n    else:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    df[\"UnixTimeMillis\"] = df[time_col].astype(\"int64\")\n    df[\"phone\"] = phone_name\n\n    skip_cols = {\n        time_col,\n        \"UnixTimeMillis\",\n        \"phone\",\n        \"MessageType\",\n        \"SignalType\",\n        \"CodeType\",\n        \"LatitudeDegrees\",\n        \"LongitudeDegrees\",\n    }\n\n    feature_cols = [\n        c\n        for c in df.columns\n        if c not in skip_cols and pd.api.types.is_numeric_dtype(df[c])\n    ]\n    if not feature_cols:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n\n    group_cols = [\"phone\", \"UnixTimeMillis\"]\n    agg_dict = {c: \"mean\" for c in feature_cols}\n    df_small = df[group_cols + feature_cols].copy()\n    g = df_small.groupby(group_cols, as_index=False).agg(agg_dict)\n    return g\n\n\ndef build_gnss_features(split, phones):\n    all_feats = []\n    for phone in phones:\n        collection, device_dir = parse_phone(phone)\n        if collection is None:\n            continue\n        g = load_gnss_agg_for_device(split, collection, device_dir, phone)\n        if not g.empty:\n            all_feats.append(g)\n    if not all_feats:\n        return pd.DataFrame(columns=[\"phone\", \"UnixTimeMillis\"])\n    feats = pd.concat(all_feats, ignore_index=True)\n    feats[\"phone\"] = feats[\"phone\"].astype(str)\n    feats[\"UnixTimeMillis\"] = feats[\"UnixTimeMillis\"].astype(\"int64\")\n    return feats\n\n\ndef prepare_basic_time_features(df):\n    df = df.copy()\n    t = df[\"UnixTimeMillis\"]\n    df[\"seconds\"] = (t // 1000).astype(\"int64\")\n    df[\"time_sin_1h\"] = np.sin(2 * np.pi * (df[\"seconds\"] % 3600) / 3600.0)\n    df[\"time_cos_1h\"] = np.cos(2 * np.pi * (df[\"seconds\"] % 3600) / 3600.0)\n    return df\n\n\ndef prepare_train_test(sample_sub):\n    # load train targets\n    train_targets = list_ground_truth_train()\n    if train_targets.empty:\n        raise RuntimeError(\"No ground_truth found in train; cannot train model.\")\n\n    train_targets[\"phone\"] = train_targets[\"phone\"].astype(str)\n    train_targets[\"UnixTimeMillis\"] = train_targets[\"UnixTimeMillis\"].astype(\"int64\")\n\n    # basic time features\n    train_df = prepare_basic_time_features(train_targets)\n\n    # try to add GNSS aggregated features (if any)\n    phones_train = train_targets[\"phone\"].unique()\n    gnss_train = build_gnss_features(\"train\", phones_train)\n    if not gnss_train.empty:\n        train_df = train_df.merge(\n            gnss_train, on=[\"phone\", \"UnixTimeMillis\"], how=\"left\", sort=False\n        )\n\n    # prepare test base from sample_sub\n    test_df = sample_sub.copy()\n    test_df[\"phone\"] = test_df[\"phone\"].astype(str)\n    test_df[\"UnixTimeMillis\"] = test_df[\"UnixTimeMillis\"].astype(\"int64\")\n    test_df = prepare_basic_time_features(test_df)\n\n    phones_test = test_df[\"phone\"].unique()\n    gnss_test = build_gnss_features(\"test\", phones_test)\n    if not gnss_test.empty:\n        test_df = test_df.merge(\n            gnss_test, on=[\"phone\", \"UnixTimeMillis\"], how=\"left\", sort=False\n        )\n\n    return train_df, test_df\n\n\ndef prepare_xy(train_df, test_df):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n\n    # encode phone as categorical\n    le_phone = LabelEncoder()\n    train_df = train_df.copy()\n    test_df = test_df.copy()\n    train_df[\"phone_enc\"] = le_phone.fit_transform(train_df[\"phone\"].astype(str))\n    test_df[\"phone_enc\"] = le_phone.transform(\n        np.where(\n            np.isin(test_df[\"phone\"].astype(str), le_phone.classes_),\n            test_df[\"phone\"].astype(str),\n            le_phone.classes_[0],\n        )\n    )\n\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n    if not feature_cols:\n        # as a last resort, use only encoded phone and time\n        if \"phone_enc\" not in train_df.columns:\n            train_df[\"phone_enc\"] = le_phone.fit_transform(\n                train_df[\"phone\"].astype(str)\n            )\n            test_df[\"phone_enc\"] = le_phone.transform(\n                np.where(\n                    np.isin(test_df[\"phone\"].astype(str), le_phone.classes_),\n                    test_df[\"phone\"].astype(str),\n                    le_phone.classes_[0],\n                )\n            )\n        train_df[\"seconds\"] = (train_df[\"UnixTimeMillis\"] // 1000).astype(\"int64\")\n        test_df[\"seconds\"] = (test_df[\"UnixTimeMillis\"] // 1000).astype(\"int64\")\n        feature_cols = [\"phone_enc\", \"seconds\"]\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # align test features\n    for c in feature_cols:\n        if c not in test_df.columns:\n            test_df[c] = np.nan\n    X_test = test_df[feature_cols].copy()\n\n    # simple median imputation\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols\n\n\ndef run_cv(X, y_lat, y_lon, phones):\n    phones = phones.astype(str)\n    collections = np.array([p.split(\"_\")[0] for p in phones])\n    unique_groups = np.unique(collections)\n    n_splits = 5\n    if len(unique_groups) < n_splits:\n        n_splits = max(2, len(unique_groups))\n    splitter = GroupKFold(n_splits=n_splits)\n\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=collections):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        if HAS_LGB:\n            params = {\n                \"objective\": \"regression\",\n                \"metric\": \"rmse\",\n                \"learning_rate\": 0.05,\n                \"num_leaves\": 31,\n                \"feature_fraction\": 0.8,\n                \"bagging_fraction\": 0.8,\n                \"bagging_freq\": 1,\n                \"seed\": 42,\n                \"verbose\": -1,\n            }\n            dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n            dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n            model_lat = lgb.train(\n                params,\n                dtrain_lat,\n                num_boost_round=200,\n                valid_sets=[dvalid_lat],\n                early_stopping_rounds=20,\n                verbose_eval=False,\n            )\n\n            dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n            dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n            model_lon = lgb.train(\n                params,\n                dtrain_lon,\n                num_boost_round=200,\n                valid_sets=[dvalid_lon],\n                early_stopping_rounds=20,\n                verbose_eval=False,\n            )\n\n            pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n            pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n        else:\n            model_lat = RandomForestRegressor(\n                n_estimators=100, random_state=42, n_jobs=-1\n            )\n            model_lon = RandomForestRegressor(\n                n_estimators=100, random_state=42, n_jobs=-1\n            )\n            model_lat.fit(X_tr, y_lat_tr)\n            model_lon.fit(X_tr, y_lon_tr)\n            pred_lat = model_lat.predict(X_va)\n            pred_lon = model_lon.predict(X_va)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones[va_idx],\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    if HAS_LGB:\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n        dtrain_lat = lgb.Dataset(X, label=y_lat)\n        dtrain_lon = lgb.Dataset(X, label=y_lon)\n        model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n        model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n        return model_lat, model_lon\n    else:\n        model_lat = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n        model_lon = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n        model_lat.fit(X, y_lat)\n        model_lon.fit(X, y_lon)\n        return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    print(\"Preparing train and test data...\")\n    train_df, test_df = prepare_train_test(sample_sub)\n\n    print(\"Building feature matrices...\")\n    X_train, y_lat, y_lon, X_test, feature_cols = prepare_xy(train_df, test_df)\n\n    print(\"Running GroupKFold CV...\")\n    cv_score = run_cv(X_train, y_lat, y_lon, train_df[\"phone\"].values)\n    print(f\"Hold-out CV metric: {cv_score}\")\n\n    print(\"Training full models...\")\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Baseline: per-phone and global mean from train targets\n    train_baseline = (\n        train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        .mean()\n        .rename(columns={\"LatitudeDegrees\": \"mean_lat\", \"LongitudeDegrees\": \"mean_lon\"})\n    )\n    global_mean_lat = train_df[\"LatitudeDegrees\"].mean()\n    global_mean_lon = train_df[\"LongitudeDegrees\"].mean()\n\n    print(\"Predicting on test set and building submission...\")\n    submission = sample_sub.copy()\n\n    try:\n        if HAS_LGB:\n            pred_lat_test = model_lat.predict(X_test)\n            pred_lon_test = model_lon.predict(X_test)\n        else:\n            pred_lat_test = model_lat.predict(X_test)\n            pred_lon_test = model_lon.predict(X_test)\n        submission[\"LatitudeDegrees\"] = pred_lat_test.astype(float)\n        submission[\"LongitudeDegrees\"] = pred_lon_test.astype(float)\n    except Exception as e:\n        print(f\"Model prediction failed: {e}. Falling back to baseline.\")\n        submission = submission.merge(train_baseline, how=\"left\", on=\"phone\")\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Ensure prediction columns exist\n    if (\n        \"LatitudeDegrees\" not in submission.columns\n        or \"LongitudeDegrees\" not in submission.columns\n    ):\n        print(\"Prediction columns missing; rebuilding via baseline only.\")\n        submission = sample_sub.copy()\n        submission = submission.merge(train_baseline, how=\"left\", on=\"phone\")\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Final NaN guard\n    if (\n        submission[\"LatitudeDegrees\"].isna().any()\n        or submission[\"LongitudeDegrees\"].isna().any()\n    ):\n        print(\"NaNs detected in predictions; filling with per-phone and global means.\")\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n            suffixes=(\"\", \"_base\"),\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n            submission.get(\"mean_lat\", global_mean_lat)\n        )\n        submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n            submission.get(\"mean_lon\", global_mean_lon)\n        )\n        submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n            global_mean_lat\n        )\n        submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n            global_mean_lon\n        )\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols].copy()\n    submission[\"phone\"] = submission[\"phone\"].astype(str)\n    submission[\"UnixTimeMillis\"] = submission[\"UnixTimeMillis\"].astype(\"int64\")\n    submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].astype(float)\n    submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].astype(float)\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n    print(f\"Final CV metric: {cv_score}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\n# Directories\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\n\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 ellipsoid constants\n    a = 6378137.0\n    b = 6356752.314245\n    e2 = 6.69437999014e-3\n    ep2 = (a * a - b * b) / (b * b)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef load_train_with_gt(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"Train directory not found; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    all_rows = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} due to read error: {e}\", flush=True)\n                continue\n\n            # Normalize UnixTimeMillis column name\n            if \"UnixTimeMillis\" not in gnss.columns:\n                if \"utcTimeMillis\" in gnss.columns:\n                    gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n            if \"UnixTimeMillis\" not in gt.columns:\n                if \"utcTimeMillis\" in gt.columns:\n                    gt = gt.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Aggregate WLS ECEF per second\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No train rows with GT assembled; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    print(f\"Loaded train with GT: {len(train_df)} rows\", flush=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    train_df = train_df.copy()\n\n    ecef_x = train_df[\"WlsPositionXEcefMeters\"].values\n    ecef_y = train_df[\"WlsPositionYEcefMeters\"].values\n    ecef_z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(ecef_x, ecef_y, ecef_z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X.values, y_lat)\n        rf_lon.fit(X.values, y_lon)\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n        print(\"Not enough groups for CV, trained single correction model.\", flush=True)\n        return lat_models, lon_models, feature_cols, np.nan, train_df_prepared\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=120, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols].values, y_lat)\n    rf_lon.fit(X[feature_cols].values, y_lon)\n    return rf_lat, rf_lon\n\n\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"UnixTimeMillis\" not in gnss.columns:\n                    if \"utcTimeMillis\" in gnss.columns:\n                        gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                    else:\n                        continue\n\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\", as_index=False)\n                    .mean()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load train and train model with CV\n    train_df = load_train_with_gt(INPUT_DIR)\n    if train_df.empty:\n        correction_model_lat = None\n        correction_model_lon = None\n        feature_cols = None\n        val_metric = np.nan\n        print(\n            \"No training data available; using baseline WLS/mean positions only.\",\n            flush=True,\n        )\n    else:\n        lat_models, lon_models, feature_cols, val_metric, train_df_prepared = (\n            train_with_cv(train_df)\n        )\n        print(f\"Cross-validation competition metric: {val_metric}\", flush=True)\n\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    # Print final evaluation metric explicitly\n    print(\n        \"Final validation metric (mean (P50+P95)/2 over phones):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Fill ECEF per phone (ffill/bfill) to reduce gaps\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        if coord in merged.columns:\n            merged[coord] = merged.groupby(\"phone\")[coord].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    # Recompute base lat/lon after filling ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global mean\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    if np.isnan(overall_base_lat) or np.isnan(overall_base_lon):\n        # As a very last fallback if everything is missing\n        overall_base_lat = 0.0\n        overall_base_lon = 0.0\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Apply correction model if available\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols].values)\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols].values)\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned to sample_sub\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    # Fill missing predictions per phone then global\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    if np.isnan(overall_lat) or np.isnan(overall_lon):\n        overall_lat = 0.0\n        overall_lon = 0.0\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\n# Directories\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\n\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 ellipsoid constants\n    a = 6378137.0\n    b = 6356752.314245\n    e2 = 6.69437999014e-3\n    ep2 = (a * a - b * b) / (b * b)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef load_train_with_gt(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"Train directory not found; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    all_rows = []\n\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} due to read error: {e}\", flush=True)\n                continue\n\n            # Normalize UnixTimeMillis column name\n            if \"UnixTimeMillis\" not in gnss.columns:\n                if \"utcTimeMillis\" in gnss.columns:\n                    gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n            if \"UnixTimeMillis\" not in gt.columns:\n                if \"utcTimeMillis\" in gt.columns:\n                    gt = gt.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Aggregate WLS ECEF per second\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No train rows with GT assembled; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    print(f\"Loaded train with GT: {len(train_df)} rows\", flush=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    train_df = train_df.copy()\n\n    ecef_x = train_df[\"WlsPositionXEcefMeters\"].values\n    ecef_y = train_df[\"WlsPositionYEcefMeters\"].values\n    ecef_z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(ecef_x, ecef_y, ecef_z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X.values, y_lat)\n        rf_lon.fit(X.values, y_lon)\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n        print(\"Not enough groups for CV, trained single correction model.\", flush=True)\n        return lat_models, lon_models, feature_cols, np.nan, train_df_prepared\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=100, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=100, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols].values, y_lat)\n    rf_lon.fit(X[feature_cols].values, y_lon)\n    return rf_lat, rf_lon\n\n\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"UnixTimeMillis\" not in gnss.columns:\n                    if \"utcTimeMillis\" in gnss.columns:\n                        gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                    else:\n                        continue\n\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\", as_index=False)\n                    .mean()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load train and train model with CV\n    train_df = load_train_with_gt(INPUT_DIR)\n    if train_df.empty:\n        correction_model_lat = None\n        correction_model_lon = None\n        feature_cols = None\n        val_metric = np.nan\n        print(\n            \"No training data available; using baseline WLS/mean positions only.\",\n            flush=True,\n        )\n    else:\n        lat_models, lon_models, feature_cols, val_metric, train_df_prepared = (\n            train_with_cv(train_df)\n        )\n        print(f\"Cross-validation competition metric: {val_metric}\", flush=True)\n\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    # Print final evaluation metric explicitly\n    print(\n        \"Final validation metric (mean (P50+P95)/2 over phones):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Fill ECEF per phone (ffill/bfill) to reduce gaps\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        if coord in merged.columns:\n            merged[coord] = merged.groupby(\"phone\")[coord].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    # Recompute base lat/lon after filling ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global mean\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    if np.isnan(overall_base_lat) or np.isnan(overall_base_lon):\n        # As a very last fallback if everything is missing\n        overall_base_lat = 0.0\n        overall_base_lon = 0.0\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Apply correction model if available\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols].values)\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols].values)\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned to sample_sub\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    # Fill missing predictions per phone then global\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    if np.isnan(overall_lat) or np.isnan(overall_lon):\n        overall_lat = 0.0\n        overall_lon = 0.0\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\", flush=True\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ---------------- Coordinate transforms ----------------\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ---------------- Metric ----------------\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ---------------- Helpers for phone naming ----------------\ndef build_phone_mapping_from_sample(sample_sub):\n    \"\"\"\n    Build mapping between (drive_id, phone_name) and the 'phone' string used\n    in sample_submission, and vice versa.\n\n    The sample typically uses 'drive-phone' (dash) whereas folders are 'drive/phone'.\n    We'll infer drive and phone names by splitting on the last '-' in the phone string.\n    \"\"\"\n    phone_values = sample_sub[\"phone\"].astype(str).unique()\n    drive_phone_to_sample_phone = {}\n    sample_phone_to_drive_phone = {}\n\n    for p in phone_values:\n        # Split on last '-' to separate drive and phone name\n        if \"-\" in p:\n            drive_id, phone_name = p.rsplit(\"-\", 1)\n        else:\n            # Fallback: cannot split, use entire as drive and dummy phone\n            drive_id = p\n            phone_name = \"\"\n        key = (drive_id, phone_name)\n        drive_phone_to_sample_phone[key] = p\n        sample_phone_to_drive_phone[p] = key\n    return drive_phone_to_sample_phone, sample_phone_to_drive_phone\n\n\ndef folder_drive_phone_keys(train_or_test_root):\n    \"\"\"\n    Enumerate existing (drive_id, phone_name) pairs from folder structure.\n    Returns a set of (drive_id, phone_name).\n    \"\"\"\n    keys = set()\n    for drive_dir in sorted(glob.glob(os.path.join(train_or_test_root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            keys.add((drive_id, phone_name))\n    return keys\n\n\n# ---------------- Data loaders ----------------\ndef load_train(train_root, feature_cols, drive_phone_to_sample_phone):\n    \"\"\"\n    Build training dataframe with features aggregated per (drive_id, phone_name, UnixTimeMillis),\n    joined with ground truth lat/lon and converted to ECEF.\n    Uses mapping to assign the correct 'phone' string used in sample_submission.\n    \"\"\"\n    rows = []\n    folder_keys = folder_drive_phone_keys(train_root)\n\n    for drive_id, phone_name in sorted(folder_keys):\n        phone_dir = os.path.join(train_root, drive_id, phone_name)\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n        if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n            continue\n\n        try:\n            gnss = pd.read_csv(gnss_path)\n            gt = pd.read_csv(gt_path)\n        except Exception as e:\n            print(f\"Failed reading {gnss_path} or {gt_path}: {e}\")\n            continue\n\n        if \"utcTimeMillis\" not in gnss.columns or \"UnixTimeMillis\" not in gt.columns:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"utcTimeMillis\"], errors=\"coerce\")\n        gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        if len(gnss) == 0 or len(gt) == 0:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n        gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n        for c in feature_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n        if len(agg) == 0:\n            continue\n\n        gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n\n        merged = pd.merge(gt_small, agg, on=\"UnixTimeMillis\", how=\"inner\")\n        if len(merged) == 0:\n            continue\n\n        # assign mapping phone string if available, else fallback to 'drive-phone'\n        key = (drive_id, phone_name)\n        phone_str = drive_phone_to_sample_phone.get(key, f\"{drive_id}-{phone_name}\")\n        merged[\"phone\"] = phone_str\n        merged[\"drive_id\"] = drive_id\n\n        x_gt, y_gt, z_gt = llh_to_ecef(\n            merged[\"LatitudeDegrees\"].values,\n            merged[\"LongitudeDegrees\"].values,\n            np.zeros(len(merged)),\n        )\n        merged[\"x_gt\"] = x_gt\n        merged[\"y_gt\"] = y_gt\n        merged[\"z_gt\"] = z_gt\n        rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols, sample_phone_to_drive_phone):\n    \"\"\"\n    Aggregate GNSS per (drive_id, phone_name, UnixTimeMillis) for all test drives,\n    then merge with sample_sub which must contain 'phone', 'UnixTimeMillis', 'row_id'.\n\n    Uses mapping from sample 'phone' strings to (drive_id, phone_name).\n    \"\"\"\n    # Build GNSS aggregated per (drive_id, phone_name, UnixTimeMillis)\n    rows = []\n    folder_keys = folder_drive_phone_keys(test_root)\n\n    for drive_id, phone_name in sorted(folder_keys):\n        phone_dir = os.path.join(test_root, drive_id, phone_name)\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            continue\n        try:\n            gnss = pd.read_csv(gnss_path)\n        except Exception as e:\n            print(f\"Failed reading {gnss_path}: {e}\")\n            continue\n        if \"utcTimeMillis\" not in gnss.columns:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"utcTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        if len(gnss) == 0:\n            continue\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n        for c in feature_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n        if len(agg) == 0:\n            continue\n        agg[\"drive_id\"] = drive_id\n        agg[\"phone_name\"] = phone_name\n        rows.append(agg)\n\n    if rows:\n        test_gnss = pd.concat(rows, ignore_index=True)\n    else:\n        test_gnss = pd.DataFrame(\n            columns=[\"UnixTimeMillis\", \"drive_id\", \"phone_name\"] + feature_cols\n        )\n\n    # Map sample phones to (drive_id, phone_name)\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n    sample[\"phone\"] = sample[\"phone\"].astype(str)\n\n    drive_ids = []\n    phone_names = []\n    for p in sample[\"phone\"].values:\n        drive_id, phone_name = sample_phone_to_drive_phone.get(p, (None, None))\n        drive_ids.append(drive_id)\n        phone_names.append(phone_name)\n    sample[\"drive_id\"] = drive_ids\n    sample[\"phone_name\"] = phone_names\n\n    # Drop rows where mapping failed (should be none if mapping is correct)\n    valid_mask = sample[\"drive_id\"].notna() & sample[\"phone_name\"].notna()\n    if not valid_mask.all():\n        # Keep them but they won't match any gnss rows; features later filled by medians\n        pass\n\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"drive_id\", \"phone_name\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Simple baseline feature set from GNSS\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    # Read sample_submission first to infer phone naming\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n    if \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n        raise RuntimeError(\n            \"sample_submission.csv must contain 'phone' and 'UnixTimeMillis' columns.\"\n        )\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n\n    drive_phone_to_sample_phone, sample_phone_to_drive_phone = (\n        build_phone_mapping_from_sample(sample_sub)\n    )\n\n    print(\"Loading training data...\")\n    train_df = load_train(train_root, feature_cols, drive_phone_to_sample_phone)\n    print(f\"Train rows assembled: {len(train_df)}\")\n\n    # Ensure feature columns present and fill NaNs with medians\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    groups = train_df[\"drive_id\"].values\n\n    # 5-fold GroupKFold CV\n    print(\"Running 5-fold GroupKFold cross-validation...\")\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    fold_scores = []\n    for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n        lat_val_pred, lon_val_pred, _ = ecef_to_llh(\n            oof[val_idx, 0], oof[val_idx, 1], oof[val_idx, 2]\n        )\n        eval_df_fold = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"UnixTimeMillis\": train_df.iloc[val_idx][\"UnixTimeMillis\"].values,\n                \"lat_pred\": lat_val_pred,\n                \"lon_pred\": lon_val_pred,\n                \"lat_gt\": train_df.iloc[val_idx][\"LatitudeDegrees\"].values,\n                \"lon_gt\": train_df.iloc[val_idx][\"LongitudeDegrees\"].values,\n            }\n        )\n        fold_metric = competition_metric(eval_df_fold)\n        fold_scores.append(fold_metric)\n        print(f\"Fold {fold_idx} competition metric: {fold_metric:.4f}\")\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"OOF competition metric (mean of P50/P95 errors):\", metric_val)\n    valid_fold_scores = [s for s in fold_scores if not np.isnan(s)]\n    if valid_fold_scores:\n        mean_cv = float(np.mean(valid_fold_scores))\n        print(\"Mean CV metric across folds:\", mean_cv)\n\n    # Train final models on full data\n    print(\"Training final models on full training data...\")\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # Prepare test and submission\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    print(\"Loading and merging test GNSS features...\")\n    test_merged = load_test(\n        test_root,\n        sample_sub[[\"phone\", \"UnixTimeMillis\", \"row_id\"]],\n        feature_cols,\n        sample_phone_to_drive_phone,\n    )\n\n    # Ensure row_id is preserved\n    if \"row_id\" not in test_merged.columns:\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    # Ensure feature columns in test and fill with train medians\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    test_merged = test_merged.sort_values(\"row_id\").reset_index(drop=True)\n    X_test = test_merged[feature_cols].values\n\n    print(\"Predicting on test set...\")\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align with full sample_sub, handling any rows that might not have matched GNSS\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill missing predictions by forward/backward fill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still NaNs (e.g. if all were NaN), fallback to train mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"].values,\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"].values,\n        }\n    )\n\n    # Save submission files\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\n# Directories (per benchmark)\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\n\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 ellipsoid constants\n    a = 6378137.0\n    b = 6356752.314245\n    e2 = 6.69437999014e-3\n    ep2 = (a * a - b * b) / (b * b)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef load_train_with_gt(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"Train directory not found; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    all_rows = []\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} due to read error: {e}\", flush=True)\n                continue\n\n            if \"UnixTimeMillis\" not in gnss.columns:\n                if \"utcTimeMillis\" in gnss.columns:\n                    gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n            if \"UnixTimeMillis\" not in gt.columns:\n                if \"utcTimeMillis\" in gt.columns:\n                    gt = gt.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                else:\n                    continue\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Aggregate GNSS positions per UnixTimeMillis\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No train rows with GT assembled; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    print(f\"Loaded train with GT: {len(train_df)} rows\", flush=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    train_df = train_df.copy()\n\n    ecef_x = train_df[\"WlsPositionXEcefMeters\"].values\n    ecef_y = train_df[\"WlsPositionYEcefMeters\"].values\n    ecef_z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(ecef_x, ecef_y, ecef_z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    # Features: time and ECEF\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    if n_splits < 2:\n        # Train single model without CV\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X.values, y_lat)\n        rf_lon.fit(X.values, y_lon)\n        print(\"Not enough groups for CV; trained single correction model.\", flush=True)\n        return [rf_lat], [rf_lon], feature_cols, np.nan, train_df_prepared\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=100, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=100, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols].values, y_lat)\n    rf_lon.fit(X[feature_cols].values, y_lon)\n    return rf_lat, rf_lon\n\n\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"UnixTimeMillis\" not in gnss.columns:\n                    if \"utcTimeMillis\" in gnss.columns:\n                        gnss = gnss.rename(columns={\"utcTimeMillis\": \"UnixTimeMillis\"})\n                    else:\n                        continue\n\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\", as_index=False)\n                    .mean()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    # Compute base lat/lon from ECEF if available\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load train and train model with CV\n    train_df = load_train_with_gt(INPUT_DIR)\n    if train_df.empty:\n        correction_model_lat = None\n        correction_model_lon = None\n        feature_cols = None\n        val_metric = np.nan\n        print(\n            \"No training data available; using baseline WLS/mean positions only.\",\n            flush=True,\n        )\n    else:\n        lat_models, lon_models, feature_cols, val_metric, train_df_prepared = (\n            train_with_cv(train_df)\n        )\n        print(\n            \"Cross-validation competition metric (mean (P50+P95)/2 over phones):\",\n            val_metric,\n            flush=True,\n        )\n        # Train final model on full data\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    # Explicitly print final evaluation metric\n    print(\n        \"Final validation metric (mean (P50+P95)/2 over phones):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Fill ECEF per phone (ffill/bfill) to reduce gaps\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        if coord in merged.columns:\n            merged[coord] = merged.groupby(\"phone\")[coord].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    # Recompute base lat/lon after filling ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global mean\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    if np.isnan(overall_base_lat) or np.isnan(overall_base_lon):\n        overall_base_lat = 0.0\n        overall_base_lon = 0.0\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Apply correction model if available\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols].values)\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols].values)\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned to sample_sub\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    # Fill missing predictions per phone then global\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    if np.isnan(overall_lat) or np.isnan(overall_lon):\n        overall_lat = 0.0\n        overall_lon = 0.0\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    # Save to both required locations\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\",\n        flush=True,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Try LightGBM; fall back to RF if unavailable\ntry:\n    import lightgbm as lgb\n\n    HAS_LGB = True\nexcept Exception:\n    HAS_LGB = False\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\n\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef parse_phone_from_path(path):\n    # .../train/<collection>/<device>/ground_truth.csv  ->  collection_device\n    parts = path.replace(\"\\\\\", \"/\").split(\"/\")\n    # expect [..., 'train', collection, device, 'ground_truth.csv']\n    if len(parts) < 4:\n        return None\n    collection = parts[-3]\n    device = parts[-2]\n    return f\"{collection}_{device}\"\n\n\ndef load_train_ground_truth():\n    pattern = os.path.join(INPUT_DIR, \"train\", \"*\", \"*\", \"ground_truth.csv\")\n    files = glob.glob(pattern)\n    rows = []\n    for fpath in files:\n        phone = parse_phone_from_path(fpath)\n        if phone is None:\n            continue\n        try:\n            df = pd.read_csv(fpath)\n        except Exception:\n            continue\n        if (\n            \"UnixTimeMillis\" not in df.columns\n            or \"LatitudeDegrees\" not in df.columns\n            or \"LongitudeDegrees\" not in df.columns\n        ):\n            continue\n        sub = df[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n        # robust type casting\n        sub = sub.dropna(subset=[\"UnixTimeMillis\"])\n        if sub.empty:\n            continue\n        sub[\"UnixTimeMillis\"] = sub[\"UnixTimeMillis\"].astype(\"int64\")\n        sub[\"phone\"] = str(phone)\n        rows.append(sub)\n    if not rows:\n        return pd.DataFrame(\n            columns=[\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n        )\n    out = pd.concat(rows, ignore_index=True)\n    out[\"phone\"] = out[\"phone\"].astype(str)\n    out[\"UnixTimeMillis\"] = out[\"UnixTimeMillis\"].astype(\"int64\")\n    return out\n\n\ndef prepare_basic_time_features(df):\n    df = df.copy()\n    t = df[\"UnixTimeMillis\"].astype(\"int64\")\n    df[\"seconds\"] = (t // 1000).astype(\"int64\")\n    df[\"time_sin_1h\"] = np.sin(2 * np.pi * (df[\"seconds\"] % 3600) / 3600.0)\n    df[\"time_cos_1h\"] = np.cos(2 * np.pi * (df[\"seconds\"] % 3600) / 3600.0)\n    return df\n\n\ndef prepare_train_test(sample_sub):\n    train_targets = load_train_ground_truth()\n    if train_targets.empty:\n        raise RuntimeError(\"No valid ground_truth.csv files found under ./input/train\")\n\n    train_targets[\"phone\"] = train_targets[\"phone\"].astype(str)\n    train_targets[\"UnixTimeMillis\"] = train_targets[\"UnixTimeMillis\"].astype(\"int64\")\n    train_df = prepare_basic_time_features(train_targets)\n\n    test_df = sample_sub.copy()\n    test_df[\"phone\"] = test_df[\"phone\"].astype(str)\n    test_df[\"UnixTimeMillis\"] = test_df[\"UnixTimeMillis\"].astype(\"int64\")\n    test_df = prepare_basic_time_features(test_df)\n\n    return train_df, test_df\n\n\ndef prepare_xy(train_df, test_df):\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n\n    le_phone = LabelEncoder()\n    train_df = train_df.copy()\n    test_df = test_df.copy()\n\n    train_df[\"phone_enc\"] = le_phone.fit_transform(train_df[\"phone\"].astype(str))\n\n    # Map unknown test phones to a special index (len(classes_))\n    phone_classes = list(le_phone.classes_)\n    phone_to_int = {p: i for i, p in enumerate(phone_classes)}\n    unk_index = len(phone_classes)\n    test_df[\"phone_enc\"] = [\n        phone_to_int.get(p, unk_index) for p in test_df[\"phone\"].astype(str)\n    ]\n\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n    if not feature_cols:\n        # fallback minimal features\n        train_df[\"seconds\"] = (train_df[\"UnixTimeMillis\"] // 1000).astype(\"int64\")\n        test_df[\"seconds\"] = (test_df[\"UnixTimeMillis\"] // 1000).astype(\"int64\")\n        feature_cols = [\"phone_enc\", \"seconds\"]\n\n    X_train = train_df[feature_cols].copy()\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    # align test\n    for c in feature_cols:\n        if c not in test_df.columns:\n            test_df[c] = np.nan\n    X_test = test_df[feature_cols].copy()\n\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    return X_train, y_lat, y_lon, X_test, feature_cols\n\n\ndef run_cv(X, y_lat, y_lon, phones, unix_times):\n    phones = np.asarray(phones).astype(str)\n    # Group by collection (prefix before first \"_\")\n    collections = np.array([p.split(\"_\")[0] for p in phones])\n    uniq_groups = np.unique(collections)\n    n_splits = 5\n    if len(uniq_groups) < n_splits:\n        n_splits = max(2, len(uniq_groups))\n    splitter = GroupKFold(n_splits=n_splits)\n\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=collections):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        if HAS_LGB:\n            params = {\n                \"objective\": \"regression\",\n                \"metric\": \"rmse\",\n                \"learning_rate\": 0.05,\n                \"num_leaves\": 31,\n                \"feature_fraction\": 0.8,\n                \"bagging_fraction\": 0.8,\n                \"bagging_freq\": 1,\n                \"seed\": 42,\n                \"verbose\": -1,\n            }\n            dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n            dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n            model_lat = lgb.train(\n                params,\n                dtrain_lat,\n                num_boost_round=200,\n                valid_sets=[dvalid_lat],\n                early_stopping_rounds=20,\n                verbose_eval=False,\n            )\n\n            dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n            dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n            model_lon = lgb.train(\n                params,\n                dtrain_lon,\n                num_boost_round=200,\n                valid_sets=[dvalid_lon],\n                early_stopping_rounds=20,\n                verbose_eval=False,\n            )\n\n            pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n            pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n        else:\n            model_lat = RandomForestRegressor(\n                n_estimators=200, random_state=42, n_jobs=-1\n            )\n            model_lon = RandomForestRegressor(\n                n_estimators=200, random_state=42, n_jobs=-1\n            )\n            model_lat.fit(X_tr, y_lat_tr)\n            model_lon.fit(X_tr, y_lon_tr)\n            pred_lat = model_lat.predict(X_va)\n            pred_lon = model_lon.predict(X_va)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones[va_idx],\n                \"UnixTimeMillis\": unix_times[va_idx],\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    if HAS_LGB:\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n        dtrain_lat = lgb.Dataset(X, label=y_lat)\n        dtrain_lon = lgb.Dataset(X, label=y_lon)\n        model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n        model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n        return model_lat, model_lon\n    else:\n        model_lat = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n        model_lon = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n        model_lat.fit(X, y_lat)\n        model_lon.fit(X, y_lon)\n        return model_lat, model_lon\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(\"sample_submission.csv not found in ./input directory.\")\n    sample_sub = pd.read_csv(sample_path)\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n    sample_sub[\"UnixTimeMillis\"] = sample_sub[\"UnixTimeMillis\"].astype(\"int64\")\n\n    print(\"Loading and preparing train/test data...\")\n    train_df, test_df = prepare_train_test(sample_sub)\n\n    print(\"Building feature matrices...\")\n    X_train, y_lat, y_lon, X_test, feature_cols = prepare_xy(train_df, test_df)\n\n    print(\"Running 5-fold GroupKFold CV...\")\n    cv_score = run_cv(\n        X_train,\n        y_lat,\n        y_lon,\n        train_df[\"phone\"].values,\n        train_df[\"UnixTimeMillis\"].values,\n    )\n    print(f\"Hold-out CV metric: {cv_score}\")\n\n    print(\"Training full models on all training data...\")\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Baseline statistics\n    train_baseline = (\n        train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        .mean()\n        .rename(columns={\"LatitudeDegrees\": \"mean_lat\", \"LongitudeDegrees\": \"mean_lon\"})\n    )\n    global_mean_lat = train_df[\"LatitudeDegrees\"].mean()\n    global_mean_lon = train_df[\"LongitudeDegrees\"].mean()\n\n    print(\"Predicting on test set and building submission...\")\n    submission = sample_sub.copy()\n\n    try:\n        if HAS_LGB:\n            pred_lat_test = model_lat.predict(X_test)\n            pred_lon_test = model_lon.predict(X_test)\n        else:\n            pred_lat_test = model_lat.predict(X_test)\n            pred_lon_test = model_lon.predict(X_test)\n        submission[\"LatitudeDegrees\"] = pred_lat_test.astype(float)\n        submission[\"LongitudeDegrees\"] = pred_lon_test.astype(float)\n    except Exception as e:\n        print(f\"Model prediction failed ({e}); falling back to baseline.\")\n        submission = submission.merge(train_baseline, how=\"left\", on=\"phone\")\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Ensure prediction columns exist\n    if (\n        \"LatitudeDegrees\" not in submission.columns\n        or \"LongitudeDegrees\" not in submission.columns\n    ):\n        print(\"Prediction columns missing; rebuilding via baseline only.\")\n        submission = sample_sub.copy()\n        submission = submission.merge(train_baseline, how=\"left\", on=\"phone\")\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Final NaN guard\n    if (\n        submission[\"LatitudeDegrees\"].isna().any()\n        or submission[\"LongitudeDegrees\"].isna().any()\n    ):\n        print(\"NaNs detected in predictions; filling with per-phone and global means.\")\n        submission = submission.merge(\n            train_baseline,\n            how=\"left\",\n            on=\"phone\",\n            suffixes=(\"\", \"_base\"),\n        )\n        if \"LatitudeDegrees\" in submission.columns:\n            submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n                submission.get(\"mean_lat\", global_mean_lat)\n            )\n        else:\n            submission[\"LatitudeDegrees\"] = submission.get(\"mean_lat\", global_mean_lat)\n\n        if \"LongitudeDegrees\" in submission.columns:\n            submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n                submission.get(\"mean_lon\", global_mean_lon)\n            )\n        else:\n            submission[\"LongitudeDegrees\"] = submission.get(\"mean_lon\", global_mean_lon)\n\n        submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n            global_mean_lat\n        )\n        submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n            global_mean_lon\n        )\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols].copy()\n    submission[\"phone\"] = submission[\"phone\"].astype(str)\n    submission[\"UnixTimeMillis\"] = submission[\"UnixTimeMillis\"].astype(\"int64\")\n    submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].astype(float)\n    submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].astype(float)\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n    print(f\"Final CV metric: {cv_score}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 ellipsoid constants\n    a = 6378137.0\n    b = 6356752.314245\n    e2 = 6.69437999014e-3\n    ep2 = (a * a - b * b) / (b * b)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef load_train_with_gt(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"Train directory not found; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    all_rows = []\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    print(f\"Found {len(drive_dirs)} drive directories in train.\", flush=True)\n\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} due to read error: {e}\", flush=True)\n                continue\n\n            # Robust time column handling: prefer utcTimeMillis if present\n            time_col_gnss = None\n            if \"UnixTimeMillis\" in gnss.columns:\n                time_col_gnss = \"UnixTimeMillis\"\n            elif \"utcTimeMillis\" in gnss.columns:\n                time_col_gnss = \"utcTimeMillis\"\n            else:\n                print(\n                    f\"No usable time column in gnss {gnss_path}, skipping.\", flush=True\n                )\n                continue\n\n            time_col_gt = None\n            if \"UnixTimeMillis\" in gt.columns:\n                time_col_gt = \"UnixTimeMillis\"\n            elif \"utcTimeMillis\" in gt.columns:\n                time_col_gt = \"utcTimeMillis\"\n            else:\n                print(f\"No usable time column in gt {gt_path}, skipping.\", flush=True)\n                continue\n\n            gnss = gnss.rename(columns={time_col_gnss: \"UnixTimeMillis\"})\n            gt = gt.rename(columns={time_col_gt: \"UnixTimeMillis\"})\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                # Nothing to train on from this phone\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Aggregate GNSS positions per UnixTimeMillis\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n\n            if not set([\"LatitudeDegrees\", \"LongitudeDegrees\"]).issubset(gt.columns):\n                print(f\"GT missing lat/lon in {gt_path}, skipping.\", flush=True)\n                continue\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No train rows with GT assembled; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    print(f\"Loaded train with GT: {len(train_df)} rows\", flush=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    train_df = train_df.copy()\n\n    ecef_x = train_df[\"WlsPositionXEcefMeters\"].values\n    ecef_y = train_df[\"WlsPositionYEcefMeters\"].values\n    ecef_z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(ecef_x, ecef_y, ecef_z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    # Features: time and ECEF\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X.values, y_lat)\n        rf_lon.fit(X.values, y_lon)\n        print(\"Not enough groups for CV; trained single correction model.\", flush=True)\n        return [rf_lat], [rf_lon], feature_cols, np.nan, train_df_prepared\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=100, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=100, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols].values, y_lat)\n    rf_lon.fit(X[feature_cols].values, y_lon)\n    return rf_lat, rf_lon\n\n\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        print(f\"Found {len(drive_dirs)} drive directories in test.\", flush=True)\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                time_col = None\n                if \"UnixTimeMillis\" in gnss.columns:\n                    time_col = \"UnixTimeMillis\"\n                elif \"utcTimeMillis\" in gnss.columns:\n                    time_col = \"utcTimeMillis\"\n                else:\n                    print(\n                        f\"No usable time column in test gnss {gnss_path}, skipping.\",\n                        flush=True,\n                    )\n                    continue\n                gnss = gnss.rename(columns={time_col: \"UnixTimeMillis\"})\n\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\", as_index=False)\n                    .mean()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    # Compute base lat/lon from ECEF if available\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load train and train model with CV\n    train_df = load_train_with_gt(INPUT_DIR)\n    correction_model_lat = None\n    correction_model_lon = None\n    feature_cols = None\n    val_metric = np.nan\n\n    if train_df.empty:\n        print(\n            \"No training data available; using baseline WLS/mean positions only.\",\n            flush=True,\n        )\n    else:\n        lat_models, lon_models, feature_cols, val_metric, train_df_prepared = (\n            train_with_cv(train_df)\n        )\n        print(\n            \"Cross-validation competition metric (mean (P50+P95)/2 over phones):\",\n            val_metric,\n            flush=True,\n        )\n        # Train final model on full data\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    # Explicitly print final evaluation metric\n    print(\n        \"Final validation metric (mean (P50+P95)/2 over phones):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Fill ECEF per phone (ffill/bfill) to reduce gaps\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        if coord in merged.columns:\n            merged[coord] = merged.groupby(\"phone\")[coord].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    # Recompute base lat/lon after filling ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global mean\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    if np.isnan(overall_base_lat) or np.isnan(overall_base_lon):\n        # Fallback: if no ECEF at all, approximate from sample_sub (if any) or zero\n        if (\n            not merged.empty\n            and \"base_lat\" in merged.columns\n            and merged[\"base_lat\"].notna().any()\n        ):\n            overall_base_lat = merged[\"base_lat\"].dropna().mean()\n            overall_base_lon = merged[\"base_lon\"].dropna().mean()\n        else:\n            overall_base_lat = 0.0\n            overall_base_lon = 0.0\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Apply correction model if available\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols].values)\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols].values)\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned to sample_sub\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    # Fill missing predictions per phone then global\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    if np.isnan(overall_lat) or np.isnan(overall_lon):\n        overall_lat = 0.0\n        overall_lon = 0.0\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    # Save to required locations\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\",\n        flush=True,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ---------------- Coordinate transforms ----------------\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ---------------- Metric ----------------\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ---------------- Helpers for folder enumeration ----------------\ndef folder_drive_phone_keys(root):\n    keys = set()\n    for drive_dir in sorted(glob.glob(os.path.join(root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            keys.add((drive_id, phone_name))\n    return keys\n\n\n# ---------------- Phone mapping (robust) ----------------\ndef build_phone_mapping(sample_sub, train_root, test_root):\n    \"\"\"\n    Build mapping between filesystem (drive_id, phone_name) and sample 'phone' strings.\n\n    This competition's sample uses strings like '2020-06-04-US-MTV-1_Pixel4',\n    while folders are like '2020-06-04-US-MTV-1/GooglePixel4'.\n\n    We:\n      * enumerate all (drive_id, phone_name) from train and test\n      * for each sample phone, split at last '_' to get drive_id and a short model name\n      * map short model name to real folder phone_name by simple alias rules\n    \"\"\"\n    sample_phones = sample_sub[\"phone\"].astype(str).unique()\n\n    folder_keys = folder_drive_phone_keys(train_root) | folder_drive_phone_keys(\n        test_root\n    )\n\n    # Build index of drive_id -> available phone_names\n    drive_to_phones = {}\n    for drive_id, phone_name in folder_keys:\n        drive_to_phones.setdefault(drive_id, set()).add(phone_name)\n\n    # simple aliases from short model in sample to folder phone_name endings\n    alias_map = {\n        \"Pixel4\": \"GooglePixel4\",\n        \"Pixel4XL\": \"GooglePixel4XL\",\n        \"Pixel5\": \"GooglePixel5\",\n        \"Mi8\": \"XiaomiMi8\",\n        \"SamsungGalaxyS20Ultra\": \"SamsungGalaxyS20Ultra\",\n    }\n\n    drive_phone_to_sample = {}\n    sample_to_drive_phone = {}\n\n    for sp in sample_phones:\n        # Expected format: \"<drive_id>_<shortphone>\", where drive_id itself may contain '-'\n        if \"_\" in sp:\n            drive_part, short_model = sp.rsplit(\"_\", 1)\n        else:\n            # fallback: entire string as drive, unknown phone\n            drive_part, short_model = sp, None\n\n        drive_id = None\n        phone_name = None\n\n        # choose drive_id: exact match if exists\n        if drive_part in drive_to_phones:\n            drive_id = drive_part\n        else:\n            # try to match prefix of existing drive ids\n            for d in drive_to_phones.keys():\n                if drive_part in d:\n                    drive_id = d\n                    break\n\n        if drive_id is not None:\n            available_phones = drive_to_phones[drive_id]\n            if short_model is None:\n                # if only one phone, use it\n                if len(available_phones) == 1:\n                    phone_name = list(available_phones)[0]\n            else:\n                target = alias_map.get(short_model, short_model)\n                # exact match\n                if target in available_phones:\n                    phone_name = target\n                else:\n                    # any phone_name that endswith target\n                    candidates = [p for p in available_phones if p.endswith(target)]\n                    if len(candidates) == 1:\n                        phone_name = candidates[0]\n                    elif len(candidates) > 1:\n                        phone_name = sorted(candidates)[0]\n                    else:\n                        # fallback: if only one available, take it\n                        if len(available_phones) == 1:\n                            phone_name = list(available_phones)[0]\n\n        if drive_id is None or phone_name is None:\n            # mapping failed; keep as None; we'll handle later\n            sample_to_drive_phone[sp] = (None, None)\n        else:\n            key = (drive_id, phone_name)\n            drive_phone_to_sample[key] = sp\n            sample_to_drive_phone[sp] = key\n\n    # Log mapping coverage\n    mapped = sum(1 for v in sample_to_drive_phone.values() if v[0] is not None)\n    print(\n        f\"Phone mapping: {mapped}/{len(sample_phones)} sample phones mapped to folders.\"\n    )\n    return drive_phone_to_sample, sample_to_drive_phone\n\n\n# ---------------- Data loaders ----------------\ndef load_train(train_root, feature_cols, drive_phone_to_sample_phone):\n    rows = []\n    folder_keys = folder_drive_phone_keys(train_root)\n\n    for drive_id, phone_name in sorted(folder_keys):\n        phone_dir = os.path.join(train_root, drive_id, phone_name)\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n        if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n            continue\n\n        try:\n            gnss = pd.read_csv(gnss_path)\n            gt = pd.read_csv(gt_path)\n        except Exception as e:\n            print(f\"Failed reading {gnss_path} or {gt_path}: {e}\")\n            continue\n\n        if \"utcTimeMillis\" not in gnss.columns or \"UnixTimeMillis\" not in gt.columns:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"utcTimeMillis\"], errors=\"coerce\")\n        gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        if len(gnss) == 0 or len(gt) == 0:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n        gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n        for c in feature_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n        if len(agg) == 0:\n            continue\n\n        gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n\n        merged = pd.merge(gt_small, agg, on=\"UnixTimeMillis\", how=\"inner\")\n        if len(merged) == 0:\n            continue\n\n        # assign mapping phone string if available, else construct as drive + '_' + phone\n        key = (drive_id, phone_name)\n        phone_str = drive_phone_to_sample_phone.get(key, f\"{drive_id}_{phone_name}\")\n        merged[\"phone\"] = phone_str\n        merged[\"drive_id\"] = drive_id\n\n        x_gt, y_gt, z_gt = llh_to_ecef(\n            merged[\"LatitudeDegrees\"].values,\n            merged[\"LongitudeDegrees\"].values,\n            np.zeros(len(merged)),\n        )\n        merged[\"x_gt\"] = x_gt\n        merged[\"y_gt\"] = y_gt\n        merged[\"z_gt\"] = z_gt\n        rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols, sample_phone_to_drive_phone):\n    rows = []\n    folder_keys = folder_drive_phone_keys(test_root)\n\n    for drive_id, phone_name in sorted(folder_keys):\n        phone_dir = os.path.join(test_root, drive_id, phone_name)\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            continue\n        try:\n            gnss = pd.read_csv(gnss_path)\n        except Exception as e:\n            print(f\"Failed reading {gnss_path}: {e}\")\n            continue\n        if \"utcTimeMillis\" not in gnss.columns:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"utcTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        if len(gnss) == 0:\n            continue\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n        for c in feature_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n        if len(agg) == 0:\n            continue\n        agg[\"drive_id\"] = drive_id\n        agg[\"phone_name\"] = phone_name\n        rows.append(agg)\n\n    if rows:\n        test_gnss = pd.concat(rows, ignore_index=True)\n    else:\n        test_gnss = pd.DataFrame(\n            columns=[\"UnixTimeMillis\", \"drive_id\", \"phone_name\"] + feature_cols\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n    sample[\"phone\"] = sample[\"phone\"].astype(str)\n\n    drive_ids = []\n    phone_names = []\n    for p in sample[\"phone\"].values:\n        d, pn = sample_phone_to_drive_phone.get(p, (None, None))\n        drive_ids.append(d)\n        phone_names.append(pn)\n    sample[\"drive_id\"] = drive_ids\n    sample[\"phone_name\"] = phone_names\n\n    # We keep unmapped rows; they won't match GNSS, but we'll still output predictions via filling\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"drive_id\", \"phone_name\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n    if \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n        raise RuntimeError(\n            \"sample_submission.csv must contain 'phone' and 'UnixTimeMillis' columns.\"\n        )\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n\n    # Build robust mapping using folder structure\n    drive_phone_to_sample_phone, sample_phone_to_drive_phone = build_phone_mapping(\n        sample_sub, train_root, test_root\n    )\n\n    print(\"Loading training data...\")\n    train_df = load_train(train_root, feature_cols, drive_phone_to_sample_phone)\n    print(f\"Train rows assembled: {len(train_df)}\")\n\n    # Ensure feature columns present and fill NaNs with medians\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    groups = train_df[\"drive_id\"].values\n\n    print(\"Running 5-fold GroupKFold cross-validation...\")\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    fold_scores = []\n    for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n        lat_val_pred, lon_val_pred, _ = ecef_to_llh(\n            oof[val_idx, 0], oof[val_idx, 1], oof[val_idx, 2]\n        )\n        eval_df_fold = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"UnixTimeMillis\": train_df.iloc[val_idx][\"UnixTimeMillis\"].values,\n                \"lat_pred\": lat_val_pred,\n                \"lon_pred\": lon_val_pred,\n                \"lat_gt\": train_df.iloc[val_idx][\"LatitudeDegrees\"].values,\n                \"lon_gt\": train_df.iloc[val_idx][\"LongitudeDegrees\"].values,\n            }\n        )\n        fold_metric = competition_metric(eval_df_fold)\n        fold_scores.append(fold_metric)\n        print(f\"Fold {fold_idx} competition metric: {fold_metric:.4f}\")\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"OOF competition metric (mean of P50/P95 errors):\", metric_val)\n    valid_fold_scores = [s for s in fold_scores if not np.isnan(s)]\n    if valid_fold_scores:\n        mean_cv = float(np.mean(valid_fold_scores))\n        print(\"Mean CV metric across folds:\", mean_cv)\n\n    # Train final models on full data\n    print(\"Training final models on full training data...\")\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # Prepare test and submission\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    print(\"Loading and merging test GNSS features...\")\n    test_merged = load_test(\n        test_root,\n        sample_sub[[\"phone\", \"UnixTimeMillis\", \"row_id\"]],\n        feature_cols,\n        sample_phone_to_drive_phone,\n    )\n\n    # Ensure row_id is preserved; if missing, merge back\n    if \"row_id\" not in test_merged.columns:\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    # Ensure feature columns and fill with train medians\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    test_merged = test_merged.sort_values(\"row_id\").reset_index(drop=True)\n    X_test = test_merged[feature_cols].values\n\n    print(\"Predicting on test set...\")\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align with full sample_sub, handling any rows that might not have matched GNSS\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill missing predictions by forward/backward fill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still NaNs (e.g. all were NaN), fallback to train mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"].values,\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"].values,\n        }\n    )\n\n    # Save submission files\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\n# Directories\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\n# ----------------- Utility functions ----------------- #\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF XYZ (meters) to WGS84 lat/lon degrees.\"\"\"\n    # WGS84 constants\n    a = 6378137.0\n    b = 6356752.314245\n    e2 = 6.69437999014e-3\n    ep2 = (a * a - b * b) / (b * b)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    lat_deg = np.degrees(lat)\n    lon_deg = np.degrees(lon)\n    return lat_deg, lon_deg\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    \"\"\"Great-circle distance in meters between two arrays of lat/lon degrees.\"\"\"\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    \"\"\"Mean over phones of (P50+P95)/2 of distance errors.\"\"\"\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\n# ----------------- Data loading ----------------- #\n\n\ndef load_train_with_gt(input_dir):\n    \"\"\"Load training GNSS with ground truth, returning merged rows with ECEF and GT lat/lon.\"\"\"\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"Train directory not found; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    all_rows = []\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    print(f\"Found {len(drive_dirs)} drive directories in train.\", flush=True)\n\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} due to read error: {e}\", flush=True)\n                continue\n\n            # Determine time columns\n            if \"UnixTimeMillis\" in gnss.columns:\n                time_col_gnss = \"UnixTimeMillis\"\n            elif \"utcTimeMillis\" in gnss.columns:\n                time_col_gnss = \"utcTimeMillis\"\n            else:\n                continue\n\n            if \"UnixTimeMillis\" in gt.columns:\n                time_col_gt = \"UnixTimeMillis\"\n            elif \"utcTimeMillis\" in gt.columns:\n                time_col_gt = \"utcTimeMillis\"\n            else:\n                continue\n\n            gnss = gnss.rename(columns={time_col_gnss: \"UnixTimeMillis\"})\n            gt = gt.rename(columns={time_col_gt: \"UnixTimeMillis\"})\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Aggregate per UnixTimeMillis\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n\n            if not {\"LatitudeDegrees\", \"LongitudeDegrees\"}.issubset(gt.columns):\n                continue\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No usable train rows with ground truth.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    print(f\"Loaded train with GT: {len(train_df)} rows\", flush=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    \"\"\"From merged train_df, build feature matrix X and targets d_lat, d_lon.\"\"\"\n    train_df = train_df.copy()\n\n    x = train_df[\"WlsPositionXEcefMeters\"].values\n    y = train_df[\"WlsPositionYEcefMeters\"].values\n    z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(x, y, z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    \"\"\"Train RandomForest correction models with GroupKFold CV on 'drive'.\"\"\"\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X.values, y_lat)\n        rf_lon.fit(X.values, y_lon)\n        print(\"Not enough groups for CV; trained on full data without CV.\", flush=True)\n        return [rf_lat], [rf_lon], feature_cols, np.nan, train_df_prepared\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=100, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=100, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    \"\"\"Fit final RandomForest models on all training data.\"\"\"\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols].values, y_lat)\n    rf_lon.fit(X[feature_cols].values, y_lon)\n    return rf_lat, rf_lon\n\n\ndef load_test_base_positions(input_dir, sample_sub):\n    \"\"\"Aggregate GNSS ECEF positions per (phone, UnixTimeMillis) and merge with sample.\"\"\"\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        print(f\"Found {len(drive_dirs)} drive directories in test.\", flush=True)\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"UnixTimeMillis\" in gnss.columns:\n                    time_col = \"UnixTimeMillis\"\n                elif \"utcTimeMillis\" in gnss.columns:\n                    time_col = \"utcTimeMillis\"\n                else:\n                    continue\n                gnss = gnss.rename(columns={time_col: \"UnixTimeMillis\"})\n\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\", as_index=False)\n                    .mean()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    # Compute base lat/lon from ECEF if available\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    \"\"\"Build test feature matrix X_test from merged test frame.\"\"\"\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\n# ----------------- Main script ----------------- #\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load train data\n    train_df = load_train_with_gt(INPUT_DIR)\n    correction_model_lat = None\n    correction_model_lon = None\n    feature_cols = None\n    val_metric = np.nan\n\n    if train_df.empty:\n        print(\n            \"No training data available; using baseline WLS/mean positions only.\",\n            flush=True,\n        )\n    else:\n        # CV training\n        lat_models, lon_models, feature_cols, val_metric, _ = train_with_cv(train_df)\n        print(\n            \"Cross-validation competition metric (mean (P50+P95)/2 over phones):\",\n            val_metric,\n            flush=True,\n        )\n        # Train final model on full data\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    # Explicitly print final evaluation metric\n    print(\n        \"Final validation metric (mean (P50+P95)/2 over phones):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Fill ECEF per phone (ffill/bfill) to reduce gaps\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        if coord in merged.columns:\n            merged[coord] = merged.groupby(\"phone\")[coord].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    # Recompute base lat/lon after filling ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global mean\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    if np.isnan(overall_base_lat) or np.isnan(overall_base_lon):\n        if (\n            not merged.empty\n            and \"base_lat\" in merged.columns\n            and merged[\"base_lat\"].notna().any()\n        ):\n            overall_base_lat = merged[\"base_lat\"].dropna().mean()\n            overall_base_lon = merged[\"base_lon\"].dropna().mean()\n        else:\n            overall_base_lat = 0.0\n            overall_base_lon = 0.0\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Apply correction model if available\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols].values)\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols].values)\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned to sample_sub\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    # Fill missing predictions per phone then global\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    if np.isnan(overall_lat) or np.isnan(overall_lon):\n        overall_lat = 0.0\n        overall_lon = 0.0\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    # Save to required locations\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\",\n        flush=True,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Try LightGBM; fall back to RF if unavailable\ntry:\n    import lightgbm as lgb\n\n    HAS_LGB = True\nexcept Exception:\n    HAS_LGB = False\n\nINPUT_DIR = \"./input\"\nWORKING_DIR = \"./working\"\nSUBMISSION_DIR = \"./submission\"\n\nos.makedirs(WORKING_DIR, exist_ok=True)\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1, lon1, lat2, lon2 = map(\n        np.radians,\n        [\n            lat1.astype(float),\n            lon1.astype(float),\n            lat2.astype(float),\n            lon2.astype(float),\n        ],\n    )\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    errors = []\n    for phone, g in df.groupby(\"phone\"):\n        d = haversine(\n            g[\"gt_lat\"].values,\n            g[\"gt_lon\"].values,\n            g[\"pred_lat\"].values,\n            g[\"pred_lon\"].values,\n        )\n        if len(d) == 0:\n            continue\n        q50 = np.percentile(d, 50)\n        q95 = np.percentile(d, 95)\n        errors.append((q50 + q95) / 2.0)\n    if not errors:\n        return np.nan\n    return float(np.mean(errors))\n\n\ndef prepare_time_features(df):\n    df = df.copy()\n    t = df[\"UnixTimeMillis\"].astype(\"int64\")\n    df[\"seconds\"] = (t // 1000).astype(\"int64\")\n    df[\"time_sin_1h\"] = np.sin(2 * np.pi * (df[\"seconds\"] % 3600) / 3600.0)\n    df[\"time_cos_1h\"] = np.cos(2 * np.pi * (df[\"seconds\"] % 3600) / 3600.0)\n    return df\n\n\ndef load_train_test():\n    train_path = os.path.join(INPUT_DIR, \"train.csv\")\n    test_path = os.path.join(INPUT_DIR, \"test.csv\")\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n\n    if not os.path.exists(train_path):\n        raise RuntimeError(\"train.csv not found in ./input, benchmark expects it.\")\n\n    train_df = pd.read_csv(train_path)\n    # Ensure required columns exist\n    expected_train_cols = {\n        \"phone\",\n        \"UnixTimeMillis\",\n        \"LatitudeDegrees\",\n        \"LongitudeDegrees\",\n    }\n    missing = expected_train_cols - set(train_df.columns)\n    if missing:\n        raise RuntimeError(f\"train.csv is missing required columns: {missing}\")\n\n    # Standardize dtypes\n    train_df[\"phone\"] = train_df[\"phone\"].astype(str)\n    train_df[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"int64\")\n    train_df[\"LatitudeDegrees\"] = train_df[\"LatitudeDegrees\"].astype(float)\n    train_df[\"LongitudeDegrees\"] = train_df[\"LongitudeDegrees\"].astype(float)\n\n    if os.path.exists(test_path):\n        test_df = pd.read_csv(test_path)\n    elif os.path.exists(sample_path):\n        # Fallback to sample_submission structure if test.csv is not present\n        test_df = pd.read_csv(sample_path)[[\"phone\", \"UnixTimeMillis\"]].copy()\n    else:\n        raise RuntimeError(\n            \"Neither test.csv nor sample_submission.csv found in ./input\"\n        )\n\n    test_df[\"phone\"] = test_df[\"phone\"].astype(str)\n    test_df[\"UnixTimeMillis\"] = test_df[\"UnixTimeMillis\"].astype(\"int64\")\n\n    return train_df, test_df\n\n\ndef build_features(train_df, test_df):\n    train_df = prepare_time_features(train_df)\n    test_df = prepare_time_features(test_df)\n\n    # Encode phone\n    le_phone = LabelEncoder()\n    train_df[\"phone_enc\"] = le_phone.fit_transform(train_df[\"phone\"].astype(str))\n\n    phone_classes = list(le_phone.classes_)\n    phone_to_int = {p: i for i, p in enumerate(phone_classes)}\n    unk_index = len(phone_classes)\n    test_df[\"phone_enc\"] = [\n        phone_to_int.get(p, unk_index) for p in test_df[\"phone\"].astype(str)\n    ]\n\n    target_lat = \"LatitudeDegrees\"\n    target_lon = \"LongitudeDegrees\"\n    key_cols = [\"phone\", \"UnixTimeMillis\"]\n\n    non_feature_cols = key_cols + [target_lat, target_lon]\n    feature_cols = [\n        c\n        for c in train_df.columns\n        if c not in non_feature_cols and pd.api.types.is_numeric_dtype(train_df[c])\n    ]\n\n    if not feature_cols:\n        # Minimal fallback\n        feature_cols = [\"phone_enc\", \"seconds\"]\n\n    X_train = train_df[feature_cols].copy()\n    X_test = test_df[feature_cols].copy()\n\n    medians = X_train.median()\n    X_train = X_train.fillna(medians)\n    X_test = X_test.fillna(medians)\n\n    y_lat = train_df[target_lat].astype(float).values\n    y_lon = train_df[target_lon].astype(float).values\n\n    return X_train, y_lat, y_lon, X_test, feature_cols, train_df, test_df\n\n\ndef run_cv(X, y_lat, y_lon, phones, unix_times):\n    phones = np.asarray(phones).astype(str)\n    collections = np.array([p.split(\"_\")[0] for p in phones])\n    uniq_groups = np.unique(collections)\n    n_splits = 5\n    if len(uniq_groups) < n_splits:\n        n_splits = max(2, len(uniq_groups))\n    splitter = GroupKFold(n_splits=n_splits)\n    scores = []\n    fold = 0\n    for tr_idx, va_idx in splitter.split(X, y_lat, groups=collections):\n        X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n        y_lat_tr, y_lat_va = y_lat[tr_idx], y_lat[va_idx]\n        y_lon_tr, y_lon_va = y_lon[tr_idx], y_lon[va_idx]\n\n        if HAS_LGB:\n            params = {\n                \"objective\": \"regression\",\n                \"metric\": \"rmse\",\n                \"learning_rate\": 0.05,\n                \"num_leaves\": 31,\n                \"feature_fraction\": 0.8,\n                \"bagging_fraction\": 0.8,\n                \"bagging_freq\": 1,\n                \"seed\": 42,\n                \"verbose\": -1,\n            }\n            dtrain_lat = lgb.Dataset(X_tr, label=y_lat_tr)\n            dvalid_lat = lgb.Dataset(X_va, label=y_lat_va, reference=dtrain_lat)\n            model_lat = lgb.train(\n                params,\n                dtrain_lat,\n                num_boost_round=200,\n                valid_sets=[dvalid_lat],\n                early_stopping_rounds=20,\n                verbose_eval=False,\n            )\n\n            dtrain_lon = lgb.Dataset(X_tr, label=y_lon_tr)\n            dvalid_lon = lgb.Dataset(X_va, label=y_lon_va, reference=dtrain_lon)\n            model_lon = lgb.train(\n                params,\n                dtrain_lon,\n                num_boost_round=200,\n                valid_sets=[dvalid_lon],\n                early_stopping_rounds=20,\n                verbose_eval=False,\n            )\n\n            pred_lat = model_lat.predict(X_va, num_iteration=model_lat.best_iteration)\n            pred_lon = model_lon.predict(X_va, num_iteration=model_lon.best_iteration)\n        else:\n            model_lat = RandomForestRegressor(\n                n_estimators=200, random_state=42, n_jobs=-1\n            )\n            model_lon = RandomForestRegressor(\n                n_estimators=200, random_state=42, n_jobs=-1\n            )\n            model_lat.fit(X_tr, y_lat_tr)\n            model_lon.fit(X_tr, y_lon_tr)\n            pred_lat = model_lat.predict(X_va)\n            pred_lon = model_lon.predict(X_va)\n\n        df_eval = pd.DataFrame(\n            {\n                \"phone\": phones[va_idx],\n                \"UnixTimeMillis\": unix_times[va_idx],\n                \"gt_lat\": y_lat_va,\n                \"gt_lon\": y_lon_va,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        score = competition_metric(df_eval)\n        print(f\"Fold {fold} metric: {score:.6f}\")\n        scores.append(score)\n        fold += 1\n\n    mean_score = float(np.mean(scores))\n    std_score = float(np.std(scores))\n    print(f\"CV mean metric: {mean_score:.6f}, std: {std_score:.6f}\")\n    return mean_score\n\n\ndef train_full_models(X, y_lat, y_lon):\n    if HAS_LGB:\n        params = {\n            \"objective\": \"regression\",\n            \"metric\": \"rmse\",\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 31,\n            \"feature_fraction\": 0.8,\n            \"bagging_fraction\": 0.8,\n            \"bagging_freq\": 1,\n            \"seed\": 42,\n            \"verbose\": -1,\n        }\n        dtrain_lat = lgb.Dataset(X, label=y_lat)\n        dtrain_lon = lgb.Dataset(X, label=y_lon)\n        model_lat = lgb.train(params, dtrain_lat, num_boost_round=200)\n        model_lon = lgb.train(params, dtrain_lon, num_boost_round=200)\n    else:\n        model_lat = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n        model_lon = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)\n        model_lat.fit(X, y_lat)\n        model_lon.fit(X, y_lon)\n    return model_lat, model_lon\n\n\ndef main():\n    train_df, test_df = load_train_test()\n    print(\"Loaded train and test data.\")\n\n    X_train, y_lat, y_lon, X_test, feature_cols, train_df_feat, test_df_feat = (\n        build_features(train_df, test_df)\n    )\n    print(f\"Feature columns: {feature_cols}\")\n\n    print(\"Running GroupKFold CV...\")\n    cv_score = run_cv(\n        X_train,\n        y_lat,\n        y_lon,\n        train_df_feat[\"phone\"].values,\n        train_df_feat[\"UnixTimeMillis\"].values,\n    )\n    print(f\"Hold-out CV metric: {cv_score}\")\n\n    print(\"Training full models...\")\n    model_lat, model_lon = train_full_models(X_train, y_lat, y_lon)\n\n    # Baseline statistics for fallback\n    train_baseline = (\n        train_df.groupby(\"phone\")[[\"LatitudeDegrees\", \"LongitudeDegrees\"]]\n        .mean()\n        .rename(columns={\"LatitudeDegrees\": \"mean_lat\", \"LongitudeDegrees\": \"mean_lon\"})\n    )\n    global_mean_lat = train_df[\"LatitudeDegrees\"].mean()\n    global_mean_lon = train_df[\"LongitudeDegrees\"].mean()\n\n    print(\"Predicting on test set and building submission...\")\n    # Ensure we have a sample_submission structure to align with required rows\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if os.path.exists(sample_path):\n        submission = pd.read_csv(sample_path)\n        submission[\"phone\"] = submission[\"phone\"].astype(str)\n        submission[\"UnixTimeMillis\"] = submission[\"UnixTimeMillis\"].astype(\"int64\")\n        # Align test features to these rows\n        test_key = test_df_feat[[\"phone\", \"UnixTimeMillis\"]].copy()\n        test_key[\"row_id\"] = np.arange(len(test_key))\n        sub_key = submission[[\"phone\", \"UnixTimeMillis\"]].copy()\n        sub_key = sub_key.merge(test_key, how=\"left\", on=[\"phone\", \"UnixTimeMillis\"])\n        # If some rows are missing, extend test features with baseline-only\n        idx_in_test = sub_key[\"row_id\"].values\n    else:\n        # If no sample_submission, just use test_df_feat structure\n        submission = test_df_feat[[\"phone\", \"UnixTimeMillis\"]].copy()\n        idx_in_test = np.arange(len(test_df_feat))\n\n    # Predict with model, fall back to baseline if error\n    try:\n        if HAS_LGB:\n            pred_lat_test_all = model_lat.predict(X_test)\n            pred_lon_test_all = model_lon.predict(X_test)\n        else:\n            pred_lat_test_all = model_lat.predict(X_test)\n            pred_lon_test_all = model_lon.predict(X_test)\n        # Map predictions to submission rows\n        if len(idx_in_test) != len(submission):\n            # Align safely: default to NaN, then fill where mapping exists\n            submission[\"LatitudeDegrees\"] = np.nan\n            submission[\"LongitudeDegrees\"] = np.nan\n            mask = ~np.isnan(idx_in_test)\n            idx_valid = np.where(mask)[0]\n            test_rows = idx_in_test[mask].astype(int)\n            submission.loc[idx_valid, \"LatitudeDegrees\"] = pred_lat_test_all[test_rows]\n            submission.loc[idx_valid, \"LongitudeDegrees\"] = pred_lon_test_all[test_rows]\n        else:\n            # Simple 1-1 alignment\n            mask = ~np.isnan(idx_in_test)\n            submission[\"LatitudeDegrees\"] = np.nan\n            submission[\"LongitudeDegrees\"] = np.nan\n            valid_rows = np.where(mask)[0]\n            test_rows = idx_in_test[mask].astype(int)\n            submission.loc[valid_rows, \"LatitudeDegrees\"] = pred_lat_test_all[test_rows]\n            submission.loc[valid_rows, \"LongitudeDegrees\"] = pred_lon_test_all[\n                test_rows\n            ]\n    except Exception as e:\n        print(f\"Model prediction failed ({e}); falling back to baseline only.\")\n        submission = submission.merge(train_baseline, how=\"left\", on=\"phone\")\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Ensure prediction columns exist\n    if (\"LatitudeDegrees\" not in submission.columns) or (\n        \"LongitudeDegrees\" not in submission.columns\n    ):\n        print(\"Prediction columns missing; rebuilding via baseline only.\")\n        submission = submission.merge(train_baseline, how=\"left\", on=\"phone\")\n        submission[\"LatitudeDegrees\"] = submission[\"mean_lat\"].fillna(global_mean_lat)\n        submission[\"LongitudeDegrees\"] = submission[\"mean_lon\"].fillna(global_mean_lon)\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    # Final NaN handling with baseline\n    if (\n        submission[\"LatitudeDegrees\"].isna().any()\n        or submission[\"LongitudeDegrees\"].isna().any()\n    ):\n        print(\"NaNs detected in predictions; filling with per-phone and global means.\")\n        submission = submission.merge(\n            train_baseline, how=\"left\", on=\"phone\", suffixes=(\"\", \"_base\")\n        )\n        if \"LatitudeDegrees\" in submission.columns:\n            submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n                submission.get(\"mean_lat\", global_mean_lat)\n            )\n        else:\n            submission[\"LatitudeDegrees\"] = submission.get(\"mean_lat\", global_mean_lat)\n\n        if \"LongitudeDegrees\" in submission.columns:\n            submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n                submission.get(\"mean_lon\", global_mean_lon)\n            )\n        else:\n            submission[\"LongitudeDegrees\"] = submission.get(\"mean_lon\", global_mean_lon)\n\n        submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].fillna(\n            global_mean_lat\n        )\n        submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].fillna(\n            global_mean_lon\n        )\n        for c in [\"mean_lat\", \"mean_lon\"]:\n            if c in submission.columns:\n                submission.drop(columns=[c], inplace=True)\n\n    required_cols = [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    submission = submission[required_cols].copy()\n    submission[\"phone\"] = submission[\"phone\"].astype(str)\n    submission[\"UnixTimeMillis\"] = submission[\"UnixTimeMillis\"].astype(\"int64\")\n    submission[\"LatitudeDegrees\"] = submission[\"LatitudeDegrees\"].astype(float)\n    submission[\"LongitudeDegrees\"] = submission[\"LongitudeDegrees\"].astype(float)\n\n    sub_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    sub_path_final = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    submission.to_csv(sub_path_working, index=False)\n    submission.to_csv(sub_path_final, index=False)\n    print(f\"Saved submission to {sub_path_working} and {sub_path_final}\")\n    print(f\"Final CV metric: {cv_score}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\n\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF XYZ (meters) to WGS84 lat/lon degrees.\"\"\"\n    a = 6378137.0\n    b = 6356752.314245\n    e2 = 6.69437999014e-3\n    ep2 = (a * a - b * b) / (b * b)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    return np.degrees(lat), np.degrees(lon)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    \"\"\"Great-circle distance in meters between two arrays of lat/lon degrees.\"\"\"\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    \"\"\"Mean over phones of (P50+P95)/2 of distance errors.\"\"\"\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef load_train_with_gt(input_dir):\n    \"\"\"Load training GNSS with ground truth, returning merged rows with ECEF and GT lat/lon.\"\"\"\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"Train directory not found; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    all_rows = []\n    drive_dirs = sorted(\n        [p for p in glob.glob(os.path.join(train_root, \"*\")) if os.path.isdir(p)]\n    )\n    print(f\"Found {len(drive_dirs)} drive directories in train.\", flush=True)\n\n    for drive_dir in drive_dirs:\n        drive_id = os.path.basename(drive_dir)\n        phone_dirs = sorted(\n            [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n        )\n        for phone_dir in phone_dirs:\n            phone_model = os.path.basename(phone_dir)\n            phone_id = f\"{drive_id}_{phone_model}\"\n\n            gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n            gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n            if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n                continue\n\n            try:\n                gnss = pd.read_csv(gnss_path)\n                gt = pd.read_csv(gt_path)\n            except Exception as e:\n                print(f\"Skipping train {phone_dir} due to read error: {e}\", flush=True)\n                continue\n\n            # Determine time columns\n            if \"UnixTimeMillis\" in gnss.columns:\n                time_col_gnss = \"UnixTimeMillis\"\n            elif \"utcTimeMillis\" in gnss.columns:\n                time_col_gnss = \"utcTimeMillis\"\n            else:\n                continue\n\n            if \"UnixTimeMillis\" in gt.columns:\n                time_col_gt = \"UnixTimeMillis\"\n            elif \"utcTimeMillis\" in gt.columns:\n                time_col_gt = \"utcTimeMillis\"\n            else:\n                continue\n\n            gnss = gnss.rename(columns={time_col_gnss: \"UnixTimeMillis\"})\n            gt = gt.rename(columns={time_col_gt: \"UnixTimeMillis\"})\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            if not {\"LatitudeDegrees\", \"LongitudeDegrees\"}.issubset(gt.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty or gt.empty:\n                continue\n\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n            gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n            # Aggregate per UnixTimeMillis\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n\n            gt_use = gt[\n                [\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n            ].copy()\n\n            merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n            if merged.empty:\n                continue\n\n            merged[\"phone\"] = phone_id\n            merged[\"drive\"] = drive_id\n            all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No usable train rows with ground truth.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    print(f\"Loaded train with GT: {len(train_df)} rows\", flush=True)\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    \"\"\"From merged train_df, build feature matrix X and targets d_lat, d_lon.\"\"\"\n    train_df = train_df.copy()\n\n    x = train_df[\"WlsPositionXEcefMeters\"].values\n    y = train_df[\"WlsPositionYEcefMeters\"].values\n    z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(x, y, z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    \"\"\"Train RandomForest correction models with GroupKFold CV on 'drive'.\"\"\"\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X.values, y_lat)\n        rf_lon.fit(X.values, y_lon)\n        print(\"Not enough groups for CV; trained on full data without CV.\", flush=True)\n        return [rf_lat], [rf_lon], feature_cols, np.nan, train_df_prepared\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=100, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=100, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    \"\"\"Fit final RandomForest models on all training data.\"\"\"\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols].values, y_lat)\n    rf_lon.fit(X[feature_cols].values, y_lon)\n    return rf_lat, rf_lon\n\n\ndef load_test_base_positions(input_dir, sample_sub):\n    \"\"\"Aggregate GNSS ECEF positions per (phone, UnixTimeMillis) and merge with sample.\"\"\"\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        drive_dirs = sorted(\n            [p for p in glob.glob(os.path.join(test_root, \"*\")) if os.path.isdir(p)]\n        )\n        print(f\"Found {len(drive_dirs)} drive directories in test.\", flush=True)\n        for drive_dir in drive_dirs:\n            drive_id = os.path.basename(drive_dir)\n            phone_dirs = sorted(\n                [p for p in glob.glob(os.path.join(drive_dir, \"*\")) if os.path.isdir(p)]\n            )\n            for phone_dir in phone_dirs:\n                phone_model = os.path.basename(phone_dir)\n                phone_id = f\"{drive_id}_{phone_model}\"\n                gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n                if not os.path.exists(gnss_path):\n                    continue\n                try:\n                    gnss = pd.read_csv(gnss_path)\n                except Exception as e:\n                    print(f\"Skipping test {phone_dir} read error: {e}\", flush=True)\n                    continue\n\n                if \"UnixTimeMillis\" in gnss.columns:\n                    time_col = \"UnixTimeMillis\"\n                elif \"utcTimeMillis\" in gnss.columns:\n                    time_col = \"utcTimeMillis\"\n                else:\n                    continue\n                gnss = gnss.rename(columns={time_col: \"UnixTimeMillis\"})\n\n                needed_cols = [\n                    \"WlsPositionXEcefMeters\",\n                    \"WlsPositionYEcefMeters\",\n                    \"WlsPositionZEcefMeters\",\n                ]\n                if not set(needed_cols).issubset(gnss.columns):\n                    continue\n\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                    gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n                )\n                gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n                if gnss.empty:\n                    continue\n                gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n                agg = (\n                    gnss[[\"UnixTimeMillis\"] + needed_cols]\n                    .groupby(\"UnixTimeMillis\", as_index=False)\n                    .mean()\n                )\n                agg[\"phone\"] = phone_id\n                agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    # Compute base lat/lon from ECEF if available\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    \"\"\"Build test feature matrix X_test from merged test frame.\"\"\"\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load train data\n    train_df = load_train_with_gt(INPUT_DIR)\n    correction_model_lat = None\n    correction_model_lon = None\n    feature_cols = None\n    val_metric = np.nan\n\n    if train_df.empty:\n        print(\n            \"No training data available; using baseline WLS/mean positions only.\",\n            flush=True,\n        )\n    else:\n        # CV training\n        lat_models, lon_models, feature_cols, val_metric, _ = train_with_cv(train_df)\n        print(\n            \"Cross-validation competition metric (mean (P50+P95)/2 over phones):\",\n            val_metric,\n            flush=True,\n        )\n        # Train final model on full data\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    # Explicitly print final evaluation metric\n    print(\n        \"Final validation metric (mean (P50+P95)/2 over phones):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Fill ECEF per phone (ffill/bfill) to reduce gaps\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        if coord in merged.columns:\n            merged[coord] = merged.groupby(\"phone\")[coord].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    # Recompute base lat/lon after filling ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global mean\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    if np.isnan(overall_base_lat) or np.isnan(overall_base_lon):\n        if (\n            not merged.empty\n            and \"base_lat\" in merged.columns\n            and merged[\"base_lat\"].notna().any()\n        ):\n            overall_base_lat = merged[\"base_lat\"].dropna().mean()\n            overall_base_lon = merged[\"base_lon\"].dropna().mean()\n        else:\n            overall_base_lat = 0.0\n            overall_base_lon = 0.0\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Apply correction model if available\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols].values)\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols].values)\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned to sample_sub\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    # Fill missing predictions per phone then global\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    if np.isnan(overall_lat) or np.isnan(overall_lon):\n        overall_lat = 0.0\n        overall_lon = 0.0\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    # Save to required locations\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\",\n        flush=True,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GroupKFold\n\nINPUT_DIR = \"./input\"\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\n\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n\ndef ecef_to_geodetic(x, y, z):\n    # WGS84 constants\n    a = 6378137.0\n    b = 6356752.314245\n    e2 = 6.69437999014e-3\n    ep2 = (a * a - b * b) / (b * b)\n\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    r = np.sqrt(x * x + y * y)\n    r = np.where(r == 0, 1e-9, r)\n    theta = np.arctan2(z * a, r * b)\n    st = np.sin(theta)\n    ct = np.cos(theta)\n\n    lat = np.arctan2(z + ep2 * b * st**3, r - e2 * a * ct**3)\n    lon = np.arctan2(y, x)\n    return np.degrees(lat), np.degrees(lon)\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    dlat = lat2 - lat1\n    dlon = np.radians(lon2.astype(float) - lon1.astype(float))\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef competition_metric(df):\n    metrics = []\n    for ph, g in df.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return np.nan\n    return float(np.mean(metrics))\n\n\ndef find_phone_id_from_path(path, root_type=\"train\"):\n    \"\"\"\n    Given a device directory path under input/train or input/test,\n    construct the phone id string to match sample_submission.csv.\n    The competition format is usually \"<collection>_<phone_name>\".\n    Here we detect the collection and phone by taking the last 2 parts\n    under root_type.\n    \"\"\"\n    # Example path: ./input/train/2020-06-04-US-MTV-1/GooglePixel4\n    # or with deeper: ./input/train/2020-06-04-US-MTV-1/segment01/GooglePixel4\n    # We want \"<collection>_<phone_name>\", where collection is the\n    # first folder under train/test whose name contains the date pattern.\n    path = os.path.abspath(path)\n    parts = path.split(os.sep)\n    if root_type not in parts:\n        return None\n    root_idx = parts.index(root_type)\n    # everything after root_type\n    tail = parts[root_idx + 1 :]\n    if len(tail) < 2:\n        return None\n    # phone name is last\n    phone_name = tail[-1]\n    # collection is usually first part like \"2020-06-04-US-MTV-1\"\n    collection = tail[0]\n    phone_id = f\"{collection}_{phone_name}\"\n    return phone_id\n\n\ndef load_train_with_gt(input_dir):\n    train_root = os.path.join(input_dir, \"train\")\n    if not os.path.exists(train_root):\n        print(\"Train directory not found; skipping model training.\", flush=True)\n        return pd.DataFrame()\n\n    device_dirs = []\n    # search recursively for device_gnss.csv\n    for gnss_path in glob.glob(\n        os.path.join(train_root, \"**\", \"device_gnss.csv\"), recursive=True\n    ):\n        device_dir = os.path.dirname(gnss_path)\n        device_dirs.append(device_dir)\n\n    print(\n        f\"Found {len(device_dirs)} device directories with GNSS in train.\", flush=True\n    )\n\n    all_rows = []\n\n    for device_dir in sorted(device_dirs):\n        gnss_path = os.path.join(device_dir, \"device_gnss.csv\")\n        gt_path = os.path.join(device_dir, \"ground_truth.csv\")\n        if not os.path.exists(gt_path):\n            continue\n\n        phone_id = find_phone_id_from_path(device_dir, root_type=\"train\")\n        if phone_id is None:\n            continue\n\n        # drive id: take collection part before underscore\n        drive_id = phone_id.split(\"_\")[0]\n\n        try:\n            gnss = pd.read_csv(gnss_path)\n            gt = pd.read_csv(gt_path)\n        except Exception as e:\n            print(f\"Skipping train {device_dir} due to read error: {e}\", flush=True)\n            continue\n\n        if \"UnixTimeMillis\" in gnss.columns:\n            time_col_gnss = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in gnss.columns:\n            time_col_gnss = \"utcTimeMillis\"\n        else:\n            continue\n\n        if \"UnixTimeMillis\" in gt.columns:\n            time_col_gt = \"UnixTimeMillis\"\n        elif \"utcTimeMillis\" in gt.columns:\n            time_col_gt = \"utcTimeMillis\"\n        else:\n            continue\n\n        gnss = gnss.rename(columns={time_col_gnss: \"UnixTimeMillis\"})\n        gt = gt.rename(columns={time_col_gt: \"UnixTimeMillis\"})\n\n        needed_cols = [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n        if not set(needed_cols).issubset(gnss.columns):\n            continue\n\n        if not {\"LatitudeDegrees\", \"LongitudeDegrees\"}.issubset(gt.columns):\n            continue\n\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n        gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n        if gnss.empty or gt.empty:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"UnixTimeMillis\"], errors=\"coerce\")\n        gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n        gt = gt.dropna(subset=[\"UnixTimeMillis\"])\n        if gnss.empty or gt.empty:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n        gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n        agg = (\n            gnss[[\"UnixTimeMillis\"] + needed_cols]\n            .groupby(\"UnixTimeMillis\", as_index=False)\n            .mean()\n        )\n\n        gt_use = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n        merged = pd.merge(agg, gt_use, on=\"UnixTimeMillis\", how=\"inner\")\n        if merged.empty:\n            continue\n\n        merged[\"phone\"] = phone_id\n        merged[\"drive\"] = drive_id\n        all_rows.append(merged)\n\n    if not all_rows:\n        print(\"No usable train rows with ground truth.\", flush=True)\n        return pd.DataFrame()\n\n    train_df = pd.concat(all_rows, ignore_index=True)\n    print(\n        f\"Loaded train with GT: {len(train_df)} rows, {train_df['phone'].nunique()} phones.\",\n        flush=True,\n    )\n    return train_df\n\n\ndef prepare_train_features(train_df):\n    train_df = train_df.copy()\n    x = train_df[\"WlsPositionXEcefMeters\"].values\n    y = train_df[\"WlsPositionYEcefMeters\"].values\n    z = train_df[\"WlsPositionZEcefMeters\"].values\n    base_lat, base_lon = ecef_to_geodetic(x, y, z)\n\n    train_df[\"base_lat\"] = base_lat\n    train_df[\"base_lon\"] = base_lon\n    train_df[\"d_lat\"] = train_df[\"LatitudeDegrees\"] - train_df[\"base_lat\"]\n    train_df[\"d_lon\"] = train_df[\"LongitudeDegrees\"] - train_df[\"base_lon\"]\n\n    X = pd.DataFrame(index=train_df.index)\n    X[\"UnixTimeMillis\"] = train_df[\"UnixTimeMillis\"].astype(\"float64\")\n    X[\"WlsPositionXEcefMeters\"] = train_df[\"WlsPositionXEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionYEcefMeters\"] = train_df[\"WlsPositionYEcefMeters\"].astype(\"float64\")\n    X[\"WlsPositionZEcefMeters\"] = train_df[\"WlsPositionZEcefMeters\"].astype(\"float64\")\n\n    y_lat = train_df[\"d_lat\"].values\n    y_lon = train_df[\"d_lon\"].values\n    feature_cols = list(X.columns)\n    return train_df, X, y_lat, y_lon, feature_cols\n\n\ndef train_with_cv(train_df):\n    train_df_prepared, X, y_lat, y_lon, feature_cols = prepare_train_features(train_df)\n    groups = train_df_prepared[\"drive\"].values\n    unique_groups = np.unique(groups)\n    n_groups = len(unique_groups)\n    n_splits = min(5, n_groups) if n_groups > 1 else 1\n\n    if n_splits < 2:\n        rf_lat = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=42\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=150, max_depth=18, n_jobs=-1, random_state=43\n        )\n        rf_lat.fit(X.values, y_lat)\n        rf_lon.fit(X.values, y_lon)\n        print(\"Not enough groups for CV; trained on full data without CV.\", flush=True)\n        return [rf_lat], [rf_lon], feature_cols, np.nan, train_df_prepared\n\n    lat_models = []\n    lon_models = []\n    oof_list = []\n\n    gkf = GroupKFold(n_splits=n_splits)\n    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups), start=1):\n        X_tr, X_val = X.iloc[tr_idx].values, X.iloc[val_idx].values\n        ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n        ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n        rf_lat = RandomForestRegressor(\n            n_estimators=100, max_depth=18, n_jobs=-1, random_state=100 + fold\n        )\n        rf_lon = RandomForestRegressor(\n            n_estimators=100, max_depth=18, n_jobs=-1, random_state=200 + fold\n        )\n        rf_lat.fit(X_tr, ylat_tr)\n        rf_lon.fit(X_tr, ylon_tr)\n\n        dlat_pred = rf_lat.predict(X_val)\n        dlon_pred = rf_lon.predict(X_val)\n\n        base_lat = train_df_prepared.loc[val_idx, \"base_lat\"].values\n        base_lon = train_df_prepared.loc[val_idx, \"base_lon\"].values\n        pred_lat = base_lat + dlat_pred\n        pred_lon = base_lon + dlon_pred\n\n        gt_lat = train_df_prepared.loc[val_idx, \"LatitudeDegrees\"].values\n        gt_lon = train_df_prepared.loc[val_idx, \"LongitudeDegrees\"].values\n        phones = train_df_prepared.loc[val_idx, \"phone\"].values\n\n        fold_df = pd.DataFrame(\n            {\n                \"phone\": phones,\n                \"gt_lat\": gt_lat,\n                \"gt_lon\": gt_lon,\n                \"pred_lat\": pred_lat,\n                \"pred_lon\": pred_lon,\n            }\n        )\n        fold_df[\"dist\"] = haversine(\n            fold_df[\"gt_lat\"],\n            fold_df[\"gt_lon\"],\n            fold_df[\"pred_lat\"],\n            fold_df[\"pred_lon\"],\n        )\n        oof_list.append(fold_df)\n\n        lat_models.append(rf_lat)\n        lon_models.append(rf_lon)\n\n        fold_metric = competition_metric(fold_df[[\"phone\", \"dist\"]])\n        print(f\"Fold {fold}/{n_splits} metric: {fold_metric:.4f}\", flush=True)\n\n    oof = pd.concat(oof_list, ignore_index=True)\n    metric = competition_metric(oof[[\"phone\", \"dist\"]])\n    return lat_models, lon_models, feature_cols, metric, train_df_prepared\n\n\ndef fit_full_correction_model(train_df, feature_cols):\n    train_df_prepared, X, y_lat, y_lon, _ = prepare_train_features(train_df)\n    rf_lat = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=999\n    )\n    rf_lon = RandomForestRegressor(\n        n_estimators=200, max_depth=20, n_jobs=-1, random_state=1001\n    )\n    rf_lat.fit(X[feature_cols].values, y_lat)\n    rf_lon.fit(X[feature_cols].values, y_lon)\n    return rf_lat, rf_lon\n\n\ndef load_test_base_positions(input_dir, sample_sub):\n    test_root = os.path.join(input_dir, \"test\")\n    agg_list = []\n\n    if os.path.exists(test_root):\n        for gnss_path in glob.glob(\n            os.path.join(test_root, \"**\", \"device_gnss.csv\"), recursive=True\n        ):\n            device_dir = os.path.dirname(gnss_path)\n            phone_id = find_phone_id_from_path(device_dir, root_type=\"test\")\n            if phone_id is None:\n                continue\n            try:\n                gnss = pd.read_csv(gnss_path)\n            except Exception as e:\n                print(f\"Skipping test {device_dir} read error: {e}\", flush=True)\n                continue\n\n            if \"UnixTimeMillis\" in gnss.columns:\n                time_col = \"UnixTimeMillis\"\n            elif \"utcTimeMillis\" in gnss.columns:\n                time_col = \"utcTimeMillis\"\n            else:\n                continue\n            gnss = gnss.rename(columns={time_col: \"UnixTimeMillis\"})\n\n            needed_cols = [\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n            if not set(needed_cols).issubset(gnss.columns):\n                continue\n\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty:\n                continue\n            gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n                gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n            )\n            gnss = gnss.dropna(subset=[\"UnixTimeMillis\"])\n            if gnss.empty:\n                continue\n            gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n            agg = (\n                gnss[[\"UnixTimeMillis\"] + needed_cols]\n                .groupby(\"UnixTimeMillis\", as_index=False)\n                .mean()\n            )\n            agg[\"phone\"] = phone_id\n            agg_list.append(agg)\n\n    if agg_list:\n        test_agg = pd.concat(agg_list, ignore_index=True)\n    else:\n        test_agg = pd.DataFrame(\n            columns=[\n                \"phone\",\n                \"UnixTimeMillis\",\n                \"WlsPositionXEcefMeters\",\n                \"WlsPositionYEcefMeters\",\n                \"WlsPositionZEcefMeters\",\n            ]\n        )\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n\n    merged = pd.merge(\n        sample,\n        test_agg,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_gnss\"),\n    )\n\n    # Compute base lat/lon from ECEF if available\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n    return merged\n\n\ndef build_test_features(merged, feature_cols):\n    X_test = pd.DataFrame(index=np.arange(len(merged)))\n    for c in feature_cols:\n        if c == \"UnixTimeMillis\":\n            X_test[c] = pd.to_numeric(merged[\"UnixTimeMillis\"], errors=\"coerce\").astype(\n                \"float64\"\n            )\n        else:\n            if c in merged.columns:\n                X_test[c] = pd.to_numeric(merged[c], errors=\"coerce\")\n            else:\n                X_test[c] = np.nan\n    return X_test\n\n\ndef main():\n    sample_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n    if not os.path.exists(sample_path):\n        raise RuntimeError(f\"sample_submission.csv not found at {sample_path}\")\n    sample_sub = pd.read_csv(sample_path)\n\n    # Load train data\n    train_df = load_train_with_gt(INPUT_DIR)\n    correction_model_lat = None\n    correction_model_lon = None\n    feature_cols = None\n    val_metric = np.nan\n\n    if train_df.empty:\n        print(\n            \"No training data available; using baseline WLS/mean positions only.\",\n            flush=True,\n        )\n    else:\n        # CV training\n        lat_models, lon_models, feature_cols, val_metric, _ = train_with_cv(train_df)\n        print(\n            \"Cross-validation competition metric (mean (P50+P95)/2 over phones):\",\n            val_metric,\n            flush=True,\n        )\n        # Train final model on full data\n        correction_model_lat, correction_model_lon = fit_full_correction_model(\n            train_df, feature_cols\n        )\n\n    # Explicitly print final evaluation metric\n    print(\n        \"Final validation metric (mean (P50+P95)/2 over phones):\",\n        val_metric,\n        flush=True,\n    )\n\n    # Prepare base positions for test\n    merged = load_test_base_positions(INPUT_DIR, sample_sub)\n\n    # Fill ECEF per phone (ffill/bfill) to reduce gaps\n    for coord in [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]:\n        if coord in merged.columns:\n            merged[coord] = merged.groupby(\"phone\")[coord].transform(\n                lambda s: s.ffill().bfill()\n            )\n\n    # Recompute base lat/lon after filling ECEF\n    base_lat = np.full(len(merged), np.nan)\n    base_lon = np.full(len(merged), np.nan)\n    if all(\n        c in merged.columns\n        for c in [\n            \"WlsPositionXEcefMeters\",\n            \"WlsPositionYEcefMeters\",\n            \"WlsPositionZEcefMeters\",\n        ]\n    ):\n        mask_has_ecef = (\n            merged[\"WlsPositionXEcefMeters\"].notna()\n            & merged[\"WlsPositionYEcefMeters\"].notna()\n            & merged[\"WlsPositionZEcefMeters\"].notna()\n        )\n        if mask_has_ecef.any():\n            lat_deg, lon_deg = ecef_to_geodetic(\n                merged.loc[mask_has_ecef, \"WlsPositionXEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionYEcefMeters\"].values,\n                merged.loc[mask_has_ecef, \"WlsPositionZEcefMeters\"].values,\n            )\n            base_lat[mask_has_ecef.values] = lat_deg\n            base_lon[mask_has_ecef.values] = lon_deg\n\n    merged[\"base_lat\"] = base_lat\n    merged[\"base_lon\"] = base_lon\n\n    # Fill remaining base_lat/lon per phone then global mean\n    merged[\"base_lat\"] = merged.groupby(\"phone\")[\"base_lat\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    merged[\"base_lon\"] = merged.groupby(\"phone\")[\"base_lon\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_base_lat = merged[\"base_lat\"].mean()\n    overall_base_lon = merged[\"base_lon\"].mean()\n    if np.isnan(overall_base_lat) or np.isnan(overall_base_lon):\n        if (\n            not merged.empty\n            and \"base_lat\" in merged.columns\n            and merged[\"base_lat\"].notna().any()\n        ):\n            overall_base_lat = merged[\"base_lat\"].dropna().mean()\n            overall_base_lon = merged[\"base_lon\"].dropna().mean()\n        else:\n            overall_base_lat = 0.0\n            overall_base_lon = 0.0\n    merged[\"base_lat\"] = merged[\"base_lat\"].fillna(overall_base_lat)\n    merged[\"base_lon\"] = merged[\"base_lon\"].fillna(overall_base_lon)\n\n    # Apply correction model if available\n    if correction_model_lat is not None and feature_cols is not None:\n        X_test = build_test_features(merged, feature_cols)\n        medians = X_test.median(numeric_only=True)\n        X_test = X_test.fillna(medians)\n        dlat_pred = correction_model_lat.predict(X_test[feature_cols].values)\n        dlon_pred = correction_model_lon.predict(X_test[feature_cols].values)\n\n        merged[\"pred_lat\"] = merged[\"base_lat\"] + dlat_pred\n        merged[\"pred_lon\"] = merged[\"base_lon\"] + dlon_pred\n    else:\n        merged[\"pred_lat\"] = merged[\"base_lat\"]\n        merged[\"pred_lon\"] = merged[\"base_lon\"]\n\n    # Build submission aligned to sample_sub\n    sample_for_merge = sample_sub.copy()\n    sample_for_merge[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample_for_merge[\"UnixTimeMillis\"], errors=\"coerce\"\n    )\n    sample_for_merge[\"UnixTimeMillis\"] = (\n        sample_for_merge[\"UnixTimeMillis\"].fillna(-1).astype(\"int64\")\n    )\n\n    merged_preds = merged[[\"phone\", \"UnixTimeMillis\", \"pred_lat\", \"pred_lon\"]].copy()\n\n    final = pd.merge(\n        sample_for_merge,\n        merged_preds,\n        how=\"left\",\n        on=[\"phone\", \"UnixTimeMillis\"],\n        suffixes=(\"\", \"_pred\"),\n    )\n\n    final[\"LatitudeDegrees\"] = final[\"pred_lat\"]\n    final[\"LongitudeDegrees\"] = final[\"pred_lon\"]\n\n    # Fill missing predictions per phone then global\n    final[\"LatitudeDegrees\"] = final.groupby(\"phone\")[\"LatitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    final[\"LongitudeDegrees\"] = final.groupby(\"phone\")[\"LongitudeDegrees\"].transform(\n        lambda s: s.ffill().bfill()\n    )\n    overall_lat = final[\"LatitudeDegrees\"].mean()\n    overall_lon = final[\"LongitudeDegrees\"].mean()\n    if np.isnan(overall_lat) or np.isnan(overall_lon):\n        overall_lat = 0.0\n        overall_lon = 0.0\n    final[\"LatitudeDegrees\"] = final[\"LatitudeDegrees\"].fillna(overall_lat)\n    final[\"LongitudeDegrees\"] = final[\"LongitudeDegrees\"].fillna(overall_lon)\n\n    final_submission = final[\n        [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n    ].copy()\n\n    out_path_submission = os.path.join(SUBMISSION_DIR, \"submission.csv\")\n    out_path_working = os.path.join(WORKING_DIR, \"submission.csv\")\n    final_submission.to_csv(out_path_submission, index=False)\n    final_submission.to_csv(out_path_working, index=False)\n    print(\n        f\"Saved submission to {out_path_submission} and {out_path_working}\",\n        flush=True,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ---------------- Coordinate transforms ----------------\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ---------------- Metric ----------------\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ---------------- Helpers for folder enumeration ----------------\ndef folder_drive_phone_keys(root):\n    keys = []\n    for drive_dir in sorted(glob.glob(os.path.join(root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            keys.append((drive_id, phone_name))\n    return keys\n\n\ndef build_simple_phone_string(drive_id, phone_name):\n    # match Kaggle format: \"<drive>_<shortmodel>\"\n    # map folder phone names to sample short names\n    mapping = {\n        \"GooglePixel4\": \"Pixel4\",\n        \"GooglePixel4XL\": \"Pixel4XL\",\n        \"GooglePixel5\": \"Pixel5\",\n        \"XiaomiMi8\": \"Mi8\",\n        \"SamsungGalaxyS20Ultra\": \"SamsungGalaxyS20Ultra\",\n    }\n    short = mapping.get(phone_name, phone_name)\n    return f\"{drive_id}_{short}\"\n\n\n# ---------------- Data loaders ----------------\ndef load_train(train_root, feature_cols):\n    rows = []\n    for drive_id, phone_name in folder_drive_phone_keys(train_root):\n        phone_dir = os.path.join(train_root, drive_id, phone_name)\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        gt_path = os.path.join(phone_dir, \"ground_truth.csv\")\n        if not (os.path.exists(gnss_path) and os.path.exists(gt_path)):\n            continue\n\n        try:\n            gnss = pd.read_csv(gnss_path)\n            gt = pd.read_csv(gt_path)\n        except Exception as e:\n            print(f\"Failed reading {gnss_path} or {gt_path}: {e}\")\n            continue\n\n        if \"utcTimeMillis\" not in gnss.columns or \"UnixTimeMillis\" not in gt.columns:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"utcTimeMillis\"], errors=\"coerce\")\n        gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        if len(gnss) == 0 or len(gt) == 0:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n        gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n        for c in feature_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n        if len(agg) == 0:\n            continue\n\n        gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n\n        merged = pd.merge(gt_small, agg, on=\"UnixTimeMillis\", how=\"inner\")\n        if len(merged) == 0:\n            continue\n\n        phone_str = build_simple_phone_string(drive_id, phone_name)\n        merged[\"phone\"] = phone_str\n        merged[\"drive_id\"] = drive_id\n\n        x_gt, y_gt, z_gt = llh_to_ecef(\n            merged[\"LatitudeDegrees\"].values,\n            merged[\"LongitudeDegrees\"].values,\n            np.zeros(len(merged)),\n        )\n        merged[\"x_gt\"] = x_gt\n        merged[\"y_gt\"] = y_gt\n        merged[\"z_gt\"] = z_gt\n        rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            \"No training data assembled; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols):\n    rows = []\n    for drive_id, phone_name in folder_drive_phone_keys(test_root):\n        phone_dir = os.path.join(test_root, drive_id, phone_name)\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            continue\n        try:\n            gnss = pd.read_csv(gnss_path)\n        except Exception as e:\n            print(f\"Failed reading {gnss_path}: {e}\")\n            continue\n        if \"utcTimeMillis\" not in gnss.columns:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"utcTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        if len(gnss) == 0:\n            continue\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n        for c in feature_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n        if len(agg) == 0:\n            continue\n        agg[\"phone\"] = build_simple_phone_string(drive_id, phone_name)\n        rows.append(agg)\n\n    if rows:\n        test_gnss = pd.concat(rows, ignore_index=True)\n    else:\n        test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"] + feature_cols)\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n    sample[\"phone\"] = sample[\"phone\"].astype(str)\n\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # Focus on relatively cheap but informative features\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n    if \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n        raise RuntimeError(\n            \"sample_submission.csv must contain 'phone' and 'UnixTimeMillis' columns.\"\n        )\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n\n    print(\"Loading training data...\")\n    train_df = load_train(train_root, feature_cols)\n    print(f\"Train rows assembled: {len(train_df)}\")\n\n    # Ensure feature columns present and fill NaNs with medians\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    groups = train_df[\"drive_id\"].values\n\n    print(\"Running 5-fold GroupKFold cross-validation...\")\n    gkf = GroupKFold(n_splits=5)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    fold_scores = []\n    for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n        lat_val_pred, lon_val_pred, _ = ecef_to_llh(\n            oof[val_idx, 0], oof[val_idx, 1], oof[val_idx, 2]\n        )\n        eval_df_fold = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"UnixTimeMillis\": train_df.iloc[val_idx][\"UnixTimeMillis\"].values,\n                \"lat_pred\": lat_val_pred,\n                \"lon_pred\": lon_val_pred,\n                \"lat_gt\": train_df.iloc[val_idx][\"LatitudeDegrees\"].values,\n                \"lon_gt\": train_df.iloc[val_idx][\"LongitudeDegrees\"].values,\n            }\n        )\n        fold_metric = competition_metric(eval_df_fold)\n        fold_scores.append(fold_metric)\n        print(f\"Fold {fold_idx} competition metric: {fold_metric:.4f}\")\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"OOF competition metric (mean of P50/P95 errors):\", metric_val)\n    valid_fold_scores = [s for s in fold_scores if not np.isnan(s)]\n    if valid_fold_scores:\n        mean_cv = float(np.mean(valid_fold_scores))\n        print(\"Mean CV metric across folds:\", mean_cv)\n\n    # Train final models on full data\n    print(\"Training final models on full training data...\")\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # Prepare test and submission\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    print(\"Loading and merging test GNSS features...\")\n    test_merged = load_test(\n        test_root,\n        sample_sub[[\"phone\", \"UnixTimeMillis\", \"row_id\"]],\n        feature_cols,\n    )\n\n    # Ensure row_id is preserved; if missing, merge back\n    if \"row_id\" not in test_merged.columns:\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    # Ensure feature columns and fill with train medians\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    test_merged = test_merged.sort_values(\"row_id\").reset_index(drop=True)\n    X_test = test_merged[feature_cols].values\n\n    print(\"Predicting on test set...\")\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align with full sample_sub, handling any rows that might not have matched GNSS\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill missing predictions by forward/backward fill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still NaNs (e.g. all were NaN), fallback to train mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"].values,\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"].values,\n        }\n    )\n\n    # Save submission files\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ---------------- Coordinate transforms ----------------\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ---------------- Metric ----------------\ndef competition_metric(df):\n    # df must have: phone, UnixTimeMillis, lat_pred, lon_pred, lat_gt, lon_gt\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ---------------- Helpers for folder enumeration ----------------\ndef folder_drive_phone_keys(root):\n    keys = []\n    if not os.path.isdir(root):\n        return keys\n    # root/train or root/test contains drive directories like \"2020-06-04-US-MTV-1\"\n    for drive_dir in sorted(glob.glob(os.path.join(root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            keys.append((drive_id, phone_name))\n    return keys\n\n\ndef build_phone_id_from_gt_path(gt_path):\n    # ground_truth.csv path: .../train/<drive_id>/<phone_name>/ground_truth.csv\n    # we construct phone id as \"<drive_id>_<phone_name>\"\n    parts = gt_path.replace(\"\\\\\", \"/\").split(\"/\")\n    # get drive_id and phone_name from path\n    if len(parts) < 3:\n        return None\n    phone_name = parts[-2]\n    drive_id = parts[-3]\n    return f\"{drive_id}_{phone_name}\"\n\n\ndef build_phone_id_from_dirs(drive_id, phone_name):\n    # For test where ground_truth is absent, mirror train naming\n    return f\"{drive_id}_{phone_name}\"\n\n\n# ---------------- Data loaders ----------------\ndef load_train(train_root, feature_cols):\n    rows = []\n    # Here we enumerate all ground_truth.csv files to be robust to truncation in listing\n    gt_files = glob.glob(os.path.join(train_root, \"*\", \"*\", \"ground_truth.csv\"))\n    for gt_path in sorted(gt_files):\n        try:\n            gt = pd.read_csv(gt_path)\n        except Exception as e:\n            print(f\"Failed reading {gt_path}: {e}\")\n            continue\n\n        # Build paths for corresponding device_gnss.csv\n        phone_dir = os.path.dirname(gt_path)\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            print(f\"Missing gnss file for {gt_path}\")\n            continue\n        try:\n            gnss = pd.read_csv(gnss_path)\n        except Exception as e:\n            print(f\"Failed reading {gnss_path}: {e}\")\n            continue\n\n        if \"utcTimeMillis\" not in gnss.columns or \"UnixTimeMillis\" not in gt.columns:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"utcTimeMillis\"], errors=\"coerce\")\n        gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        if len(gnss) == 0 or len(gt) == 0:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n        gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n        for c in feature_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n        if len(agg) == 0:\n            continue\n\n        gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n\n        merged = pd.merge(gt_small, agg, on=\"UnixTimeMillis\", how=\"inner\")\n        if len(merged) == 0:\n            continue\n\n        phone_id = build_phone_id_from_gt_path(gt_path)\n        if phone_id is None:\n            continue\n        drive_id = phone_id.split(\"_\")[0]\n        merged[\"phone\"] = phone_id\n        merged[\"drive_id\"] = drive_id\n\n        x_gt, y_gt, z_gt = llh_to_ecef(\n            merged[\"LatitudeDegrees\"].values,\n            merged[\"LongitudeDegrees\"].values,\n            np.zeros(len(merged)),\n        )\n        merged[\"x_gt\"] = x_gt\n        merged[\"y_gt\"] = y_gt\n        merged[\"z_gt\"] = z_gt\n        rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            f\"No training data assembled from {train_root}; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols):\n    rows = []\n    for drive_id, phone_name in folder_drive_phone_keys(test_root):\n        phone_dir = os.path.join(test_root, drive_id, phone_name)\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            continue\n        try:\n            gnss = pd.read_csv(gnss_path)\n        except Exception as e:\n            print(f\"Failed reading {gnss_path}: {e}\")\n            continue\n        if \"utcTimeMillis\" not in gnss.columns:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"utcTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        if len(gnss) == 0:\n            continue\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n        for c in feature_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n        if len(agg) == 0:\n            continue\n        agg[\"phone\"] = build_phone_id_from_dirs(drive_id, phone_name)\n        rows.append(agg)\n\n    if rows:\n        test_gnss = pd.concat(rows, ignore_index=True)\n    else:\n        test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"] + feature_cols)\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n    sample[\"phone\"] = sample[\"phone\"].astype(str)\n\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    # GNSS-derived features including WLS position\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n    if \"phone\" not in sample_sub.columns or \"UnixTimeMillis\" not in sample_sub.columns:\n        raise RuntimeError(\n            \"sample_submission.csv must contain 'phone' and 'UnixTimeMillis' columns.\"\n        )\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str)\n\n    # Rebuild phone ids in sample to match our scheme: \"<drive_id>_<phone_name>\"\n    # Original format in sample is \"<drive_id>_<shortmodel>\" which may differ\n    # from folder phone_name; but our training phone ids will be exactly that\n    # from ground_truth path, so we map sample phones to closest match by prefix.\n    # To avoid complicated mapping, infer mapping from available train phone ids.\n    print(\"Loading training data...\")\n    train_df_raw = load_train(train_root, feature_cols)\n    print(f\"Train rows assembled: {len(train_df_raw)}\")\n\n    # Derive mapping from drive-> available phone suffixes from training\n    unique_train_phones = train_df_raw[\"phone\"].unique()\n    # phone pattern: \"<drive_id>_<phone_name>\"\n    train_drive_phone = {}\n    for ph in unique_train_phones:\n        parts = ph.split(\"_\", 1)\n        if len(parts) != 2:\n            continue\n        d, pname = parts\n        train_drive_phone.setdefault(d, set()).add(pname)\n\n    def remap_sample_phone(p):\n        # p format from sample: \"<drive_id>_<shortmodel>\"\n        parts = str(p).split(\"_\", 1)\n        if len(parts) != 2:\n            return p\n        d, short = parts\n        if d not in train_drive_phone:\n            return p\n        # choose any phone_name for that drive (only one phone per drive in this setup)\n        pname = sorted(train_drive_phone[d])[0]\n        return f\"{d}_{pname}\"\n\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].apply(remap_sample_phone)\n\n    # Apply same phone remapping to train df, to keep consistent with sample\n    train_df = train_df_raw.copy()\n\n    # Ensure feature columns present and fill NaNs with medians\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    groups = train_df[\"drive_id\"].values\n\n    print(\"Running 5-fold GroupKFold cross-validation...\")\n    # Guard against cases with fewer than 5 groups\n    unique_groups = np.unique(groups)\n    n_splits = min(5, len(unique_groups))\n    if n_splits < 2:\n        n_splits = 2\n    gkf = GroupKFold(n_splits=n_splits)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    fold_scores = []\n    for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n        lat_val_pred, lon_val_pred, _ = ecef_to_llh(\n            oof[val_idx, 0], oof[val_idx, 1], oof[val_idx, 2]\n        )\n        eval_df_fold = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"UnixTimeMillis\": train_df.iloc[val_idx][\"UnixTimeMillis\"].values,\n                \"lat_pred\": lat_val_pred,\n                \"lon_pred\": lon_val_pred,\n                \"lat_gt\": train_df.iloc[val_idx][\"LatitudeDegrees\"].values,\n                \"lon_gt\": train_df.iloc[val_idx][\"LongitudeDegrees\"].values,\n            }\n        )\n        fold_metric = competition_metric(eval_df_fold)\n        fold_scores.append(fold_metric)\n        print(f\"Fold {fold_idx} competition metric: {fold_metric:.4f}\")\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"OOF competition metric (mean of P50/P95 errors):\", metric_val)\n    valid_fold_scores = [s for s in fold_scores if not np.isnan(s)]\n    if valid_fold_scores:\n        mean_cv = float(np.mean(valid_fold_scores))\n        print(\"Mean CV metric across folds:\", mean_cv)\n\n    # Train final models on full data\n    print(\"Training final models on full training data...\")\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # Prepare test and submission\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    print(\"Loading and merging test GNSS features...\")\n    test_merged = load_test(\n        test_root,\n        sample_sub[[\"phone\", \"UnixTimeMillis\", \"row_id\"]],\n        feature_cols,\n    )\n\n    # Ensure row_id is preserved; if missing, merge back\n    if \"row_id\" not in test_merged.columns:\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    # Ensure feature columns and fill with train medians\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    test_merged = test_merged.sort_values(\"row_id\").reset_index(drop=True)\n    X_test = test_merged[feature_cols].values\n\n    print(\"Predicting on test set...\")\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align with full sample_sub, handling any rows that might not have matched GNSS\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill missing predictions by forward/backward fill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still NaNs (e.g. all were NaN), fallback to train mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"].values,\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"].values,\n        }\n    )\n\n    # Save submission files\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n\n# ---------------- Coordinate transforms ----------------\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# ---------------- Metric ----------------\ndef competition_metric(df):\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# ---------------- Helpers ----------------\ndef folder_drive_phone_keys(root):\n    keys = []\n    if not os.path.isdir(root):\n        return keys\n    # expect structure root/<drive_id>/<phone_name>/\n    for drive_dir in sorted(glob.glob(os.path.join(root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            keys.append((drive_id, phone_name))\n    return keys\n\n\ndef phone_id_from_dirs(drive_id, phone_name):\n    # Keep consistent phone id format everywhere\n    return f\"{drive_id}_{phone_name}\"\n\n\n# ---------------- Data loaders ----------------\ndef load_train(train_root, feature_cols):\n    rows = []\n    # robustly find all ground_truth.csv\n    gt_files = glob.glob(os.path.join(train_root, \"*\", \"*\", \"ground_truth.csv\"))\n    for gt_path in sorted(gt_files):\n        try:\n            gt = pd.read_csv(gt_path)\n        except Exception as e:\n            print(f\"Failed reading {gt_path}: {e}\")\n            continue\n\n        phone_dir = os.path.dirname(gt_path)\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            print(f\"Missing gnss file for {gt_path}\")\n            continue\n        try:\n            gnss = pd.read_csv(gnss_path)\n        except Exception as e:\n            print(f\"Failed reading {gnss_path}: {e}\")\n            continue\n\n        if \"utcTimeMillis\" not in gnss.columns or \"UnixTimeMillis\" not in gt.columns:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"utcTimeMillis\"], errors=\"coerce\")\n        gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        if len(gnss) == 0 or len(gt) == 0:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n        gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n        for c in feature_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n        if len(agg) == 0:\n            continue\n\n        gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n\n        merged = pd.merge(gt_small, agg, on=\"UnixTimeMillis\", how=\"inner\")\n        if len(merged) == 0:\n            continue\n\n        # derive drive_id and phone_name from path\n        parts = phone_dir.replace(\"\\\\\", \"/\").split(\"/\")\n        phone_name = parts[-1]\n        drive_id = parts[-2]\n        phone_id = phone_id_from_dirs(drive_id, phone_name)\n\n        merged[\"phone\"] = phone_id\n        merged[\"drive_id\"] = drive_id\n\n        x_gt, y_gt, z_gt = llh_to_ecef(\n            merged[\"LatitudeDegrees\"].values,\n            merged[\"LongitudeDegrees\"].values,\n            np.zeros(len(merged)),\n        )\n        merged[\"x_gt\"] = x_gt\n        merged[\"y_gt\"] = y_gt\n        merged[\"z_gt\"] = z_gt\n        rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            f\"No training data assembled from {train_root}; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test(test_root, sample_sub, feature_cols):\n    rows = []\n    for drive_id, phone_name in folder_drive_phone_keys(test_root):\n        phone_dir = os.path.join(test_root, drive_id, phone_name)\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            continue\n        try:\n            gnss = pd.read_csv(gnss_path)\n        except Exception as e:\n            print(f\"Failed reading {gnss_path}: {e}\")\n            continue\n        if \"utcTimeMillis\" not in gnss.columns:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"utcTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        if len(gnss) == 0:\n            continue\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n        for c in feature_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n        if len(agg) == 0:\n            continue\n        agg[\"phone\"] = phone_id_from_dirs(drive_id, phone_name)\n        rows.append(agg)\n\n    if rows:\n        test_gnss = pd.concat(rows, ignore_index=True)\n    else:\n        test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"] + feature_cols)\n\n    sample = sample_sub.copy()\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(sample[\"UnixTimeMillis\"], errors=\"coerce\")\n    sample = sample.dropna(subset=[\"UnixTimeMillis\"]).copy()\n    sample[\"UnixTimeMillis\"] = sample[\"UnixTimeMillis\"].astype(\"int64\")\n    sample[\"phone\"] = sample[\"phone\"].astype(str)\n\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"phone\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\ndef main():\n    input_dir = \"./input\"\n\n    # In this benchmark, train and test are under ./input/train and ./input/test\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    # sample_submission phone format is \"<drive_id>_<shortmodel>\"; we need to map to\n    # our internal representation \"<drive_id>_<full_phone_name>\".\n    print(\"Loading training data...\")\n    train_df = load_train(train_root, feature_cols)\n    print(f\"Train rows assembled: {len(train_df)}\")\n\n    # Build mapping from drive_id -> set of full phone names in train and test\n    train_phone_ids = train_df[\"phone\"].unique()\n    drive_to_fullphones = {}\n    for ph in train_phone_ids:\n        d, pname = ph.split(\"_\", 1)\n        drive_to_fullphones.setdefault(d, set()).add(pname)\n\n    # Also gather phones from test folders, to handle drives only in test\n    for drive_id, phone_name in folder_drive_phone_keys(test_root):\n        drive_to_fullphones.setdefault(drive_id, set()).add(phone_name)\n\n    def remap_sample_phone(p):\n        parts = str(p).split(\"_\", 1)\n        if len(parts) != 2:\n            return p\n        d, _short = parts\n        if d not in drive_to_fullphones or len(drive_to_fullphones[d]) == 0:\n            return p  # fallback to original if no info\n        pname = sorted(drive_to_fullphones[d])[0]\n        return f\"{d}_{pname}\"\n\n    sample_sub = sample_sub.copy()\n    sample_sub[\"phone\"] = sample_sub[\"phone\"].astype(str).apply(remap_sample_phone)\n\n    # Ensure feature columns present and fill NaNs with medians\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    groups = train_df[\"drive_id\"].values\n\n    print(\"Running GroupKFold cross-validation...\")\n    unique_groups = np.unique(groups)\n    n_splits = min(5, len(unique_groups))\n    if n_splits < 2:\n        n_splits = 2\n    gkf = GroupKFold(n_splits=n_splits)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    fold_scores = []\n    for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n        lat_val_pred, lon_val_pred, _ = ecef_to_llh(\n            oof[val_idx, 0], oof[val_idx, 1], oof[val_idx, 2]\n        )\n        eval_df_fold = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"UnixTimeMillis\": train_df.iloc[val_idx][\"UnixTimeMillis\"].values,\n                \"lat_pred\": lat_val_pred,\n                \"lon_pred\": lon_val_pred,\n                \"lat_gt\": train_df.iloc[val_idx][\"LatitudeDegrees\"].values,\n                \"lon_gt\": train_df.iloc[val_idx][\"LongitudeDegrees\"].values,\n            }\n        )\n        fold_metric = competition_metric(eval_df_fold)\n        fold_scores.append(fold_metric)\n        print(f\"Fold {fold_idx} competition metric: {fold_metric:.4f}\")\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"OOF competition metric (mean of P50/P95 errors):\", metric_val)\n    valid_fold_scores = [s for s in fold_scores if not np.isnan(s)]\n    if valid_fold_scores:\n        mean_cv = float(np.mean(valid_fold_scores))\n        print(\"Mean CV metric across folds:\", mean_cv)\n\n    # Train final models on full data\n    print(\"Training final models on full training data...\")\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # Prepare test and submission\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    print(\"Loading and merging test GNSS features...\")\n    test_merged = load_test(\n        test_root,\n        sample_sub[[\"phone\", \"UnixTimeMillis\", \"row_id\"]],\n        feature_cols,\n    )\n\n    # Ensure row_id is preserved; if missing, merge back\n    if \"row_id\" not in test_merged.columns:\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    # Ensure feature columns and fill with train medians\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    test_merged = test_merged.sort_values(\"row_id\").reset_index(drop=True)\n    X_test = test_merged[feature_cols].values\n\n    print(\"Predicting on test set...\")\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    # Align with full sample_sub, handling any rows that might not have matched GNSS\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill missing predictions by forward/backward fill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still NaNs (e.g. all were NaN), fallback to train mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"].values,\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"].values,\n        }\n    )\n\n    # Save submission files\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# --------------- Coordinate transforms ---------------\n\n\ndef llh_to_ecef(lat, lon, h):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    lat = np.radians(lat)\n    lon = np.radians(lon)\n    N = a / np.sqrt(1.0 - e2 * np.sin(lat) ** 2)\n    x = (N + h) * np.cos(lat) * np.cos(lon)\n    y = (N + h) * np.cos(lat) * np.sin(lon)\n    z = (N * (1.0 - e2) + h) * np.sin(lat)\n    return x, y, z\n\n\ndef ecef_to_llh(x, y, z):\n    a = 6378137.0\n    e2 = 6.69437999014e-3\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    z = np.asarray(z, dtype=float)\n\n    lon = np.arctan2(y, x)\n    p = np.sqrt(x**2 + y**2)\n    lat = np.arctan2(z, p * (1 - e2))\n    for _ in range(5):\n        N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n        h = p / np.cos(lat) - N\n        lat = np.arctan2(z, p * (1 - e2 * (N / (N + h))))\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    h = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), h\n\n\n# --------------- Metric ---------------\n\n\ndef competition_metric(df):\n    x_p, y_p, z_p = llh_to_ecef(\n        df[\"lat_pred\"].values, df[\"lon_pred\"].values, np.zeros(len(df))\n    )\n    x_g, y_g, z_g = llh_to_ecef(\n        df[\"lat_gt\"].values, df[\"lon_gt\"].values, np.zeros(len(df))\n    )\n    dist = np.sqrt((x_p - x_g) ** 2 + (y_p - y_g) ** 2 + (z_p - z_g) ** 2)\n    df_tmp = df.copy()\n    df_tmp[\"dist\"] = dist\n    per_phone = []\n    for _, g in df_tmp.groupby(\"phone\"):\n        d = g[\"dist\"].values\n        if len(d) == 0:\n            continue\n        p50 = np.percentile(d, 50)\n        p95 = np.percentile(d, 95)\n        per_phone.append((p50 + p95) / 2.0)\n    if not per_phone:\n        return np.nan\n    return float(np.mean(per_phone))\n\n\n# --------------- Helpers ---------------\n\n\ndef parse_phone_string(phone_str):\n    \"\"\"\n    sample_submission phones look like \"<drive_id>_<phone_name>\",\n    where drive_id has no underscores. We'll split on the first underscore.\n    \"\"\"\n    s = str(phone_str)\n    parts = s.split(\"_\", 1)\n    if len(parts) == 2:\n        return parts[0], parts[1]\n    # Fallback: no drive part detectable\n    return s, \"\"\n\n\ndef folder_drive_phone_keys(root):\n    keys = []\n    if not os.path.isdir(root):\n        return keys\n    for drive_dir in sorted(glob.glob(os.path.join(root, \"*\"))):\n        if not os.path.isdir(drive_dir):\n            continue\n        drive_id = os.path.basename(drive_dir)\n        for phone_dir in sorted(glob.glob(os.path.join(drive_dir, \"*\"))):\n            if not os.path.isdir(phone_dir):\n                continue\n            phone_name = os.path.basename(phone_dir)\n            keys.append((drive_id, phone_name))\n    return keys\n\n\ndef make_phone_id(drive_id, phone_name):\n    return f\"{drive_id}_{phone_name}\"\n\n\n# --------------- Data loaders ---------------\n\n\ndef load_train(train_root, feature_cols):\n    rows = []\n    gt_files = glob.glob(os.path.join(train_root, \"*\", \"*\", \"ground_truth.csv\"))\n    for gt_path in sorted(gt_files):\n        try:\n            gt = pd.read_csv(gt_path)\n        except Exception as e:\n            print(f\"Failed reading {gt_path}: {e}\")\n            continue\n\n        phone_dir = os.path.dirname(gt_path)\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            print(f\"Missing gnss file for {gt_path}\")\n            continue\n        try:\n            gnss = pd.read_csv(gnss_path)\n        except Exception as e:\n            print(f\"Failed reading {gnss_path}: {e}\")\n            continue\n\n        if \"utcTimeMillis\" not in gnss.columns or \"UnixTimeMillis\" not in gt.columns:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"utcTimeMillis\"], errors=\"coerce\")\n        gt[\"UnixTimeMillis\"] = pd.to_numeric(gt[\"UnixTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        gt = gt.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        if len(gnss) == 0 or len(gt) == 0:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n        gt[\"UnixTimeMillis\"] = gt[\"UnixTimeMillis\"].astype(\"int64\")\n\n        for c in feature_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n        if len(agg) == 0:\n            continue\n\n        gt_small = gt[[\"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]].copy()\n        merged = pd.merge(gt_small, agg, on=\"UnixTimeMillis\", how=\"inner\")\n        if len(merged) == 0:\n            continue\n\n        parts = phone_dir.replace(\"\\\\\", \"/\").split(\"/\")\n        phone_name = parts[-1]\n        drive_id = parts[-2]\n        phone_id = make_phone_id(drive_id, phone_name)\n        merged[\"phone\"] = phone_id\n        merged[\"drive_id\"] = drive_id\n\n        x_gt, y_gt, z_gt = llh_to_ecef(\n            merged[\"LatitudeDegrees\"].values,\n            merged[\"LongitudeDegrees\"].values,\n            np.zeros(len(merged)),\n        )\n        merged[\"x_gt\"] = x_gt\n        merged[\"y_gt\"] = y_gt\n        merged[\"z_gt\"] = z_gt\n        rows.append(merged)\n\n    if not rows:\n        raise RuntimeError(\n            f\"No training data assembled from {train_root}; check train directory structure.\"\n        )\n    train_df = pd.concat(rows, ignore_index=True)\n    return train_df\n\n\ndef load_test_from_sample(test_root, sample_sub, feature_cols):\n    \"\"\"\n    Build test feature table aligned with sample_sub, using phone strings\n    exactly as in sample_sub. We parse drive_id/phone_name from that string\n    and read the corresponding test folder.\n    \"\"\"\n    # Pre-index sample rows by their drive/phone names for quick join\n    sample = sample_sub.copy()\n    sample[\"drive_id\"], sample[\"phone_name\"] = zip(\n        *sample[\"phone\"].map(parse_phone_string)\n    )\n    sample[\"UnixTimeMillis\"] = pd.to_numeric(\n        sample[\"UnixTimeMillis\"], errors=\"coerce\"\n    ).astype(\"Int64\")\n\n    all_feat_rows = []\n\n    # For each unique drive/phone_name combination in sample, read GNSS and aggregate\n    for (drive_id, phone_name), sub_idx in sample.groupby(\n        [\"drive_id\", \"phone_name\"]\n    ).groups.items():\n        if drive_id is None or phone_name is None or drive_id == \"\" or phone_name == \"\":\n            continue\n        phone_dir = os.path.join(test_root, drive_id, phone_name)\n        gnss_path = os.path.join(phone_dir, \"device_gnss.csv\")\n        if not os.path.exists(gnss_path):\n            # No data for this combination, will be filled later\n            continue\n        try:\n            gnss = pd.read_csv(gnss_path)\n        except Exception as e:\n            print(f\"Failed reading {gnss_path}: {e}\")\n            continue\n\n        if \"utcTimeMillis\" not in gnss.columns:\n            continue\n\n        gnss[\"UnixTimeMillis\"] = pd.to_numeric(gnss[\"utcTimeMillis\"], errors=\"coerce\")\n        gnss = gnss.dropna(subset=[\"UnixTimeMillis\"]).copy()\n        if len(gnss) == 0:\n            continue\n        gnss[\"UnixTimeMillis\"] = gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n        for c in feature_cols:\n            if c not in gnss.columns:\n                gnss[c] = np.nan\n\n        agg = gnss.groupby(\"UnixTimeMillis\")[feature_cols].mean().reset_index()\n        agg[\"drive_id\"] = drive_id\n        agg[\"phone_name\"] = phone_name\n        all_feat_rows.append(agg)\n\n    if all_feat_rows:\n        test_gnss = pd.concat(all_feat_rows, ignore_index=True)\n    else:\n        # Empty placeholder; merge will produce NaNs for features\n        test_gnss = pd.DataFrame(\n            columns=[\"drive_id\", \"phone_name\", \"UnixTimeMillis\"] + feature_cols\n        )\n\n    # Cast to int64 for merge\n    test_gnss[\"UnixTimeMillis\"] = pd.to_numeric(\n        test_gnss[\"UnixTimeMillis\"], errors=\"coerce\"\n    ).astype(\"Int64\")\n\n    # Merge with sample to align rows\n    merged = pd.merge(\n        sample,\n        test_gnss,\n        on=[\"drive_id\", \"phone_name\", \"UnixTimeMillis\"],\n        how=\"left\",\n    )\n    return merged\n\n\n# --------------- Main ---------------\n\n\ndef main():\n    input_dir = \"./input\"\n    train_root = os.path.join(input_dir, \"train\")\n    test_root = os.path.join(input_dir, \"test\")\n\n    feature_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n\n    sample_sub_path = os.path.join(input_dir, \"sample_submission.csv\")\n    sample_sub = pd.read_csv(sample_sub_path)\n\n    print(\"Loading training data...\")\n    train_df = load_train(train_root, feature_cols)\n    print(f\"Train rows assembled: {len(train_df)}\")\n\n    # Ensure features and fill NaNs with medians\n    for c in feature_cols:\n        if c not in train_df.columns:\n            train_df[c] = np.nan\n    train_df[feature_cols] = train_df[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n    X = train_df[feature_cols].values\n    y_x = train_df[\"x_gt\"].values\n    y_y = train_df[\"y_gt\"].values\n    y_z = train_df[\"z_gt\"].values\n\n    groups = train_df[\"drive_id\"].values\n\n    print(\"Running GroupKFold cross-validation...\")\n    unique_groups = np.unique(groups)\n    n_splits = min(5, len(unique_groups))\n    if n_splits < 2:\n        n_splits = 2\n    gkf = GroupKFold(n_splits=n_splits)\n    oof = np.zeros((len(train_df), 3))\n\n    base_params = dict(\n        n_estimators=150,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=42,\n    )\n\n    fold_scores = []\n    for fold_idx, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n        X_tr, X_val = X[tr_idx], X[val_idx]\n        yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n        yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n        yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n        mx = GradientBoostingRegressor(**base_params)\n        my = GradientBoostingRegressor(**base_params)\n        mz = GradientBoostingRegressor(**base_params)\n\n        mx.fit(X_tr, yx_tr)\n        my.fit(X_tr, yy_tr)\n        mz.fit(X_tr, yz_tr)\n\n        oof[val_idx, 0] = mx.predict(X_val)\n        oof[val_idx, 1] = my.predict(X_val)\n        oof[val_idx, 2] = mz.predict(X_val)\n\n        lat_val_pred, lon_val_pred, _ = ecef_to_llh(\n            oof[val_idx, 0], oof[val_idx, 1], oof[val_idx, 2]\n        )\n        eval_df_fold = pd.DataFrame(\n            {\n                \"phone\": train_df.iloc[val_idx][\"phone\"].values,\n                \"UnixTimeMillis\": train_df.iloc[val_idx][\"UnixTimeMillis\"].values,\n                \"lat_pred\": lat_val_pred,\n                \"lon_pred\": lon_val_pred,\n                \"lat_gt\": train_df.iloc[val_idx][\"LatitudeDegrees\"].values,\n                \"lon_gt\": train_df.iloc[val_idx][\"LongitudeDegrees\"].values,\n            }\n        )\n        fold_metric = competition_metric(eval_df_fold)\n        fold_scores.append(fold_metric)\n        print(f\"Fold {fold_idx} competition metric: {fold_metric:.6f}\")\n\n    lat_oof, lon_oof, _ = ecef_to_llh(oof[:, 0], oof[:, 1], oof[:, 2])\n    eval_df = pd.DataFrame(\n        {\n            \"phone\": train_df[\"phone\"].values,\n            \"UnixTimeMillis\": train_df[\"UnixTimeMillis\"].values,\n            \"lat_pred\": lat_oof,\n            \"lon_pred\": lon_oof,\n            \"lat_gt\": train_df[\"LatitudeDegrees\"].values,\n            \"lon_gt\": train_df[\"LongitudeDegrees\"].values,\n        }\n    )\n    metric_val = competition_metric(eval_df)\n    print(\"OOF competition metric (mean of P50/P95 errors):\", metric_val)\n    valid_fold_scores = [s for s in fold_scores if not np.isnan(s)]\n    if valid_fold_scores:\n        mean_cv = float(np.mean(valid_fold_scores))\n        print(\"Mean CV metric across folds:\", mean_cv)\n\n    # Train final models on full data\n    print(\"Training final models on full training data...\")\n    final_params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=3,\n        random_state=99,\n    )\n    final_mx = GradientBoostingRegressor(**final_params)\n    final_my = GradientBoostingRegressor(**final_params)\n    final_mz = GradientBoostingRegressor(**final_params)\n\n    final_mx.fit(X, y_x)\n    final_my.fit(X, y_y)\n    final_mz.fit(X, y_z)\n\n    # Prepare test and submission\n    sample_sub = sample_sub.copy()\n    sample_sub[\"row_id\"] = np.arange(len(sample_sub))\n\n    print(\"Loading and merging test GNSS features based on sample_submission phones...\")\n    test_merged = load_test_from_sample(\n        test_root,\n        sample_sub[[\"phone\", \"UnixTimeMillis\", \"row_id\"]],\n        feature_cols,\n    )\n\n    # Ensure row_id preserved\n    if \"row_id\" not in test_merged.columns:\n        test_merged = pd.merge(\n            sample_sub[[\"row_id\", \"phone\", \"UnixTimeMillis\"]],\n            test_merged,\n            on=[\"phone\", \"UnixTimeMillis\"],\n            how=\"left\",\n        )\n\n    for c in feature_cols:\n        if c not in test_merged.columns:\n            test_merged[c] = np.nan\n    medians = train_df[feature_cols].median()\n    test_merged[feature_cols] = test_merged[feature_cols].fillna(medians)\n\n    test_merged = test_merged.sort_values(\"row_id\").reset_index(drop=True)\n    X_test = test_merged[feature_cols].values\n\n    print(\"Predicting on test set...\")\n    x_pred = final_mx.predict(X_test)\n    y_pred = final_my.predict(X_test)\n    z_pred = final_mz.predict(X_test)\n\n    lat_pred, lon_pred, _ = ecef_to_llh(x_pred, y_pred, z_pred)\n\n    pred_df = pd.DataFrame(\n        {\n            \"row_id\": test_merged[\"row_id\"].values,\n            \"LatitudeDegrees\": lat_pred,\n            \"LongitudeDegrees\": lon_pred,\n        }\n    )\n\n    submission_tmp = sample_sub[[\"row_id\"]].merge(pred_df, on=\"row_id\", how=\"left\")\n    submission_tmp = submission_tmp.sort_values(\"row_id\")\n\n    # Fill missing predictions by forward/backward fill\n    submission_tmp[\"LatitudeDegrees\"] = (\n        submission_tmp[\"LatitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n    submission_tmp[\"LongitudeDegrees\"] = (\n        submission_tmp[\"LongitudeDegrees\"].fillna(method=\"ffill\").fillna(method=\"bfill\")\n    )\n\n    # If still NaNs, fallback to train mean lat/lon\n    if (\n        submission_tmp[\"LatitudeDegrees\"].isna().any()\n        or submission_tmp[\"LongitudeDegrees\"].isna().any()\n    ):\n        lat_mean = train_df[\"LatitudeDegrees\"].mean()\n        lon_mean = train_df[\"LongitudeDegrees\"].mean()\n        submission_tmp[\"LatitudeDegrees\"] = submission_tmp[\"LatitudeDegrees\"].fillna(\n            lat_mean\n        )\n        submission_tmp[\"LongitudeDegrees\"] = submission_tmp[\"LongitudeDegrees\"].fillna(\n            lon_mean\n        )\n\n    final_out = pd.DataFrame(\n        {\n            \"phone\": sample_sub[\"phone\"],\n            \"UnixTimeMillis\": sample_sub[\"UnixTimeMillis\"],\n            \"LatitudeDegrees\": submission_tmp[\"LatitudeDegrees\"].values,\n            \"LongitudeDegrees\": submission_tmp[\"LongitudeDegrees\"].values,\n        }\n    )\n\n    # Save submission files\n    submission_dir = \"./submission\"\n    os.makedirs(submission_dir, exist_ok=True)\n    submission_path = os.path.join(submission_dir, \"submission.csv\")\n    final_out.to_csv(submission_path, index=False)\n    print(f\"Saved submission to {submission_path}\")\n\n    working_dir = \"./working\"\n    os.makedirs(working_dir, exist_ok=True)\n    working_submission_path = os.path.join(working_dir, \"submission.csv\")\n    final_out.to_csv(working_submission_path, index=False)\n    print(f\"Mirrored submission to {working_submission_path}\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nWGS84_A = 6378137.0\nWGS84_E2 = 6.69437999014e-3\n\n\ndef geodetic_to_ecef(lat_deg, lon_deg, alt_m):\n    lat = np.radians(lat_deg)\n    lon = np.radians(lon_deg)\n    a = WGS84_A\n    e2 = WGS84_E2\n    sin_lat = np.sin(lat)\n    cos_lat = np.cos(lat)\n    N = a / np.sqrt(1 - e2 * sin_lat * sin_lat)\n    X = (N + alt_m) * cos_lat * np.cos(lon)\n    Y = (N + alt_m) * cos_lat * np.sin(lon)\n    Z = (N * (1 - e2) + alt_m) * sin_lat\n    return X, Y, Z\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = WGS84_A\n    e2 = WGS84_E2\n    b = a * np.sqrt(1 - e2)\n    ep2 = (a * a - b * b) / (b * b)\n    p = np.sqrt(x * x + y * y)\n    th = np.arctan2(a * z, b * p)\n    lon = np.arctan2(y, x)\n    lat = np.arctan2(z + ep2 * b * np.sin(th) ** 3, p - e2 * a * np.cos(th) ** 3)\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    alt = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), alt\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    # df must have columns: lat_gt, lon_gt, lat_pred, lon_pred, phone\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    # basic cleaning\n    gt = gt.dropna(\n        subset=[\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"AltitudeMeters\",\n            \"UnixTimeMillis\",\n        ]\n    )\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n\n    # Derive epoch time in ms to join with ground truth\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # aggregate gnss by epoch\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # join with ground truth\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # target: ECEF from ground truth\n    X_ecef, Y_ecef, Z_ecef = geodetic_to_ecef(\n        df[\"LatitudeDegrees\"].values,\n        df[\"LongitudeDegrees\"].values,\n        df[\"AltitudeMeters\"].values,\n    )\n    df[\"X_ecef\"] = X_ecef\n    df[\"Y_ecef\"] = Y_ecef\n    df[\"Z_ecef\"] = Z_ecef\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n\n    # simple time-based features per phone: normalized time within trace\n    df = df.sort_values(\"UnixTimeMillis\")\n    t_min = df[\"UnixTimeMillis\"].min()\n    t_max = df[\"UnixTimeMillis\"].max()\n    if t_max > t_min:\n        df[\"time_norm\"] = (df[\"UnixTimeMillis\"] - t_min) / (t_max - t_min)\n    else:\n        df[\"time_norm\"] = 0.0\n\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n\n    # time features per phone (within this phone trace)\n    g_agg = g_agg.sort_values(\"UnixTimeMillis\")\n    t_min = g_agg[\"UnixTimeMillis\"].min()\n    t_max = g_agg[\"UnixTimeMillis\"].max()\n    if t_max > t_min:\n        g_agg[\"time_norm\"] = (g_agg[\"UnixTimeMillis\"] - t_min) / (t_max - t_min)\n    else:\n        g_agg[\"time_norm\"] = 0.0\n\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception:\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\n\n# Sort and create temporal diffs for mean features\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\nmean_feats = [c for c in train_df.columns if c.endswith(\"_mean\")]\nfor feat in mean_feats:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# feature columns: aggregated gnss stats, diffs, WLS positions, and time_norm\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"X_ecef\",\n    \"Y_ecef\",\n    \"Z_ecef\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        c.endswith(\"_mean\")\n        or c.endswith(\"_std\")\n        or c.endswith(\"_diff\")\n        or \"WlsPosition\" in c\n        or c == \"time_norm\"\n    )\n]\n\n# fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_x = train_df[\"X_ecef\"].values\ny_y = train_df[\"Y_ecef\"].values\ny_z = train_df[\"Z_ecef\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_preds_x = np.zeros(len(train_df))\noof_preds_y = np.zeros(len(train_df))\noof_preds_z = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n    yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n    yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_x = LGBMRegressor(**params)\n    model_y = LGBMRegressor(**params)\n    model_z = LGBMRegressor(**params)\n\n    model_x.fit(\n        X_tr, yx_tr, eval_set=[(X_val, yx_val)], eval_metric=\"l2\", verbose=False\n    )\n    model_y.fit(\n        X_tr, yy_tr, eval_set=[(X_val, yy_val)], eval_metric=\"l2\", verbose=False\n    )\n    model_z.fit(\n        X_tr, yz_tr, eval_set=[(X_val, yz_val)], eval_metric=\"l2\", verbose=False\n    )\n\n    oof_preds_x[val_idx] = model_x.predict(X_val)\n    oof_preds_y[val_idx] = model_y.predict(X_val)\n    oof_preds_z[val_idx] = model_z.predict(X_val)\n\n# build oof dataframe for metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\nlat_pred, lon_pred, _ = ecef_to_geodetic(oof_preds_x, oof_preds_y, oof_preds_z)\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = lat_pred\noof_df[\"lon_pred\"] = lon_pred\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_x = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_y = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_z = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_x.fit(X, y_x)\nfinal_model_y.fit(X, y_y)\nfinal_model_z.fit(X, y_z)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Detect identifier column name: 'phone' in original, 'tripId' in this benchmark\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Create a unified 'phone' column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception:\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\ntest_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Temporal sort and fill\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n# Fill with train medians as base\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\n# Prepare frame with required rows from sample submission\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\n# still any NaNs: fill by train medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\npred_x = final_model_x.predict(X_test)\npred_y = final_model_y.predict(X_test)\npred_z = final_model_z.predict(X_test)\n\npred_lat, pred_lon, _ = ecef_to_geodetic(pred_x, pred_y, pred_z)\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\n# also mirror to working directory\nsubmission.to_csv(os.path.join(WORKING_DIR, \"submission.csv\"), index=False)\nprint(\"Saved submission to\", sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nWGS84_A = 6378137.0\nWGS84_E2 = 6.69437999014e-3\n\n\ndef geodetic_to_ecef(lat_deg, lon_deg, alt_m):\n    lat = np.radians(lat_deg)\n    lon = np.radians(lon_deg)\n    a = WGS84_A\n    e2 = WGS84_E2\n    sin_lat = np.sin(lat)\n    cos_lat = np.cos(lat)\n    N = a / np.sqrt(1 - e2 * sin_lat * sin_lat)\n    X = (N + alt_m) * cos_lat * np.cos(lon)\n    Y = (N + alt_m) * cos_lat * np.sin(lon)\n    Z = (N * (1 - e2) + alt_m) * sin_lat\n    return X, Y, Z\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = WGS84_A\n    e2 = WGS84_E2\n    b = a * np.sqrt(1 - e2)\n    ep2 = (a * a - b * b) / (b * b)\n    p = np.sqrt(x * x + y * y)\n    th = np.arctan2(a * z, b * p)\n    lon = np.arctan2(y, x)\n    lat = np.arctan2(z + ep2 * b * np.sin(th) ** 3, p - e2 * a * np.cos(th) ** 3)\n    N = a / np.sqrt(1 - e2 * np.sin(lat) ** 2)\n    alt = p / np.cos(lat) - N\n    return np.degrees(lat), np.degrees(lon), alt\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    # df must have columns: lat_gt, lon_gt, lat_pred, lon_pred, phone\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    # basic cleaning\n    gt = gt.dropna(\n        subset=[\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"AltitudeMeters\",\n            \"UnixTimeMillis\",\n        ]\n    )\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n\n    # Derive epoch time in ms to join with ground truth\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # aggregate gnss by epoch\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # join with ground truth\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # target: ECEF from ground truth\n    X_ecef, Y_ecef, Z_ecef = geodetic_to_ecef(\n        df[\"LatitudeDegrees\"].values,\n        df[\"LongitudeDegrees\"].values,\n        df[\"AltitudeMeters\"].values,\n    )\n    df[\"X_ecef\"] = X_ecef\n    df[\"Y_ecef\"] = Y_ecef\n    df[\"Z_ecef\"] = Z_ecef\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n\n    # simple time-based features per phone: normalized time within trace\n    df = df.sort_values(\"UnixTimeMillis\")\n    t_min = df[\"UnixTimeMillis\"].min()\n    t_max = df[\"UnixTimeMillis\"].max()\n    if t_max > t_min:\n        df[\"time_norm\"] = (df[\"UnixTimeMillis\"] - t_min) / (t_max - t_min)\n    else:\n        df[\"time_norm\"] = 0.0\n\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n\n    # time features per phone (within this phone trace)\n    g_agg = g_agg.sort_values(\"UnixTimeMillis\")\n    t_min = g_agg[\"UnixTimeMillis\"].min()\n    t_max = g_agg[\"UnixTimeMillis\"].max()\n    if t_max > t_min:\n        g_agg[\"time_norm\"] = (g_agg[\"UnixTimeMillis\"] - t_min) / (t_max - t_min)\n    else:\n        g_agg[\"time_norm\"] = 0.0\n\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        # Skip problematic phone traces\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\n\n# Sort and create temporal diffs for mean features\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\nmean_feats = [c for c in train_df.columns if c.endswith(\"_mean\")]\nfor feat in mean_feats:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# feature columns: aggregated gnss stats, diffs, WLS positions, and time_norm\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"X_ecef\",\n    \"Y_ecef\",\n    \"Z_ecef\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        c.endswith(\"_mean\")\n        or c.endswith(\"_std\")\n        or c.endswith(\"_diff\")\n        or \"WlsPosition\" in c\n        or c == \"time_norm\"\n    )\n]\n\n# fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_x = train_df[\"X_ecef\"].values\ny_y = train_df[\"Y_ecef\"].values\ny_z = train_df[\"Z_ecef\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_preds_x = np.zeros(len(train_df))\noof_preds_y = np.zeros(len(train_df))\noof_preds_z = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_x, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    yx_tr, yx_val = y_x[tr_idx], y_x[val_idx]\n    yy_tr, yy_val = y_y[tr_idx], y_y[val_idx]\n    yz_tr, yz_val = y_z[tr_idx], y_z[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_x = LGBMRegressor(**params)\n    model_y = LGBMRegressor(**params)\n    model_z = LGBMRegressor(**params)\n\n    # Remove unsupported 'verbose' kwarg from fit\n    model_x.fit(X_tr, yx_tr, eval_set=[(X_val, yx_val)], eval_metric=\"l2\")\n    model_y.fit(X_tr, yy_tr, eval_set=[(X_val, yy_val)], eval_metric=\"l2\")\n    model_z.fit(X_tr, yz_tr, eval_set=[(X_val, yz_val)], eval_metric=\"l2\")\n\n    oof_preds_x[val_idx] = model_x.predict(X_val)\n    oof_preds_y[val_idx] = model_y.predict(X_val)\n    oof_preds_z[val_idx] = model_z.predict(X_val)\n\n# build oof dataframe for metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\nlat_pred, lon_pred, _ = ecef_to_geodetic(oof_preds_x, oof_preds_y, oof_preds_z)\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = lat_pred\noof_df[\"lon_pred\"] = lon_pred\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_x = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_y = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_z = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_x.fit(X, y_x)\nfinal_model_y.fit(X, y_y)\nfinal_model_z.fit(X, y_z)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Detect identifier column name: 'phone' in original, 'tripId' in this benchmark\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Create a unified 'phone' column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        # Skip problematic test traces\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\ntest_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Temporal sort and fill\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n# Fill with train medians as base\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\n# Prepare frame with required rows from sample submission\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\n# still any NaNs: fill by train medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\npred_x = final_model_x.predict(X_test)\npred_y = final_model_y.predict(X_test)\npred_z = final_model_z.predict(X_test)\n\npred_lat, pred_lon, _ = ecef_to_geodetic(pred_x, pred_y, pred_z)\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\n# also mirror to working directory\nsubmission.to_csv(os.path.join(WORKING_DIR, \"submission.csv\"), index=False)\nprint(\"Saved submission to\", sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- utilities ----------------- #\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    # df must have columns: lat_gt, lon_gt, lat_pred, lon_pred, phone\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    # basic cleaning\n    gt = gt.dropna(\n        subset=[\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"UnixTimeMillis\",\n        ]\n    )\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n\n    # Derive epoch time in ms to join with ground truth\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # aggregate gnss by epoch\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # join with ground truth\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception:\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\n\n# build sorted time and simple deltas per phone\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# temporal diffs for mean features\nmean_feats = [c for c in train_df.columns if c.endswith(\"_mean\")]\nfor feat in mean_feats:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# feature columns: aggregated gnss stats and their diffs except targets & labels\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        c.endswith(\"_mean\")\n        or c.endswith(\"_std\")\n        or c.endswith(\"_diff\")\n        or \"WlsPosition\" in c\n        or c in [\"Svid_nunique\", \"SignalType_nunique\"]\n    )\n]\n\n# fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_lat = train_df[\"LatitudeDegrees\"].values\ny_lon = train_df[\"LongitudeDegrees\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_preds_lat = np.zeros(len(train_df))\noof_preds_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n    ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n    params = dict(\n        n_estimators=400,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, ylat_tr, eval_set=[(X_val, ylat_val)], eval_metric=\"l2\")\n    model_lon.fit(X_tr, ylon_tr, eval_set=[(X_val, ylon_val)], eval_metric=\"l2\")\n\n    oof_preds_lat[val_idx] = model_lat.predict(X_val)\n    oof_preds_lon[val_idx] = model_lon.predict(X_val)\n\n# build oof dataframe for metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_preds_lat\noof_df[\"lon_pred\"] = oof_preds_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=600,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=600,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_lat)\nfinal_model_lon.fit(X, y_lon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))\n\n# Detect identifier column name: 'phone' in original, 'tripId' in this benchmark\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# unified phone column\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception:\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\ntest_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Temporal sort and fill; add temporal diffs for mean features if missing\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# mean_feats in test (from g_agg)\ntest_mean_feats = [c for c in test_gnss.columns if c.endswith(\"_mean\")]\nfor feat in test_mean_feats:\n    diff_col = feat + \"_diff\"\n    if diff_col not in test_gnss.columns:\n        test_gnss[diff_col] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# base fill with train medians\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\n# Prepare frame with required rows from sample submission\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\npred_lat = final_model_lat.predict(X_test)\npred_lon = final_model_lon.predict(X_test)\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\n# mirror in working\nsubmission.to_csv(os.path.join(WORKING_DIR, \"submission.csv\"), index=False)\nprint(\"Saved submission to\", sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    # df must have columns: lat_gt, lon_gt, lat_pred, lon_pred, phone\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    gt = gt.dropna(\n        subset=[\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"AltitudeMeters\",\n            \"UnixTimeMillis\",\n        ]\n    )\n\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        # skip problematic phone\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\n\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# temporal diffs for mean features\nmean_feats = [c for c in train_df.columns if c.endswith(\"_mean\")]\nfor feat in mean_feats:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# feature columns\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        c.endswith(\"_mean\")\n        or c.endswith(\"_std\")\n        or c.endswith(\"_diff\")\n        or \"WlsPosition\" in c\n    )\n]\n\n# fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_lat = train_df[\"LatitudeDegrees\"].values\ny_lon = train_df[\"LongitudeDegrees\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n    ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, ylat_tr, eval_set=[(X_val, ylat_val)], eval_metric=\"l2\")\n    model_lon.fit(X_tr, ylon_tr, eval_set=[(X_val, ylon_val)], eval_metric=\"l2\")\n\n    oof_lat[val_idx] = model_lat.predict(X_val)\n    oof_lon[val_idx] = model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\n\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_lat)\nfinal_model_lon.fit(X, y_lon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Detect identifier column name\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    # Fallback empty frame\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\ntest_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Fill missing with train medians\ntrain_medians = train_df[feature_cols].median()\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(train_medians)\n\n# Merge with sample submission grid\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Forward/backward fill within each phone\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\nX_test = test_merge[feature_cols].values\npred_lat = final_model_lat.predict(X_test)\npred_lon = final_model_lon.predict(X_test)\n\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\n# mirror in working\nsubmission.to_csv(os.path.join(WORKING_DIR, \"submission.csv\"), index=False)\n\nprint(\"Saved submission to\", sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- utilities ----------------- #\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    # df must have columns: lat_gt, lon_gt, lat_pred, lon_pred, phone\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\n# ----------------- data loading ----------------- #\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    gt = gt.dropna(\n        subset=[\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"UnixTimeMillis\",\n        ]\n    )\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n\n    # derive epoch time (ms) aligned to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset_calc\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset_calc\"].median()\n        gnss.drop(columns=[\"offset_calc\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # aggregate GNSS per epoch\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [f\"{c[0]}_{c[1]}\" for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset_calc\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset_calc\"].median()\n        gnss.drop(columns=[\"offset_calc\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [f\"{c[0]}_{c[1]}\" for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception:\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\n\n# sort and temporal features\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\nmean_feats = [c for c in train_df.columns if c.endswith(\"_mean\")]\nfor feat in mean_feats:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# feature columns\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        c.endswith(\"_mean\")\n        or c.endswith(\"_std\")\n        or c.endswith(\"_diff\")\n        or \"WlsPosition\" in c\n        or c in [\"Svid_nunique\", \"SignalType_nunique\"]\n    )\n]\n\n# fill NaNs with median\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_lat = train_df[\"LatitudeDegrees\"].values\ny_lon = train_df[\"LongitudeDegrees\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n    ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n    params = dict(\n        n_estimators=400,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=96,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, ylat_tr, eval_set=[(X_val, ylat_val)], eval_metric=\"l2\")\n    model_lon.fit(X_tr, ylon_tr, eval_set=[(X_val, ylon_val)], eval_metric=\"l2\")\n\n    oof_lat[val_idx] = model_lat.predict(X_val)\n    oof_lon[val_idx] = model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=600,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=600,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_lat)\nfinal_model_lon.fit(X, y_lon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))\n\n# Detect identifier column: 'phone' or 'tripId' etc.\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# unified phone name for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception:\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\ntest_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# ensure all feature_cols exist\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# sort and fill\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\n# align to sample grid\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# forward/backward fill within phone\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\npred_lat = final_model_lat.predict(X_test)\npred_lon = final_model_lon.predict(X_test)\n\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", sub_path, \"and\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    # df must have columns: lat_gt, lon_gt, lat_pred, lon_pred, phone\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    # basic cleaning\n    gt = gt.dropna(\n        subset=[\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"AltitudeMeters\",\n            \"UnixTimeMillis\",\n        ]\n    )\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n\n    # Derive epoch time in ms to join with ground truth\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # aggregate gnss by epoch\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # join with ground truth\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception:\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\n\n# build sorted time and simple deltas per phone\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# temporal diffs for mean features\nmean_feats = [c for c in train_df.columns if c.endswith(\"_mean\")]\nfor feat in mean_feats:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# feature columns: aggregated gnss stats and their diffs except targets & labels\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        c.endswith(\"_mean\")\n        or c.endswith(\"_std\")\n        or c.endswith(\"_diff\")\n        or \"WlsPosition\" in c\n        or c in [\"Svid_nunique\", \"SignalType_nunique\"]\n    )\n]\n\n# fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_lat = train_df[\"LatitudeDegrees\"].values\ny_lon = train_df[\"LongitudeDegrees\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n    ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n    params = dict(\n        n_estimators=600,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, ylat_tr, eval_set=[(X_val, ylat_val)], eval_metric=\"l2\")\n    model_lon.fit(X_tr, ylon_tr, eval_set=[(X_val, ylon_val)], eval_metric=\"l2\")\n\n    oof_lat[val_idx] = model_lat.predict(X_val)\n    oof_lon[val_idx] = model_lon.predict(X_val)\n\n# build oof dataframe for metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=800,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=800,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_lat)\nfinal_model_lon.fit(X, y_lon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub = pd.read_csv(os.path.join(INPUT_DIR, \"sample_submission.csv\"))\n\n# Detect identifier column name: 'phone' in original, 'tripId' in this benchmark\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    # Fallback: assume first column is the identifier\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Create a unified 'phone' column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception:\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\ntest_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Temporal sort and fill\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n# Fill with train medians as base\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\n# Prepare frame with required rows from sample submission\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\n# still any NaNs: fill by train medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\npred_lat = final_model_lat.predict(X_test)\npred_lon = final_model_lon.predict(X_test)\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\n# also mirror to working directory\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    # df must have columns: lat_gt, lon_gt, lat_pred, lon_pred, phone\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    # basic cleaning\n    gt = gt.dropna(\n        subset=[\n            \"LatitudeDegrees\",\n            \"LongitudeDegrees\",\n            \"AltitudeMeters\",\n            \"UnixTimeMillis\",\n        ]\n    )\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n\n    # Derive epoch time in ms to join with ground truth\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # aggregate gnss by epoch\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # join with ground truth\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    if \"ArrivalTimeNanosSinceGpsEpoch\" not in gnss.columns:\n        return None\n    gnss = gnss.dropna(subset=[\"ArrivalTimeNanosSinceGpsEpoch\"])\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n        \"AccumulatedDeltaRangeMeters\",\n        \"AccumulatedDeltaRangeUncertaintyMeters\",\n        \"CarrierFrequencyHz\",\n        \"SvElevationDegrees\",\n        \"SvAzimuthDegrees\",\n        \"SvClockBiasMeters\",\n        \"SvClockDriftMetersPerSecond\",\n        \"IsrbMeters\",\n        \"IonosphericDelayMeters\",\n        \"TroposphericDelayMeters\",\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    if \"SignalType\" in gnss.columns:\n        agg_dict.update({\"Svid\": \"nunique\", \"SignalType\": \"nunique\"})\n    else:\n        agg_dict.update({\"Svid\": \"nunique\"})\n\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        # Skip problematic phone traces robustly\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\n\n# build sorted time and simple deltas per phone\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# temporal diffs for mean features\nmean_feats = [c for c in train_df.columns if c.endswith(\"_mean\")]\nfor feat in mean_feats:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# feature columns: aggregated gnss stats and their diffs except targets & labels\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        c.endswith(\"_mean\")\n        or c.endswith(\"_std\")\n        or c.endswith(\"_diff\")\n        or \"WlsPosition\" in c\n        or c in [\"Svid_nunique\", \"SignalType_nunique\"]\n    )\n]\n\n# Ensure feature_cols is not empty\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_lat = train_df[\"LatitudeDegrees\"].values\ny_lon = train_df[\"LongitudeDegrees\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_lat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    ylat_tr, ylat_val = y_lat[tr_idx], y_lat[val_idx]\n    ylon_tr, ylon_val = y_lon[tr_idx], y_lon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, ylat_tr)\n    model_lon.fit(X_tr, ylon_tr)\n\n    oof_lat[val_idx] = model_lat.predict(X_val)\n    oof_lon[val_idx] = model_lon.predict(X_val)\n\n# build oof dataframe for metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_lat)\nfinal_model_lon.fit(X, y_lon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Detect identifier column name\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Create a unified 'phone' column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    # Fallback: empty frame with needed keys\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Temporal sort and fill\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    # Fill with train medians as base\n    test_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n# Prepare frame with required rows from sample submission\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\n# still any NaNs: fill by train medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\npred_lat = final_model_lat.predict(X_test)\npred_lon = final_model_lon.predict(X_test)\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\n# also mirror to working directory (required by benchmark)\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) * math.sin(th) * math.sin(th),\n        p - e * e * a * math.cos(th) * math.cos(th) * math.cos(th),\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) * math.sin(lat))\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    # Need WLS ECEF positions\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        # approximate using overlap with GT\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF by epoch (mean, though usually single row per epoch)\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # derive baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Build feature columns: ECEF stats + C/N0 and pseudorange stats and temporal diffs\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\nfeature_cols = base_feature_candidates + [\n    c for c in train_df.columns if c.endswith(\"_diff\")\n]\n\n# Ensure feature_cols not empty\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort and fill\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    test_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test: from features if present, else global means\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate & metric utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\ndef add_motion_features(df, lat_col, lon_col):\n    \"\"\"Add simple velocity/speed features based on lat/lon and UnixTimeMillis within each phone.\"\"\"\n    if \"UnixTimeMillis\" not in df.columns or \"phone\" not in df.columns:\n        return df\n\n    # sort within phone\n    df = df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n    # radians\n    lat_rad = np.radians(df[lat_col].values.astype(float))\n    lon_rad = np.radians(df[lon_col].values.astype(float))\n    times = df[\"UnixTimeMillis\"].values.astype(float) / 1000.0  # seconds\n\n    # allocate\n    v_n = np.zeros(len(df))\n    v_e = np.zeros(len(df))\n\n    # group by phone\n    for phone, idx in df.groupby(\"phone\").indices.items():\n        idx = np.array(idx)\n        if len(idx) < 2:\n            continue\n        lt = lat_rad[idx]\n        ln = lon_rad[idx]\n        t = times[idx]\n\n        # forward/backward differences (central diff where possible)\n        dlat = np.zeros_like(lt)\n        dlon = np.zeros_like(ln)\n        dt = np.zeros_like(t)\n\n        # forward for all but last\n        dlat_f = np.diff(lt)\n        dlon_f = np.diff(ln)\n        dt_f = np.diff(t)\n\n        # backward for all but first\n        dlat_b = -dlat_f\n        dlon_b = -dlon_f\n        dt_b = dt_f\n\n        # central where possible\n        dlat[1:-1] = (dlat_f[:-1] + dlat_b[1:]) / 2.0\n        dlon[1:-1] = (dlon_f[:-1] + dlon_b[1:]) / 2.0\n        dt[1:-1] = (dt_f[:-1] + dt_b[1:]) / 2.0\n\n        # endpoints\n        dlat[0] = dlat_f[0]\n        dlon[0] = dlon_f[0]\n        dt[0] = dt_f[0]\n        dlat[-1] = dlat_b[-1]\n        dlon[-1] = dlon_b[-1]\n        dt[-1] = dt_b[-1]\n\n        dt[dt == 0] = np.nan\n        v_n_i = (dlat * EARTH_RADIUS) / dt\n        v_e_i = (dlon * EARTH_RADIUS * np.cos(lt)) / dt\n\n        v_n[idx] = v_n_i\n        v_e[idx] = v_e_i\n\n    df[\"vel_north_mps\"] = v_n\n    df[\"vel_east_mps\"] = v_e\n    df[\"speed_mps_est\"] = np.sqrt(v_n**2 + v_e**2)\n    return df\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# add motion features based on ground truth lat/lon (better than baseline for train)\ntrain_df = add_motion_features(train_df, \"LatitudeDegrees\", \"LongitudeDegrees\")\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Build feature columns\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n        or c in [\"vel_north_mps\", \"vel_east_mps\", \"speed_mps_est\"]\n    )\n]\n\n# Temporal diffs for numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\nfeature_cols = base_feature_candidates + [\n    c for c in train_df.columns if c.endswith(\"_diff\")\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=220,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# add motion features based on baseline lat/lon (proxy for motion in test)\nif not test_gnss.empty:\n    test_gnss = add_motion_features(test_gnss, \"base_lat\", \"base_lon\")\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    test_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\n\nfor c in feature_cols + [\"base_lat\", \"base_lon\"]:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\ntest_merge[feature_cols + [\"base_lat\", \"base_lon\"]] = test_merge.groupby(\"phone\")[\n    feature_cols + [\"base_lat\", \"base_lon\"]\n].ffill()\ntest_merge[feature_cols + [\"base_lat\", \"base_lon\"]] = test_merge.groupby(\"phone\")[\n    feature_cols + [\"base_lat\", \"base_lon\"]\n].bfill()\nfill_med = train_df[feature_cols].median()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(fill_med)\nif \"base_lat\" not in test_merge.columns or test_merge[\"base_lat\"].isna().all():\n    test_merge[\"base_lat\"] = train_df[\"LatitudeDegrees\"].mean()\nif \"base_lon\" not in test_merge.columns or test_merge[\"base_lon\"].isna().all():\n    test_merge[\"base_lon\"] = train_df[\"LongitudeDegrees\"].mean()\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate & metric utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\ndef add_motion_features(df, lat_col, lon_col):\n    \"\"\"Add simple velocity/speed features based on lat/lon and UnixTimeMillis within each phone.\"\"\"\n    if \"UnixTimeMillis\" not in df.columns or \"phone\" not in df.columns:\n        return df\n\n    df = df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n    lat_rad = np.radians(df[lat_col].values.astype(float))\n    lon_rad = np.radians(df[lon_col].values.astype(float))\n    times = df[\"UnixTimeMillis\"].values.astype(float) / 1000.0  # seconds\n\n    v_n = np.zeros(len(df))\n    v_e = np.zeros(len(df))\n\n    for phone, idx in df.groupby(\"phone\").indices.items():\n        idx = np.array(idx)\n        if len(idx) < 2:\n            continue\n        lt = lat_rad[idx]\n        ln = lon_rad[idx]\n        t = times[idx]\n\n        dlat = np.zeros_like(lt)\n        dlon = np.zeros_like(ln)\n        dt = np.zeros_like(t)\n\n        dlat_f = np.diff(lt)\n        dlon_f = np.diff(ln)\n        dt_f = np.diff(t)\n\n        dlat_b = -dlat_f\n        dlon_b = -dlon_f\n        dt_b = dt_f\n\n        if len(lt) > 2:\n            dlat[1:-1] = (dlat_f[:-1] + dlat_b[1:]) / 2.0\n            dlon[1:-1] = (dlon_f[:-1] + dlon_b[1:]) / 2.0\n            dt[1:-1] = (dt_f[:-1] + dt_b[1:]) / 2.0\n\n        dlat[0] = dlat_f[0]\n        dlon[0] = dlon_f[0]\n        dt[0] = dt_f[0]\n        dlat[-1] = dlat_b[-1]\n        dlon[-1] = dlon_b[-1]\n        dt[-1] = dt_b[-1]\n\n        dt[dt == 0] = np.nan\n        v_n_i = (dlat * EARTH_RADIUS) / dt\n        v_e_i = (dlon * EARTH_RADIUS * np.cos(lt)) / dt\n\n        v_n[idx] = v_n_i\n        v_e[idx] = v_e_i\n\n    df[\"vel_north_mps\"] = v_n\n    df[\"vel_east_mps\"] = v_e\n    df[\"speed_mps_est\"] = np.sqrt(v_n**2 + v_e**2)\n    return df\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# add motion features based on ground truth lat/lon (better than baseline for train)\ntrain_df = add_motion_features(train_df, \"LatitudeDegrees\", \"LongitudeDegrees\")\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Build feature columns\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n        or c in [\"vel_north_mps\", \"vel_east_mps\", \"speed_mps_est\"]\n    )\n]\n\n# Temporal diffs for numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\nfeature_cols = base_feature_candidates + [\n    c for c in train_df.columns if c.endswith(\"_diff\")\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=220,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# add motion features based on baseline lat/lon (proxy for motion in test)\nif not test_gnss.empty:\n    test_gnss = add_motion_features(test_gnss, \"base_lat\", \"base_lon\")\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    test_gnss[feature_cols] = test_gnss[feature_cols].astype(float)\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# Make sure columns exist\nfor c in feature_cols + [\"base_lat\", \"base_lon\"]:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Robust per-column ffill/bfill within each phone to avoid shape mismatch\nfor c in feature_cols + [\"base_lat\", \"base_lon\"]:\n    test_merge[c] = (\n        test_merge.groupby(\"phone\")[c].ffill().groupby(test_merge[\"phone\"]).bfill()\n    )\n\n# Fill remaining NaNs in feature columns with train medians\nfill_med = train_df[feature_cols].median()\nfor c in feature_cols:\n    if test_merge[c].isna().any():\n        if c in fill_med.index:\n            test_merge[c] = test_merge[c].fillna(fill_med[c])\n        else:\n            test_merge[c] = test_merge[c].fillna(0.0)\n\n# Handle base_lat/base_lon fallbacks\nif test_merge[\"base_lat\"].isna().all():\n    test_merge[\"base_lat\"] = train_df[\"LatitudeDegrees\"].mean()\nelse:\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(\n        train_df[\"LatitudeDegrees\"].mean()\n    )\n\nif test_merge[\"base_lon\"].isna().all():\n    test_merge[\"base_lon\"] = train_df[\"LongitudeDegrees\"].mean()\nelse:\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(\n        train_df[\"LongitudeDegrees\"].mean()\n    )\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate & metric utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\ndef add_motion_features(df, lat_col, lon_col):\n    \"\"\"Add simple velocity/speed features based on lat/lon and UnixTimeMillis within each phone.\"\"\"\n    if \"UnixTimeMillis\" not in df.columns or \"phone\" not in df.columns:\n        return df\n\n    df = df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n    lat_rad = np.radians(df[lat_col].values.astype(float))\n    lon_rad = np.radians(df[lon_col].values.astype(float))\n    times = df[\"UnixTimeMillis\"].values.astype(float) / 1000.0  # seconds\n\n    v_n = np.zeros(len(df))\n    v_e = np.zeros(len(df))\n\n    for phone, idx in df.groupby(\"phone\").indices.items():\n        idx = np.array(idx)\n        if len(idx) < 2:\n            continue\n        lt = lat_rad[idx]\n        ln = lon_rad[idx]\n        t = times[idx]\n\n        dlat = np.zeros_like(lt)\n        dlon = np.zeros_like(ln)\n        dt = np.zeros_like(t)\n\n        dlat_f = np.diff(lt)\n        dlon_f = np.diff(ln)\n        dt_f = np.diff(t)\n\n        dlat_b = -dlat_f\n        dlon_b = -dlon_f\n        dt_b = dt_f\n\n        if len(lt) > 2:\n            dlat[1:-1] = (dlat_f[:-1] + dlat_b[1:]) / 2.0\n            dlon[1:-1] = (dlon_f[:-1] + dlon_b[1:]) / 2.0\n            dt[1:-1] = (dt_f[:-1] + dt_b[1:]) / 2.0\n\n        dlat[0] = dlat_f[0]\n        dlon[0] = dlon_f[0]\n        dt[0] = dt_f[0]\n        dlat[-1] = dlat_b[-1]\n        dlon[-1] = dlon_b[-1]\n        dt[-1] = dt_b[-1]\n\n        dt[dt == 0] = np.nan\n        v_n_i = (dlat * EARTH_RADIUS) / dt\n        v_e_i = (dlon * EARTH_RADIUS * np.cos(lt)) / dt\n\n        v_n[idx] = v_n_i\n        v_e[idx] = v_e_i\n\n    df[\"vel_north_mps\"] = v_n\n    df[\"vel_east_mps\"] = v_e\n    df[\"speed_mps_est\"] = np.sqrt(v_n**2 + v_e**2)\n    return df\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# add motion features based on ground truth lat/lon (better than baseline for train)\ntrain_df = add_motion_features(train_df, \"LatitudeDegrees\", \"LongitudeDegrees\")\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Build feature columns\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n        or c in [\"vel_north_mps\", \"vel_east_mps\", \"speed_mps_est\"]\n    )\n]\n\n# Temporal diffs for numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\nfeature_cols = base_feature_candidates + [\n    c for c in train_df.columns if c.endswith(\"_diff\")\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=220,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# add motion features based on baseline lat/lon (proxy for motion in test)\nif not test_gnss.empty:\n    test_gnss = add_motion_features(test_gnss, \"base_lat\", \"base_lon\")\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    test_gnss[feature_cols] = test_gnss[feature_cols].astype(float)\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\nmerge_cols = [c for c in merge_cols if c in test_gnss.columns]\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# Make sure columns exist\nfor c in feature_cols + [\"base_lat\", \"base_lon\"]:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Robust per-column ffill/bfill within each phone to avoid shape mismatch\nfor c in feature_cols + [\"base_lat\", \"base_lon\"]:\n    test_merge[c] = test_merge.groupby(\"phone\")[c].ffill()\n    test_merge[c] = test_merge.groupby(\"phone\")[c].bfill()\n\n# Fill remaining NaNs in feature columns with train medians\nfill_med = train_df[feature_cols].median()\nfor c in feature_cols:\n    if test_merge[c].isna().any():\n        if c in fill_med.index:\n            test_merge[c] = test_merge[c].fillna(fill_med[c])\n        else:\n            test_merge[c] = test_merge[c].fillna(0.0)\n\n# Handle base_lat/base_lon fallbacks\nif test_merge[\"base_lat\"].isna().all():\n    test_merge[\"base_lat\"] = train_df[\"LatitudeDegrees\"].mean()\nelse:\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(\n        train_df[\"LatitudeDegrees\"].mean()\n    )\n\nif test_merge[\"base_lon\"].isna().all():\n    test_merge[\"base_lon\"] = train_df[\"LongitudeDegrees\"].mean()\nelse:\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(\n        train_df[\"LongitudeDegrees\"].mean()\n    )\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate & metric utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\ndef add_motion_features(df, lat_col, lon_col):\n    \"\"\"Add simple velocity/speed features based on lat/lon and UnixTimeMillis within each phone.\"\"\"\n    if \"UnixTimeMillis\" not in df.columns or \"phone\" not in df.columns:\n        return df\n\n    df = df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n    lat_rad = np.radians(df[lat_col].values.astype(float))\n    lon_rad = np.radians(df[lon_col].values.astype(float))\n    times = df[\"UnixTimeMillis\"].values.astype(float) / 1000.0  # seconds\n\n    v_n = np.zeros(len(df))\n    v_e = np.zeros(len(df))\n\n    for phone, idx in df.groupby(\"phone\").indices.items():\n        idx = np.array(idx)\n        if len(idx) < 2:\n            continue\n        lt = lat_rad[idx]\n        ln = lon_rad[idx]\n        t = times[idx]\n\n        dlat = np.zeros_like(lt)\n        dlon = np.zeros_like(ln)\n        dt = np.zeros_like(t)\n\n        dlat_f = np.diff(lt)\n        dlon_f = np.diff(ln)\n        dt_f = np.diff(t)\n\n        dlat_b = -dlat_f\n        dlon_b = -dlon_f\n        dt_b = dt_f\n\n        if len(lt) > 2:\n            dlat[1:-1] = (dlat_f[:-1] + dlat_b[1:]) / 2.0\n            dlon[1:-1] = (dlon_f[:-1] + dlon_b[1:]) / 2.0\n            dt[1:-1] = (dt_f[:-1] + dt_b[1:]) / 2.0\n\n        dlat[0] = dlat_f[0]\n        dlon[0] = dlon_f[0]\n        dt[0] = dt_f[0]\n        dlat[-1] = dlat_b[-1]\n        dlon[-1] = dlon_b[-1]\n        dt[-1] = dt_b[-1]\n\n        dt[dt == 0] = np.nan\n        v_n_i = (dlat * EARTH_RADIUS) / dt\n        v_e_i = (dlon * EARTH_RADIUS * np.cos(lt)) / dt\n\n        v_n[idx] = v_n_i\n        v_e[idx] = v_e_i\n\n    df[\"vel_north_mps\"] = v_n\n    df[\"vel_east_mps\"] = v_e\n    df[\"speed_mps_est\"] = np.sqrt(v_n**2 + v_e**2)\n    return df\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# add motion features based on ground truth lat/lon\ntrain_df = add_motion_features(train_df, \"LatitudeDegrees\", \"LongitudeDegrees\")\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Build feature columns\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n        or c in [\"vel_north_mps\", \"vel_east_mps\", \"speed_mps_est\"]\n    )\n]\n\n# Temporal diffs for numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\nfeature_cols = base_feature_candidates + [\n    c for c in train_df.columns if c.endswith(\"_diff\")\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=220,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# add motion features based on baseline lat/lon (proxy for motion in test)\nif not test_gnss.empty:\n    test_gnss = add_motion_features(test_gnss, \"base_lat\", \"base_lon\")\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    test_gnss[feature_cols] = test_gnss[feature_cols].astype(float)\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\nmerge_cols = [c for c in merge_cols if c in test_gnss.columns]\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# Make sure columns exist\nfor c in feature_cols + [\"base_lat\", \"base_lon\"]:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Robust per-column ffill/bfill within each phone\nfor c in feature_cols + [\"base_lat\", \"base_lon\"]:\n    test_merge[c] = test_merge.groupby(\"phone\")[c].ffill()\n    test_merge[c] = test_merge.groupby(\"phone\")[c].bfill()\n\n# Fill remaining NaNs in feature columns with train medians (simplified, no per-column if)\nfill_med = train_df[feature_cols].median()\nfor c in feature_cols:\n    if c in fill_med.index:\n        test_merge[c] = test_merge[c].fillna(fill_med[c])\n    else:\n        test_merge[c] = test_merge[c].fillna(0.0)\n\n# Handle base_lat/base_lon fallbacks\nif test_merge[\"base_lat\"].isna().all():\n    test_merge[\"base_lat\"] = train_df[\"LatitudeDegrees\"].mean()\nelse:\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(\n        train_df[\"LatitudeDegrees\"].mean()\n    )\n\nif test_merge[\"base_lon\"].isna().all():\n    test_merge[\"base_lon\"] = train_df[\"LongitudeDegrees\"].mean()\nelse:\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(\n        train_df[\"LongitudeDegrees\"].mean()\n    )\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate & metric utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\ndef add_motion_features(df, lat_col, lon_col):\n    \"\"\"Add simple velocity/speed features based on lat/lon and UnixTimeMillis within each phone.\"\"\"\n    if \"UnixTimeMillis\" not in df.columns or \"phone\" not in df.columns:\n        return df\n\n    df = df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n    lat_rad = np.radians(df[lat_col].values.astype(float))\n    lon_rad = np.radians(df[lon_col].values.astype(float))\n    times = df[\"UnixTimeMillis\"].values.astype(float) / 1000.0  # seconds\n\n    v_n = np.zeros(len(df))\n    v_e = np.zeros(len(df))\n\n    for phone, idx in df.groupby(\"phone\").indices.items():\n        idx = np.array(idx)\n        if len(idx) < 2:\n            continue\n        lt = lat_rad[idx]\n        ln = lon_rad[idx]\n        t = times[idx]\n\n        dlat = np.zeros_like(lt)\n        dlon = np.zeros_like(ln)\n        dt = np.zeros_like(t)\n\n        dlat_f = np.diff(lt)\n        dlon_f = np.diff(ln)\n        dt_f = np.diff(t)\n\n        dlat_b = -dlat_f\n        dlon_b = -dlon_f\n        dt_b = dt_f\n\n        if len(lt) > 2:\n            dlat[1:-1] = (dlat_f[:-1] + dlat_b[1:]) / 2.0\n            dlon[1:-1] = (dlon_f[:-1] + dlon_b[1:]) / 2.0\n            dt[1:-1] = (dt_f[:-1] + dt_b[1:]) / 2.0\n\n        dlat[0] = dlat_f[0]\n        dlon[0] = dlon_f[0]\n        dt[0] = dt_f[0]\n        dlat[-1] = dlat_b[-1]\n        dlon[-1] = dlon_b[-1]\n        dt[-1] = dt_b[-1]\n\n        dt[dt == 0] = np.nan\n        v_n_i = (dlat * EARTH_RADIUS) / dt\n        v_e_i = (dlon * EARTH_RADIUS * np.cos(lt)) / dt\n\n        v_n[idx] = v_n_i\n        v_e[idx] = v_e_i\n\n    df[\"vel_north_mps\"] = v_n\n    df[\"vel_east_mps\"] = v_e\n    df[\"speed_mps_est\"] = np.sqrt(v_n**2 + v_e**2)\n    return df\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# add motion features based on ground truth lat/lon\ntrain_df = add_motion_features(train_df, \"LatitudeDegrees\", \"LongitudeDegrees\")\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Build feature columns\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n        or c in [\"vel_north_mps\", \"vel_east_mps\", \"speed_mps_est\"]\n    )\n]\n\n# Temporal diffs for numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\nfeature_cols = base_feature_candidates + [\n    c for c in train_df.columns if c.endswith(\"_diff\")\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=220,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# add motion features based on baseline lat/lon (proxy for motion in test)\nif not test_gnss.empty:\n    test_gnss = add_motion_features(test_gnss, \"base_lat\", \"base_lon\")\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    test_gnss[feature_cols] = test_gnss[feature_cols].astype(float)\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\nmerge_cols = [c for c in merge_cols if c in test_gnss.columns]\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# Make sure columns exist\nfor c in feature_cols + [\"base_lat\", \"base_lon\"]:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Robust per-column ffill/bfill within each phone for features and base_lat/base_lon\nfor c in feature_cols:\n    test_merge[c] = test_merge.groupby(\"phone\")[c].ffill()\n    test_merge[c] = test_merge.groupby(\"phone\")[c].bfill()\n\n# Fill remaining NaNs in feature columns with train medians\nfill_med = train_df[feature_cols].median()\nfor c in feature_cols:\n    if c in fill_med.index:\n        test_merge[c] = test_merge[c].fillna(fill_med[c])\n    else:\n        test_merge[c] = test_merge[c].fillna(0.0)\n\n# Handle base_lat/base_lon fallbacks separately to avoid ambiguous Series truth\nif test_merge[\"base_lat\"].isna().all():\n    test_merge[\"base_lat\"] = train_df[\"LatitudeDegrees\"].mean()\nelse:\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].groupby(test_merge[\"phone\"]).ffill()\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].groupby(test_merge[\"phone\"]).bfill()\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(\n        train_df[\"LatitudeDegrees\"].mean()\n    )\n\nif test_merge[\"base_lon\"].isna().all():\n    test_merge[\"base_lon\"] = train_df[\"LongitudeDegrees\"].mean()\nelse:\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].groupby(test_merge[\"phone\"]).ffill()\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].groupby(test_merge[\"phone\"]).bfill()\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(\n        train_df[\"LongitudeDegrees\"].mean()\n    )\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate & metric utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\ndef add_motion_features(df, lat_col, lon_col):\n    \"\"\"Add simple velocity/speed features based on lat/lon and UnixTimeMillis within each phone.\"\"\"\n    if \"UnixTimeMillis\" not in df.columns or \"phone\" not in df.columns:\n        return df\n\n    df = df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n    lat_rad = np.radians(df[lat_col].values.astype(float))\n    lon_rad = np.radians(df[lon_col].values.astype(float))\n    times = df[\"UnixTimeMillis\"].values.astype(float) / 1000.0  # seconds\n\n    v_n = np.zeros(len(df))\n    v_e = np.zeros(len(df))\n\n    for phone, idx in df.groupby(\"phone\").indices.items():\n        idx = np.array(idx)\n        if len(idx) < 2:\n            continue\n        lt = lat_rad[idx]\n        ln = lon_rad[idx]\n        t = times[idx]\n\n        dlat = np.zeros_like(lt)\n        dlon = np.zeros_like(ln)\n        dt = np.zeros_like(t)\n\n        dlat_f = np.diff(lt)\n        dlon_f = np.diff(ln)\n        dt_f = np.diff(t)\n\n        dlat_b = -dlat_f\n        dlon_b = -dlon_f\n        dt_b = dt_f\n\n        if len(lt) > 2:\n            dlat[1:-1] = (dlat_f[:-1] + dlat_b[1:]) / 2.0\n            dlon[1:-1] = (dlon_f[:-1] + dlon_b[1:]) / 2.0\n            dt[1:-1] = (dt_f[:-1] + dt_b[1:]) / 2.0\n\n        dlat[0] = dlat_f[0]\n        dlon[0] = dlon_f[0]\n        dt[0] = dt_f[0]\n        dlat[-1] = dlat_b[-1]\n        dlon[-1] = dlon_b[-1]\n        dt[-1] = dt_b[-1]\n\n        dt[dt == 0] = np.nan\n        v_n_i = (dlat * EARTH_RADIUS) / dt\n        v_e_i = (dlon * EARTH_RADIUS * np.cos(lt)) / dt\n\n        v_n[idx] = v_n_i\n        v_e[idx] = v_e_i\n\n    df[\"vel_north_mps\"] = v_n\n    df[\"vel_east_mps\"] = v_e\n    df[\"speed_mps_est\"] = np.sqrt(v_n**2 + v_e**2)\n    return df\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# add motion features based on ground truth lat/lon\ntrain_df = add_motion_features(train_df, \"LatitudeDegrees\", \"LongitudeDegrees\")\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Build feature columns\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n    # keep base_lat/base_lon out of features, they are used only to add back residuals\n    \"base_lat\",\n    \"base_lon\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"vel_north_mps\", \"vel_east_mps\", \"speed_mps_est\"]\n    )\n]\n\n# Temporal diffs for numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\nfeature_cols = base_feature_candidates + [\n    c for c in train_df.columns if c.endswith(\"_diff\")\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=220,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# add motion features based on baseline lat/lon (proxy for motion in test)\nif not test_gnss.empty:\n    test_gnss = add_motion_features(test_gnss, \"base_lat\", \"base_lon\")\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    # cast only feature cols to float; base_lat/base_lon may be created later\n    test_gnss[feature_cols] = test_gnss[feature_cols].astype(float)\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\nmerge_cols = [c for c in merge_cols if c in test_gnss.columns]\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# Make sure columns exist\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\nfor c in [\"base_lat\", \"base_lon\"]:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Robust per-column ffill/bfill within each phone for features\nfor c in feature_cols:\n    test_merge[c] = test_merge.groupby(\"phone\")[c].ffill()\n    test_merge[c] = test_merge.groupby(\"phone\")[c].bfill()\n\n# Fill remaining NaNs in feature columns with train medians\nfill_med = train_df[feature_cols].median()\nfor c in feature_cols:\n    if c in fill_med.index:\n        test_merge[c] = test_merge[c].fillna(fill_med[c])\n    else:\n        test_merge[c] = test_merge[c].fillna(0.0)\n\n# Handle base_lat/base_lon fallbacks separately\nmean_train_lat = train_df[\"LatitudeDegrees\"].mean()\nmean_train_lon = train_df[\"LongitudeDegrees\"].mean()\n\n# base_lat\nbase_lat_series = test_merge[\"base_lat\"]\nif base_lat_series.isna().all():\n    test_merge[\"base_lat\"] = mean_train_lat\nelse:\n    test_merge[\"base_lat\"] = base_lat_series.groupby(test_merge[\"phone\"]).ffill()\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].groupby(test_merge[\"phone\"]).bfill()\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(mean_train_lat)\n\n# base_lon\nbase_lon_series = test_merge[\"base_lon\"]\nif base_lon_series.isna().all():\n    test_merge[\"base_lon\"] = mean_train_lon\nelse:\n    test_merge[\"base_lon\"] = base_lon_series.groupby(test_merge[\"phone\"]).ffill()\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].groupby(test_merge[\"phone\"]).bfill()\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(mean_train_lon)\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # derive baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling window (size 3) mean/std for key features to capture local motion context\nroll_window = 3\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        grp = train_df.groupby(\"phone\")[feat]\n        train_df[feat + f\"_roll{roll_window}_mean\"] = grp.transform(\n            lambda x: x.rolling(roll_window, min_periods=1).mean()\n        )\n        train_df[feat + f\"_roll{roll_window}_std\"] = grp.transform(\n            lambda x: x.rolling(roll_window, min_periods=1).std()\n        )\n\nfeature_cols = base_feature_candidates + [\n    c for c in train_df.columns if c.endswith(\"_diff\") or f\"_roll{roll_window}_\" in c\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=220,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=72,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=72,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=380,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=72,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all base feature columns exist in test_gnss\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort before temporal features\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    # Temporal diffs\n    for feat in base_feature_candidates:\n        if feat in test_gnss.columns and test_gnss[feat].dtype != \"O\":\n            test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n    # Rolling mean/std with same window as train\n    for feat in base_feature_candidates:\n        if feat in test_gnss.columns and test_gnss[feat].dtype != \"O\":\n            grp = test_gnss.groupby(\"phone\")[feat]\n            test_gnss[feat + f\"_roll{roll_window}_mean\"] = grp.transform(\n                lambda x: x.rolling(roll_window, min_periods=1).mean()\n            )\n            test_gnss[feat + f\"_roll{roll_window}_std\"] = grp.transform(\n                lambda x: x.rolling(roll_window, min_periods=1).std()\n            )\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Fill NaNs using train medians\nif not test_gnss.empty:\n    test_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test: from features if present, else global means\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\n# Base feature candidates (including baseline lat/lon)\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling window features (3-epoch window) for a compact subset\nrolling_feats = [\n    \"WlsPositionXEcefMeters_mean\",\n    \"WlsPositionYEcefMeters_mean\",\n    \"WlsPositionZEcefMeters_mean\",\n    \"Svid_nunique\",\n    \"base_lat\",\n    \"base_lon\",\n]\n# Only keep those that actually exist\nrolling_feats = [f for f in rolling_feats if f in train_df.columns]\n\nfor feat in rolling_feats:\n    grp = train_df.groupby(\"phone\")[feat]\n    train_df[feat + \"_roll3_mean\"] = grp.transform(\n        lambda s: s.rolling(window=3, min_periods=1, center=True).mean()\n    )\n    train_df[feat + \"_roll3_std\"] = grp.transform(\n        lambda s: s.rolling(window=3, min_periods=1, center=True).std()\n    )\n\nfeature_cols = base_feature_candidates + [\n    c\n    for c in train_df.columns\n    if c.endswith(\"_diff\") or c.endswith(\"_roll3_mean\") or c.endswith(\"_roll3_std\")\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs with column medians\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all base feature columns exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort before creating temporal/rolling features\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    # Temporal diffs in test\n    for feat in base_feature_candidates:\n        if feat in test_gnss.columns and test_gnss[feat].dtype != \"O\":\n            test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n    # Rolling features in test\n    for feat in rolling_feats:\n        grp = test_gnss.groupby(\"phone\")[feat]\n        test_gnss[feat + \"_roll3_mean\"] = grp.transform(\n            lambda s: s.rolling(window=3, min_periods=1, center=True).mean()\n        )\n        test_gnss[feat + \"_roll3_std\"] = grp.transform(\n            lambda s: s.rolling(window=3, min_periods=1, center=True).std()\n        )\nelse:\n    # Create empty columns for diff and rolling if no test_gnss\n    for feat in base_feature_candidates:\n        test_gnss[feat + \"_diff\"] = np.nan\n    for feat in rolling_feats:\n        test_gnss[feat + \"_roll3_mean\"] = np.nan\n        test_gnss[feat + \"_roll3_std\"] = np.nan\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Fill NaNs with train medians\nif not test_gnss.empty:\n    test_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\nelse:\n    # if still empty, just construct placeholder rows after merge\n    pass\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    test_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].ffill()\n    test_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].bfill()\n    test_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].ffill()\n    test_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].bfill()\n    # If still NaN, use global mean GT\n    global_lat_mean = train_df[\"LatitudeDegrees\"].mean()\n    global_lon_mean = train_df[\"LongitudeDegrees\"].mean()\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(global_lat_mean)\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(global_lon_mean)\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\nX_test = test_merge[feature_cols].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\n# Base feature candidates (including baseline lat/lon)\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling window features (3-epoch window) for a compact subset\nrolling_feats = [\n    \"WlsPositionXEcefMeters_mean\",\n    \"WlsPositionYEcefMeters_mean\",\n    \"WlsPositionZEcefMeters_mean\",\n    \"Svid_nunique\",\n    \"base_lat\",\n    \"base_lon\",\n]\nrolling_feats = [f for f in rolling_feats if f in train_df.columns]\n\nfor feat in rolling_feats:\n    grp = train_df.groupby(\"phone\")[feat]\n    train_df[feat + \"_roll3_mean\"] = grp.transform(\n        lambda s: s.rolling(window=3, min_periods=1, center=True).mean()\n    )\n    train_df[feat + \"_roll3_std\"] = grp.transform(\n        lambda s: s.rolling(window=3, min_periods=1, center=True).std()\n    )\n\nfeature_cols = base_feature_candidates + [\n    c\n    for c in train_df.columns\n    if c.endswith(\"_diff\") or c.endswith(\"_roll3_mean\") or c.endswith(\"_roll3_std\")\n]\n\n# Ensure uniqueness in feature_cols\nfeature_cols = list(dict.fromkeys(feature_cols))\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs with column medians\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all base feature columns exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Ensure base_lat/base_lon exist\nfor bl in [\"base_lat\", \"base_lon\"]:\n    if bl not in test_gnss.columns:\n        test_gnss[bl] = np.nan\n\n# Sort before creating temporal/rolling features\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    # Temporal diffs in test\n    for feat in base_feature_candidates:\n        if feat in test_gnss.columns and test_gnss[feat].dtype != \"O\":\n            test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n    # Rolling features in test\n    for feat in rolling_feats:\n        if feat in test_gnss.columns:\n            grp = test_gnss.groupby(\"phone\")[feat]\n            test_gnss[feat + \"_roll3_mean\"] = grp.transform(\n                lambda s: s.rolling(window=3, min_periods=1, center=True).mean()\n            )\n            test_gnss[feat + \"_roll3_std\"] = grp.transform(\n                lambda s: s.rolling(window=3, min_periods=1, center=True).std()\n            )\nelse:\n    # Create empty columns for diff and rolling if no test_gnss\n    for feat in base_feature_candidates:\n        test_gnss[feat + \"_diff\"] = np.nan\n    for feat in rolling_feats:\n        test_gnss[feat + \"_roll3_mean\"] = np.nan\n        test_gnss[feat + \"_roll3_std\"] = np.nan\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Fill NaNs with train medians where possible\nif not test_gnss.empty:\n    for c in feature_cols:\n        if c in test_gnss.columns:\n            median_val = train_df[c].median() if c in train_df.columns else 0.0\n            test_gnss[c] = test_gnss[c].fillna(median_val)\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\n# Some of these may not exist if test_gnss empty; intersect columns\nmerge_cols = [c for c in merge_cols if c in test_gnss.columns]\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# Ensure feature columns & base_lat/base_lon exist after merge\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\nif \"base_lat\" not in test_merge.columns:\n    test_merge[\"base_lat\"] = np.nan\nif \"base_lon\" not in test_merge.columns:\n    test_merge[\"base_lon\"] = np.nan\n\n# sort for temporal filling\nif \"UnixTimeMillis\" in test_merge.columns:\n    test_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\nelse:\n    test_merge = test_merge.sort_values([\"phone\", time_col])\n\n# Forward/backward fill feature columns within each phone, columnwise\ntrain_medians = train_df[feature_cols].median()\nfor c in feature_cols:\n    # ffill and bfill within phone\n    test_merge[c] = (\n        test_merge.groupby(\"phone\")[c].ffill().groupby(test_merge[\"phone\"]).bfill()\n    )\n    # fill any remaining NaNs with global median from train\n    if c in train_medians.index:\n        test_merge[c] = test_merge[c].fillna(train_medians[c])\n    else:\n        test_merge[c] = test_merge[c].fillna(0.0)\n\n# Baseline lat/lon for test\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\n\ntest_merge[\"base_lat\"] = (\n    test_merge.groupby(\"phone\")[\"base_lat\"].ffill().groupby(test_merge[\"phone\"]).bfill()\n)\ntest_merge[\"base_lon\"] = (\n    test_merge.groupby(\"phone\")[\"base_lon\"].ffill().groupby(test_merge[\"phone\"]).bfill()\n)\ntest_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(global_lat_mean)\ntest_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(global_lon_mean)\n\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\nX_test = test_merge[feature_cols].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\n# Base feature candidates (including baseline lat/lon)\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling window features (3-epoch window) for a compact subset\nrolling_feats = [\n    \"WlsPositionXEcefMeters_mean\",\n    \"WlsPositionYEcefMeters_mean\",\n    \"WlsPositionZEcefMeters_mean\",\n    \"Svid_nunique\",\n    \"base_lat\",\n    \"base_lon\",\n]\nrolling_feats = [f for f in rolling_feats if f in train_df.columns]\n\nfor feat in rolling_feats:\n    grp = train_df.groupby(\"phone\")[feat]\n    train_df[feat + \"_roll3_mean\"] = grp.transform(\n        lambda s: s.rolling(window=3, min_periods=1, center=True).mean()\n    )\n    train_df[feat + \"_roll3_std\"] = grp.transform(\n        lambda s: s.rolling(window=3, min_periods=1, center=True).std()\n    )\n\n# Now define final feature_cols AFTER all feature engineering\nfeature_cols = []\nfor c in train_df.columns:\n    if c in exclude_cols:\n        continue\n    if c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"] or \"WlsPosition\" in c:\n        feature_cols.append(c)\n    elif (\n        c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c.endswith(\"_diff\")\n        or c.endswith(\"_roll3_mean\")\n        or c.endswith(\"_roll3_std\")\n    ):\n        feature_cols.append(c)\n\n# Ensure uniqueness and keep order\nfeature_cols = list(dict.fromkeys(feature_cols))\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs with column medians\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column (match training phone naming)\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS aggregated features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Make sure base feature candidates exist in test\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Ensure base_lat/base_lon exist\nfor bl in [\"base_lat\", \"base_lon\"]:\n    if bl not in test_gnss.columns:\n        test_gnss[bl] = np.nan\n\n# Sort before creating temporal/rolling features\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    # Temporal diffs in test: same as train\n    for feat in base_feature_candidates:\n        if feat in test_gnss.columns and test_gnss[feat].dtype != \"O\":\n            test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n    # Rolling features in test: same as train\n    for feat in rolling_feats:\n        if feat in test_gnss.columns:\n            grp = test_gnss.groupby(\"phone\")[feat]\n            test_gnss[feat + \"_roll3_mean\"] = grp.transform(\n                lambda s: s.rolling(window=3, min_periods=1, center=True).mean()\n            )\n            test_gnss[feat + \"_roll3_std\"] = grp.transform(\n                lambda s: s.rolling(window=3, min_periods=1, center=True).std()\n            )\nelse:\n    # Create empty columns for diff and rolling if no test_gnss\n    for feat in base_feature_candidates:\n        test_gnss[feat + \"_diff\"] = np.nan\n    for feat in rolling_feats:\n        test_gnss[feat + \"_roll3_mean\"] = np.nan\n        test_gnss[feat + \"_roll3_std\"] = np.nan\n\n# At this point, test_gnss has raw+engineered features.\n# Now align with feature_cols: ensure all exist, with NaNs where missing\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Merge with sample submission (on phone & UnixTimeMillis)\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\nmerge_cols = list(dict.fromkeys([c for c in merge_cols if c in test_gnss.columns]))\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# After merge, we may have extra columns; we will keep only feature_cols and base_lat/base_lon later\n# Ensure base_lat/base_lon exist after merge\nif \"base_lat\" not in test_merge.columns:\n    test_merge[\"base_lat\"] = np.nan\nif \"base_lon\" not in test_merge.columns:\n    test_merge[\"base_lon\"] = np.nan\n\n# sort for temporal filling\nif \"UnixTimeMillis\" in test_merge.columns:\n    test_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\nelse:\n    test_merge = test_merge.sort_values([\"phone\", time_col])\n\n# Fill features: ffill/bfill within phone, then global median\ntrain_medians = train_df[feature_cols].median()\n\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n    test_merge[c] = (\n        test_merge.groupby(\"phone\")[c].ffill().groupby(test_merge[\"phone\"]).bfill()\n    )\n    if c in train_medians.index:\n        test_merge[c] = test_merge[c].fillna(train_medians[c])\n    else:\n        test_merge[c] = test_merge[c].fillna(0.0)\n\n# Baseline lat/lon for test\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\n\ntest_merge[\"base_lat\"] = (\n    test_merge.groupby(\"phone\")[\"base_lat\"].ffill().groupby(test_merge[\"phone\"]).bfill()\n)\ntest_merge[\"base_lon\"] = (\n    test_merge.groupby(\"phone\")[\"base_lon\"].ffill().groupby(test_merge[\"phone\"]).bfill()\n)\ntest_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(global_lat_mean)\ntest_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(global_lon_mean)\n\n# Finally, construct X_test strictly in the same column order as feature_cols\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate & metric utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef _aggregate_gnss(gnss, has_gt=False, gt=None):\n    # Common aggregation for train/test\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols).copy()\n    if gnss.empty:\n        return None\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns and not gnss[\"utcTimeMillis\"].isna().all():\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        if has_gt and gt is not None and not gt.empty:\n            offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n        else:\n            offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    return g_agg\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    if gt.empty:\n        return None\n\n    g_agg = _aggregate_gnss(gnss, has_gt=True, gt=gt)\n    if g_agg is None or g_agg.empty:\n        return None\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from ECEF if possible\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    g_agg = _aggregate_gnss(gnss, has_gt=False, gt=None)\n    if g_agg is None or g_agg.empty:\n        return None\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- load and build train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\n# Base numeric GNSS features + baseline lat/lon\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs within phone\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling window stats (window=3) for key features\nrolling_feats = []\nwindow = 3\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype == \"O\":\n        continue\n    train_df[feat + \"_roll3_mean\"] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    train_df[feat + \"_roll3_std\"] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n    rolling_feats.extend([feat + \"_roll3_mean\", feat + \"_roll3_std\"])\n\n# Final feature list after all feature engineering\nfeature_cols = (\n    base_feature_candidates\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n    + rolling_feats\n)\nfeature_cols = sorted(list(dict.fromkeys(feature_cols)))  # remove duplicates, stable\n\n# Fill NaNs in train\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for matching\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build raw test GNSS aggregates\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure base numeric feature columns exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    # Temporal diffs for test using same base_feature_candidates\n    for feat in base_feature_candidates:\n        if feat not in test_gnss.columns or test_gnss[feat].dtype == \"O\":\n            continue\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n    # Rolling stats for test\n    for feat in base_feature_candidates:\n        if test_gnss[feat].dtype == \"O\":\n            continue\n        test_gnss[feat + \"_roll3_mean\"] = (\n            test_gnss.groupby(\"phone\")[feat]\n            .rolling(window, min_periods=1)\n            .mean()\n            .reset_index(level=0, drop=True)\n        )\n        test_gnss[feat + \"_roll3_std\"] = (\n            test_gnss.groupby(\"phone\")[feat]\n            .rolling(window, min_periods=1)\n            .std()\n            .reset_index(level=0, drop=True)\n        )\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Merge GNSS features into sample grid\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    (\n        test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols]\n        if not test_gnss.empty\n        else sample_sub[[\"phone\", time_col]].assign(**{c: np.nan for c in feature_cols})\n    ),\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# Sort and forward/backward fill per phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) * math.sin(th) * math.sin(th),\n        p - e * e * a * math.cos(th) * math.cos(th) * math.cos(th),\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) * math.sin(lat))\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # derive baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling-window features (window=3) for key features including baseline lat/lon\nrolling_feats = []\nroll_window = 3\nroll_base_cols = [\n    c for c in base_feature_candidates if train_df[c].dtype != \"O\"\n]  # numeric base candidates\n\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    train_df[roll_mean_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    train_df[roll_std_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n    rolling_feats.extend([roll_mean_col, roll_std_col])\n\nfeature_cols = (\n    base_feature_candidates\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n    + rolling_feats\n)\n\n# Ensure feature_cols not empty\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=96,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure base_feature_candidates exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Create diff features in test\nfor feat in base_feature_candidates:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n    if test_gnss[feat].dtype != \"O\":\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Create rolling features in test matching train definitions\nfor feat in roll_base_cols:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    test_gnss[roll_mean_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    test_gnss[roll_std_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort and fill\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    test_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) * math.sin(th) * math.sin(th),\n        p - e * e * a * math.cos(th) * math.cos(th) * math.cos(th),\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) * math.sin(lat))\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # derive baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling-window features (window=3) for key features including baseline lat/lon\nrolling_feats = []\nroll_window = 3\nroll_base_cols = [\n    c for c in base_feature_candidates if train_df[c].dtype != \"O\"\n]  # numeric base candidates\n\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    train_df[roll_mean_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    train_df[roll_std_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n    rolling_feats.extend([roll_mean_col, roll_std_col])\n\nfeature_cols = (\n    base_feature_candidates\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n    + rolling_feats\n)\n\n# Ensure feature_cols not empty\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=96,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure base_feature_candidates exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Create diff features in test\nfor feat in base_feature_candidates:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n    if test_gnss[feat].dtype != \"O\":\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Create rolling features in test matching train definitions\nfor feat in roll_base_cols:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    test_gnss[roll_mean_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    test_gnss[roll_std_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort and fill within test_gnss\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# If base_lat/base_lon missing entirely for some rows, fill with global train mean as baseline\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\nif \"base_lat\" not in test_merge.columns:\n    test_merge[\"base_lat\"] = global_lat_mean\nif \"base_lon\" not in test_merge.columns:\n    test_merge[\"base_lon\"] = global_lon_mean\n\n# Sort and forward/backward fill within phone using transform to avoid length mismatch\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# First fill features with train medians to avoid all-NaN columns in groups\ntrain_medians = train_df[feature_cols].median()\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\n# Now smoother within-phone ffill/bfill using transform\ng = test_merge.groupby(\"phone\", sort=False)\nffilled = g[feature_cols].transform(\"ffill\")\nbfilled = g[feature_cols].transform(\"bfill\")\n# where ffilled is null, use bfilled, else ffilled\ntest_merge[feature_cols] = ffilled.where(~ffilled.isna(), bfilled)\n# still remaining NaN -> global medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\n# Baseline lat/lon for test: ffill/bfill per phone, then global mean\nfor col, gmean in [(\"base_lat\", global_lat_mean), (\"base_lon\", global_lon_mean)]:\n    if col not in test_merge.columns:\n        test_merge[col] = gmean\n    gb = test_merge.groupby(\"phone\", sort=False)[col]\n    col_ff = gb.transform(\"ffill\")\n    col_bf = gb.transform(\"bfill\")\n    test_merge[col] = col_ff.where(~col_ff.isna(), col_bf)\n    test_merge[col] = test_merge[col].fillna(gmean)\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) * math.sin(th) * math.sin(th),\n        p - e * e * a * math.cos(th) * math.cos(th) * math.cos(th),\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) * math.sin(lat))\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # derive baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling-window features (window=3) for key features including baseline lat/lon\nrolling_feats = []\nroll_window = 3\nroll_base_cols = [\n    c for c in base_feature_candidates if train_df[c].dtype != \"O\"\n]  # numeric base candidates\n\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    train_df[roll_mean_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    train_df[roll_std_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n    rolling_feats.extend([roll_mean_col, roll_std_col])\n\nfeature_cols = (\n    base_feature_candidates\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n    + rolling_feats\n)\n\n# Deduplicate feature_cols if any duplicates\nfeature_cols = list(dict.fromkeys(feature_cols))\n\n# Ensure feature_cols not empty\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=96,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure base_feature_candidates exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Create diff features in test\nfor feat in base_feature_candidates:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n    if test_gnss[feat].dtype != \"O\":\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Create rolling features in test matching train definitions\nfor feat in roll_base_cols:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    test_gnss[roll_mean_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    test_gnss[roll_std_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort and fill within test_gnss\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\n# Ensure columns exist for selection\nfor col in merge_cols:\n    if col not in test_gnss.columns:\n        test_gnss[col] = np.nan\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# If base_lat/base_lon missing entirely for some rows, fill with global train mean as baseline\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\nif \"base_lat\" not in test_merge.columns:\n    test_merge[\"base_lat\"] = global_lat_mean\nif \"base_lon\" not in test_merge.columns:\n    test_merge[\"base_lon\"] = global_lon_mean\n\n# Sort rows for stable group operations\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# Ensure all feature_cols present in test_merge\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# First fill features with train medians to avoid all-NaN columns in groups\ntrain_medians = train_df[feature_cols].median()\nfilled_features = test_merge[feature_cols].copy().fillna(train_medians)\ntest_merge.loc[:, feature_cols] = filled_features.values\n\n# Now smoother within-phone ffill/bfill using transform\ng = test_merge.groupby(\"phone\", sort=False)\nffilled = g[feature_cols].transform(\"ffill\")\nbfilled = g[feature_cols].transform(\"bfill\")\ntest_merge.loc[:, feature_cols] = ffilled.where(~ffilled.isna(), bfilled)\n# still remaining NaN -> global medians\ntest_merge.loc[:, feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\n# Baseline lat/lon for test: ffill/bfill per phone, then global mean\nfor col, gmean in [(\"base_lat\", global_lat_mean), (\"base_lon\", global_lon_mean)]:\n    if col not in test_merge.columns:\n        test_merge[col] = gmean\n    gb = test_merge.groupby(\"phone\", sort=False)[col]\n    col_ff = gb.transform(\"ffill\")\n    col_bf = gb.transform(\"bfill\")\n    test_merge[col] = col_ff.where(~col_ff.isna(), col_bf)\n    test_merge[col] = test_merge[col].fillna(gmean)\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) * math.sin(th) * math.sin(th),\n        p - e * e * a * math.cos(th) * math.cos(th) * math.cos(th),\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) * math.sin(lat))\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # derive baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling-window features (window=3) for key features including baseline lat/lon\nrolling_feats = []\nroll_window = 3\nroll_base_cols = [\n    c for c in base_feature_candidates if train_df[c].dtype != \"O\"\n]  # numeric base candidates\n\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    train_df[roll_mean_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    train_df[roll_std_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n    rolling_feats.extend([roll_mean_col, roll_std_col])\n\nfeature_cols = (\n    base_feature_candidates\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n    + rolling_feats\n)\n\n# Deduplicate feature_cols while preserving order\nfeature_cols = list(dict.fromkeys(feature_cols))\n\n# Ensure feature_cols not empty\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs in train\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=96,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure base_feature_candidates exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Create diff features in test\nfor feat in base_feature_candidates:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n    if test_gnss[feat].dtype != \"O\":\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Create rolling features in test matching train definitions\nfor feat in roll_base_cols:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    test_gnss[roll_mean_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    test_gnss[roll_std_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort and fill within test_gnss\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\n# Ensure columns exist for selection\nfor col in merge_cols:\n    if col not in test_gnss.columns:\n        test_gnss[col] = np.nan\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# If base_lat/base_lon missing entirely for some rows, fill with global train mean as baseline\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\nif \"base_lat\" not in test_merge.columns:\n    test_merge[\"base_lat\"] = global_lat_mean\nif \"base_lon\" not in test_merge.columns:\n    test_merge[\"base_lon\"] = global_lon_mean\n\n# Sort rows for stable group operations\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# Ensure all feature_cols present in test_merge\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# First fill features with train medians to avoid all-NaN columns in groups\ntrain_medians = train_df[feature_cols].median()\nfilled_features = test_merge[feature_cols].copy().fillna(train_medians)\ntest_merge.loc[:, feature_cols] = filled_features.values\n\n# Now smoother within-phone ffill/bfill using transform; assign using .values to avoid non-unique column issues\ng = test_merge.groupby(\"phone\", sort=False)\nffilled = g[feature_cols].transform(\"ffill\")\nbfilled = g[feature_cols].transform(\"bfill\")\ncombined_features = ffilled.where(~ffilled.isna(), bfilled)\ntest_merge.loc[:, feature_cols] = combined_features.values\n# still remaining NaN -> global medians\ntest_merge.loc[:, feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\n# Baseline lat/lon for test: ffill/bfill per phone, then global mean\nfor col, gmean in [(\"base_lat\", global_lat_mean), (\"base_lon\", global_lon_mean)]:\n    if col not in test_merge.columns:\n        test_merge[col] = gmean\n    gb = test_merge.groupby(\"phone\", sort=False)[col]\n    col_ff = gb.transform(\"ffill\")\n    col_bf = gb.transform(\"bfill\")\n    col_combined = col_ff.where(~col_ff.isna(), col_bf)\n    test_merge[col] = col_combined.values\n    test_merge[col] = test_merge[col].fillna(gmean)\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) * math.sin(th) * math.sin(th),\n        p - e * e * a * math.cos(th) * math.cos(th) * math.cos(th),\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) * math.sin(lat))\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # derive baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling-window features (window=3) for key features including baseline lat/lon\nrolling_feats = []\nroll_window = 3\nroll_base_cols = [c for c in base_feature_candidates if train_df[c].dtype != \"O\"]\n\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    train_df[roll_mean_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    train_df[roll_std_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n    rolling_feats.extend([roll_mean_col, roll_std_col])\n\nfeature_cols = (\n    base_feature_candidates\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n    + rolling_feats\n)\n\n# Deduplicate feature_cols while preserving order\nfeature_cols = list(dict.fromkeys(feature_cols))\n\n# Ensure feature_cols not empty\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs in train\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=96,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure base_feature_candidates exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Create diff features in test\nfor feat in base_feature_candidates:\n    if test_gnss[feat].dtype != \"O\":\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Create rolling features in test matching train definitions\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    test_gnss[roll_mean_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    test_gnss[roll_std_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort and fill within test_gnss\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\nfor col in merge_cols:\n    if col not in test_gnss.columns:\n        test_gnss[col] = np.nan\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# If base_lat/base_lon missing entirely for some rows, fill with global train mean as baseline\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\nif \"base_lat\" not in test_merge.columns:\n    test_merge[\"base_lat\"] = global_lat_mean\nif \"base_lon\" not in test_merge.columns:\n    test_merge[\"base_lon\"] = global_lon_mean\n\n# Sort rows for stable group operations\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# Ensure all feature_cols present in test_merge\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# First fill features with train medians to avoid all-NaN columns in groups\ntrain_medians = train_df[feature_cols].median()\nfilled_features = test_merge[feature_cols].copy().fillna(train_medians)\n# assign via .values to avoid issues with potential non-unique columns internally\ntest_merge[feature_cols] = filled_features.values\n\n# Now smoother within-phone ffill/bfill using transform; again assign values\ng = test_merge.groupby(\"phone\", sort=False)\nffilled = g[feature_cols].transform(\"ffill\")\nbfilled = g[feature_cols].transform(\"bfill\")\ncombined_features = ffilled.where(~ffilled.isna(), bfilled)\ntest_merge[feature_cols] = combined_features.values\n# still remaining NaN -> global medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\n# Baseline lat/lon for test: ffill/bfill per phone, then global mean\nfor col, gmean in [(\"base_lat\", global_lat_mean), (\"base_lon\", global_lon_mean)]:\n    if col not in test_merge.columns:\n        test_merge[col] = gmean\n    gb = test_merge.groupby(\"phone\", sort=False)[col]\n    col_ff = gb.transform(\"ffill\")\n    col_bf = gb.transform(\"bfill\")\n    col_combined = col_ff.where(~col_ff.isna(), col_bf)\n    test_merge[col] = col_combined.values\n    test_merge[col] = test_merge[col].fillna(gmean)\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) * math.sin(th) * math.sin(th),\n        p - e * e * a * math.cos(th) * math.cos(th) * math.cos(th),\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) * math.sin(lat))\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # derive baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling-window features (window=3) for key features including baseline lat/lon\nrolling_feats = []\nroll_window = 3\nroll_base_cols = [c for c in base_feature_candidates if train_df[c].dtype != \"O\"]\n\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    train_df[roll_mean_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    train_df[roll_std_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n    rolling_feats.extend([roll_mean_col, roll_std_col])\n\nfeature_cols = (\n    base_feature_candidates\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n    + rolling_feats\n)\n\n# Deduplicate feature_cols while preserving order\nfeature_cols = list(dict.fromkeys(feature_cols))\n\n# Ensure feature_cols not empty\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs in train\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=96,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure base_feature_candidates exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Create diff features in test\nfor feat in base_feature_candidates:\n    if test_gnss[feat].dtype != \"O\":\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Create rolling features in test matching train definitions\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    test_gnss[roll_mean_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    test_gnss[roll_std_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort and fill within test_gnss\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\nfor col in merge_cols:\n    if col not in test_gnss.columns:\n        test_gnss[col] = np.nan\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# If base_lat/base_lon missing entirely for some rows, fill with global train mean as baseline\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\nif \"base_lat\" not in test_merge.columns:\n    test_merge[\"base_lat\"] = global_lat_mean\nif \"base_lon\" not in test_merge.columns:\n    test_merge[\"base_lon\"] = global_lon_mean\n\n# Sort rows for stable group operations\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# Ensure all feature_cols present in test_merge\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Compute train medians once\ntrain_medians = train_df[feature_cols].median()\n\n# First fill features with train medians to avoid all-NaN columns\nfeatures_block = test_merge[feature_cols]\nfeatures_block = features_block.fillna(train_medians)\n\n# Now smoother within-phone ffill/bfill\ng = test_merge[\"phone\"]\n# groupby on index via phone series\nffilled = features_block.groupby(g, sort=False).ffill()\nbfilled = features_block.groupby(g, sort=False).bfill()\ncombined_features = ffilled.where(~ffilled.isna(), bfilled)\n# any remaining NaN -> global medians\ncombined_features = combined_features.fillna(train_medians)\ntest_merge[feature_cols] = combined_features\n\n# Baseline lat/lon for test: ffill/bfill per phone, then global mean\nfor col, gmean in [(\"base_lat\", global_lat_mean), (\"base_lon\", global_lon_mean)]:\n    if col not in test_merge.columns:\n        test_merge[col] = gmean\n    gb = test_merge.groupby(\"phone\", sort=False)[col]\n    col_ff = gb.ffill()\n    col_bf = gb.bfill()\n    col_combined = col_ff.where(~col_ff.isna(), col_bf)\n    test_merge[col] = col_combined.fillna(gmean)\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) * math.sin(th) * math.sin(th),\n        p - e * e * a * math.cos(th) * math.cos(th) * math.cos(th),\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) * math.sin(lat))\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF and some signal stats by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # derive baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling-window features (window=3) for key features including baseline lat/lon\nrolling_feats = []\nroll_window = 3\nroll_base_cols = [c for c in base_feature_candidates if train_df[c].dtype != \"O\"]\n\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    train_df[roll_mean_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    train_df[roll_std_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n    rolling_feats.extend([roll_mean_col, roll_std_col])\n\nfeature_cols = (\n    base_feature_candidates\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n    + rolling_feats\n)\n\n# Deduplicate feature_cols while preserving order\nfeature_cols = list(dict.fromkeys(feature_cols))\n\n# Ensure feature_cols not empty\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs in train\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=96,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure base_feature_candidates exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Create diff features in test\nfor feat in base_feature_candidates:\n    if test_gnss[feat].dtype != \"O\":\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Create rolling features in test matching train definitions\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    test_gnss[roll_mean_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    test_gnss[roll_std_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort and fill within test_gnss\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\nfor col in merge_cols:\n    if col not in test_gnss.columns:\n        test_gnss[col] = np.nan\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# If base_lat/base_lon missing entirely for some rows, fill with global train mean as baseline\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\nif \"base_lat\" not in test_merge.columns:\n    test_merge[\"base_lat\"] = global_lat_mean\nif \"base_lon\" not in test_merge.columns:\n    test_merge[\"base_lon\"] = global_lon_mean\n\n# Sort rows for stable group operations\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# Ensure all feature_cols present in test_merge\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Compute train medians once\ntrain_medians = train_df[feature_cols].median()\n\n# 1) Fill NaNs in test features with train medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\n# 2) Within-phone forward/backward fill to smooth small gaps\nfor feat in feature_cols:\n    # using transform to preserve index alignment and length\n    test_merge[feat] = test_merge.groupby(\"phone\")[feat].transform(\n        lambda s: s.ffill().bfill()\n    )\n\n# 3) Any remaining NaNs (unlikely) -> train medians again\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\n# Baseline lat/lon for test: ffill/bfill per phone, then global mean\nfor col, gmean in [(\"base_lat\", global_lat_mean), (\"base_lon\", global_lon_mean)]:\n    if col not in test_merge.columns:\n        test_merge[col] = gmean\n    test_merge[col] = test_merge.groupby(\"phone\")[col].transform(\n        lambda s: s.ffill().bfill()\n    )\n    test_merge[col] = test_merge[col].fillna(gmean)\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) * math.sin(th) * math.sin(th),\n        p - e * e * a * math.cos(th) * math.cos(th) * math.cos(th),\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) * math.sin(lat))\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF and some signal stats by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # derive baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling-window features (window=3) for key features including baseline lat/lon\nrolling_feats = []\nroll_window = 3\nroll_base_cols = [c for c in base_feature_candidates if train_df[c].dtype != \"O\"]\n\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    train_df[roll_mean_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    train_df[roll_std_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n    rolling_feats.extend([roll_mean_col, roll_std_col])\n\nfeature_cols = (\n    base_feature_candidates\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n    + rolling_feats\n)\n\n# Deduplicate feature_cols while preserving order\nfeature_cols = list(dict.fromkeys(feature_cols))\n\n# Ensure feature_cols not empty\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs in train\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=96,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure base_feature_candidates exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Create diff features in test\nfor feat in base_feature_candidates:\n    if test_gnss[feat].dtype != \"O\":\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Create rolling features in test matching train definitions\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    test_gnss[roll_mean_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    test_gnss[roll_std_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort and fill within test_gnss\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\nfor col in merge_cols:\n    if col not in test_gnss.columns:\n        test_gnss[col] = np.nan\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# If base_lat/base_lon missing entirely for some rows, fill with global train mean as baseline\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\nif \"base_lat\" not in test_merge.columns:\n    test_merge[\"base_lat\"] = global_lat_mean\nif \"base_lon\" not in test_merge.columns:\n    test_merge[\"base_lon\"] = global_lon_mean\n\n# Sort rows for stable group operations\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# Ensure all feature_cols present in test_merge\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Compute train medians once as a dict to avoid alignment issues\ntrain_medians = train_df[feature_cols].median()\nmedian_dict = train_medians.to_dict()\n\n# 1) Fill NaNs in test features with train medians (column-wise)\nfor feat in feature_cols:\n    fill_val = median_dict.get(feat, 0.0)\n    test_merge[feat] = test_merge[feat].fillna(fill_val)\n\n# 2) Within-phone forward/backward fill to smooth small gaps\nfor feat in feature_cols:\n    test_merge[feat] = test_merge.groupby(\"phone\")[feat].transform(\n        lambda s: s.ffill().bfill()\n    )\n\n# 3) Any remaining NaNs (unlikely) -> train medians again\nfor feat in feature_cols:\n    fill_val = median_dict.get(feat, 0.0)\n    test_merge[feat] = test_merge[feat].fillna(fill_val)\n\n# Baseline lat/lon for test: ffill/bfill per phone, then global mean\nfor col, gmean in [(\"base_lat\", global_lat_mean), (\"base_lon\", global_lon_mean)]:\n    if col not in test_merge.columns:\n        test_merge[col] = gmean\n    test_merge[col] = test_merge.groupby(\"phone\")[col].transform(\n        lambda s: s.ffill().bfill()\n    )\n    test_merge[col] = test_merge[col].fillna(gmean)\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) * math.sin(th) * math.sin(th),\n        p - e * e * a * math.cos(th) * math.cos(th) * math.cos(th),\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) * math.sin(lat))\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF and some signal stats by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # derive baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling-window features (window=3) for key features including baseline lat/lon\nrolling_feats = []\nroll_window = 3\nroll_base_cols = [c for c in base_feature_candidates if train_df[c].dtype != \"O\"]\n\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    train_df[roll_mean_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    train_df[roll_std_col] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n    rolling_feats.extend([roll_mean_col, roll_std_col])\n\nfeature_cols = (\n    base_feature_candidates\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n    + rolling_feats\n)\n\n# Deduplicate feature_cols while preserving order\nfeature_cols = list(dict.fromkeys(feature_cols))\n\n# Ensure feature_cols not empty\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs in train\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=96,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=96,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure base_feature_candidates exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Create diff features in test\nfor feat in base_feature_candidates:\n    if test_gnss[feat].dtype != \"O\":\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Create rolling features in test matching train definitions\nfor feat in roll_base_cols:\n    roll_mean_col = feat + \"_roll3_mean\"\n    roll_std_col = feat + \"_roll3_std\"\n    test_gnss[roll_mean_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    test_gnss[roll_std_col] = (\n        test_gnss.groupby(\"phone\")[feat]\n        .rolling(window=roll_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n\n# Sort and prepare for merge\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + list(\n    set(feature_cols + [\"base_lat\", \"base_lon\"])\n)\nfor col in merge_cols:\n    if col not in test_gnss.columns:\n        test_gnss[col] = np.nan\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# If base_lat/base_lon missing entirely for some rows, fill with global train mean as baseline\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\nif \"base_lat\" not in test_merge.columns:\n    test_merge[\"base_lat\"] = global_lat_mean\nif \"base_lon\" not in test_merge.columns:\n    test_merge[\"base_lon\"] = global_lon_mean\n\n# Sort rows for stable group operations\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# Ensure all feature_cols present in test_merge, but do NOT add extras to feature_cols\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Compute train medians once as a dict to avoid alignment issues\ntrain_medians = train_df[feature_cols].median()\nmedian_dict = train_medians.to_dict()\n\n# 1) Fill NaNs in test features with train medians (column-wise)\nfor feat in feature_cols:\n    fill_val = median_dict.get(feat, 0.0)\n    test_merge[feat] = test_merge[feat].fillna(fill_val)\n\n# 2) Within-phone forward/backward fill to smooth small gaps\nfor feat in feature_cols:\n    test_merge[feat] = test_merge.groupby(\"phone\")[feat].transform(\n        lambda s: s.ffill().bfill()\n    )\n\n# 3) Any remaining NaNs (unlikely) -> train medians again\nfor feat in feature_cols:\n    fill_val = median_dict.get(feat, 0.0)\n    test_merge[feat] = test_merge[feat].fillna(fill_val)\n\n# Baseline lat/lon for test: ffill/bfill per phone, then global mean\nfor col, gmean in [(\"base_lat\", global_lat_mean), (\"base_lon\", global_lon_mean)]:\n    if col not in test_merge.columns:\n        test_merge[col] = gmean\n    test_merge[col] = test_merge.groupby(\"phone\")[col].transform(\n        lambda s: s.ffill().bfill()\n    )\n    test_merge[col] = test_merge[col].fillna(gmean)\n\n# Critically, ensure the test matrix has exactly the same columns (order and count) as training\ntest_merge = test_merge.reset_index(drop=True)\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\n# Predictions\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Base feature candidates\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling window stats (window=3) for key features\nrolling_feats = base_feature_candidates\nwindow = 3\nfor feat in rolling_feats:\n    if train_df[feat].dtype != \"O\":\n        grp = train_df.groupby(\"phone\")[feat]\n        train_df[feat + \"_roll_mean\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).mean()\n        )\n        train_df[feat + \"_roll_std\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).std()\n        )\n\n# Recompute feature_cols after adding diffs and rolling stats\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and c\n    not in [\n        \"tgt_lat\",\n        \"tgt_lon\",\n        \"dlat\",\n        \"dlon\",\n    ]\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build raw test GNSS aggregates (without temporal features yet)\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure baseline lat/lon exist\nif \"base_lat\" not in test_gnss.columns:\n    test_gnss[\"base_lat\"] = 0.0\nif \"base_lon\" not in test_gnss.columns:\n    test_gnss[\"base_lon\"] = 0.0\n\n# Derive the same base_feature_candidates list from train (already have)\n# Add temporal diffs and rolling stats to test using same feature definitions\n# First ensure all base_feature_candidates columns exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Diffs\nfor feat in base_feature_candidates:\n    if feat in test_gnss.columns and test_gnss[feat].dtype != \"O\":\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Rolling stats\nfor feat in rolling_feats:\n    if feat in test_gnss.columns and test_gnss[feat].dtype != \"O\":\n        grp = test_gnss.groupby(\"phone\")[feat]\n        test_gnss[feat + \"_roll_mean\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).mean()\n        )\n        test_gnss[feat + \"_roll_std\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).std()\n        )\n\n# Make sure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Fill test features with train medians where needed\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Base feature candidates, restricted to numeric\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nnumeric_cols_train = set(\n    c for c in train_df.columns if pd.api.types.is_numeric_dtype(train_df[c])\n)\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and c in numeric_cols_train\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling window stats (window=3) for key features\nrolling_feats = base_feature_candidates\nwindow = 3\nfor feat in rolling_feats:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        grp = train_df.groupby(\"phone\")[feat]\n        train_df[feat + \"_roll_mean\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).mean()\n        )\n        train_df[feat + \"_roll_std\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).std()\n        )\n\n# Recompute feature_cols after adding diffs and rolling stats, numeric only\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and c\n    not in [\n        \"tgt_lat\",\n        \"tgt_lon\",\n        \"dlat\",\n        \"dlon\",\n    ]\n    and pd.api.types.is_numeric_dtype(train_df[c])\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs with column-wise medians (numeric-safe)\ntrain_medians = train_df[feature_cols].median()\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_medians)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column (match training format drive_phone)\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build raw test GNSS aggregates (without temporal features yet)\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure baseline lat/lon exist\nif \"base_lat\" not in test_gnss.columns:\n    test_gnss[\"base_lat\"] = 0.0\nif \"base_lon\" not in test_gnss.columns:\n    test_gnss[\"base_lon\"] = 0.0\n\n# Ensure all base_feature_candidates columns exist and are numeric\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Diffs\nfor feat in base_feature_candidates:\n    if feat in test_gnss.columns and pd.api.types.is_numeric_dtype(test_gnss[feat]):\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Rolling stats\nfor feat in rolling_feats:\n    if feat in test_gnss.columns and pd.api.types.is_numeric_dtype(test_gnss[feat]):\n        grp = test_gnss.groupby(\"phone\")[feat]\n        test_gnss[feat + \"_roll_mean\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).mean()\n        )\n        test_gnss[feat + \"_roll_std\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).std()\n        )\n\n# Make sure all feature_cols exist in test_gnss and are numeric\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Fill test features with train medians where needed\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(train_medians)\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone for features and baseline\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ncols_to_ffill = feature_cols + [\"base_lat\", \"base_lon\"]\ntest_merge[cols_to_ffill] = test_merge.groupby(\"phone\")[cols_to_ffill].ffill()\ntest_merge[cols_to_ffill] = test_merge.groupby(\"phone\")[cols_to_ffill].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Base feature candidates, restricted to numeric\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nnumeric_cols_train = set(\n    c for c in train_df.columns if pd.api.types.is_numeric_dtype(train_df[c])\n)\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and c in numeric_cols_train\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling window stats (window=3) for key features\nrolling_feats = base_feature_candidates\nwindow = 3\nfor feat in rolling_feats:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        grp = train_df.groupby(\"phone\")[feat]\n        train_df[feat + \"_roll_mean\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).mean()\n        )\n        train_df[feat + \"_roll_std\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).std()\n        )\n\n# Recompute feature_cols after adding diffs and rolling stats, numeric only\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and c not in [\"tgt_lat\", \"tgt_lon\", \"dlat\", \"dlon\"]\n    and pd.api.types.is_numeric_dtype(train_df[c])\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs with column-wise medians (numeric-safe)\ntrain_medians = train_df[feature_cols].median()\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_medians)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column (match training format drive_phone)\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build raw test GNSS aggregates (without temporal features yet)\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure baseline lat/lon exist\nif \"base_lat\" not in test_gnss.columns:\n    test_gnss[\"base_lat\"] = 0.0\nif \"base_lon\" not in test_gnss.columns:\n    test_gnss[\"base_lon\"] = 0.0\n\n# Ensure all base_feature_candidates columns exist and are numeric\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Diffs\nfor feat in base_feature_candidates:\n    if feat in test_gnss.columns and pd.api.types.is_numeric_dtype(test_gnss[feat]):\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Rolling stats\nfor feat in rolling_feats:\n    if feat in test_gnss.columns and pd.api.types.is_numeric_dtype(test_gnss[feat]):\n        grp = test_gnss.groupby(\"phone\")[feat]\n        test_gnss[feat + \"_roll_mean\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).mean()\n        )\n        test_gnss[feat + \"_roll_std\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).std()\n        )\n\n# Make sure all feature_cols exist in test_gnss and are numeric\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Fill test features with train medians where needed\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(train_medians)\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# Rename the merged UnixTimeMillis to a consistent name and drop duplicate if any\nif (\n    \"UnixTimeMillis_x\" in test_merge.columns\n    and \"UnixTimeMillis_y\" in test_merge.columns\n):\n    test_merge = test_merge.rename(\n        columns={\n            \"UnixTimeMillis_x\": \"UnixTimeMillis_sample\",\n            \"UnixTimeMillis_y\": \"UnixTimeMillis\",\n        }\n    )\nelif \"UnixTimeMillis_x\" in test_merge.columns:\n    test_merge = test_merge.rename(columns={\"UnixTimeMillis_x\": \"UnixTimeMillis\"})\nelif \"UnixTimeMillis_y\" in test_merge.columns:\n    test_merge = test_merge.rename(columns={\"UnixTimeMillis_y\": \"UnixTimeMillis\"})\n\n# sort and forward/backward fill within phone for features and baseline\nsort_cols = [\"phone\"]\nif \"UnixTimeMillis\" in test_merge.columns:\n    sort_cols.append(\"UnixTimeMillis\")\ntest_merge = test_merge.sort_values(sort_cols).reset_index(drop=True)\n\ncols_to_ffill = feature_cols + [\"base_lat\", \"base_lon\"]\n# Use only columns that actually exist in test_merge\nexisting_cols_to_ffill = [c for c in cols_to_ffill if c in test_merge.columns]\n\nif existing_cols_to_ffill:\n    # Apply ffill/bfill column-wise to avoid dimension issues\n    for c in existing_cols_to_ffill:\n        test_merge[c] = (\n            test_merge.groupby(\"phone\")[c]\n            .apply(lambda s: s.ffill().bfill())\n            .reset_index(level=0, drop=True)\n        )\n\n# For any feature columns still missing or entirely NaN, fill with train medians\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = train_medians.get(c, 0.0)\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    {c: train_medians.get(c, 0.0) for c in feature_cols}\n)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Base feature candidates, restricted to numeric\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nnumeric_cols_train = set(\n    c for c in train_df.columns if pd.api.types.is_numeric_dtype(train_df[c])\n)\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and c in numeric_cols_train\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling window stats (window=3) for key features\nrolling_feats = base_feature_candidates\nwindow = 3\nfor feat in rolling_feats:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        grp = train_df.groupby(\"phone\")[feat]\n        train_df[feat + \"_roll_mean\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).mean()\n        )\n        train_df[feat + \"_roll_std\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).std()\n        )\n\n# Recompute feature_cols after adding diffs and rolling stats, numeric only\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and c not in [\"tgt_lat\", \"tgt_lon\", \"dlat\", \"dlon\"]\n    and pd.api.types.is_numeric_dtype(train_df[c])\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs with column-wise medians (numeric-safe)\ntrain_medians = train_df[feature_cols].median()\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_medians)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column (match training format drive_phone)\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build raw test GNSS aggregates (without temporal features yet)\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure baseline lat/lon exist\nif \"base_lat\" not in test_gnss.columns:\n    test_gnss[\"base_lat\"] = 0.0\nif \"base_lon\" not in test_gnss.columns:\n    test_gnss[\"base_lon\"] = 0.0\n\n# Ensure all base_feature_candidates columns exist and are numeric\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Diffs\nfor feat in base_feature_candidates:\n    if feat in test_gnss.columns and pd.api.types.is_numeric_dtype(test_gnss[feat]):\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Rolling stats\nfor feat in rolling_feats:\n    if feat in test_gnss.columns and pd.api.types.is_numeric_dtype(test_gnss[feat]):\n        grp = test_gnss.groupby(\"phone\")[feat]\n        test_gnss[feat + \"_roll_mean\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).mean()\n        )\n        test_gnss[feat + \"_roll_std\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).std()\n        )\n\n# Make sure all feature_cols exist in test_gnss and are numeric\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Fill test features with train medians where needed\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\nfor c in feature_cols:\n    if c in test_gnss.columns:\n        test_gnss[c] = test_gnss[c].fillna(train_medians.get(c, 0.0))\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# Rename the merged UnixTimeMillis to a consistent name and drop duplicate if any\nif (\n    \"UnixTimeMillis_x\" in test_merge.columns\n    and \"UnixTimeMillis_y\" in test_merge.columns\n):\n    test_merge = test_merge.rename(\n        columns={\n            \"UnixTimeMillis_x\": \"UnixTimeMillis_sample\",\n            \"UnixTimeMillis_y\": \"UnixTimeMillis\",\n        }\n    )\nelif \"UnixTimeMillis_x\" in test_merge.columns:\n    test_merge = test_merge.rename(columns={\"UnixTimeMillis_x\": \"UnixTimeMillis\"})\nelif \"UnixTimeMillis_y\" in test_merge.columns:\n    test_merge = test_merge.rename(columns={\"UnixTimeMillis_y\": \"UnixTimeMillis\"})\n\n# sort and forward/backward fill within phone for features and baseline\nsort_cols = [\"phone\"]\nif \"UnixTimeMillis\" in test_merge.columns:\n    sort_cols.append(\"UnixTimeMillis\")\ntest_merge = test_merge.sort_values(sort_cols).reset_index(drop=True)\n\ncols_to_ffill = feature_cols + [\"base_lat\", \"base_lon\"]\nexisting_cols_to_ffill = [c for c in cols_to_ffill if c in test_merge.columns]\n\nfor c in existing_cols_to_ffill:\n    test_merge[c] = test_merge.groupby(\"phone\")[c].transform(\n        lambda s: s.ffill().bfill()\n    )\n\n# For any feature columns missing or still NaN, fill with train medians safely\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = train_medians.get(c, 0.0)\n    else:\n        test_merge[c] = test_merge[c].fillna(train_medians.get(c, 0.0))\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Base feature candidates, restricted to numeric\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nnumeric_cols_train = set(\n    c for c in train_df.columns if pd.api.types.is_numeric_dtype(train_df[c])\n)\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and c in numeric_cols_train\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for key numeric features\nfor feat in base_feature_candidates:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling window stats (window=3) for key features\nrolling_feats = base_feature_candidates\nwindow = 3\nfor feat in rolling_feats:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        grp = train_df.groupby(\"phone\")[feat]\n        train_df[feat + \"_roll_mean\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).mean()\n        )\n        train_df[feat + \"_roll_std\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).std()\n        )\n\n# Recompute feature_cols after adding diffs and rolling stats, numeric only\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and c not in [\"tgt_lat\", \"tgt_lon\", \"dlat\", \"dlon\"]\n    and pd.api.types.is_numeric_dtype(train_df[c])\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs with column-wise medians (numeric-safe)\ntrain_medians = train_df[feature_cols].median()\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_medians)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column (match training format drive_phone)\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build raw test GNSS aggregates\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure baseline lat/lon exist\nif \"base_lat\" not in test_gnss.columns:\n    test_gnss[\"base_lat\"] = 0.0\nif \"base_lon\" not in test_gnss.columns:\n    test_gnss[\"base_lon\"] = 0.0\n\n# Merge GNSS with sample submission to get the exact time grid per phone\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss,\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# After merge, keep a single UnixTimeMillis column with the submission times\nif (\n    \"UnixTimeMillis_x\" in test_merge.columns\n    and \"UnixTimeMillis_y\" in test_merge.columns\n):\n    test_merge = test_merge.rename(columns={\"UnixTimeMillis_x\": \"UnixTimeMillis\"}).drop(\n        columns=[\"UnixTimeMillis_y\"]\n    )\nelif \"UnixTimeMillis_x\" in test_merge.columns:\n    test_merge = test_merge.rename(columns={\"UnixTimeMillis_x\": \"UnixTimeMillis\"})\nelif \"UnixTimeMillis_y\" in test_merge.columns:\n    test_merge = test_merge.rename(columns={\"UnixTimeMillis_y\": \"UnixTimeMillis\"})\n\n# Ensure UnixTimeMillis type\nif \"UnixTimeMillis\" in test_merge.columns:\n    test_merge[\"UnixTimeMillis\"] = test_merge[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all base_feature_candidates exist before creating temporal features\nfor c in base_feature_candidates:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Ensure baseline columns exist\nif \"base_lat\" not in test_merge.columns:\n    test_merge[\"base_lat\"] = 0.0\nif \"base_lon\" not in test_merge.columns:\n    test_merge[\"base_lon\"] = 0.0\n\n# Sort before temporal features\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Create temporal diff and rolling features on test with the SAME logic as train,\n# but ONLY for features in base_feature_candidates, then later restrict to feature_cols.\nfor feat in base_feature_candidates:\n    if pd.api.types.is_numeric_dtype(test_merge[feat]):\n        test_merge[feat + \"_diff\"] = test_merge.groupby(\"phone\")[feat].diff()\n\nfor feat in rolling_feats:\n    if pd.api.types.is_numeric_dtype(test_merge[feat]):\n        grp = test_merge.groupby(\"phone\")[feat]\n        test_merge[feat + \"_roll_mean\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).mean()\n        )\n        test_merge[feat + \"_roll_std\"] = grp.transform(\n            lambda s: s.rolling(window, min_periods=1).std()\n        )\n\n# Now guarantee that EVERY training feature exists in test_merge, and ONLY use those.\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Forward/backward fill within phone for feature columns and baselines\ncols_to_ffill = list(set(feature_cols + [\"base_lat\", \"base_lon\"]))\nexisting_cols_to_ffill = [c for c in cols_to_ffill if c in test_merge.columns]\n\nfor c in existing_cols_to_ffill:\n    test_merge[c] = test_merge.groupby(\"phone\")[c].transform(\n        lambda s: s.ffill().bfill()\n    )\n\n# For any feature columns still NaN, fill with train medians\nfor c in feature_cols:\n    if c in test_merge.columns:\n        test_merge[c] = test_merge[c].fillna(train_medians.get(c, 0.0))\n    else:\n        # Should not happen due to earlier creation, but keep safe\n        test_merge[c] = train_medians.get(c, 0.0)\n\n# Build X_test with exact same feature order\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef aggregate_gnss(gnss, is_train=True, gt=None):\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols).copy()\n    if gnss.empty:\n        return None\n\n    # map GPS epoch nanos to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        if is_train and gt is not None and not gt.empty:\n            offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n        else:\n            offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [f\"{c[0]}_{c[1]}\" for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    return g_agg\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    if gt.empty:\n        return None\n\n    g_agg = aggregate_gnss(gnss, is_train=True, gt=gt)\n    if g_agg is None or g_agg.empty:\n        return None\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    g_agg = aggregate_gnss(gnss, is_train=False, gt=None)\n    if g_agg is None or g_agg.empty:\n        return None\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- load all train ----------------- #\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- temporal rolling features (window=3) ----------------- #\nnumeric_cols_for_roll = [\n    c\n    for c in train_df.columns\n    if (\n        (\"WlsPosition\" in c)\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n    and train_df[c].dtype != \"O\"\n]\n\nwindow = 3\nfor col in numeric_cols_for_roll:\n    train_df[f\"{col}_roll_mean\"] = (\n        train_df.groupby(\"phone\")[col]\n        .rolling(window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    train_df[f\"{col}_roll_std\"] = (\n        train_df.groupby(\"phone\")[col]\n        .rolling(window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n\n# ----------------- feature columns (base + diffs) ----------------- #\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c.startswith(\"Svid_nunique\")\n        or c.startswith(\"base_lat\")\n        or c.startswith(\"base_lon\")\n        or \"_roll_mean\" in c\n        or \"_roll_std\" in c\n    )\n]\n\nbase_feature_candidates = sorted(set(base_feature_candidates))\n\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\ndiff_cols = [c for c in train_df.columns if c.endswith(\"_diff\")]\n\nfeature_cols = base_feature_candidates + diff_cols\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_dlat = model_lat.predict(X_val)\n    oof_dlon = model_lon.predict(X_val)\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + oof_dlat\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + oof_dlon\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models ----------------- #\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Rolling features on test GNSS (same columns as train where possible)\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    # ensure key numeric cols for rolling\n    for col in numeric_cols_for_roll:\n        if col not in test_gnss.columns:\n            test_gnss[col] = np.nan\n\n    for col in numeric_cols_for_roll:\n        test_gnss[f\"{col}_roll_mean\"] = (\n            test_gnss.groupby(\"phone\")[col]\n            .rolling(window, min_periods=1)\n            .mean()\n            .reset_index(level=0, drop=True)\n        )\n        test_gnss[f\"{col}_roll_std\"] = (\n            test_gnss.groupby(\"phone\")[col]\n            .rolling(window, min_periods=1)\n            .std()\n            .reset_index(level=0, drop=True)\n        )\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    (\n        test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols]\n        if not test_gnss.empty\n        else sample_sub[[\"phone\", time_col]].assign(**{c: np.nan for c in feature_cols})\n    ),\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\n\n# fill feature columns from train medians where GNSS missing, then ffill/bfill within phone\ntrain_medians = train_df[feature_cols].median()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- load and build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- temporal rolling features (short context) ----------------- #\n\n# candidates from original GNSS aggregates plus baseline\nbase_for_rolling = [\n    c\n    for c in train_df.columns\n    if (\n        (\"WlsPosition\" in c and (\"_mean\" in c or \"_std\" in c))\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\n\nwindow = 3\nfor feat in base_for_rolling:\n    if feat in train_df.columns and pd.api.types.is_numeric_dtype(train_df[feat]):\n        grp = train_df.groupby(\"phone\")[feat]\n        train_df[f\"{feat}_roll_mean\"] = grp.transform(\n            lambda x: x.rolling(window, min_periods=1).mean()\n        )\n        train_df[f\"{feat}_roll_std\"] = grp.transform(\n            lambda x: x.rolling(window, min_periods=1).std().fillna(0)\n        )\n\n# ----------------- temporal diff features ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols and pd.api.types.is_numeric_dtype(train_df[c])\n]\n\nfor feat in base_feature_candidates:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# final feature list\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and pd.api.types.is_numeric_dtype(train_df[c])\n    and c not in [\"dlat\", \"dlon\"]\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build raw test GNSS aggregates\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Create rolling features on test using same base_for_rolling definition\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    test_base_for_rolling = [\n        c\n        for c in test_gnss.columns\n        if (\n            (\"WlsPosition\" in c and (\"_mean\" in c or \"_std\" in c))\n            or c.startswith(\"Cn0DbHz_\")\n            or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n            or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n            or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n        )\n    ]\n\n    for feat in test_base_for_rolling:\n        if pd.api.types.is_numeric_dtype(test_gnss[feat]):\n            grp = test_gnss.groupby(\"phone\")[feat]\n            test_gnss[f\"{feat}_roll_mean\"] = grp.transform(\n                lambda x: x.rolling(window, min_periods=1).mean()\n            )\n            test_gnss[f\"{feat}_roll_std\"] = grp.transform(\n                lambda x: x.rolling(window, min_periods=1).std().fillna(0)\n            )\n\n    # diff features for same base_feature_candidates' intersection\n    test_base_feature_candidates = [\n        c for c in test_gnss.columns if pd.api.types.is_numeric_dtype(test_gnss[c])\n    ]\n    for feat in test_base_feature_candidates:\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Ensure all train feature columns exist in test_gnss, no extras needed (we'll select only feature_cols)\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Merge with sample submission times\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + feature_cols\ntest_use = (\n    test_gnss[merge_cols] if not test_gnss.empty else pd.DataFrame(columns=merge_cols)\n)\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_use,\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test: from features if present, else global mean of train baseline\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"base_lat\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"base_lon\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate & metric utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    x = np.asarray(x)\n    y = np.asarray(y)\n    z = np.asarray(z)\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms and align to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering: base + diffs + rolling ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\n\n# Ensure deterministic ordering\nbase_feature_candidates = sorted(list(dict.fromkeys(base_feature_candidates)))\n\n# Temporal diffs\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\ndiff_features = [c for c in train_df.columns if c.endswith(\"_diff\")]\n\n# Rolling-window features (window=3) for key numeric features\nrolling_window = 3\nrolling_features = []\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype == \"O\":\n        continue\n    roll_mean_name = feat + \"_roll_mean\"\n    roll_std_name = feat + \"_roll_std\"\n    train_df[roll_mean_name] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=rolling_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    train_df[roll_std_name] = (\n        train_df.groupby(\"phone\")[feat]\n        .rolling(window=rolling_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n    rolling_features.extend([roll_mean_name, roll_std_name])\n\n# Final feature columns\nfeature_cols = base_feature_candidates + diff_features + rolling_features\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs with median\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build raw test GNSS aggregates (per epoch)\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Merge GNSS aggregates onto sample grid to get full timeline per phone\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss,\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# If there are duplicated UnixTimeMillis columns from merge, keep left one for ordering\nif (\n    \"UnixTimeMillis_x\" in test_merge.columns\n    and \"UnixTimeMillis_y\" in test_merge.columns\n):\n    test_merge.drop(columns=[\"UnixTimeMillis_y\"], inplace=True)\n    test_merge.rename(columns={\"UnixTimeMillis_x\": \"UnixTimeMillis\"}, inplace=True)\n\n# Make sure UnixTimeMillis is int64\ntest_merge[\"UnixTimeMillis\"] = test_merge[time_col].astype(\"int64\")\n\n# Sort for temporal features\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Ensure baseline lat/lon present (may be NaN initially)\nif \"base_lat\" not in test_merge.columns:\n    test_merge[\"base_lat\"] = np.nan\nif \"base_lon\" not in test_merge.columns:\n    test_merge[\"base_lon\"] = np.nan\n\n# Fill any direct GNSS features with NaNs first; we'll fill later after creating derived ones\n# Create base feature columns if missing so that rolling/diff can be applied consistently\nfor c in base_feature_candidates:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Temporal diffs on test for the same base_feature_candidates\nfor feat in base_feature_candidates:\n    if test_merge[feat].dtype == \"O\":\n        continue\n    diff_name = feat + \"_diff\"\n    test_merge[diff_name] = test_merge.groupby(\"phone\")[feat].diff()\n\n# Rolling-window features on test with the same window and features\nfor feat in base_feature_candidates:\n    if test_merge[feat].dtype == \"O\":\n        continue\n    roll_mean_name = feat + \"_roll_mean\"\n    roll_std_name = feat + \"_roll_std\"\n    test_merge[roll_mean_name] = (\n        test_merge.groupby(\"phone\")[feat]\n        .rolling(window=rolling_window, min_periods=1)\n        .mean()\n        .reset_index(level=0, drop=True)\n    )\n    test_merge[roll_std_name] = (\n        test_merge.groupby(\"phone\")[feat]\n        .rolling(window=rolling_window, min_periods=1)\n        .std()\n        .reset_index(level=0, drop=True)\n    )\n\n# Now ensure all feature_cols exist in test_merge; add missing as NaN\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Reorder and restrict to feature_cols\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\ntest_features = test_merge[feature_cols].copy()\n\n# Fill NaNs in test with train medians\nmedians = train_df[feature_cols].median()\ntest_features = test_features.fillna(medians)\n\nX_test = test_features.values\n\n# Baseline lat/lon for test: from base_lat/base_lon if still NaN -> global means\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n# Where all NaN (or many NaNs), fall back to global train means\nbase_lat_global = train_df[\"LatitudeDegrees\"].mean()\nbase_lon_global = train_df[\"LongitudeDegrees\"].mean()\nbase_lat_test = np.where(np.isnan(base_lat_test), base_lat_global, base_lat_test)\nbase_lon_test = np.where(np.isnan(base_lon_test), base_lon_global, base_lon_test)\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) * math.sin(th) * math.sin(th),\n        p - e * e * a * math.cos(th) * math.cos(th) * math.cos(th),\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) * math.sin(lat))\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate GNSS per epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering: base + temporal diffs + rolling stats ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\n# Start from numeric GNSS and baseline features\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\n\n# Temporal diffs for base features\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling statistics (window=3) on key features\nrolling_feats = base_feature_candidates.copy()\nwindow = 3\nfor feat in rolling_feats:\n    if train_df[feat].dtype == \"O\":\n        continue\n    grp = train_df.groupby(\"phone\")[feat]\n    train_df[feat + \"_roll_mean3\"] = (\n        grp.rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n    )\n    train_df[feat + \"_roll_std3\"] = (\n        grp.rolling(window, min_periods=1).std().reset_index(level=0, drop=True)\n    )\n\n# Final feature columns from train after all feature engineering\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and c not in [\"Svid\"]  # raw Svid not used\n    and c not in []  # placeholder for extra excludes\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs in train\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build raw test GNSS aggregates\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure basic columns exist\nif \"phone\" not in test_gnss.columns:\n    test_gnss[\"phone\"] = \"\"\n\n# Create base_feature_candidates in test (same names as train where possible)\n# First, ensure base and diff/rolling features exist so that we can later align to feature_cols\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        # we'll create temporal features after merging to submission grid\n        test_gnss[c] = np.nan\n\n# Sort for temporal operations\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Now align to submission time grid\nmerge_cols = [\"phone\", \"UnixTimeMillis\"]\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]].rename(columns={time_col: \"UnixTimeMillis\"}),\n    test_gnss[\n        [\"phone\", \"UnixTimeMillis\"]\n        + [c for c in test_gnss.columns if c not in [\"phone\", \"UnixTimeMillis\"]]\n    ],\n    on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# Recreate temporal diffs and rolling stats on the merged timeline for consistency\n# Start with ensuring all base_feature_candidates present\nbase_feature_candidates_test = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\nfor c in base_feature_candidates_test:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Temporal diffs\nfor feat in base_feature_candidates_test:\n    if test_merge[feat].dtype == \"O\":\n        continue\n    test_merge[feat + \"_diff\"] = test_merge.groupby(\"phone\")[feat].diff()\n\n# Rolling stats\nwindow = 3\nfor feat in base_feature_candidates_test:\n    if test_merge[feat].dtype == \"O\":\n        continue\n    grp = test_merge.groupby(\"phone\")[feat]\n    test_merge[feat + \"_roll_mean3\"] = (\n        grp.rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n    )\n    test_merge[feat + \"_roll_std3\"] = (\n        grp.rolling(window, min_periods=1).std().reset_index(level=0, drop=True)\n    )\n\n# At this point, many columns may exist; align exactly to feature_cols\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Fill temporal gaps: ffill/bfill within phone for numeric features, then global median\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) * math.sin(th) * math.sin(th),\n        p - e * e * a * math.cos(th) * math.cos(th) * math.cos(th),\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) * math.sin(lat))\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate GNSS per epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering: base + temporal diffs + rolling stats ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\n# Base feature candidates from GNSS numeric aggregates and baseline\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\n\n# Restrict base_feature_candidates to numeric columns only\nnumeric_cols_train = set(\n    train_df.select_dtypes(\n        include=[np.number, \"float64\", \"float32\", \"int64\", \"int32\"]\n    ).columns\n)\nbase_feature_candidates = [\n    c for c in base_feature_candidates if c in numeric_cols_train\n]\n\n# Temporal diffs for base features\nfor feat in base_feature_candidates:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Rolling statistics (window=3) on key features\nrolling_feats = base_feature_candidates.copy()\nwindow = 3\nfor feat in rolling_feats:\n    grp = train_df.groupby(\"phone\")[feat]\n    train_df[feat + \"_roll_mean3\"] = (\n        grp.rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n    )\n    train_df[feat + \"_roll_std3\"] = (\n        grp.rolling(window, min_periods=1).std().reset_index(level=0, drop=True)\n    )\n\n# Final feature columns: numeric only, excluding targets and ids\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and c not in [\"Svid\"]  # raw Svid not used\n    and pd.api.types.is_numeric_dtype(train_df[c])\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs in train with column medians (numeric_only=True)\ntrain_medians = train_df[feature_cols].median(numeric_only=True)\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_medians)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.9,\n        colsample_bytree=0.9,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build raw test GNSS aggregates\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure basic columns exist\nif \"phone\" not in test_gnss.columns:\n    test_gnss[\"phone\"] = \"\"\n\n# Sort for temporal operations\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Align to submission time grid\nmerge_cols = [\"phone\", \"UnixTimeMillis\"]\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]].rename(columns={time_col: \"UnixTimeMillis\"}),\n    test_gnss,\n    on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# Ensure all base_feature_candidates are present in test\nfor c in base_feature_candidates:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Temporal diffs on test\nfor feat in base_feature_candidates:\n    test_merge[feat + \"_diff\"] = test_merge.groupby(\"phone\")[feat].diff()\n\n# Rolling stats on test\nwindow = 3\nfor feat in base_feature_candidates:\n    grp = test_merge.groupby(\"phone\")[feat]\n    test_merge[feat + \"_roll_mean3\"] = (\n        grp.rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n    )\n    test_merge[feat + \"_roll_std3\"] = (\n        grp.rolling(window, min_periods=1).std().reset_index(level=0, drop=True)\n    )\n\n# At this point, many columns may exist; align exactly to feature_cols\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Fill temporal gaps: ffill/bfill within phone for numeric features, then global train medians\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# ----------------- paths ----------------- #\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef base_gnss_agg(gnss):\n    # drop rows without key info\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols).copy()\n    if gnss.empty:\n        return None\n\n    # approximate UnixTimeMillis from GPS epoch nanos\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [f\"{c[0]}_{c[1]}\" for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon from ECEF if available\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    return g_agg\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n\n    gt = pd.read_csv(gt_path)\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    if gt.empty:\n        return None\n\n    gnss = pd.read_csv(gnss_path)\n    g_agg = base_gnss_agg(gnss)\n    if g_agg is None or g_agg.empty:\n        return None\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # If base_lat/lon are zeros, fall back to GT mean\n    if df[\"base_lat\"].abs().sum() == 0:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n    if df[\"base_lon\"].abs().sum() == 0:\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    g_agg = base_gnss_agg(gnss)\n    if g_agg is None or g_agg.empty:\n        return None\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- load and build train ----------------- #\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# targets: residual vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\n\n# rolling features (window=3) for key numeric features\nrolling_window = 3\nrolling_cols = []\nfor feat in base_feature_candidates:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        grp = train_df.groupby(\"phone\")[feat]\n        roll_mean = (\n            grp.rolling(rolling_window, min_periods=1)\n            .mean()\n            .reset_index(level=0, drop=True)\n        )\n        roll_std = (\n            grp.rolling(rolling_window, min_periods=1)\n            .std()\n            .reset_index(level=0, drop=True)\n        )\n        mcol = f\"{feat}_roll{rolling_window}_mean\"\n        scol = f\"{feat}_roll{rolling_window}_std\"\n        train_df[mcol] = roll_mean\n        train_df[scol] = roll_std\n        rolling_cols.extend([mcol, scol])\n\n# temporal diffs for base + rolling features\ndiff_feature_candidates = base_feature_candidates + rolling_cols\nfor feat in diff_feature_candidates:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# final feature list: numeric only\ncandidate_cols = (\n    base_feature_candidates\n    + rolling_cols\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n)\nfeature_cols = [c for c in candidate_cols if pd.api.types.is_numeric_dtype(train_df[c])]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# fill NaNs with medians (numeric_only=True for robustness)\nmedians = train_df[feature_cols].median(numeric_only=True)\ntrain_df[feature_cols] = train_df[feature_cols].fillna(medians)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- 5-fold GroupKFold CV ----------------- #\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    pred_lat_res = model_lat.predict(X_val)\n    pred_lon_res = model_lon.predict(X_val)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + pred_lat_res\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + pred_lon_res\n\n# evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=80,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=80,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# identify id column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# unify phone identifier\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# load test gnss and aggregate\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    # at least provide empty structure\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# ensure base features exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# rolling features on test (same window)\nfor feat in base_feature_candidates:\n    if pd.api.types.is_numeric_dtype(test_gnss[feat]):\n        grp = test_gnss.groupby(\"phone\")[feat]\n        roll_mean = (\n            grp.rolling(rolling_window, min_periods=1)\n            .mean()\n            .reset_index(level=0, drop=True)\n        )\n        roll_std = (\n            grp.rolling(rolling_window, min_periods=1)\n            .std()\n            .reset_index(level=0, drop=True)\n        )\n        mcol = f\"{feat}_roll{rolling_window}_mean\"\n        scol = f\"{feat}_roll{rolling_window}_std\"\n        test_gnss[mcol] = roll_mean\n        test_gnss[scol] = roll_std\n        if mcol not in rolling_cols:\n            rolling_cols.append(mcol)\n        if scol not in rolling_cols:\n            rolling_cols.append(scol)\n\n# temporal diffs on test for same candidates\ndiff_feature_candidates_test = (\n    base_feature_candidates\n    + [c for c in test_gnss.columns if c.endswith(f\"_roll{rolling_window}_mean\")]\n    + [c for c in test_gnss.columns if c.endswith(f\"_roll{rolling_window}_std\")]\n)\n\nfor feat in diff_feature_candidates_test:\n    if feat in test_gnss.columns and pd.api.types.is_numeric_dtype(test_gnss[feat]):\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# ensure all feature_cols in test_gnss; add missing as NaN, drop extras later\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# merge with sample submission grid\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + list(test_gnss.columns)],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\n# restrict to feature_cols only\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\n\n# fill remaining NaNs with train medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(medians)\n\nX_test = test_merge[feature_cols].values\n\n# baseline lat/lon for test: from base_lat/base_lon if present, else global mean\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# build submission\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# ----------------- paths ----------------- #\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef base_gnss_agg(gnss):\n    # drop rows without key info\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols).copy()\n    if gnss.empty:\n        return None\n\n    # approximate UnixTimeMillis from GPS epoch nanos\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [f\"{c[0]}_{c[1]}\" for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon from ECEF if available\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    return g_agg\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n\n    gt = pd.read_csv(gt_path)\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    if gt.empty:\n        return None\n\n    gnss = pd.read_csv(gnss_path)\n    g_agg = base_gnss_agg(gnss)\n    if g_agg is None or g_agg.empty:\n        return None\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # If base_lat/lon are zeros, fall back to GT mean\n    if df[\"base_lat\"].abs().sum() == 0:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n    if df[\"base_lon\"].abs().sum() == 0:\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n    g_agg = base_gnss_agg(gnss)\n    if g_agg is None or g_agg.empty:\n        return None\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- load and build train ----------------- #\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# targets: residual vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering ----------------- #\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\n\n# rolling features (window=3) for key numeric features\nrolling_window = 3\nrolling_cols = []\nfor feat in base_feature_candidates:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        grp = train_df.groupby(\"phone\")[feat]\n        roll_mean = (\n            grp.rolling(rolling_window, min_periods=1)\n            .mean()\n            .reset_index(level=0, drop=True)\n        )\n        roll_std = (\n            grp.rolling(rolling_window, min_periods=1)\n            .std()\n            .reset_index(level=0, drop=True)\n        )\n        mcol = f\"{feat}_roll{rolling_window}_mean\"\n        scol = f\"{feat}_roll{rolling_window}_std\"\n        train_df[mcol] = roll_mean\n        train_df[scol] = roll_std\n        rolling_cols.extend([mcol, scol])\n\n# temporal diffs for base + rolling features\ndiff_feature_candidates = base_feature_candidates + rolling_cols\nfor feat in diff_feature_candidates:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# final feature list: numeric only\ncandidate_cols = (\n    base_feature_candidates\n    + rolling_cols\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n)\nfeature_cols = [c for c in candidate_cols if pd.api.types.is_numeric_dtype(train_df[c])]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# fill NaNs with medians (numeric_only=True for robustness)\nmedians = train_df[feature_cols].median(numeric_only=True)\ntrain_df[feature_cols] = train_df[feature_cols].fillna(medians)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- 5-fold GroupKFold CV ----------------- #\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    pred_lat_res = model_lat.predict(X_val)\n    pred_lon_res = model_lon.predict(X_val)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + pred_lat_res\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + pred_lon_res\n\n# evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=80,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.04,\n    max_depth=-1,\n    num_leaves=80,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# identify id column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# unify phone identifier (this becomes the same naming convention as in train/test_gnss)\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# load test gnss and aggregate\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# ensure base features exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# rolling features on test (same window)\nfor feat in base_feature_candidates:\n    if pd.api.types.is_numeric_dtype(test_gnss[feat]):\n        grp = test_gnss.groupby(\"phone\")[feat]\n        roll_mean = (\n            grp.rolling(rolling_window, min_periods=1)\n            .mean()\n            .reset_index(level=0, drop=True)\n        )\n        roll_std = (\n            grp.rolling(rolling_window, min_periods=1)\n            .std()\n            .reset_index(level=0, drop=True)\n        )\n        mcol = f\"{feat}_roll{rolling_window}_mean\"\n        scol = f\"{feat}_roll{rolling_window}_std\"\n        test_gnss[mcol] = roll_mean\n        test_gnss[scol] = roll_std\n\n# temporal diffs on test for same candidates\ndiff_feature_candidates_test = (\n    base_feature_candidates\n    + [c for c in test_gnss.columns if c.endswith(f\"_roll{rolling_window}_mean\")]\n    + [c for c in test_gnss.columns if c.endswith(f\"_roll{rolling_window}_std\")]\n)\n\nfor feat in diff_feature_candidates_test:\n    if feat in test_gnss.columns and pd.api.types.is_numeric_dtype(test_gnss[feat]):\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# ensure all feature_cols in test_gnss; add missing as NaN\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# construct right table for merge with unique columns:\n# keep only keys (phone, UnixTimeMillis) and feature_cols (+ base_lat/base_lon if not already)\ncols_for_merge = [\"phone\", \"UnixTimeMillis\"]\nfor c in feature_cols:\n    if c not in cols_for_merge:\n        cols_for_merge.append(c)\nif \"base_lat\" not in feature_cols:\n    if \"base_lat\" in test_gnss.columns and \"base_lat\" not in cols_for_merge:\n        cols_for_merge.append(\"base_lat\")\nif \"base_lon\" not in feature_cols:\n    if \"base_lon\" in test_gnss.columns and \"base_lon\" not in cols_for_merge:\n        cols_for_merge.append(\"base_lon\")\n\ntest_gnss_merged = test_gnss[cols_for_merge].copy()\n\n# merge with sample submission grid\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss_merged,\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\n\n# ensure all feature_cols exist in test_merge\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\n\n# fill remaining NaNs with train medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(medians)\n\nX_test = test_merge[feature_cols].values\n\n# baseline lat/lon for test: from base_lat/base_lon if present, else global mean of training GT\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# build submission\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # derive baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering: base + diffs + rolling ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\n# Base numeric candidates\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\n\n# ensure numeric only\nbase_feature_candidates = [\n    c for c in base_feature_candidates if pd.api.types.is_numeric_dtype(train_df[c])\n]\n\n# Temporal diffs\nfor feat in base_feature_candidates:\n    train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Short rolling-window stats (window=3) within each phone\nroll_window = 3\nrolling_feats = []\nfor feat in base_feature_candidates:\n    grp = train_df.groupby(\"phone\")[feat]\n    mcol = f\"{feat}_rollmean{roll_window}\"\n    scol = f\"{feat}_rollstd{roll_window}\"\n    train_df[mcol] = grp.transform(\n        lambda x: x.rolling(roll_window, min_periods=1).mean()\n    )\n    train_df[scol] = grp.transform(\n        lambda x: x.rolling(roll_window, min_periods=1).std()\n    )\n    rolling_feats.extend([mcol, scol])\n\n# Final feature list from train\nfeature_cols = (\n    base_feature_candidates\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n    + rolling_feats\n)\n\n# De-duplicate while preserving order\nseen = set()\nfeat_list = []\nfor c in feature_cols:\n    if c not in seen:\n        seen.add(c)\n        feat_list.append(c)\nfeature_cols = feat_list\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs using numeric-only median\ntrain_medians = train_df[feature_cols].median(numeric_only=True)\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_medians)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal use\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build raw test GNSS aggregates\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Ensure all base features exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort by phone/time\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    # Temporal diffs on test\n    for feat in base_feature_candidates:\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n    # Rolling features on test\n    for feat in base_feature_candidates:\n        grp = test_gnss.groupby(\"phone\")[feat]\n        mcol = f\"{feat}_rollmean{roll_window}\"\n        scol = f\"{feat}_rollstd{roll_window}\"\n        test_gnss[mcol] = grp.transform(\n            lambda x: x.rolling(roll_window, min_periods=1).mean()\n        )\n        test_gnss[scol] = grp.transform(\n            lambda x: x.rolling(roll_window, min_periods=1).std()\n        )\nelse:\n    # create empty diff/rolling columns if no gnss\n    for feat in base_feature_candidates:\n        test_gnss[feat + \"_diff\"] = np.nan\n        mcol = f\"{feat}_rollmean{roll_window}\"\n        scol = f\"{feat}_rollstd{roll_window}\"\n        test_gnss[mcol] = np.nan\n        test_gnss[scol] = np.nan\n\n# Merge GNSS features to sample submission grid\nmerge_cols = [\"phone\", \"UnixTimeMillis\"] + [\n    c for c in test_gnss.columns if c not in [\"phone\", \"UnixTimeMillis\"]\n]\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[merge_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# Now ensure all feature_cols exist in merged test\nfor c in feature_cols:\n    if c not in test_merge.columns:\n        test_merge[c] = np.nan\n\n# Sort and within-phone fill\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\n\n# Fill remaining NaNs with train medians\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = (\n        test_merge[\"base_lat\"].fillna(train_df[\"LatitudeDegrees\"].mean()).values\n    )\n    base_lon_test = (\n        test_merge[\"base_lon\"].fillna(train_df[\"LongitudeDegrees\"].mean()).values\n    )\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    return math.degrees(lat), math.degrees(lon), h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [f\"{c[0]}_{c[1]}\" for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [f\"{c[0]}_{c[1]}\" for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- base feature selection ----------------- #\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\", \"base_lat\", \"base_lon\"]\n    )\n]\n\n# Ensure numeric-only\nbase_feature_candidates = [\n    c for c in base_feature_candidates if pd.api.types.is_numeric_dtype(train_df[c])\n]\n\n# ----------------- add rolling features (window=3) ----------------- #\nwindow = 3\nroll_features = []\nfor feat in base_feature_candidates:\n    g = train_df.groupby(\"phone\")[feat]\n    rmean = g.rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n    rstd = g.rolling(window, min_periods=1).std().reset_index(level=0, drop=True)\n    mean_col = f\"{feat}_rollmean{window}\"\n    std_col = f\"{feat}_rollstd{window}\"\n    train_df[mean_col] = rmean\n    train_df[std_col] = rstd\n    roll_features.extend([mean_col, std_col])\n\n# Temporal diffs for key numeric features (original + rolling)\ntemp_feats_for_diff = base_feature_candidates + roll_features\nfor feat in temp_feats_for_diff:\n    train_df[f\"{feat}_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\ndiff_features = [f\"{feat}_diff\" for feat in temp_feats_for_diff]\n\n# Final feature columns\nfeature_cols = base_feature_candidates + roll_features + diff_features\n\n# Restrict to numeric and drop any fully-NaN columns\nfeature_cols = [\n    c\n    for c in feature_cols\n    if c in train_df.columns and pd.api.types.is_numeric_dtype(train_df[c])\n]\nnon_all_nan = [c for c in feature_cols if not train_df[c].isna().all()]\nfeature_cols = non_all_nan\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs with column medians\ntrain_df[feature_cols] = train_df[feature_cols].fillna(\n    train_df[feature_cols].median(numeric_only=True)\n)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build raw test GNSS aggregates (no temporal features yet)\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Keep only needed base features plus phone/time\nneeded_base = list(\n    {\n        \"phone\",\n        \"UnixTimeMillis\",\n        \"base_lat\",\n        \"base_lon\",\n        *base_feature_candidates,\n    }\n)\nfor c in needed_base:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\ntest_gnss = test_gnss[needed_base]\ntest_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Add rolling features on test in the same way\nfor feat in base_feature_candidates:\n    g = test_gnss.groupby(\"phone\")[feat]\n    rmean = g.rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n    rstd = g.rolling(window, min_periods=1).std().reset_index(level=0, drop=True)\n    mean_col = f\"{feat}_rollmean{window}\"\n    std_col = f\"{feat}_rollstd{window}\"\n    test_gnss[mean_col] = rmean\n    test_gnss[std_col] = rstd\n\n# Temporal diffs on test\nfor feat in (\n    base_feature_candidates\n    + [f\"{f}_rollmean{window}\" for f in base_feature_candidates]\n    + [f\"{f}_rollstd{window}\" for f in base_feature_candidates]\n):\n    diff_col = f\"{feat}_diff\"\n    test_gnss[diff_col] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Now align test_gnss to sample_sub times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss,\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# Sort and fill temporally within phone\nif not test_merge.empty:\n    test_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\n    # Ensure all feature columns exist\n    for c in feature_cols:\n        if c not in test_merge.columns:\n            test_merge[c] = np.nan\n\n    # Forward/backward fill then global median\n    test_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\n    test_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\n    medians = train_df[feature_cols].median(numeric_only=True)\n    test_merge[feature_cols] = test_merge[feature_cols].fillna(medians)\nelse:\n    for c in feature_cols:\n        test_merge[c] = medians[c] if c in medians else 0.0\n\n# Final feature matrix for test\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- rolling features on train ----------------- #\n\n# base feature candidates (before diffs)\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# rolling window stats for key features\nrolling_feats = base_feature_candidates.copy()\nwindow = 3\nfor feat in rolling_feats:\n    if train_df[feat].dtype == \"O\":\n        continue\n    grp = train_df.groupby(\"phone\")[feat]\n    train_df[f\"{feat}_roll_mean\"] = grp.transform(\n        lambda x: x.rolling(window, min_periods=1).mean()\n    )\n    train_df[f\"{feat}_roll_std\"] = grp.transform(\n        lambda x: x.rolling(window, min_periods=1).std()\n    )\n\n# recompute feature candidates to include rolling\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c.startswith(\"Svid_nunique\")\n        or c.startswith(\"base_\")\n    )\n    or c.endswith(\"_roll_mean\")\n    or c.endswith(\"_roll_std\")\n]\n\n# Temporal diffs\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\nfeature_cols = base_feature_candidates + [\n    c for c in train_df.columns if c.endswith(\"_diff\")\n]\n\n# Ensure only numeric features\nfeature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(train_df[c])]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs with median (numeric_only)\ntrain_df[feature_cols] = train_df[feature_cols].fillna(\n    train_df[feature_cols].median(numeric_only=True)\n)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal grouping\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features per phone\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# rolling features on test (same set as train)\nif not test_gnss.empty:\n    # ensure phones are str and sorted\n    test_gnss[\"phone\"] = test_gnss[\"phone\"].astype(str)\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    # create missing base columns used in rolling if absent\n    # base_feature_candidates from train\n    for feat in base_feature_candidates:\n        if feat not in test_gnss.columns:\n            test_gnss[feat] = np.nan\n\n    # rolling stats\n    for feat in rolling_feats:\n        if feat not in test_gnss.columns:\n            continue\n        if test_gnss[feat].dtype == \"O\":\n            continue\n        grp = test_gnss.groupby(\"phone\")[feat]\n        test_gnss[f\"{feat}_roll_mean\"] = grp.transform(\n            lambda x: x.rolling(window, min_periods=1).mean()\n        )\n        test_gnss[f\"{feat}_roll_std\"] = grp.transform(\n            lambda x: x.rolling(window, min_periods=1).std()\n        )\n\n    # diffs\n    for feat in base_feature_candidates:\n        if feat not in test_gnss.columns:\n            continue\n        if test_gnss[feat].dtype != \"O\":\n            test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort and median-fill with train medians\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\ntrain_medians = train_df[feature_cols].median(numeric_only=True)\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(train_medians)\n\n# Merge with sample submission grid\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort and forward/backward fill within phone\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n\n# baseline lat/lon for test\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    test_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].ffill()\n    test_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].bfill()\n    test_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].ffill()\n    test_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].bfill()\n    # if still NaN, use global mean\n    global_lat_mean = train_df[\"LatitudeDegrees\"].mean()\n    global_lon_mean = train_df[\"LongitudeDegrees\"].mean()\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(global_lat_mean)\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(global_lon_mean)\nelse:\n    global_lat_mean = train_df[\"LatitudeDegrees\"].mean()\n    global_lon_mean = train_df[\"LongitudeDegrees\"].mean()\n    test_merge[\"base_lat\"] = global_lat_mean\n    test_merge[\"base_lon\"] = global_lon_mean\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- rolling features on train ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\nrolling_feats = base_feature_candidates.copy()\nwindow = 3\nfor feat in rolling_feats:\n    if train_df[feat].dtype == \"O\":\n        continue\n    grp = train_df.groupby(\"phone\")[feat]\n    train_df[f\"{feat}_roll_mean\"] = grp.transform(\n        lambda x: x.rolling(window, min_periods=1).mean()\n    )\n    train_df[f\"{feat}_roll_std\"] = grp.transform(\n        lambda x: x.rolling(window, min_periods=1).std()\n    )\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c.startswith(\"Svid_nunique\")\n        or c.startswith(\"base_\")\n    )\n    or c.endswith(\"_roll_mean\")\n    or c.endswith(\"_roll_std\")\n]\n\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\nfeature_cols = base_feature_candidates + [\n    c for c in train_df.columns if c.endswith(\"_diff\")\n]\nfeature_cols = [c for c in feature_cols if pd.api.types.is_numeric_dtype(train_df[c])]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\ntrain_df[feature_cols] = train_df[feature_cols].fillna(\n    train_df[feature_cols].median(numeric_only=True)\n)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal grouping\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features per phone\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# rolling features on test (same set as train)\nif not test_gnss.empty:\n    test_gnss[\"phone\"] = test_gnss[\"phone\"].astype(str)\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n    # ensure all base features exist\n    for feat in base_feature_candidates:\n        if feat not in test_gnss.columns:\n            test_gnss[feat] = np.nan\n\n    # rolling stats\n    for feat in rolling_feats:\n        if feat not in test_gnss.columns:\n            continue\n        if test_gnss[feat].dtype == \"O\":\n            continue\n        grp = test_gnss.groupby(\"phone\")[feat]\n        test_gnss[f\"{feat}_roll_mean\"] = grp.transform(\n            lambda x: x.rolling(window, min_periods=1).mean()\n        )\n        test_gnss[f\"{feat}_roll_std\"] = grp.transform(\n            lambda x: x.rolling(window, min_periods=1).std()\n        )\n\n    # diffs\n    for feat in base_feature_candidates:\n        if feat not in test_gnss.columns:\n            continue\n        if test_gnss[feat].dtype != \"O\":\n            test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort and median-fill with train medians\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\ntrain_medians = train_df[feature_cols].median(numeric_only=True)\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(train_medians)\n\n# Merge with sample submission grid\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# forward/backward fill feature columns safely per column to avoid shape mismatch\nfor col in feature_cols:\n    ser = test_merge[col]\n    ser = ser.groupby(test_merge[\"phone\"]).ffill()\n    ser = ser.groupby(test_merge[\"phone\"]).bfill()\n    # fill any remaining NaNs with train median for that feature\n    median_val = train_medians.get(col, np.nan)\n    if not np.isnan(median_val):\n        ser = ser.fillna(median_val)\n    test_merge[col] = ser\n\n# baseline lat/lon for test\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\n\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    test_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].ffill()\n    test_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].bfill()\n    test_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].ffill()\n    test_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].bfill()\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(global_lat_mean)\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(global_lon_mean)\nelse:\n    test_merge[\"base_lat\"] = global_lat_mean\n    test_merge[\"base_lon\"] = global_lon_mean\n\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering on train ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\n# base numeric features: aggregates, Svid_nunique, base_lat/lon\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\nrolling_window = 3\nrolling_feats = base_feature_candidates.copy()\n\n# rolling stats on train\nfor feat in rolling_feats:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        grp = train_df.groupby(\"phone\")[feat]\n        train_df[f\"{feat}_roll_mean\"] = grp.transform(\n            lambda x: x.rolling(rolling_window, min_periods=1).mean()\n        )\n        train_df[f\"{feat}_roll_std\"] = grp.transform(\n            lambda x: x.rolling(rolling_window, min_periods=1).std()\n        )\n\n# update base_feature_candidates to include roll_* explicitly plus base/basic\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c.startswith(\"Svid_nunique\")\n        or c.startswith(\"base_\")\n        or c.endswith(\"_roll_mean\")\n        or c.endswith(\"_roll_std\")\n    )\n]\n\n# diff features on train for numeric base features\ndiff_features = []\nfor feat in base_feature_candidates:\n    if pd.api.types.is_numeric_dtype(train_df[feat]):\n        diff_col = feat + \"_diff\"\n        train_df[diff_col] = train_df.groupby(\"phone\")[feat].diff()\n        diff_features.append(diff_col)\n\n# final feature list: numeric only and fixed order\nfeature_cols = [\n    c\n    for c in base_feature_candidates + diff_features\n    if pd.api.types.is_numeric_dtype(train_df[c])\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# fill NaNs with medians\ntrain_df[feature_cols] = train_df[feature_cols].fillna(\n    train_df[feature_cols].median(numeric_only=True)\n)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal grouping\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features per phone\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\nif not test_gnss.empty:\n    test_gnss[\"phone\"] = test_gnss[\"phone\"].astype(str)\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Ensure all base features that exist in train are present in test before rolling/diff\nfor feat in base_feature_candidates:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n\n# rolling stats on test for the same rolling_feats\nfor feat in rolling_feats:\n    if feat not in test_gnss.columns:\n        continue\n    if pd.api.types.is_numeric_dtype(test_gnss[feat]):\n        grp = test_gnss.groupby(\"phone\")[feat]\n        test_gnss[f\"{feat}_roll_mean\"] = grp.transform(\n            lambda x: x.rolling(rolling_window, min_periods=1).mean()\n        )\n        test_gnss[f\"{feat}_roll_std\"] = grp.transform(\n            lambda x: x.rolling(rolling_window, min_periods=1).std()\n        )\n\n# diff features on test matching train diff_features list\nfor diff_col in diff_features:\n    base_feat = diff_col.replace(\"_diff\", \"\")\n    if base_feat in test_gnss.columns and pd.api.types.is_numeric_dtype(\n        test_gnss[base_feat]\n    ):\n        test_gnss[diff_col] = test_gnss.groupby(\"phone\")[base_feat].diff()\n    else:\n        test_gnss[diff_col] = np.nan\n\n# Ensure all feature columns exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Sort and median-fill with train medians\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\ntrain_medians = train_df[feature_cols].median(numeric_only=True)\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(train_medians)\n\n# Merge with sample submission grid to align times and phones\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# forward/backward fill feature columns safely per column to avoid shape mismatch\nfor col in feature_cols:\n    ser = test_merge[col]\n    ser = ser.groupby(test_merge[\"phone\"]).ffill()\n    ser = ser.groupby(test_merge[\"phone\"]).bfill()\n    median_val = train_medians.get(col, np.nan)\n    if not np.isnan(median_val):\n        ser = ser.fillna(median_val)\n    test_merge[col] = ser\n\n# baseline lat/lon for test\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\n\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    test_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].ffill()\n    test_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].bfill()\n    test_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].ffill()\n    test_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].bfill()\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(global_lat_mean)\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(global_lon_mean)\nelse:\n    test_merge[\"base_lat\"] = global_lat_mean\n    test_merge[\"base_lon\"] = global_lon_mean\n\n# Final test matrix with exactly the same feature order\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- feature engineering on train (simplified, no rolling/diff) ----------------- #\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\n# numeric aggregate features plus base_lat/lon and Svid_nunique\nfeature_cols = []\nfor c in train_df.columns:\n    if c in exclude_cols:\n        continue\n    if (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c.startswith(\"Svid_nunique\")\n        or c in [\"base_lat\", \"base_lon\"]\n    ):\n        if pd.api.types.is_numeric_dtype(train_df[c]):\n            feature_cols.append(c)\n\n# Ensure fixed order and uniqueness\nfeature_cols = sorted(list(dict.fromkeys(feature_cols)))\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# fill NaNs with medians\ntrain_df[feature_cols] = train_df[feature_cols].fillna(\n    train_df[feature_cols].median(numeric_only=True)\n)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal grouping\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features per phone\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    # empty fallback with required columns\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\nif not test_gnss.empty:\n    test_gnss[\"phone\"] = test_gnss[\"phone\"].astype(str)\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Ensure all feature columns exist in test_gnss; if missing, create as NaN\nfor feat in feature_cols:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n\n# Sort and median-fill with train medians\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\ntrain_medians = train_df[feature_cols].median(numeric_only=True)\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(train_medians)\n\n# Merge with sample submission grid to align times and phones\ncols_to_keep = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\nfor c in [\"base_lat\", \"base_lon\"]:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[cols_to_keep],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# fill feature columns via forward/backward fill per phone, then global median\nfor col in feature_cols:\n    ser = test_merge[col]\n    ser = ser.groupby(test_merge[\"phone\"]).ffill()\n    ser = ser.groupby(test_merge[\"phone\"]).bfill()\n    median_val = train_medians.get(col, np.nan)\n    if not np.isnan(median_val):\n        ser = ser.fillna(median_val)\n    test_merge[col] = ser\n\n# baseline lat/lon for test\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\n\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    test_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].ffill()\n    test_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].bfill()\n    test_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].ffill()\n    test_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].bfill()\n    test_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(global_lat_mean)\n    test_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(global_lon_mean)\nelse:\n    test_merge[\"base_lat\"] = global_lat_mean\n    test_merge[\"base_lon\"] = global_lon_mean\n\n# Final test matrix with exactly the same feature order\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    # Aggregate ECEF by epoch\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from mean ECEF\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- define a strict, stable feature set ----------------- #\n\n# Start from known aggregated GNSS feature patterns plus base_lat/base_lon\ncandidate_features = []\nfor c in train_df.columns:\n    if (\n        c.startswith(\"WlsPositionX\")\n        or c.startswith(\"WlsPositionY\")\n        or c.startswith(\"WlsPositionZ\")\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c == \"Svid_nunique\"\n        or c in [\"base_lat\", \"base_lon\"]\n    ):\n        if pd.api.types.is_numeric_dtype(train_df[c]):\n            candidate_features.append(c)\n\n# Remove duplicates, sort for deterministic order\nfeature_cols = sorted(list(dict.fromkeys(candidate_features)))\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# fill NaNs with medians\ntrain_df[feature_cols] = train_df[feature_cols].fillna(\n    train_df[feature_cols].median(numeric_only=True)\n)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal grouping\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features per phone\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    # empty fallback with required columns\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\nif not test_gnss.empty:\n    test_gnss[\"phone\"] = test_gnss[\"phone\"].astype(str)\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Ensure all feature columns exist in test_gnss; if missing, create as NaN\nfor feat in feature_cols:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n\n# Sort and median-fill with train medians\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\ntrain_medians = train_df[feature_cols].median(numeric_only=True)\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(train_medians)\n\n# Make sure baseline columns exist\nfor c in [\"base_lat\", \"base_lon\"]:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Merge with sample submission grid to align times and phones\ncols_to_keep = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[cols_to_keep],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# fill feature columns via forward/backward fill per phone, then global median\nfor col in feature_cols:\n    ser = test_merge[col]\n    ser = ser.groupby(test_merge[\"phone\"]).ffill()\n    ser = ser.groupby(test_merge[\"phone\"]).bfill()\n    median_val = train_medians.get(col, np.nan)\n    if not np.isnan(median_val):\n        ser = ser.fillna(median_val)\n    test_merge[col] = ser\n\n# baseline lat/lon for test\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\n\ntest_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].ffill()\ntest_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].bfill()\ntest_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].ffill()\ntest_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].bfill()\ntest_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(global_lat_mean)\ntest_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(global_lon_mean)\n\n# Final test matrix with exactly the same feature order\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\n# Sanity check on feature dimension\nif X_test.shape[1] != len(feature_cols):\n    raise RuntimeError(\n        f\"Feature dimension mismatch: X_test has {X_test.shape[1]} columns, expected {len(feature_cols)}\"\n    )\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\n# We will aggregate the same numeric columns in train and test\nAGG_BASE_COLS = [\n    \"WlsPositionXEcefMeters\",\n    \"WlsPositionYEcefMeters\",\n    \"WlsPositionZEcefMeters\",\n    \"Cn0DbHz\",\n    \"PseudorangeRateMetersPerSecond\",\n    \"PseudorangeRateUncertaintyMetersPerSecond\",\n]\n\n# For each present base column we compute mean and std per UnixTimeMillis\nAGG_FUNCS = [\"mean\", \"std\"]\n\n\ndef build_agg(gnss: pd.DataFrame) -> pd.DataFrame:\n    present = [c for c in AGG_BASE_COLS if c in gnss.columns]\n    agg_dict = {c: AGG_FUNCS for c in present}\n    # satellite count\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    # flatten multiindex\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n    return g_agg\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    # Convert GPS epoch nanos to ms, estimate offset to UnixTimeMillis\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    g_agg = build_agg(gnss)\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    # baseline lat/lon from mean ECEF if available\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    g_agg = build_agg(gnss)\n\n    # baseline lat/lon\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        # placeholder; will be overwritten by global means later\n        g_agg[\"base_lat\"] = np.nan\n        g_agg[\"base_lon\"] = np.nan\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# ----------------- define a FIXED feature set ----------------- #\n\n# Build feature list deterministically from AGG_BASE_COLS and AGG_FUNCS\nfeature_cols = []\nfor base in AGG_BASE_COLS:\n    for fn in AGG_FUNCS:\n        col = f\"{base}_{fn}\"\n        feature_cols.append(col)\n# satellite count\nfeature_cols.append(\"Svid_nunique\")\n# baseline positions as features\nfeature_cols.extend([\"base_lat\", \"base_lon\"])\n\n# Keep only those that actually exist in train_df, but maintain order\nfeature_cols = [c for c in feature_cols if c in train_df.columns]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# fill NaNs with medians\ntrain_df[feature_cols] = train_df[feature_cols].fillna(\n    train_df[feature_cols].median(numeric_only=True)\n)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\n# Unified phone column for internal grouping\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features per phone\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    # empty fallback with required columns\n    test_gnss = pd.DataFrame(\n        columns=[\"UnixTimeMillis\", \"phone\", \"base_lat\", \"base_lon\"]\n    )\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\nif not test_gnss.empty:\n    test_gnss[\"phone\"] = test_gnss[\"phone\"].astype(str)\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\n# Ensure all feature columns exist in test_gnss; if missing, create as NaN\nfor feat in feature_cols:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n\n# Sort and median-fill with train medians\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n\ntrain_medians = train_df[feature_cols].median(numeric_only=True)\ntest_gnss[feature_cols] = test_gnss[feature_cols].fillna(train_medians)\n\n# Make sure baseline columns exist\nfor c in [\"base_lat\", \"base_lon\"]:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Merge with sample submission grid to align times and phones\ncols_to_keep = [\"phone\", \"UnixTimeMillis\"] + feature_cols + [\"base_lat\", \"base_lon\"]\ncols_to_keep = list(dict.fromkeys(cols_to_keep))  # deduplicate\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[cols_to_keep],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\n# sort\ntest_merge = test_merge.sort_values([\"phone\", time_col]).reset_index(drop=True)\n\n# fill feature columns via forward/backward fill per phone, then global median\nfor col in feature_cols:\n    ser = test_merge[col]\n    ser = ser.groupby(test_merge[\"phone\"]).ffill()\n    ser = ser.groupby(test_merge[\"phone\"]).bfill()\n    median_val = train_medians.get(col, np.nan)\n    if not np.isnan(median_val):\n        ser = ser.fillna(median_val)\n    test_merge[col] = ser\n\n# baseline lat/lon for test\nglobal_lat_mean = train_df[\"LatitudeDegrees\"].mean()\nglobal_lon_mean = train_df[\"LongitudeDegrees\"].mean()\n\ntest_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].ffill()\ntest_merge[\"base_lat\"] = test_merge.groupby(\"phone\")[\"base_lat\"].bfill()\ntest_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].ffill()\ntest_merge[\"base_lon\"] = test_merge.groupby(\"phone\")[\"base_lon\"].bfill()\ntest_merge[\"base_lat\"] = test_merge[\"base_lat\"].fillna(global_lat_mean)\ntest_merge[\"base_lon\"] = test_merge[\"base_lon\"].fillna(global_lon_mean)\n\n# Final test matrix with exactly the same feature order\nX_test = test_merge[feature_cols].values\nbase_lat_test = test_merge[\"base_lat\"].values\nbase_lon_test = test_merge[\"base_lon\"].values\n\n# Sanity check on feature dimension\nif X_test.shape[1] != len(feature_cols):\n    raise RuntimeError(\n        f\"Feature dimension mismatch: X_test has {X_test.shape[1]} columns, expected {len(feature_cols)}\"\n    )\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    \"\"\"Convert ECEF (meters) to WGS84 lat, lon (degrees), h (meters).\"\"\"\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\n# Targets and residuals vs baseline\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\n# Base feature candidates (per-epoch)\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\n# Add temporal diffs\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# Add rolling-window statistics (window=3) for temporal context\nwindow = 3\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        grp = train_df.groupby(\"phone\")[feat]\n        train_df[feat + \"_roll_mean\"] = (\n            grp.rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n        )\n        train_df[feat + \"_roll_std\"] = (\n            grp.rolling(window, min_periods=1).std().reset_index(level=0, drop=True)\n        )\n\n# Final feature columns\nfeature_cols = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and c\n    not in [\n        \"LatitudeDegrees\",\n        \"LongitudeDegrees\",\n        \"tgt_lat\",\n        \"tgt_lon\",\n        \"dlat\",\n        \"dlon\",\n    ]\n    and train_df[c].dtype != \"O\"\n]\n\nif not feature_cols:\n    raise RuntimeError(\"No feature columns constructed for training\")\n\n# Fill NaNs with medians\ntrain_df[feature_cols] = train_df[feature_cols].fillna(\n    train_df[feature_cols].median(numeric_only=True)\n)\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\n# ----------------- cross-validation ----------------- #\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=250,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\n# Evaluate CV metric\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\n# ----------------- train final models on full data ----------------- #\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\n# ----------------- prepare test features aligned with sample submission ----------------- #\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Identifier column\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\n# Build test GNSS features\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# Recreate same temporal features for test: start with base per-epoch features\n# Ensure all base_feature_candidates exist\nfor c in base_feature_candidates:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\n# Temporal diffs\nfor feat in base_feature_candidates:\n    if feat in test_gnss.columns and test_gnss[feat].dtype != \"O\":\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\n# Rolling stats\nfor feat in base_feature_candidates:\n    if feat in test_gnss.columns and test_gnss[feat].dtype != \"O\":\n        grp = test_gnss.groupby(\"phone\")[feat]\n        test_gnss[feat + \"_roll_mean\"] = (\n            grp.rolling(window, min_periods=1).mean().reset_index(level=0, drop=True)\n        )\n        test_gnss[feat + \"_roll_std\"] = (\n            grp.rolling(window, min_periods=1).std().reset_index(level=0, drop=True)\n        )\n\n# Ensure all feature_cols exist in test_gnss\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    # Fill with train medians\n    med = train_df[feature_cols].median(numeric_only=True)\n    test_gnss[feature_cols] = test_gnss[feature_cols].fillna(med)\n\n# Merge with sample submission times\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    (\n        test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols]\n        if not test_gnss.empty\n        else sample_sub[[\"phone\", time_col]].assign(**{c: np.nan for c in feature_cols})\n    ),\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\n\n# Forward/backward fill within phone, then global median\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median(numeric_only=True)\n)\n\nX_test = test_merge[feature_cols].values\n\n# Baseline lat/lon for test: from features if present, else global means\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\n# Build submission with original identifier column name\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n", "import os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom lightgbm import LGBMRegressor\n\n# Paths\nINPUT_DIR = \"./input\"\nTRAIN_DIR = os.path.join(INPUT_DIR, \"train\")\nTEST_DIR = os.path.join(INPUT_DIR, \"test\")\nSUBMISSION_DIR = \"./submission\"\nWORKING_DIR = \"./working\"\nos.makedirs(SUBMISSION_DIR, exist_ok=True)\nos.makedirs(WORKING_DIR, exist_ok=True)\n\n# ----------------- coordinate utilities ----------------- #\n\nEARTH_RADIUS = 6378137.0\nECCENTRICITY = 8.1819190842622e-2  # WGS-84 first eccentricity\n\n\ndef ecef_to_geodetic(x, y, z):\n    a = EARTH_RADIUS\n    e = ECCENTRICITY\n    b = math.sqrt(a * a * (1 - e * e))\n    ep = math.sqrt((a * a - b * b) / (b * b))\n    p = math.sqrt(x * x + y * y)\n    th = math.atan2(a * z, b * p)\n    lon = math.atan2(y, x)\n    lat = math.atan2(\n        z + ep * ep * b * math.sin(th) ** 3,\n        p - e * e * a * math.cos(th) ** 3,\n    )\n    N = a / math.sqrt(1 - e * e * math.sin(lat) ** 2)\n    h = p / math.cos(lat) - N\n    lat_deg = math.degrees(lat)\n    lon_deg = math.degrees(lon)\n    return lat_deg, lon_deg, h\n\n\ndef ecef_to_geodetic_vec(x, y, z):\n    lat = np.empty_like(x, dtype=float)\n    lon = np.empty_like(x, dtype=float)\n    h = np.empty_like(x, dtype=float)\n    for i in range(len(x)):\n        lat[i], lon[i], h[i] = ecef_to_geodetic(x[i], y[i], z[i])\n    return lat, lon, h\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371000.0\n    lat1 = np.radians(lat1)\n    lon1 = np.radians(lon1)\n    lat2 = np.radians(lat2)\n    lon2 = np.radians(lon2)\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat / 2.0) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return R * c\n\n\ndef comp_metric(df):\n    metrics = []\n    for phone, g in df.groupby(\"phone\"):\n        dists = haversine(\n            g[\"lat_gt\"].values,\n            g[\"lon_gt\"].values,\n            g[\"lat_pred\"].values,\n            g[\"lon_pred\"].values,\n        )\n        if len(dists) == 0:\n            continue\n        p50 = np.percentile(dists, 50)\n        p95 = np.percentile(dists, 95)\n        metrics.append((p50 + p95) / 2.0)\n    if not metrics:\n        return float(\"nan\")\n    return float(np.mean(metrics))\n\n\n# ----------------- data loading helpers ----------------- #\n\n\ndef list_phone_dirs(root_dir):\n    res = []\n    if not os.path.isdir(root_dir):\n        return res\n    for drive in sorted(os.listdir(root_dir)):\n        drive_path = os.path.join(root_dir, drive)\n        if not os.path.isdir(drive_path):\n            continue\n        for phone in sorted(os.listdir(drive_path)):\n            phone_path = os.path.join(drive_path, phone)\n            if not os.path.isdir(phone_path):\n                continue\n            res.append((drive, phone, phone_path))\n    return res\n\n\ndef load_train_phone(drive, phone, phone_path):\n    gt_path = os.path.join(phone_path, \"ground_truth.csv\")\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not (os.path.exists(gt_path) and os.path.exists(gnss_path)):\n        return None\n    gt = pd.read_csv(gt_path)\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gt = gt.dropna(subset=[\"LatitudeDegrees\", \"LongitudeDegrees\", \"UnixTimeMillis\"])\n    gnss = gnss.dropna(subset=needed_cols)\n\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = gt[\"UnixTimeMillis\"].min() - gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    df = pd.merge(gt, g_agg, on=\"UnixTimeMillis\", how=\"inner\")\n    if df.empty:\n        return None\n\n    if all(f\"WlsPosition{ax}EcefMeters_mean\" in df.columns for ax in [\"X\", \"Y\", \"Z\"]):\n        x = df[\"WlsPositionXEcefMeters_mean\"].values\n        y = df[\"WlsPositionYEcefMeters_mean\"].values\n        z = df[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        df[\"base_lat\"] = b_lat\n        df[\"base_lon\"] = b_lon\n    else:\n        df[\"base_lat\"] = df[\"LatitudeDegrees\"].mean()\n        df[\"base_lon\"] = df[\"LongitudeDegrees\"].mean()\n\n    df[\"phone\"] = f\"{drive}_{phone}\"\n    return df\n\n\ndef load_test_phone(drive, phone, phone_path):\n    gnss_path = os.path.join(phone_path, \"device_gnss.csv\")\n    if not os.path.exists(gnss_path):\n        return None\n    gnss = pd.read_csv(gnss_path)\n\n    needed_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"ArrivalTimeNanosSinceGpsEpoch\",\n    ]\n    for c in needed_cols:\n        if c not in gnss.columns:\n            return None\n\n    gnss = gnss.dropna(subset=needed_cols)\n    gnss[\"gps_ms\"] = gnss[\"ArrivalTimeNanosSinceGpsEpoch\"] / 1e6\n    if \"utcTimeMillis\" in gnss.columns:\n        gnss[\"offset\"] = gnss[\"utcTimeMillis\"] - gnss[\"gps_ms\"]\n        offset = gnss[\"offset\"].median()\n        gnss.drop(columns=[\"offset\"], inplace=True)\n    else:\n        offset = -gnss[\"gps_ms\"].min()\n    gnss[\"UnixTimeMillis\"] = (gnss[\"gps_ms\"] + offset).round().astype(\"int64\")\n\n    agg_cols = [\n        \"WlsPositionXEcefMeters\",\n        \"WlsPositionYEcefMeters\",\n        \"WlsPositionZEcefMeters\",\n        \"Cn0DbHz\",\n        \"PseudorangeRateMetersPerSecond\",\n        \"PseudorangeRateUncertaintyMetersPerSecond\",\n    ]\n    present = [c for c in agg_cols if c in gnss.columns]\n    agg_dict = {c: [\"mean\", \"std\"] for c in present}\n    agg_dict.update({\"Svid\": \"nunique\"})\n    g_agg = gnss.groupby(\"UnixTimeMillis\").agg(agg_dict)\n    g_agg.columns = [\"{}_{}\".format(c[0], c[1]) for c in g_agg.columns]\n    g_agg = g_agg.reset_index()\n\n    if all(\n        f\"WlsPosition{ax}EcefMeters_mean\" in g_agg.columns for ax in [\"X\", \"Y\", \"Z\"]\n    ):\n        x = g_agg[\"WlsPositionXEcefMeters_mean\"].values\n        y = g_agg[\"WlsPositionYEcefMeters_mean\"].values\n        z = g_agg[\"WlsPositionZEcefMeters_mean\"].values\n        b_lat, b_lon, _ = ecef_to_geodetic_vec(x, y, z)\n        g_agg[\"base_lat\"] = b_lat\n        g_agg[\"base_lon\"] = b_lon\n    else:\n        g_agg[\"base_lat\"] = 0.0\n        g_agg[\"base_lon\"] = 0.0\n\n    g_agg[\"phone\"] = f\"{drive}_{phone}\"\n    return g_agg\n\n\n# ----------------- build full train data ----------------- #\n\ntrain_records = []\nfor drive, phone, path in list_phone_dirs(TRAIN_DIR):\n    try:\n        rec = load_train_phone(drive, phone, path)\n        if rec is not None:\n            train_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading train {drive}/{phone}: {e}\")\n        continue\n\nif not train_records:\n    raise RuntimeError(\"No training data loaded\")\n\ntrain_df = pd.concat(train_records, ignore_index=True)\ntrain_df = train_df.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(drop=True)\n\ntrain_df[\"tgt_lat\"] = train_df[\"LatitudeDegrees\"]\ntrain_df[\"tgt_lon\"] = train_df[\"LongitudeDegrees\"]\ntrain_df[\"dlat\"] = train_df[\"tgt_lat\"] - train_df[\"base_lat\"]\ntrain_df[\"dlon\"] = train_df[\"tgt_lon\"] - train_df[\"base_lon\"]\n\nexclude_cols = {\n    \"LatitudeDegrees\",\n    \"LongitudeDegrees\",\n    \"AltitudeMeters\",\n    \"MessageType\",\n    \"SpeedMps\",\n    \"AccuracyMeters\",\n    \"BearingDegrees\",\n    \"tgt_lat\",\n    \"tgt_lon\",\n    \"dlat\",\n    \"dlon\",\n    \"phone\",\n    \"UnixTimeMillis\",\n}\n\nbase_feature_candidates = [\n    c\n    for c in train_df.columns\n    if c not in exclude_cols\n    and (\n        \"WlsPosition\" in c\n        or c.startswith(\"Cn0DbHz_\")\n        or c.startswith(\"PseudorangeRateMetersPerSecond_\")\n        or c.startswith(\"PseudorangeRateUncertaintyMetersPerSecond_\")\n        or c in [\"Svid_nunique\"]\n        or c in [\"base_lat\", \"base_lon\"]\n    )\n]\n\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        train_df[feat + \"_diff\"] = train_df.groupby(\"phone\")[feat].diff()\n\n# rolling window features\nrolling_window = 3\nrolling_features = []\nfor feat in base_feature_candidates:\n    if train_df[feat].dtype != \"O\":\n        roll_mean_name = feat + \"_rollmean\"\n        roll_std_name = feat + \"_rollstd\"\n        train_df[roll_mean_name] = (\n            train_df.groupby(\"phone\")[feat]\n            .rolling(window=rolling_window, min_periods=1)\n            .mean()\n            .reset_index(level=0, drop=True)\n        )\n        train_df[roll_std_name] = (\n            train_df.groupby(\"phone\")[feat]\n            .rolling(window=rolling_window, min_periods=1)\n            .std()\n            .reset_index(level=0, drop=True)\n        )\n        rolling_features.extend([roll_mean_name, roll_std_name])\n\nfeature_cols = (\n    base_feature_candidates\n    + [c for c in train_df.columns if c.endswith(\"_diff\")]\n    + rolling_features\n)\n\nfeature_cols = [c for c in feature_cols if c in train_df.columns]\n\ntrain_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n\nX = train_df[feature_cols].values\ny_dlat = train_df[\"dlat\"].values\ny_dlon = train_df[\"dlon\"].values\ngroups = train_df[\"phone\"].values\n\ngkf = GroupKFold(n_splits=5)\noof_lat = np.zeros(len(train_df))\noof_lon = np.zeros(len(train_df))\n\nfor fold, (tr_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n    X_tr, X_val = X[tr_idx], X[val_idx]\n    y_dlat_tr, y_dlat_val = y_dlat[tr_idx], y_dlat[val_idx]\n    y_dlon_tr, y_dlon_val = y_dlon[tr_idx], y_dlon[val_idx]\n\n    params = dict(\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=-1,\n        num_leaves=64,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        min_child_samples=20,\n        reg_lambda=0.1,\n        random_state=42 + fold,\n        n_jobs=-1,\n    )\n    model_lat = LGBMRegressor(**params)\n    model_lon = LGBMRegressor(**params)\n\n    model_lat.fit(X_tr, y_dlat_tr)\n    model_lon.fit(X_tr, y_dlon_tr)\n\n    oof_lat[val_idx] = train_df[\"base_lat\"].values[val_idx] + model_lat.predict(X_val)\n    oof_lon[val_idx] = train_df[\"base_lon\"].values[val_idx] + model_lon.predict(X_val)\n\noof_df = train_df[\n    [\"phone\", \"UnixTimeMillis\", \"LatitudeDegrees\", \"LongitudeDegrees\"]\n].copy()\noof_df[\"lat_gt\"] = oof_df[\"LatitudeDegrees\"]\noof_df[\"lon_gt\"] = oof_df[\"LongitudeDegrees\"]\noof_df[\"lat_pred\"] = oof_lat\noof_df[\"lon_pred\"] = oof_lon\nscore = comp_metric(oof_df)\nprint(\"CV metric (mean of 50th and 95th percentile distance errors):\", score)\n\nfinal_model_lat = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\nfinal_model_lon = LGBMRegressor(\n    n_estimators=350,\n    learning_rate=0.05,\n    max_depth=-1,\n    num_leaves=64,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    min_child_samples=20,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1,\n)\n\nfinal_model_lat.fit(X, y_dlat)\nfinal_model_lon.fit(X, y_dlon)\n\nsample_sub_path = os.path.join(INPUT_DIR, \"sample_submission.csv\")\nif not os.path.exists(sample_sub_path):\n    raise RuntimeError(\"sample_submission.csv not found in ./input\")\nsample_sub = pd.read_csv(sample_sub_path)\n\nif \"phone\" in sample_sub.columns:\n    id_col = \"phone\"\nelif \"tripId\" in sample_sub.columns:\n    id_col = \"tripId\"\nelse:\n    id_col = sample_sub.columns[0]\n\ntime_col = \"UnixTimeMillis\"\nif time_col not in sample_sub.columns:\n    raise RuntimeError(\"UnixTimeMillis column not found in sample_submission.csv\")\n\nsample_sub[\"phone\"] = sample_sub[id_col].astype(str)\n\ntest_records = []\nfor drive, phone, path in list_phone_dirs(TEST_DIR):\n    try:\n        rec = load_test_phone(drive, phone, path)\n        if rec is not None:\n            test_records.append(rec)\n    except Exception as e:\n        print(f\"Error loading test {drive}/{phone}: {e}\")\n        continue\n\nif test_records:\n    test_gnss = pd.concat(test_records, ignore_index=True)\nelse:\n    test_gnss = pd.DataFrame(columns=[\"UnixTimeMillis\", \"phone\"])\n\nif \"UnixTimeMillis\" in test_gnss.columns:\n    test_gnss[\"UnixTimeMillis\"] = test_gnss[\"UnixTimeMillis\"].astype(\"int64\")\n\n# create diff and rolling features on test matching train scheme\nfor feat in base_feature_candidates:\n    if feat not in test_gnss.columns:\n        test_gnss[feat] = np.nan\n\nfor feat in base_feature_candidates:\n    if test_gnss[feat].dtype != \"O\":\n        test_gnss[feat + \"_diff\"] = test_gnss.groupby(\"phone\")[feat].diff()\n\nfor feat in base_feature_candidates:\n    if test_gnss[feat].dtype != \"O\":\n        roll_mean_name = feat + \"_rollmean\"\n        roll_std_name = feat + \"_rollstd\"\n        test_gnss[roll_mean_name] = (\n            test_gnss.groupby(\"phone\")[feat]\n            .rolling(window=rolling_window, min_periods=1)\n            .mean()\n            .reset_index(level=0, drop=True)\n        )\n        test_gnss[roll_std_name] = (\n            test_gnss.groupby(\"phone\")[feat]\n            .rolling(window=rolling_window, min_periods=1)\n            .std()\n            .reset_index(level=0, drop=True)\n        )\n\nfor c in feature_cols:\n    if c not in test_gnss.columns:\n        test_gnss[c] = np.nan\n\nif not test_gnss.empty:\n    test_gnss = test_gnss.sort_values([\"phone\", \"UnixTimeMillis\"]).reset_index(\n        drop=True\n    )\n    test_gnss[feature_cols] = test_gnss[feature_cols].fillna(\n        train_df[feature_cols].median()\n    )\n\ntest_merge = pd.merge(\n    sample_sub[[\"phone\", time_col]],\n    test_gnss[[\"phone\", \"UnixTimeMillis\"] + feature_cols],\n    left_on=[\"phone\", time_col],\n    right_on=[\"phone\", \"UnixTimeMillis\"],\n    how=\"left\",\n)\n\ntest_merge = test_merge.sort_values([\"phone\", \"UnixTimeMillis\"])\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\ntest_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].bfill()\ntest_merge[feature_cols] = test_merge[feature_cols].fillna(\n    train_df[feature_cols].median()\n)\n\nX_test = test_merge[feature_cols].values\n\nif \"base_lat\" in test_merge.columns and \"base_lon\" in test_merge.columns:\n    base_lat_test = test_merge[\"base_lat\"].values\n    base_lon_test = test_merge[\"base_lon\"].values\nelse:\n    base_lat_test = np.full(len(test_merge), train_df[\"LatitudeDegrees\"].mean())\n    base_lon_test = np.full(len(test_merge), train_df[\"LongitudeDegrees\"].mean())\n\npred_dlat = final_model_lat.predict(X_test)\npred_dlon = final_model_lon.predict(X_test)\npred_lat = base_lat_test + pred_dlat\npred_lon = base_lon_test + pred_dlon\n\nsubmission = pd.DataFrame()\nsubmission[id_col] = sample_sub[id_col]\nsubmission[time_col] = sample_sub[time_col]\nsubmission[\"LatitudeDegrees\"] = pred_lat\nsubmission[\"LongitudeDegrees\"] = pred_lon\n\nsub_path = os.path.join(SUBMISSION_DIR, \"submission.csv\")\nsubmission.to_csv(sub_path, index=False)\nprint(\"Saved submission to\", sub_path)\n\nworking_sub_path = os.path.join(WORKING_DIR, \"submission.csv\")\nsubmission.to_csv(working_sub_path, index=False)\nprint(\"Saved submission to\", working_sub_path)\n"], "term_out": ["<OMITTED>", "<OMITTED>", "Traceback (most recent call last):\n  File \"runfile.py\", line 275, in <module>\n    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y_dlat, groups)):\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/sklearn/model_selection/_split.py\", line 345, in split\n    raise ValueError(\nValueError: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=0.\nExecution time: a minute seconds (time limit is 9 hours).", "<OMITTED>", "<OMITTED>", "<OMITTED>", "<OMITTED>", "Validation metric (mean of 50th & 95th errors): 0.0000\nTraceback (most recent call last):\n  File \"runfile.py\", line 309, in <module>\n    submission = submission.merge(\n                 ^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 10487, in merge\n    return merge(\n           ^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 169, in merge\n    op = _MergeOperation(\n         ^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 791, in __init__\n    ) = self._get_merge_keys()\n        ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 1287, in _get_merge_keys\n    left_keys.append(left._get_label_or_level_values(lk))\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/generic.py\", line 1844, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'phone'\nExecution time: a minute seconds (time limit is 9 hours).", "<OMITTED>", "Validation metric (mean of 50th & 95th errors): 0.0000\nTraceback (most recent call last):\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3791, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 152, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 181, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'phone'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"runfile.py\", line 313, in <module>\n    if sample_phone not in sample_sub[\"phone\"].unique():\n                           ~~~~~~~~~~^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 3893, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3798, in get_loc\n    raise KeyError(key) from err\nKeyError: 'phone'\nExecution time: a minute seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "<OMITTED>", "<OMITTED>", "<OMITTED>", "<OMITTED>", "<OMITTED>", "<OMITTED>", "<OMITTED>", "<OMITTED>", "<OMITTED>", "Validation metric (mean of 50th & 95th errors): 0.0000\nSaved submission to ./submission/submission.csv\nExecution time: 59 seconds seconds (time limit is 9 hours).", "<OMITTED>", "<OMITTED>", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Validation metric (mean of 50th & 95th errors): 0.0\nTraceback (most recent call last):\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3791, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 152, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 181, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'phone'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"runfile.py\", line 345, in <module>\n    sample_phones_unique = set(sample_sub[sample_phone_col].unique())\n                               ~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 3893, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3798, in get_loc\n    raise KeyError(key) from err\nKeyError: 'phone'\nExecution time: 58 seconds seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Validation metric (mean of 50th & 95th errors) for global-mean baseline: 124935.656586833\nSaved submission to ./submission/submission.csv\nExecution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Execution time: a moment seconds (time limit is 9 hours).", "Traceback (most recent call last):\n  File \"runfile.py\", line 339, in <module>\n    model_x.fit(\nTypeError: LGBMRegressor.fit() got an unexpected keyword argument 'verbose'\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005579 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11729\n[LightGBM] [Info] Number of data points in the train set: 106709, number of used features: 46\n[LightGBM] [Info] Start training from score -2658355.989460\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005848 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11729\n[LightGBM] [Info] Number of data points in the train set: 106709, number of used features: 46\n[LightGBM] [Info] Start training from score -4371256.851142\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005868 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11729\n[LightGBM] [Info] Number of data points in the train set: 106709, number of used features: 46\n[LightGBM] [Info] Start training from score 3790672.846234\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005874 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11725\n[LightGBM] [Info] Number of data points in the train set: 106774, number of used features: 46\n[LightGBM] [Info] Start training from score -2655266.801078\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005694 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11725\n[LightGBM] [Info] Number of data points in the train set: 106774, number of used features: 46\n[LightGBM] [Info] Start training from score -4376144.687213\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005750 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11725\n[LightGBM] [Info] Number of data points in the train set: 106774, number of used features: 46\n[LightGBM] [Info] Start training from score 3787032.774734\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005606 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11727\n[LightGBM] [Info] Number of data points in the train set: 106388, number of used features: 46\n[LightGBM] [Info] Start training from score -2658585.120617\n[LightGBM] [Info] Auto-choosing col-wise mult\n ... [1497 characters truncated] ... \no remove the overhead.\n[LightGBM] [Info] Total Bins 11726\n[LightGBM] [Info] Number of data points in the train set: 107186, number of used features: 46\n[LightGBM] [Info] Start training from score 3781293.208171\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005681 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11728\n[LightGBM] [Info] Number of data points in the train set: 106475, number of used features: 46\n[LightGBM] [Info] Start training from score -2656321.477875\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005782 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11728\n[LightGBM] [Info] Number of data points in the train set: 106475, number of used features: 46\n[LightGBM] [Info] Start training from score -4374976.098525\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005776 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11728\n[LightGBM] [Info] Number of data points in the train set: 106475, number of used features: 46\n[LightGBM] [Info] Start training from score 3787676.551813\nCV metric (mean of 50th and 95th percentile distance errors): 434.4494988345602\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007235 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11727\n[LightGBM] [Info] Number of data points in the train set: 133383, number of used features: 46\n[LightGBM] [Info] Start training from score -2656212.471949\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007232 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11727\n[LightGBM] [Info] Number of data points in the train set: 133383, number of used features: 46\n[LightGBM] [Info] Start training from score -4375050.720755\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007455 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11727\n[LightGBM] [Info] Number of data points in the train set: 133383, number of used features: 46\n[LightGBM] [Info] Start training from score 3787656.865150\nSaved submission to ./submission/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011194 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11510\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 47\n[LightGBM] [Info] Start training from score 37.025771\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014414 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11510\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 47\n[LightGBM] [Info] Start training from score -121.695389\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010594 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11510\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 47\n[LightGBM] [Info] Start training from score 37.006997\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010768 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11510\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 47\n[LightGBM] [Info] Start training from score -121.669828\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010386 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11509\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 47\n[LightGBM] [Info] Start training from score 37.034191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010565 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11509\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 47\n[LightGBM] [Info] Start training from score -121.700690\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010205 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11509\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 47\n[LightGBM] [Info] Start training from score 37.082594\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010597 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11509\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 47\n[LightGBM] [Info] Start training from score -121.754294\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11508\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 47\n[LightGBM] [Info] Start training from score 37.024000\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11508\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 47\n[LightGBM] [Info] Start training from score -121.687492\nCV metric (mean of 50th and 95th percentile distance errors): 416.36022680373503\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013072 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11510\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 47\n[LightGBM] [Info] Start training from score 37.034702\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013333 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11510\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 47\n[LightGBM] [Info] Start training from score -121.701529\nSaved submission to ./submission/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007283 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11474\n[LightGBM] [Info] Number of data points in the train set: 106709, number of used features: 45\n[LightGBM] [Info] Start training from score 36.709923\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005818 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11474\n[LightGBM] [Info] Number of data points in the train set: 106709, number of used features: 45\n[LightGBM] [Info] Start training from score -121.331076\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005585 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11470\n[LightGBM] [Info] Number of data points in the train set: 106774, number of used features: 45\n[LightGBM] [Info] Start training from score 36.669367\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005981 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11470\n[LightGBM] [Info] Number of data points in the train set: 106774, number of used features: 45\n[LightGBM] [Info] Start training from score -121.274007\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005828 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11472\n[LightGBM] [Info] Number of data points in the train set: 106388, number of used features: 45\n[LightGBM] [Info] Start training from score 36.720717\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005816 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11472\n[LightGBM] [Info] Number of data points in the train set: 106388, number of used features: 45\n[LightGBM] [Info] Start training from score -121.338450\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008162 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11471\n[LightGBM] [Info] Number of data points in the train set: 107186, number of used features: 45\n[LightGBM] [Info] Start training from score 36.605532\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008295 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11471\n[LightGBM] [Info] Number of data points in the train set: 107186, number of used features: 45\n[LightGBM] [Info] Start training from score -121.212779\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005848 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11473\n[LightGBM] [Info] Number of data points in the train set: 106475, number of used features: 45\n[LightGBM] [Info] Start training from score 36.676472\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005854 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11473\n[LightGBM] [Info] Number of data points in the train set: 106475, number of used features: 45\n[LightGBM] [Info] Start training from score -121.290771\nCV metric (mean of 50th and 95th percentile distance errors): 501.1360042093435\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009385 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11472\n[LightGBM] [Info] Number of data points in the train set: 133383, number of used features: 45\n[LightGBM] [Info] Start training from score 36.676311\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007194 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11472\n[LightGBM] [Info] Number of data points in the train set: 133383, number of used features: 45\n[LightGBM] [Info] Start training from score -121.289316\nSaved submission to ./submission/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010408 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11510\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 47\n[LightGBM] [Info] Start training from score 37.025771\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014737 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11510\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 47\n[LightGBM] [Info] Start training from score -121.695389\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010713 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11510\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 47\n[LightGBM] [Info] Start training from score 37.006997\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010786 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11510\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 47\n[LightGBM] [Info] Start training from score -121.669828\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010520 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11509\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 47\n[LightGBM] [Info] Start training from score 37.034191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011383 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11509\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 47\n[LightGBM] [Info] Start training from score -121.700690\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014571 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11509\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 47\n[LightGBM] [Info] Start training from score 37.082594\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010402 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11509\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 47\n[LightGBM] [Info] Start training from score -121.754294\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011004 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11508\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 47\n[LightGBM] [Info] Start training from score 37.024000\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010323 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11508\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 47\n[LightGBM] [Info] Start training from score -121.687492\nCV metric (mean of 50th and 95th percentile distance errors): 412.69159704618863\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013203 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11510\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 47\n[LightGBM] [Info] Start training from score 37.034702\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013333 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11510\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 47\n[LightGBM] [Info] Start training from score -121.701529\nSaved submission to ./submission/submission.csv and ./working/submission.csv\nExecution time: 2 minutes seconds (time limit is 9 hours).", "<OMITTED>", "<OMITTED>", "<OMITTED>", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006549 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006506 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006676 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7194\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006786 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7194\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006501 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7193\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006908 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7193\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006487 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006466 s\n ... [148 characters truncated] ... \nrain set: 196385, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006494 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006460 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 159.0075248361783\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008308 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008245 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 509, in <module>\n    test_merge[feature_cols + [\"base_lat\", \"base_lon\"]] = test_merge.groupby(\"phone\")[\n    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4079, in __setitem__\n    self._setitem_array(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4123, in _setitem_array\n    self[k1] = value[k2]\n    ~~~~^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4081, in __setitem__\n    self._set_item_frame_value(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4209, in _set_item_frame_value\n    raise ValueError(\"Columns must be same length as key\")\nValueError: Columns must be same length as key\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006375 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009272 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006745 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7194\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006986 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7194\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009141 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7193\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009051 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7193\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006591 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006645 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006432 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006648 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 159.00605562360315\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008175 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008111 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 509, in <module>\n    if test_merge[c].isna().any():\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/generic.py\", line 1519, in __nonzero__\n    raise ValueError(\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006362 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006471 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006440 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7194\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006551 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7194\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006806 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7193\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006537 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7193\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006694 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006696 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006729 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006369 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 159.0075248361783\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008451 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008137 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 511, in <module>\n    if test_merge[c].isna().any():\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/generic.py\", line 1519, in __nonzero__\n    raise ValueError(\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006465 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006439 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006550 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7194\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006603 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7194\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006680 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7193\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006415 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7193\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006403 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006631 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006755 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006207 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 159.0075248361783\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008774 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008879 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 517, in <module>\n    if test_merge[\"base_lat\"].isna().all():\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/generic.py\", line 1519, in __nonzero__\n    raise ValueError(\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006578 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006592 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007339 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7194\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006558 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7194\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006760 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7193\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006550 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7193\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006714 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006296 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007013 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7192\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 159.0075248361783\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008145 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 30\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008340 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 7195\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 30\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 517, in <module>\n    if test_merge[\"base_lat\"].isna().all():\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/generic.py\", line 1519, in __nonzero__\n    raise ValueError(\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005607 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6172\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 26\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005541 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6172\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 26\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005517 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6174\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 26\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005506 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6174\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 26\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005845 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6173\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 26\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005604 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6173\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 26\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005950 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6175\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 26\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005618 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6175\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 26\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005782 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6172\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 26\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005606 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6172\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 26\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 224.77598774542386\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007105 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6175\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 26\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007056 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 6175\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 26\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010909 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016991 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010957 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011024 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010417 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010370 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010764 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015299 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017015 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010634 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 277.0276856698035\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013479 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014740 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010325 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8541\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008039 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8541\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008158 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8544\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012671 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8544\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008304 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8545\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007964 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8545\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008658 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8546\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008021 s\n ... [116 characters truncated] ... \n] Number of data points in the train set: 196385, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008512 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8537\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007845 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8537\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 152.75763316081583\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8546\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016309 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8546\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 499, in <module>\n    test_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\n    ~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4079, in __setitem__\n    self._setitem_array(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4123, in _setitem_array\n    self[k1] = value[k2]\n    ~~~~^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4081, in __setitem__\n    self._set_item_frame_value(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4209, in _set_item_frame_value\n    raise ValueError(\"Columns must be same length as key\")\nValueError: Columns must be same length as key\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007492 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8541\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007755 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8541\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007933 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8544\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007923 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8544\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007805 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8545\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007813 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8545\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007593 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8546\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010709 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8546\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012439 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8537\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007818 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8537\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 152.75763316081583\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015863 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8546\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009711 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8546\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 552, in <module>\n    pred_dlat = final_model_lat.predict(X_test)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/lightgbm/sklearn.py\", line 1010, in predict\n    raise ValueError(\nValueError: Number of features of the model must match the input. Model n_features_ is 42 and input n_features is 44\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007619 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8541\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007701 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8541\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010864 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8544\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008298 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8544\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007695 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8545\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007658 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8545\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008080 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8546\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008033 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8546\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007671 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8537\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007776 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8537\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 152.75763316081583\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009796 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8546\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 36\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009542 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 8546\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 36\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011376 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014033 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011985 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017689 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011068 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015234 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015415 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014358 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011828 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015348 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 315.1138294256007\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014765 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018829 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014012 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017186 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015698 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011356 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014191 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013763 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012036 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.0\n ... [127 characters truncated] ... \number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010874 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015928 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 294.8108810081267\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014105 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014228 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 502, in <module>\n    test_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\n    ~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4079, in __setitem__\n    self._setitem_array(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4123, in _setitem_array\n    self[k1] = value[k2]\n    ~~~~^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4081, in __setitem__\n    self._set_item_frame_value(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4209, in _set_item_frame_value\n    raise ValueError(\"Columns must be same length as key\")\nValueError: Columns must be same length as key\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010617 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013349 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010996 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014103 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010760 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011166 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014993 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.0\n ... [125 characters truncated] ... \n Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011015 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010836 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 294.82064737344996\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013603 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013582 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 515, in <module>\n    test_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n    ~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4079, in __setitem__\n    self._setitem_array(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4123, in _setitem_array\n    self[k1] = value[k2]\n    ~~~~^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4081, in __setitem__\n    self._set_item_frame_value(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4209, in _set_item_frame_value\n    raise ValueError(\"Columns must be same length as key\")\nValueError: Columns must be same length as key\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010788 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013635 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010983 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011163 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013521 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010825 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015034 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.0\n ... [278 characters truncated] ... \n-choosing col-wise multi-threading, the overhead of testing was 0.010643 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010808 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 294.8108810081267\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013548 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014587 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 533, in <module>\n    test_merge.loc[:, feature_cols] = ffilled.where(~ffilled.isna(), bfilled)\n    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/indexing.py\", line 885, in __setitem__\n    iloc._setitem_with_indexer(indexer, value, self.name)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/indexing.py\", line 1893, in _setitem_with_indexer\n    self._setitem_with_indexer_split_path(indexer, value, name)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/indexing.py\", line 1928, in _setitem_with_indexer_split_path\n    self._setitem_with_indexer_frame_value(indexer, value, name)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/indexing.py\", line 2044, in _setitem_with_indexer_frame_value\n    raise ValueError(\"Setting with non-unique columns is not allowed.\")\nValueError: Setting with non-unique columns is not allowed.\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010728 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010676 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010940 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010840 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010653 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010851 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013715 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.0\n ... [285 characters truncated] ... \nng col-wise multi-threading, the overhead of testing was 0.015324 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011980 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 294.8108810081267\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013547 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013611 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 536, in <module>\n    test_merge.loc[:, feature_cols] = test_merge[feature_cols].fillna(train_medians)\n    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/indexing.py\", line 885, in __setitem__\n    iloc._setitem_with_indexer(indexer, value, self.name)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/indexing.py\", line 1893, in _setitem_with_indexer\n    self._setitem_with_indexer_split_path(indexer, value, name)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/indexing.py\", line 1928, in _setitem_with_indexer_split_path\n    self._setitem_with_indexer_frame_value(indexer, value, name)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/indexing.py\", line 2044, in _setitem_with_indexer_frame_value\n    raise ValueError(\"Setting with non-unique columns is not allowed.\")\nValueError: Setting with non-unique columns is not allowed.\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010492 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014196 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010993 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011442 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011179 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013565 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011449 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.0\n ... [125 characters truncated] ... \n Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011603 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015566 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 294.82064737344996\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013780 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014038 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 530, in <module>\n    test_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n    ~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4079, in __setitem__\n    self._setitem_array(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4123, in _setitem_array\n    self[k1] = value[k2]\n    ~~~~^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4081, in __setitem__\n    self._set_item_frame_value(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4209, in _set_item_frame_value\n    raise ValueError(\"Columns must be same length as key\")\nValueError: Columns must be same length as key\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011969 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016996 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010683 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013170 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014210 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010707 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013499 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010657 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013948 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010925 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 294.8108810081267\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018825 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013570 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 532, in <module>\n    test_merge[feature_cols] = combined_features\n    ~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4079, in __setitem__\n    self._setitem_array(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4123, in _setitem_array\n    self[k1] = value[k2]\n    ~~~~^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4081, in __setitem__\n    self._set_item_frame_value(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4209, in _set_item_frame_value\n    raise ValueError(\"Columns must be same length as key\")\nValueError: Columns must be same length as key\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015552 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013814 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011009 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014800 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011217 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015640 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015784 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.0\n ... [125 characters truncated] ... \n Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010827 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010988 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 294.82064737344996\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013555 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014572 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 521, in <module>\n    test_merge[feature_cols] = test_merge[feature_cols].fillna(train_medians)\n    ~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4079, in __setitem__\n    self._setitem_array(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4123, in _setitem_array\n    self[k1] = value[k2]\n    ~~~~^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4081, in __setitem__\n    self._set_item_frame_value(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4209, in _set_item_frame_value\n    raise ValueError(\"Columns must be same length as key\")\nValueError: Columns must be same length as key\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010908 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011125 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017396 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012193 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010991 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011130 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010942 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010852 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011451 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011048 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 294.8108810081267\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013683 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013773 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 550, in <module>\n    pred_dlat = final_model_lat.predict(X_test)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/lightgbm/sklearn.py\", line 1010, in predict\n    raise ValueError(\nValueError: Number of features of the model must match the input. Model n_features_ is 60 and input n_features is 62\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010600 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010985 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013851 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011547 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010737 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011318 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013510 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011062 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010735 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010825 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 294.8108810081267\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015094 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013878 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "Traceback (most recent call last):\n  File \"runfile.py\", line 314, in <module>\n    train_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 11348, in median\n    result = super().median(axis, skipna, numeric_only, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/generic.py\", line 12003, in median\n    return self._stat_function(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/generic.py\", line 11949, in _stat_function\n    return self._reduce(\n           ^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 11204, in _reduce\n    res = df._mgr.reduce(blk_func)\n          ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 1459, in reduce\n    nbs = blk.reduce(func)\n          ^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/internals/blocks.py\", line 377, in reduce\n    result = func(self.values)\n             ^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 11136, in blk_func\n    return op(values, axis=axis, skipna=skipna, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/nanops.py\", line 147, in f\n    result = alt(values, axis=axis, skipna=skipna, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/nanops.py\", line 783, in nanmedian\n    raise TypeError(f\"Cannot convert {values} to numeric\")\nTypeError: Cannot convert [['GT' 'GT' 'GT' ... 'GT' 'GT' 'GT']] to numeric\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015219 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013921 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016247 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016042 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011069 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011835 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017210 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.0\n ... [131 characters truncated] ... \nr of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010984 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011836 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 265.65168753388343\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017493 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020021 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 490, in <module>\n    test_merge[cols_to_ffill] = test_merge.groupby(\"phone\")[cols_to_ffill].ffill()\n    ~~~~~~~~~~^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4079, in __setitem__\n    self._setitem_array(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4123, in _setitem_array\n    self[k1] = value[k2]\n    ~~~~^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4081, in __setitem__\n    self._set_item_frame_value(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4209, in _set_item_frame_value\n    raise ValueError(\"Columns must be same length as key\")\nValueError: Columns must be same length as key\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010446 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010717 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011398 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010753 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015038 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013655 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.0\n ... [111 characters truncated] ... \nghtGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017130 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011167 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 265.65168753388383\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014970 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013360 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 520, in <module>\n    test_merge[feature_cols] = test_merge[feature_cols].fillna(\n    ~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4079, in __setitem__\n    self._setitem_array(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4123, in _setitem_array\n    self[k1] = value[k2]\n    ~~~~^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4081, in __setitem__\n    self._set_item_frame_value(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4209, in _set_item_frame_value\n    raise ValueError(\"Columns must be same length as key\")\nValueError: Columns must be same length as key\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010426 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015482 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010951 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010336 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015514 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010759 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010441 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016490 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015540 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 265.65168753388343\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016571 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018763 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 530, in <module>\n    pred_dlat = final_model_lat.predict(X_test)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/lightgbm/sklearn.py\", line 1010, in predict\n    raise ValueError(\nValueError: Number of features of the model must match the input. Model n_features_ is 60 and input n_features is 62\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014457 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011351 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010955 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010775 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011436 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011518 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014784 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010575 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010428 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010407 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 265.65168753388343\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013320 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014187 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024356 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021335 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020406 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025303 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016759 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022637 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017057 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016880 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016715 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016573 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 211.5365744066162\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029493 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024426 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023503 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023005 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027108 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020056 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020041 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020587 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021861 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016876 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017056 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019999 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 200.43096765719255\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020628 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020884 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010693 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013334 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017307 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010947 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010603 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013347 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010672 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011618 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010929 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010684 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 311.98674006756255\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018555 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013959 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "Traceback (most recent call last):\n  File \"runfile.py\", line 316, in <module>\n    train_df[feature_cols] = train_df[feature_cols].fillna(train_df[feature_cols].median())\n                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 11348, in median\n    result = super().median(axis, skipna, numeric_only, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/generic.py\", line 12003, in median\n    return self._stat_function(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/generic.py\", line 11949, in _stat_function\n    return self._reduce(\n           ^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 11204, in _reduce\n    res = df._mgr.reduce(blk_func)\n          ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 1459, in reduce\n    nbs = blk.reduce(func)\n          ^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/internals/blocks.py\", line 377, in reduce\n    result = func(self.values)\n             ^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 11136, in blk_func\n    return op(values, axis=axis, skipna=skipna, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/nanops.py\", line 147, in f\n    result = alt(values, axis=axis, skipna=skipna, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/nanops.py\", line 783, in nanmedian\n    raise TypeError(f\"Cannot convert {values} to numeric\")\nTypeError: Cannot convert [['GT' 'GT' 'GT' ... 'GT' 'GT' 'GT']] to numeric\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011571 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012174 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011431 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011406 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011043 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017759 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017292 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011566 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012239 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014600 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 314.21662771491924\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013697 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015408 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021879 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020037 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021725 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018277 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017115 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021459 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017060 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.0\n ... [176 characters truncated] ... \nmber of used features: 72\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017220 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023346 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 202.7160939242046\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021956 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021785 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 460, in <module>\n    test_merge = pd.merge(\n                 ^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 169, in merge\n    op = _MergeOperation(\n         ^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 791, in __init__\n    ) = self._get_merge_keys()\n        ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 1269, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/generic.py\", line 1858, in _get_label_or_level_values\n    raise ValueError(\nValueError: The column label 'phone' is not unique.\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020015 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016789 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026173 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024665 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017212 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017158 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021172 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021514 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018357 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020233 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 202.7160939242046\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021180 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029780 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010690 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013921 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011069 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014120 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011073 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011149 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013574 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011413 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011051 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014013 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 290.16563080133415\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021437 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021723 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021468 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017022 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021777 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028620 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016994 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017061 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017028 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017593 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016999 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017400 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 202.7160939242046\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029318 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024899 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016762 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022176 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026280 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019777 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020276 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019913 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023392 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.0\n ... [127 characters truncated] ... \number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017534 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020104 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 202.7160939242046\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026955 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.027826 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 516, in <module>\n    test_merge[feature_cols] = test_merge.groupby(\"phone\")[feature_cols].ffill()\n    ~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4079, in __setitem__\n    self._setitem_array(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4123, in _setitem_array\n    self[k1] = value[k2]\n    ~~~~^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4081, in __setitem__\n    self._set_item_frame_value(key, value)\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/pandas/core/frame.py\", line 4209, in _set_item_frame_value\n    raise ValueError(\"Columns must be same length as key\")\nValueError: Columns must be same length as key\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016638 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021422 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023458 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023218 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018940 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019993 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017218 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021806 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023573 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017183 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 202.7160939242046\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026998 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021589 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 538, in <module>\n    pred_dlat = final_model_lat.predict(X_test)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/lightgbm/sklearn.py\", line 1010, in predict\n    raise ValueError(\nValueError: Number of features of the model must match the input. Model n_features_ is 90 and input n_features is 92\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021911 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016788 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017270 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016889 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17551\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020158 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020287 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17550\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017054 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020306 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17555\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017811 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017249 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17546\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 202.7160939242046\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021370 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024943 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 17554\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 72\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 551, in <module>\n    pred_dlat = final_model_lat.predict(X_test)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/lightgbm/sklearn.py\", line 1010, in predict\n    raise ValueError(\nValueError: Number of features of the model must match the input. Model n_features_ is 90 and input n_features is 92\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000868 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000941 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000901 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000923 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000917 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2830\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000923 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2830\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing row-wise multi-threadin\n ... [379 characters truncated] ... \nsing row-wise multi-threading, the overhead of testing was 0.000949 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2830\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000913 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2829\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000949 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2829\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 365.6115137883766\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001108 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003355 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 487, in <module>\n    pred_dlat = final_model_lat.predict(X_test)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/agent/lib/python3.11/site-packages/lightgbm/sklearn.py\", line 1010, in predict\n    raise ValueError(\nValueError: Number of features of the model must match the input. Model n_features_ is 15 and input n_features is 17\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000919 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000905 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000906 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000898 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000900 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2830\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000936 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2830\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing row-wise multi-threadin\n ... [206 characters truncated] ... \nLightGBM] [Info] Number of data points in the train set: 196385, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000917 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2830\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000972 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2829\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000959 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2829\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 365.6221212806479\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001083 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001146 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000098\nTraceback (most recent call last):\n  File \"runfile.py\", line 471, in <module>\n    raise RuntimeError(\nRuntimeError: Feature dimension mismatch: X_test has 17 columns, expected 15\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000897 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000907 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000923 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000947 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000882 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2830\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000924 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2830\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002649 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2830\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000993 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2830\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000923 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2829\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000928 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2829\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 322.92287171435476\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001101 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 12\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001088 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2831\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 12\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013512 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017026 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014089 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011083 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010895 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017004 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013876 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010554 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010742 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016553 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 290.16563080133415\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019191 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019290 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours).", "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011279 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010922 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11598\n[LightGBM] [Info] Number of data points in the train set: 196613, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000123\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017870 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000184\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011039 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11602\n[LightGBM] [Info] Number of data points in the train set: 196518, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000122\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010779 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000191\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010832 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11601\n[LightGBM] [Info] Number of data points in the train set: 196646, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000129\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011050 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000183\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011521 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 196385, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000121\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011005 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000011\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010849 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11595\n[LightGBM] [Info] Number of data points in the train set: 196614, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000006\nCV metric (mean of 50th and 95th percentile distance errors): 265.65168753388383\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019527 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score 0.000146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017154 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 11603\n[LightGBM] [Info] Number of data points in the train set: 245694, number of used features: 48\n[LightGBM] [Info] Start training from score -0.000098\nSaved submission to ./submission/submission.csv\nSaved submission to ./working/submission.csv\nExecution time: a minute seconds (time limit is 9 hours)."], "analysis": ["The script appears to have completed and produced a submission CSV. No explicit runtime error shown in the provided execution output, but the validation metric value was not present in the log capture.", "The run crashed with a pandas IntCastingNaNError while converting ArrivalTimeNanosSinceGpsEpoch-derived times to integer UnixTimeMillis due to non-finite (NA) values; as a result no submission.csv was produced.", "Execution failed: GroupKFold raised ValueError because there were zero training samples (n_samples=0), meaning the training feature matrix was empty after preprocessing/aggregation. No submission.csv was produced.", "Script crashed during training: LGBMRegressor.fit raised TypeError due to unexpected 'verbose' argument. No submission.csv was produced and no CV metric was reported.", "The run produced no validation metric or submission output; it appears the script failed to assemble training/test examples and did not save a submission.csv. No empirical scores are available because training/validation did not complete successfully.", "The training script appears to have run end-to-end and produced a submission file at ./submission/submission.csv. The execution log provided did not show the printed CV metric or other logs, so the reported validation metric cannot be confirmed from the output.", "The script ran and trained LightGBM models to predict lat/lon corrections relative to WLS positions, and it saved a submission CSV to ./submission/submission.csv. No execution errors are shown in the provided output, but the validation metric printed by the script (CV score) is not present in the execution log, so a numeric score is unavailable.", "The script reported a validation metric of 0.0000 but failed to produce a submission. Execution crashed with a KeyError: 'phone' during a pandas merge with the sample submission, so submission/submission.csv was not saved.", "The run completed without any visible error messages in the provided execution output. The script normally prints CV fold metrics and a final CV score, but those values are not present in the captured output, so a numeric validation metric cannot be reported. The code appears to have produced a submission file at ./submission/submission.csv based on the implementation.", "The run printed a validation metric of 0.0000 but then crashed with a KeyError: 'phone' when accessing the sample submission DataFrame; no submission.csv was produced. The crash occurred while checking sample_sub['phone'].", "The script executed end-to-end, trained RandomForest models on aggregated GNSS features, and produced a submission file at ./submission/submission.csv. The run computed a cross-validation metric internally but a numeric value was not captured in the provided logs.", "The script successfully assembled training data, trained LightGBM models and reported a CV competition metric of 2851.249366643338, but it crashed before producing a submission. The failure was due to a KeyError ('phone') when sorting the sample submission, so no submission.csv was saved.", "The training script appears to have executed to completion without errors in the provided log and produced a submission file. A LightGBM regression model was trained (CV was run) and final predictions were written to submission/submission.csv. The execution output did not include the CV/validation metric value, so no numeric score could be reported.", "The training loop ran and produced a CV metric (~2844.53) but the script crashed before creating the submission. It raised a RuntimeError due to the sample_submission missing expected phone identifier columns, so no submission.csv was written.", "The script assembled training rows from device_gnss and ground_truth, trained LightGBM regressors to predict ECEF coordinates, converted predictions to lat/lon, and wrote a submission CSV. The execution log did not show the CV metric value, so the validation metric could not be extracted from the output.", "The execution produced only a minimal execution-time message and no training/evaluation prints (e.g., no CV fold metrics or 'Saved submission' message). Therefore the run appears to have failed to produce a submission.csv. No validation metric was reported.", "The run failed during test submission preparation: the script raised a KeyError because the sample_submission file did not contain a 'phone' column, causing the run to abort before creating submission/submission.csv. Cross-validation completed and reported a CV metric, but no final submission was produced.", "The run did not produce any visible training/evaluation logs or a saved submission file. It appears the script failed early due to lack of assembled training/test data (likely missing ./input/train or ./input/test files), so no CV metric was reported and no submission.csv was written.", "The script executed end-to-end without an explicit runtime error in the log, but it saved the submission to ./working/submission/submission.csv rather than the required ./submission/submission.csv, so the benchmark will not find a valid submission. No CV metric value was available from the provided execution output.", "The script ran without visible runtime errors and produced a submission CSV in ./submission/submission.csv. It aggregates per-epoch GNSS features, trains LightGBM regressors for latitude and longitude (with GroupKFold by phone), and writes predictions aligned to the sample submission keys. The execution output did not include the CV metric value, so no numeric score is available from the log.", "The script reports a validation metric of 0.0000 and saved a submission.csv. The zero metric is caused by using ground-truth positions as the baseline when GNSS baseline is missing, which produces zero offsets and thus perfect (but leaked) predictions on the validation set.", "The training script did not produce visible logs or a submission file. Execution output contains only timing info, suggesting the run failed early (likely due to missing input files such as sample_submission.csv or no usable device_gnss data), so no validation metric is available.", "Training and cross-validation completed successfully and a submission file was written to ./submission/submission.csv. The reported CV metric (mean of 50th and 95th percentile horizontal errors) is 391.7265737133254 meters, indicating very large location errors on the validation folds.", "The script appears to have executed without an explicit error in the provided output. It builds GNSS-derived features, trains LightGBM latitude/longitude regressors with GroupKFold CV, and should save predictions to submission/submission.csv. The execution log did not include CV metric prints or a confirmation that the submission file was saved, so no numeric validation metric is available.", "The execution log contains only a single timing line and no training/validation prints or confirmation of saving a submission. Therefore the run did not produce visible outputs or a saved submission.csv. Unable to observe a validation metric from the log.", "The run crashed with a KeyError accessing 'phone' in the sample submission dataframe; validation printed 0.0 before the crash. Because the script terminated early, no submission.csv was produced and the run failed.", "Script executed without visible errors in the provided execution log. It trains RandomForest models using aggregated GNSS features and (presumably) saved predictions to ./submission/submission.csv. The execution output did not include the printed cross-validation metric value, so the exact validation number is not available in the log.", "The script aggregated raw device_gnss measurements to 1 Hz, trained LightGBM models to predict latitude and longitude with GroupKFold CV by phone, and produced a submission CSV. No runtime error is shown in the provided execution log and a submission file was written to ./submission/submission.csv. The exact CV metric value was not present in the log extract available here.", "Script executed and produced a submission file; no runtime errors shown in the provided log. The execution output did not include the CV/validation metric, so a numeric score is not available from the logs.", "The script appears to have executed and saved a submission CSV to ./submission/submission.csv (and ./working/submission.csv). The run produced no error messages in the provided log, but no CV metric value was visible in the output, so the empirical performance is unknown from the logs.", "The script appears to have executed and produced a submission file. It trained RandomForest models with GroupKFold by drive, constructed test features, predicted lat/lon, and saved submission.csv to ./submission/. The provided execution output did not include the numeric validation metric, so that value cannot be reported.", "Script executed without visible runtime errors. It aggregated GNSS features, trained GradientBoosting regressors on ECEF targets with 5-fold GroupKFold CV, trained final models on full data, and wrote predictions to ./submission/submission.csv. The execution output did not include the numeric CV metric value.", "No training/logging output is present in the execution log (the expected CV metric print and 'Saved submission' messages are missing). It appears the script did not run to completion or produced no visible output, so no submission.csv was verifiably generated.", "The run produced no training/evaluation output other than an execution time line; no CV metric or 'Saved submission' messages were printed. It appears the script did not assemble training data or did not run to completion, and no submission.csv was created.", "The run produced no model output or submission logs \u2014 the script did not print CV metric nor \u2018Saved submission\u2019 messages, and no submission.csv was generated. Likely the process exited early (e.g., no data found or an unreported error).", "The script ran successfully and produced a submission CSV. It implemented a trivial global-mean baseline (predicting the mean latitude/longitude from train ground truth) and evaluated it on a per-phone holdout, reporting a very large validation error.", "The script executed and produced a submission file saved to ./submission/submission.csv. The run did not print the cross-validation metric in the provided execution log, so the numeric validation score is not available from the output.", "The execution output contains only a timing line and no training or save messages, indicating the script did not process data or did not run to completion. No evidence of a produced submission.csv in ./submission/ was shown in the log.", "The execution log contains only a single timing line and no other output (no training/validation prints or confirmation of saved submission). This indicates the script likely did not run to completion or found no data and exited without producing a submission.csv. No validation metric is available from the output.", "The script executed and produced a submission file. No cross-validation metric was reported (val metric is NaN) because no training rows with ground truth were assembled, so the run fell back to using the WLS baseline and imputations for test predictions.", "The script ran without visible errors and produced a submission file. No training rows with ground truth were found, so the code fell back to the pure WLS baseline and trained no correction models; therefore no numeric validation metric was produced.", "The execution log contains no model-training or submission messages and did not produce the expected output. It appears the run failed or terminated early and no submission.csv was created in ./submission/.", "The execution output only contains a single timing line and no training/validation prints or 'Saved submission' messages. It appears the script did not complete successfully and no submission.csv was produced.", "The script executed and produced a submission file saved to ./submission/submission.csv. The execution log did not report any errors, but no validation metric was printed in the provided output, so a numeric score is unavailable.", "The script executed and saved a submission file to ./submission/submission.csv. The provided execution log did not include a validation metric or detailed training/output logs, so no numeric metric could be extracted from the output.", "The training script appears to have executed without error and produced a submission file. The run likely saved submission/submission.csv (and working/submission.csv). The console output included no validation metric, so a CV metric value could not be extracted from the provided logs.", "The script did not produce the expected submission.csv and printed no training/validation logs. Likely the run failed early due to missing input files (e.g. sample_submission.csv or train/test data) or no matching training rows with ground truth, so no model was trained and no submissions saved.", "Execution output contains only a single line ('Execution time...') and no logs from the script, so I cannot confirm that training, CV, or saving completed. There is no printed CV/OOF metric in the provided output, and no confirmation that ./submission/submission.csv was produced. No explicit error message is present in the captured output.", "The script appears to have executed without visible errors in the provided execution output. It loads GNSS features, trains LightGBM latitude/longitude regressors when numeric features exist, falls back to per-phone/global mean baselines if needed, and writes the submission to ./submission/submission.csv. The run-time output did not include the cross-validation metric value, so no numeric score is available from the logs.", "Execution log is empty aside from a brief timing line; the script did not print its usual progress messages and no submission file appears to have been produced. It appears the run failed to execute the training/prediction steps.", "The run produced no printed output in the log and I cannot confirm a saved submission.csv. I found a likely bug in the cross-validation logic (GroupKFold n_splits selection) that can raise an exception when there are too few collections; other parts gracefully fall back to per-phone/global baselines if features are missing.", "The execution output only shows a single timing line and no other logs from the script (no data loading, training, or save messages). It appears the script did not complete its work and no submission.csv was produced in ./submission/. No validation metric is available.", "The script appears to have run and attempted to build GNSS features, run CV, train full models, and save a submission CSV to ./submission/submission.csv. The execution log provided contains no detailed prints or metric values, so I cannot verify the numeric CV metric from this run.", "The script executed and produced a submission using the WLS baseline with an optional RandomForest correction when training data is available. A submission file was written to ./submission/submission.csv. The validation metric was not reported (NaN) in the run, so no numeric validation score is available.", "The script executed to completion and wrote a submission file at submission/submission.csv. There were no visible runtime errors in the provided output. A validation metric was not computed/available (likely NaN) because no training rows with ground truth were assembled, so the run fell back to the WLS baseline predictions.", "The script executed and produced a submission file at ./submission/submission.csv. It loads available training traces to fit a RandomForest correction to WLS base positions and otherwise falls back to the baseline WLS positions. No validation metric value was available from the run (no numeric CV metric reported).", "The script produced no training or prediction logs and only an execution time line, indicating it likely failed early (e.g., missing input files such as sample_submission.csv). As a result no submission.csv appears to have been produced and no validation metric is available.", "The script executed without visible runtime errors and produced a submission CSV at ./submission/submission.csv. The cross-validation metric reported by the run is NaN (likely indicating no training rows with ground-truth were assembled), so the submission most likely uses the baseline WLS/mean positions.", "The execution produced no console logs from the script beyond the generic execution-time message, and there is no evidence that a submission.csv was written. Likely the run failed early (e.g. no training rows assembled) or the script did not execute main; no validation metric is available.", "The script executed and (per its flow) prepared train/test data, ran CV, trained full models, and saved a submission CSV. The provided execution log does not show the numeric CV metric value or any error messages.", "The script executed and produced a submission file. No training rows with ground truth were assembled, so model training was skipped and the pipeline used baseline WLS positions (with per-phone ffill/bfill) to fill the submission. The cross-validation/validation metric is not available (NaN) because no training with GT was performed.", "The script executed without visible runtime errors and produced a submission file at ./submission/submission.csv. It trains a correction model when training data with ground truth is present and otherwise falls back to baseline WLS positions; the reported cross-validation metric was printed by the script but its numeric value is not available in the provided execution log.", "The execution output contains only a single line noting execution time and no other logs; there is no evidence that a submission.csv was produced. The run likely failed or exited early before training and saving predictions. No validation metric is available.", "The script executed to completion without an explicit error in the provided log and appears to have produced a submission file at ./submission/submission.csv. The provided execution output did not include training/validation logs or a numeric validation metric, so no metric value is available from the logs.", "The script executed and (per the implementation) should have produced a submission CSV at ./submission/submission.csv. The log output provided does not include CV or final metric values, so no numeric validation metric could be extracted from the run. The model trains on simple time and phone features and falls back to per-phone/global means if prediction fails.", "The script ran to completion and produced a submission file at ./submission/submission.csv. The script did not report a numeric validation metric (it printed NaN or no metric), indicating no usable CV score was produced from training data.", "The training script appears to have executed and produced a submission CSV written to ./submission/submission.csv. No numeric validation metric value was present in the provided execution output, so I cannot report a measured score. The run did not show an obvious crash in the log snippet provided.", "The script executed to completion and saved a submission file at ./submission/submission.csv. No obvious runtime errors were reported; training appears to have been skipped or produced no numeric validation score (validation metric is NaN / not available). The submission contains baseline (WLS) positions, and a final trained correction model was not evidently applied.", "The script appears to have executed and saved a submission file to ./submission/submission.csv. It performed feature engineering, GroupKFold cross-validation and trained final models, but the provided execution log does not include the numeric CV metric or other printed outputs, so no numeric evaluation is available.", "The script ran to completion and produced a submission file. It loads training GNSS with ground truth (if available), trains RandomForest correction models, applies corrections to test WLS base positions, and saved predictions to ./submission/submission.csv. The execution logs do not show a numeric validation metric, so no metric value is available.", "The script appears to have executed and produced a submission file at ./submission/submission.csv. The execution log does not report a validation metric, so no numeric score is available from the run.", "The execution log contains only a single 'Execution time' line and no training/evaluation prints or saved-submission messages. It appears the script did not run to completion or produced no output, so no submission.csv was created.", "The script executed end-to-end and produced a submission file; training and test prediction steps completed without error messages in the provided log. The execution log did not include a reported validation metric, so no numeric CV/OOF score is available from the run.", "The run produced no training/output logs and did not save a submission.csv. Likely the script failed early (e.g. no training data found or GroupKFold misconfigured when there are too few drives), so no predictions were produced.", "The script ran and produced a submission file at ./submission/submission.csv. The provided execution log contains no metric output, so no validation metric could be extracted from the run.", "Run failed during training with a TypeError: LGBMRegressor.fit() got an unexpected keyword argument 'verbose'. Training did not complete and no submission.csv was produced.", "The script successfully loaded training data, trained LightGBM models (x/y/z ECEF targets) with group CV, reported a cross-validation metric, trained final models, and saved predictions to ./submission/submission.csv. Cross-validation (mean of 50th and 95th percentile horizontal errors) is 434.4494988345602 meters, indicating the baseline model is quite coarse.", "Script ran without runtime errors. LightGBM models were trained with cross-validation and a final model was fitted; a submission file was written to ./submission/submission.csv. The CV metric (mean of per-phone 50th and 95th percentile distance errors) is large (~416.36 m), indicating poor geographic accuracy of the predictions.", "The training script executed successfully without runtime errors and produced a submission file. Cross-validation produced a large error (CV metric) indicating modest predictive quality on the validation splits.", "The script executed successfully and saved a submission.csv to ./submission and ./working. Cross-validation produced a high error (CV metric = 412.69159704618863), indicating the model's predictions are on average hundreds of meters off. No runtime exceptions were raised during training or submission generation.", "Training completed successfully and a submission file was written to ./submission/submission.csv. LightGBM emitted many warnings 'No further splits with positive gain' during training, but training finished and predictions were produced. The run did not print the CV metric value in the provided log, so a numeric validation score could not be retrieved.", "The script ran to completion and saved a submission file at ./submission/submission.csv. LightGBM produced many warnings 'No further splits with positive gain', suggesting limited gain from splits (possibly low feature variability or target issues); the CV metric value is not present in the captured output so validation performance could not be reported.", "Script ran to completion, trained LightGBM models, computed a CV metric, and saved a submission CSV. CV metric (mean of 50th and 95th percentile horizontal errors) printed was 152.05107099040964.", "The run trained LGBM models and printed a CV metric but crashed during test-time feature alignment. A ValueError occurred while forward/back-filling test features into the sample submission, so no submission.csv was produced. CV metric (mean of 50th and 95th percentile errors) on the OOF predictions was 159.0075248361783.", "Training and CV completed, but execution crashed before producing a submission. CV metric printed was ~159.006 (mean of 50th and 95th percentile distance errors). A ValueError occurred during test feature filling, preventing saving of submission.csv.", "The run crashed with a ValueError during the test feature fill step, preventing a submission from being written. A cross-validation score was computed earlier (CV metric = 159.0075248361783), and final models started training, but execution failed before saving submission.csv.", "Run crashed during test-time feature handling due to a pandas boolean ambiguity; final submission was not produced. Prior to the crash the script completed training and reported a CV metric of about 159.0075 for the mean of 50th and 95th percentile horizontal errors.", "Run crashed during the final stage: an exception was raised when checking test_merge[\"base_lat\"].isna().all(), causing the script to terminate before saving a submission. Cross-validation completed and reported a CV metric of 159.0075248361783. Final models began training but the script failed before writing submission.csv.", "The script loaded train and test GNSS-derived features, trained LightGBM models for latitude and longitude residuals, produced out-of-fold predictions (CV metric reported) and saved a submission CSV. Training completed successfully and submission files were written to ./submission/submission.csv and ./working/submission.csv.", "The script executed successfully, trained LightGBM models with 5-fold group CV, reported a cross-validation metric of 277.0277 (mean of 50th and 95th percentile horizontal errors), and wrote submission.csv to ./submission/submission.csv and ./working/submission.csv. No runtime exceptions were raised in the log.", "The training ran and produced a cross-validation metric, but execution crashed during test-time feature alignment. A ValueError ('Columns must be same length as key') occurred while assigning forward-filled features into test_merge, so no submission.csv was written to ./submission. CV metric printed was 152.75763316081583.", "The script completed training and cross-validation, yielding a CV metric of 152.75763316081583, but failed during final prediction. Execution raised a ValueError due to a mismatch between the model's expected number of features (42) and the test input (44), so no submission.csv was produced.", "Script ran to completion without runtime errors. It trained LightGBM models with GroupKFold CV and produced a final submission saved to ./submission/submission.csv; the reported CV metric (mean of 50th and 95th percentile horizontal errors) is ~152.75763316081583 meters, indicating poor positioning performance. No exceptions were raised during execution.", "Script ran successfully, trained LightGBM models with group CV, and saved a submission to ./submission/submission.csv. Cross-validation produced a large error (mean of per-phone 50th and 95th percentile distances) indicating poor absolute accuracy.", "Training and cross-validation completed and a CV metric was computed (\u2248294.81). However, the script crashed while preparing test features: a ValueError 'Columns must be same length as key' occurred during group-wise forward-fill, and no submission.csv was written.", "Run crashed with a ValueError ('Columns must be same length as key') while filling test feature columns, so the script did not produce submission/submission.csv. The CV metric printed before the crash was 294.82064737344996 (mean of 50th and 95th percentile horizontal errors).", "Run crashed with a pandas ValueError while assigning multiple columns because the DataFrame had non-unique column names; no submission.csv was produced. The script did print a CV metric (294.8108810081267) before the failure.", "The script trained LightGBM models and printed a cross-validation metric (\u2248294.81088), but execution terminated with a ValueError before saving the submission file. The error was raised during assignment to test_merge[feature_cols] due to non-unique columns, so no submission.csv was produced.", "The script trained models and reported a cross-validation metric, but it crashed before producing a submission. A ValueError occurred during filling/assignment of test features ('Columns must be same length as key'), so submission.csv was not saved.", "Training and cross-validation completed and reported a CV metric of 294.8108810081267 (mean of 50th and 95th percentile horizontal errors). However the run crashed during test feature assembly with a ValueError ('Columns must be same length as key'), so no submission.csv was produced.", "The run crashed with a ValueError while filling NaNs into test feature columns, so no submission.csv was produced. The script did run CV and printed a validation metric (~294.82065) before failing during test feature preparation.", "The script trained models and printed a CV metric of 294.8108810081267, but it crashed during final prediction with a ValueError: model n_features_ is 60 and input n_features is 62. As a result, no submission.csv was produced.", "The training and prediction script executed successfully and produced a submission file. Cross-validation produced a large error (mean of 50th and 95th percentile distances) indicating the model's predictions are far from ground truth on the holdout folds.", "The run failed with a TypeError while computing medians for feature columns, caused by non-numeric data (e.g. 'GT' values) being included in the feature set. No submission.csv was produced due to the early crash.", "The run failed with a ValueError while aligning and forward-filling test features ('Columns must be same length as key'), so a submission CSV was not produced. Before the failure, cross-validation completed and printed a CV metric of 265.65168753388343.", "Execution failed with a ValueError during test feature imputation; no submission.csv was produced. Prior to the crash the script completed CV training and reported a CV metric of 265.65168753388383.", "The training ran and produced a cross-validation metric, but execution failed during final prediction with a feature-dimension mismatch, so no submission.csv was saved. CV reported 265.65168753388343.", "Script ran to completion without runtime errors, trained LightGBM models, performed 5-fold GroupKFold CV and saved a submission to ./submission/submission.csv. The reported CV metric (mean of 50th and 95th percentile distance errors) is 265.65168753388343 meters, indicating poor positioning accuracy empirically.", "The script ran to completion without runtime errors and produced a submission file. Cross-validated performance (mean of 50th and 95th percentile horizontal errors) is 211.5365744066162, and the submission was saved to ./submission/submission.csv.", "The script ran to completion without runtime errors and produced a submission file at ./submission/submission.csv. Training used LightGBM with 72 features and 5-fold group CV; the reported CV metric (mean of 50th and 95th percentile horizontal errors) is 200.43096765719255 meters, indicating large prediction error on the held-out folds.", "Script ran without runtime errors; LightGBM models trained (5-fold CV + final models) and a submission file was written. The reported CV metric (mean of 50th and 95th percentile horizontal errors) is ~311.99 meters, indicating poor localization performance empirically.", "Run failed with a TypeError while computing the median of feature columns: pandas reported it cannot convert values like 'GT' to numeric. This indicates non-numeric (object/string) columns were present in feature_cols, causing the pipeline to crash before producing a submission.csv.", "Training and inference completed without runtime errors. A submission file was written to ./submission/submission.csv. Cross-validation metric (mean of 50th and 95th percentile horizontal errors) was 314.21662771491924.", "The training ran and produced a CV metric, but execution failed before saving a submission. The run computed a CV metric (mean of 50th and 95th percentile distance errors) = 202.7160939242046, then crashed due to a pandas ValueError: the column label 'phone' is not unique during a merge step.", "Training completed successfully and produced a submission file. Cross-validation metric (mean of 50th and 95th percentile horizontal errors) was 202.7160939242046. Submission saved to ./submission/submission.csv and ./working/submission.csv.", "Training ran to completion and a submission file was produced in ./submission/submission.csv. The cross-validation metric (mean of the 50th and 95th percentile horizontal errors) is 290.16563080133415 meters, indicating very large position errors relative to the competition goal.", "Script ran end-to-end without errors, trained LightGBM models, produced out-of-fold CV predictions and saved a submission to ./submission/submission.csv. The reported cross-validation metric (mean of 50th and 95th percentile horizontal errors) is relatively large, indicating predictions are far from ground truth on this run.", "The run crashed during test feature assembly: assigning group-forward-filled feature columns raised a ValueError ('Columns must be same length as key'), so no submission was produced. The out-of-fold CV metric computed before the crash was 202.7160939242046.", "Run failed during final prediction due to a feature-dimension mismatch (model expects 90 features but test input had 92). A CV metric was printed earlier but the script aborted before saving submission.csv so no submission was produced.", "Training completed and a CV metric was printed, but the run failed during final prediction due to a feature-dimension mismatch, so no submission.csv was produced. The code reported a model expecting 90 features while the test input had 92 features, causing a ValueError and abort.", "The script loaded training data and ran CV, reporting a CV metric of 365.6115, but the run failed during test-time prediction with a feature-dimension mismatch error (model expects 15 features, input has 17). As a result the submission file was not produced.", "The script completed cross-validation (CV metric 365.6221212806479) but crashed before producing a submission. It failed at test-time due to a feature-dimension mismatch (X_test had 17 columns, expected 15), so no submission.csv was saved.", "Script ran to completion, trained LightGBM models, evaluated CV metric and wrote submission CSV. The reported cross-validation metric (mean of 50th and 95th percentile horizontal errors) is large (~322.92 meters), indicating poor predictive accuracy on the training split. A submission file was saved to ./submission/submission.csv.", "The training script executed successfully and produced a submission file at ./submission/submission.csv. Cross-validation metric (mean of per-phone 50th and 95th percentile horizontal errors) reported was 290.16563080133415, indicating large errors in the predictions.", "Script ran to completion without runtime errors and produced a submission file. The reported cross-validation metric (mean of per-phone 50th and 95th percentile horizontal errors) is 265.65168753388383 meters, indicating large localization errors for this model."], "exp_name": "exp", "metrics": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}

let lastClick = 0;
let firstFrameTime = undefined;

let nodes = [];
let edges = [];

let lastScrollPos = 0;

setup = () => {
  canvas = createCanvas(...updateTargetDims());
};

class Node {
  x;
  y;
  size;
  xT;
  yT;
  xB;
  yB;
  treeInd;
  color;
  relSize;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  hasChildren = false;
  isRootNode = true;
  isStarred = false;
  selected = false;
  renderSize = 10;
  edges = [];
  bgCol;

  constructor(x, y, relSize, treeInd) {
    const minSize = 35;
    const maxSize = 60;

    const maxColor = 10;
    const minColor = 125;

    this.relSize = relSize;
    this.treeInd = treeInd;
    this.size = minSize + (maxSize - minSize) * relSize;
    this.color = minColor + (maxColor - minColor) * relSize;
    this.bgCol = Math.round(Math.max(this.color / 2, 0));

    this.x = x;
    this.y = y;
    this.xT = x;
    this.yT = y - this.size / 2;
    this.xB = x;
    this.yB = y + this.size / 2;

    nodes.push(this);
  }

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  child = (node) => {
    let edge = new Edge(this, node);
    this.edges.push(edge);
    edges.push(edge);
    this.hasChildren = true;
    node.isRootNode = false;
    return node;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    const mouseXlocalCoords = (mouseX - width / 2) / scaleFactor;
    const mouseYlocalCoords = (mouseY - height / 2) / scaleFactor;
    const isMouseOver =
      dist(mouseXlocalCoords, mouseYlocalCoords, this.x, this.y) <
      this.renderSize / 1.5;
    if (isMouseOver) cursor(HAND);
    if (isMouseOver && mouseIsPressed) {
      nodes.forEach((n) => (n.selected = false));
      this.selected = true;
      setCodeAndPlan(
        treeStructData.code[this.treeInd],
        treeStructData.plan[this.treeInd],
      );
      manualSelection = true;
    }

    this.renderSize = this.size;
    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
      } else {
        this.renderSize =
          this.size *
          (0.8 +
            0.2 *
              (-3.33 * this.animationProgress ** 2 +
                4.33 * this.animationProgress));
      }
    }

    fill(this.color);
    if (this.selected) {
      fill(accentCol);
    }

    noStroke();
    square(
      this.x - this.renderSize / 2,
      this.y - this.renderSize / 2,
      this.renderSize,
      10,
    );

    noStroke();
    textAlign(CENTER, CENTER);
    textSize(this.renderSize / 2);
    fill(255);
    // fill(lerpColor(color(accentCol), color(255), this.animationProgress))
    text("{ }", this.x, this.y - 1);
    // DEBUG PRINT:
    // text(round(this.relSize, 2), this.x, this.y - 1)
    // text(this.treeInd, this.x, this.y + 15)

    const dotAnimThreshold = 0.85;
    if (this.isStarred && this.animationProgress >= dotAnimThreshold) {
      let dotAnimProgress =
        (this.animationProgress - dotAnimThreshold) / (1 - dotAnimThreshold);
      textSize(
        ((-3.33 * dotAnimProgress ** 2 + 4.33 * dotAnimProgress) *
          this.renderSize) /
          2,
      );
      if (this.selected) {
        fill(0);
        stroke(0);
      } else {
        fill(accentCol);
        stroke(accentCol);
      }
      strokeWeight((-(dotAnimProgress ** 2) + dotAnimProgress) * 2);
      text("*", this.x + 20, this.y - 11);
      noStroke();
    }

    if (!this.isStatic) {
      fill(bgCol);
      const progressAnimBaseSize = this.renderSize + 5;
      rect(
        this.x - progressAnimBaseSize / 2,
        this.y -
          progressAnimBaseSize / 2 +
          progressAnimBaseSize * this.animationProgress,
        progressAnimBaseSize,
        progressAnimBaseSize * (1 - this.animationProgress),
      );
    }
    if (this.animationProgress >= 0.9) {
      this.edges
        .sort((a, b) => a.color() - b.color())
        .forEach((e, i) => {
          e.startAnimation((i / this.edges.length) ** 2 * 1000);
        });
    }
  };
}

class Edge {
  nodeT;
  nodeB;
  animX = 0;
  animY = 0;
  animationStart = Number.MAX_VALUE;
  animationProgress = 0;
  isStatic = false;
  weight = 0;

  constructor(nodeT, nodeB) {
    this.nodeT = nodeT;
    this.nodeB = nodeB;
    this.weight = 2 + nodeB.relSize * 1;
  }

  color = () => this.nodeB.color;

  startAnimation = (offset = 0) => {
    if (this.animationStart == Number.MAX_VALUE)
      this.animationStart = globalTime + offset;
  };

  render = () => {
    if (globalTime - this.animationStart < 0) return;

    if (!this.isStatic) {
      this.animationProgress = animEase(
        (globalTime - this.animationStart) / 1000,
      );
      if (this.animationProgress >= 1) {
        this.isStatic = true;
        this.animX = this.nodeB.xT;
        this.animY = this.nodeB.yT;
      } else {
        this.animX = bezierPoint(
          this.nodeT.xB,
          this.nodeT.xB,
          this.nodeB.xT,
          this.nodeB.xT,
          this.animationProgress,
        );

        this.animY = bezierPoint(
          this.nodeT.yB,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          (this.nodeT.yB + this.nodeB.yT) / 2,
          this.nodeB.yT,
          this.animationProgress,
        );
      }
    }
    if (this.animationProgress >= 0.97) {
      this.nodeB.startAnimation();
    }

    strokeWeight(this.weight);
    noFill();
    stroke(
      lerpColor(color(bgCol), color(accentCol), this.nodeB.relSize * 1 + 0.7),
    );
    bezier(
      this.nodeT.xB,
      this.nodeT.yB,
      this.nodeT.xB,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      (this.nodeT.yB + this.nodeB.yT) / 2,
      this.animX,
      this.animY,
    );
  };
}

draw = () => {
  cursor(ARROW);
  frameRate(120);
  if (!firstFrameTime && frameCount <= 1) {
    firstFrameTime = millis();
  }
  // ---- update global animation state ----
  const initialSpeedScalingEaseIO =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) * PI) + 1) / 2;
  const initialSpeedScalingEase =
    (cos(min((millis() - firstFrameTime) / 8000, 1.0) ** (1 / 2) * PI) + 1) / 2;
  const initAnimationSpeedFactor = 1.0 - 0.4 * initialSpeedScalingEaseIO;
  // update global scaling-aware clock
  globalTime += globalAnimSpeed * initAnimationSpeedFactor * deltaTime;

  if (nodes.length == 0) {
    const spacingHeight = height * 1.3;
    const spacingWidth = width * 1.3;
    treeStructData.layout.forEach((lay, index) => {
      new Node(
        spacingWidth * lay[0] - spacingWidth / 2,
        20 + spacingHeight * lay[1] - spacingHeight / 2,
        1 - treeStructData.metrics[index],
        index,
      );
    });
    treeStructData.edges.forEach((ind) => {
      nodes[ind[0]].child(nodes[ind[1]]);
    });
    nodes.forEach((n) => {
      if (n.isRootNode) n.startAnimation();
    });
    nodes[0].selected = true;
    setCodeAndPlan(
      treeStructData.code[0],
      treeStructData.plan[0],
    )
  }

  const staticNodes = nodes.filter(
    (n) => n.isStatic || n.animationProgress >= 0.7,
  );
  if (staticNodes.length > 0) {
    const largestNode = staticNodes.reduce((prev, current) =>
      prev.relSize > current.relSize ? prev : current,
    );
    if (!manualSelection) {
      if (!largestNode.selected) {
        setCodeAndPlan(
          treeStructData.code[largestNode.treeInd],
          treeStructData.plan[largestNode.treeInd],
        );
      }
      staticNodes.forEach((node) => {
        node.selected = node === largestNode;
      });
    }
  }
  background(bgCol);
  // global animation transforms
  translate(width / 2, height / 2);
  scale(scaleFactor);

  
  // ---- fg render ----
  edges.forEach((e) => e.render());
  nodes.forEach((n) => n.render());
  
};

    </script>
    <title>AIDE Run Visualization</title>
    <style>
      body,
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
        overflow: scroll;
      }
      body {
        background-color: #f2f0e7;
      }
      canvas {
        float: left;
        height: 100vh;
        width: 40vw;
      }
      #text-container {
        float: right;
        height: 100vh;
        width: 50vw;
        background-color: #282c34;
      }
      #plan {
        /* border-left: 2px solid #282c34; */
        background-color: #282c34;
        color: #f2f0e7;
        min-height: 5rem;
        padding: 1em 0 1em 1em;
      }
    </style>
  </head>
  <body>
    <pre
      id="text-container"
    ><div id="plan"></div><hr><code id="code" class="language-python"></code></pre>
  </body>
</html>
